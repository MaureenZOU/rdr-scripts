[
  {
    "paper_id": 0,
    "authors": "Matanel Oren, Michael Hassid, Yossef (Yossi) Adi, Roy Schwartz",
    "title": "Transformers are Multi-State RNNs",
    "paper_url": "https://ai.meta.com/research/publications/transformers-are-multi-state-rnns/",
    "pdf_link": "https://arxiv.org/pdf/2401.06104",
    "abstract": "Transformers are considered conceptually different from the previous generation of state-of-the-art NLP models—recurrent neural networks (RNNs). In this work, we demonstrate that decoder-only transformers can in fact be conceptualized as unbounded multi-state RNNs—an RNN variant with unlimited hidden state size. We further show that transformers can be converted into bounded multi-state RNNs by fixing the size of their hidden state, effectively compressing their key-value cache. We introduce a novel, training-free compression policy—Token Omission Via Attention (TOVA). Our experiments with four long range tasks and several LLMs show that TOVA outperforms several baseline compression policies. Particularly, our results are nearly on par with the full model, using in some cases only 1/8 of the original cache size, which translates to 4.8X higher throughput. Our results shed light on the connection between transformers and RNNs, and help mitigate one of LLMs’ most painful computational bottlenecks—the size of their key-value cache."
  },
  {
    "paper_id": 1,
    "authors": "Shukai Duan, Heng Ping, Nikos Kanakaris, Xiongye Xiao, Panagiotis Kyriakis, Nesreen K. Ahmed, Peiyu Zhang, Guixiang Ma, Mihai Capota, Shahin Nazarian, Theodore L. Willke, Paul Bogdan",
    "title": "A Structure-Aware Framework for Learning Device Placements on Computation Graphs",
    "paper_url": "https://ai.meta.com/research/publications/a-structure-aware-framework-for-learning-device-placements-on-computation-graphs/",
    "pdf_link": "https://arxiv.org/pdf/2405.14185",
    "abstract": "Computation graphs are Directed Acyclic Graphs (DAGs) where the nodes correspond to mathematical operations and are used widely as abstractions in optimizations of neural networks. The device placement problem aims to identify optimal allocations of those nodes to a set of (potentially heterogeneous) devices. Existing approaches rely on two types of architectures known as grouper-placer and encoder-placer, respectively. In this work, we bridge the gap between encoder-placer and grouper-placer techniques and propose a novel framework for the task of device placement, relying on smaller computation graphs extracted from the OpenVINO toolkit. The framework consists of five steps, including graph coarsening, node representation learning and policy optimization. It facilitates end-to-end training and takes into account the DAG nature of the computation graphs. We also propose a model variant, inspired by graph parsing networks and complex network analysis, enabling graph representation learning and jointed, personalized graph partitioning, using an unspecified number of groups. To train the entire framework, we use reinforcement learning using the execution time of the placement as reward. We demonstrate the flexibility and effectiveness of our approach through multiple experiments with three benchmark models, namely Inception-V3, ResNet, and BERT. The robustness of the proposed framework is also highlighted through an ablation study. The suggested placements improve the inference speed for the benchmark models by up to 58.2% over CPU execution and by up to 60.24% compared to other commonly used baselines."
  },
  {
    "paper_id": 2,
    "authors": "Haider Al-Tahan, Quentin Garrido, Randall Balestriero, Diane Bouchacourt, Caner Hazirbas, Mark Ibrahim",
    "title": "UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling",
    "paper_url": "https://ai.meta.com/research/publications/unibench-visual-reasoning-requires-rethinking-vision-language-beyond-scaling/",
    "pdf_link": "https://arxiv.org/pdf/2408.04810",
    "abstract": "Significant research efforts have been made to scale and improve vision-language model (VLM) training approaches. Yet, with an ever-growing number of benchmarks, researchers are tasked with the heavy burden of implementing each protocol, bearing a non-trivial computational cost, and making sense of how all these benchmarks translate into meaningful axes of progress. To facilitate a systematic evaluation of VLM progress, we introduce UniBench: a unified implementation of 50+ VLM benchmarks spanning a comprehensive range of carefully categorized capabilities from object recognition to spatial awareness, counting, and much more. We showcase the utility of UniBench for measuring progress by evaluating nearly 60 publicly available vision-language models, trained on scales of up to 12.8B samples. We find that while scaling training data or model size can boost many vision-language model capabilities, scaling offers little benefit for reasoning or relations. Surprisingly, we also discover today's best VLMs struggle on simple digit recognition and counting tasks, e.g. MNIST, which much simpler networks can solve. Where scale falls short, we find that more precise interventions, such as data quality or tailored-learning objectives offer more promise. For practitioners, we also offer guidance on selecting a suitable VLM for a given application. Finally, we release an easy-to-run UniBench code-base with the full set of 50+ benchmarks and comparisons across 59 models as well as a distilled, representative set of benchmarks that runs in 5 minutes on a single GPU."
  },
  {
    "paper_id": 3,
    "authors": "Jack Lin, Luyu Gao, Barlas Oguz, Wenhan Xiong, Jimmy Lin, Scott Yih, Xilun Chen",
    "title": "FLAME : Factuality-Aware Alignment for Large Language Models",
    "paper_url": "https://ai.meta.com/research/publications/flame-factuality-aware-alignment-for-large-language-models/",
    "pdf_link": "https://arxiv.org/pdf/2405.01525",
    "abstract": "Alignment is a procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e., hallucination). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps: supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new or unfamiliar knowledge can encourage hallucination. This makes SFT less factual as it trains on human-labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL often inadequately capture factuality and favor longer and more detailed responses, which inadvertently promote hallucination. Based on these observations, we propose FactuaLity-aware AlignMEnt, comprised of factuality-aware SFT and factuality-aware RL through direct preference optimization. Experiments show that our proposed FLAME guides LLMs to output more factual responses while maintaining their instruction-following capability."
  },
  {
    "paper_id": 4,
    "authors": "Andrea Tirinzoni, Ahmed Touati, Jesse Farebrother, Mateusz Guzek, Anssi Kanervisto, Yingchen Xu, Alessandro Lazaric, Matteo Pirotta",
    "title": "Zero-Shot Whole-Body Humanoid Control via Behavioral Foundation Models",
    "paper_url": "https://ai.meta.com/research/publications/zero-shot-whole-body-humanoid-control-via-behavioral-foundation-models/",
    "pdf_link": "https://openreview.net/pdf?id=9sOR0nYLtz",
    "abstract": "Unsupervised reinforcement learning (RL) aims at pre-training agents that can solve a wide range of downstream tasks in complex environments. Despite recent advancements, existing approaches suffer from several limitations: they may still require running an RL process on each downstream task to achieve a satisfactory performance, they may need access to datasets with good coverage or well-curated task-specific samples, or they may pre-train policies with unsupervised losses that are poorly correlated with the downstream tasks of interest. In this paper, we introduce a novel algorithm regularizing unsupervised RL towards imitating trajectories from unlabeled behavior dataset. The key technical novelty of our method, called Forward-Backward Representations with Conditional-Policy Regularization, is to train forward-backward representations to embed the unlabeled trajectories to the same latent space used to represent states, rewards, and policies, and use a latent-conditional discriminator to encourage policies to ``cover'' the states in the unlabeled behavior dataset. As a result, we can learn policies that are well aligned with the behaviors in the dataset, while retaining zero-shot generalization capabilities for reward-based and imitation tasks. We demonstrate the effectiveness of this new approach in a challenging humanoid control problem: leveraging observation-only motion capture datasets, we train Meta Motivo, the first humanoid behavioral foundation model that can be prompted to solve a variety of whole-body tasks, including motion tracking, goal reaching, and reward optimization. The resulting model is capable of expressing human-like behaviors and it achieves competitive performance with task-specific methods while outperforming state-of-the-art unsupervised RL and model-based baselines."
  },
  {
    "paper_id": 5,
    "authors": "Vincent-Pierre Berges, Barlas Oguz",
    "title": "Memory Layers at Scale",
    "paper_url": "https://ai.meta.com/research/publications/memory-layers-at-scale/",
    "pdf_link": "https://arxiv.org/pdf/2412.09764",
    "abstract": "Memory layers use a trainable key-value lookup mechanism to add extra parameters to a model without increasing FLOPs. Conceptually, sparsely activated memory layers complement compute-heavy dense feed-forward layers, providing dedicated capacity to store and retrieve information cheaply. This work takes memory layers beyond proof-of-concept, proving their utility at contemporary scale. On downstream tasks, language models augmented with our improved memory layer outperform dense models with more than twice the computation budget, as well as mixture-of-expert models when matched for both compute and parameters. We find gains are especially pronounced for factual tasks. We provide a fully parallelizable memory layer implementation, demonstrating scaling laws with up to 128B memory parameters, pretrained to 1 trillion tokens, comparing to base models with up to 8B parameters."
  },
  {
    "paper_id": 6,
    "authors": "Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman, Srini Iyer",
    "title": "Byte Latent Transformer: Patches Scale Better Than Tokens",
    "paper_url": "https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/",
    "pdf_link": "https://arxiv.org/pdf/2412.09871",
    "abstract": "We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented dynamically based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it. We present the first flop controlled scaling study of byte-level models up to 8B parameters with 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed-vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size."
  },
  {
    "paper_id": 7,
    "authors": "Melissa Hall, Oscar Mañas, Reyhane Askari, Mark Ibrahim, Candace Ross, Pietro Astolfi, Tariq Berrada Ifriqi, Marton Havasi, Yohann Benchetrit, Karen Ullrich, Carolina Braga, Abhishek Charnalia, Maeve Ryan, Mike Rabbat, Michal Drozdzal, Jakob Verbeek, Adriana Romero Soriano",
    "title": "EvalGIM: A Library for Evaluating Generative Image Models",
    "paper_url": "https://ai.meta.com/research/publications/evalgim-a-library-for-evaluating-generative-image-models/",
    "pdf_link": "https://arxiv.org/pdf/2412.10604",
    "abstract": "As the use of text-to-image generative models increases, so does the adoption of automatic benchmarking methods used in their evaluation. However, while metrics and datasets abound, there are few unified benchmarking libraries that provide a framework for performing evaluations across many datasets and metrics. Furthermore, the rapid introduction of increasingly robust benchmarking methods requires that evaluation libraries remain flexible to new datasets and metrics. Finally, there remains a gap in synthesizing evaluations in order to deliver actionable takeaways about model performance. To enable unified, flexible, and actionable evaluations, we introduce EvalGIM (pronounced “EvalGym”), a library for evaluating generative image models. EvalGIM contains broad support for datasets and metrics used to measure quality, diversity, and consistency of text-to-image generative models. In addition, EvalGIM is designed with flexibility for user customization as a top priority and contains a structure that allows plug-and-play additions of new datasets and metrics. To enable actionable evaluation insights, we introduce “Evaluation Exercises” that highlight takeaways for specific evaluation questions. The Evaluation Exercises contain easy-to-use and reproducible implementations of two state-of-the-art evaluation methods of text-to-image generative models: consistency-diversity-realism Pareto Fronts and disaggregated measurements of performance disparities across groups. EvalGIM also contains Evaluation Exercises that introduce two new analysis methods for text-to-image generative models: robustness analyses of model rankings and balanced evaluations across different prompt styles. In this paper, we outline the EvalGIM library and provide guidance for how others can add new datasets, metrics, and visualizations to customize the library for their own use cases. We also demonstrate the utility of EvalGIM by using its Evaluation Exercises to explore several research questions about text-to-image generative models, such as the role of re-captioning training data or the relationship between quality and diversity in early training stages. We encourage text-to-image model exploration with EvalGIM and invite contributions at https://github.com/facebookresearch/EvalGIM/."
  },
  {
    "paper_id": 8,
    "authors": "Melanie Sclar, Jane Yu, Maryam Fazel-Zarandi, Yulia Tsvetkov, Yonatan Bisk, Yejin Choi, Asli Celikyilmaz",
    "title": "Explore Theory-of-Mind: Program-Guided Adversarial Data Generation for Theory of Mind Reasoning",
    "paper_url": "https://ai.meta.com/research/publications/explore-theory-of-mind-program-guided-adversarial-data-generation-for-theory-of-mind-reasoning/",
    "pdf_link": "https://arxiv.org/pdf/2412.12175",
    "abstract": "Do large language models (LLMs) have theory of mind? A plethora of papers and benchmarks have\nbeen introduced to evaluate if current models have been able to develop this key ability of social\nintelligence. However, all rely on limited datasets with simple patterns that can potentially lead to\nproblematic blind spots in evaluation and an overestimation of model capabilities. We introduce\nExploreToM, the first framework to allow large-scale generation of diverse and challenging theory\nof mind data for robust training and evaluation. Our approach leverages an A* search over a custom\ndomain-specific language to produce complex story structures and novel, diverse, yet plausible scenarios\nto stress test the limits of LLMs. Our evaluation reveals that state-of-the-art LLMs, such as Llama-\n3.1-70B and GPT-4o, show accuracies as low as 5% on ExploreToM-generated data, highlighting\nthe need for more robust theory of mind evaluation. As our generations are a conceptual superset\nof prior work, fine-tuning on our data yields a 27-point accuracy improvement on the classic ToMi\nbenchmark (Le et al., 2019). ExploreToM also enables uncovering underlying skills and factors\nmissing for models to show theory of mind, such as unreliable state tracking or data imbalances, which\nmay contribute to models’ poor performance on benchmarks."
  },
  {
    "paper_id": 9,
    "authors": "Mubashara Akhtar, Omar Benjelloun, Costanza Conforti, Luca Foschini, Pieter Gijsbers, Joan Giner-Miguelez, Sujata Goswami, Nitisha Jain, Michalis Karamousadakis, Satyapriya Krishna, Michael Kuchnik, Sylvain Lesage, Quentin Lhoest, Pierre Marcenac, Manil Maskey, Peter Mattson, Luis Oala, Hamidah Oderinwale, Pierre Ruyssen, Tim Santos, Rajat Shinde, Elena Simperl, Arjun Suresh, Goeffry Thomas, Slava Tykhonov, Joaquin Vanschoren, Susheel Varma, Jos van der Velde, Steffen Vogler, Carole-Jean Wu, Luyao Zhang",
    "title": "Croissant: A Metadata Format for ML-Ready Datasets",
    "paper_url": "https://ai.meta.com/research/publications/croissant-a-metadata-format-for-ml-ready-datasets/",
    "pdf_link": "https://arxiv.org/pdf/2403.19546",
    "abstract": "Data is a critical resource for machine learning (ML), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that creates a shared representation across ML tools, frameworks, and platforms. Croissant makes datasets more discoverable, portable, and interoperable, thereby addressing significant challenges in ML data management. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, enabling easy loading into the most commonly-used ML frameworks, regardless of where the data is stored. Our initial evaluation by human raters shows that Croissant metadata is readable, understandable, complete, yet concise."
  },
  {
    "paper_id": 10,
    "authors": "Pierre Fernandez, Hady Elsahar, Zeki Yalniz, Alexandre Mourachko",
    "title": "Video Seal: Open and Efficient Video Watermarking",
    "paper_url": "https://ai.meta.com/research/publications/video-seal-open-and-efficient-video-watermarking/",
    "pdf_link": "https://arxiv.org/pdf/2412.09492",
    "abstract": "The proliferation of AI-generated content and sophisticated video editing tools has made it both important and challenging to moderate digital platforms. Video watermarking addresses these challenges by embedding imperceptible signals into videos allowing for identification. However, the rare open tools and methods often fall short on efficiency, robustness, and flexibility. To reduce these gaps, this paper introduces Video Seal, a comprehensive framework for neural video watermarking and a competitive open-sourced model. Our approach jointly trains an embedder and an extractor, while ensuring the watermark robustness by applying transformations in-between, e.g., video codecs. This training is multistage and includes image pre-training, hybrid post-training and extractor fine-tuning. We also introduce temporal watermark propagation, a technique to convert any image watermarking model to an efficient video watermarking model without the need to watermark every high-resolution frame. We present experimental results demonstrating the effectiveness of the approach in terms of speed, imperceptibility, and robustness. Video Seal achieves higher robustness compared to strong baselines especially under challenging distortions combining geometric transformations and video compression. Additionally, we provide new insights such as the impact of video compression during training, and how to compare methods operating at different payloads. Contributions in this work – including the codebase, models, and a public demo – are open-sourced under permissive licenses to foster further research and development in the field."
  },
  {
    "paper_id": 11,
    "authors": "The LCM team, Loic Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov, Belen Alastruey, Pierre Andrews, Mariano Coria, Guillaume Couairon, Marta R. Costa-jussa, David Dale, Hady Elsahar, Kevin Heffernan, João Maria Janeiro, Tuan Tran, Christophe Ropers, Eduardo Sánchez, Robin San Roman, Alexandre Mourachko, Safiyyah Saleem, Holger Schwenk",
    "title": "Large Concept Models: Language Modeling in a Sentence Representation Space",
    "paper_url": "https://ai.meta.com/research/publications/large-concept-models-language-modeling-in-a-sentence-representation-space/",
    "pdf_link": "https://arxiv.org/pdf/2412.08821",
    "abstract": "LLMs have revolutionized the field of artificial intelligence and have emerged as the de-facto tool for\nmany tasks. The current established technology of LLMs is to process input and generate output at\nthe token level. This is in sharp contrast to humans who operate at multiple levels of abstraction, well\nbeyond single words, to analyze information and to generate creative content. In this paper, we present\nan attempt at an architecture which operates on an explicit higher-level semantic representation,\nwhich we name a “concept”. Concepts are language- and modality-agnostic and represent a higher\nlevel idea or action in a flow. Hence, we build a“Large Concept Model”. In this study, as\nproof of feasibility, we assume that a concept corresponds to a sentence, and use an existing sentence\nembedding space, SONAR, which supports up to 200 languages in both text and speech modalities.\nThe Large Concept Model is trained to perform autoregressive sentence prediction in an embedding\nspace. We explore multiple approaches, namely MSE regression, variants of diffusion-based generation,\nand models operating in a quantized SONAR space. These explorations are performed using 1.6B\nparameter models and training data in the order of 1.3T tokens. We then scale one architecture to a\nmodel size of 7B parameters and training data of about 7.7T tokens. We perform an experimental\nevaluation on several generative tasks, namely summarization and a new task of summary expansion.\nFinally, we show that our model exhibits impressive zero-shot generalization performance to many\nlanguages, outperforming existing LLMs of the same size. The training code of our models is freely\navailable."
  },
  {
    "paper_id": 12,
    "authors": "Hu Xu, Bernie Huang, Ellen Tan, Ching-Feng Yeh, Jacob Kahn, Christine Jou, Gargi Ghosh, Omer Levy, Luke Zettlemoyer, Scott Yih, Philippe Brunet, Kim Hazelwood, Ramya Raghavendra, Daniel Li (FAIR), Saining Xie, Christoph Feichtenhofer",
    "title": "Meta CLIP 1.2",
    "paper_url": "https://ai.meta.com/research/publications/meta-clip-12/",
    "pdf_link": "https://scontent-lax3-1.xx.fbcdn.net/v/t39.2365-6/469956020_979361357571812_1075277688367756560_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=pXMdXvdFSyoQ7kNvgGQc82g&_nc_zt=14&_nc_ht=scontent-lax3-1.xx&_nc_gid=AH3oW4ngXryORl8W1v7qnh7&oh=00_AYBjHsyNUfKRUZYwhAOhDxYre4P4Oh23gEqpXAAGY7ZJyA&oe=67952607",
    "abstract": "This paper focuses on creating synthetic data to improve the quality of image captions. Existing works typically have two shortcomings. First, they caption images from scratch, ignoring existing alt-text metadata, and second, lack transparency if the captioners’ training data (e.g. GPT) is unknown. In this paper, we study a principled approach Altogether based on the key idea to edit and re-align existing alt-texts associated with the images. To generate training data, we perform human annotation where annotators start with the existing alt-text and re-align it to the image content in multiple rounds, consequently constructing captions with rich visual concepts. This differs from prior work that carries out human annotation as a one-time description task solely based on images and annotator knowledge. We train a captioner on this data that generalizes the process of re-aligning alt-texts at scale. Our results show our Altogether approach leads to richer image captions that also improve text-to-image generation and zero-shot image classification tasks."
  },
  {
    "paper_id": 13,
    "authors": "Narine Kokhlikyan, Bargav Jayaraman, Florian Bordes, Chuan Guo, Kamalika Chaudhuri",
    "title": "Measuring Deja Vu Memorization Efficiently",
    "paper_url": "https://ai.meta.com/research/publications/measuring-deja-vu-memorization-efficiently/",
    "pdf_link": "https://openreview.net/pdf?id=v8RRFNbJ43",
    "abstract": "Recent research has shown that representation learning models may accidentally memorize their training data. For example, the déjà vu method shows that for certain representation learning models and training images, it is sometimes possible to correctly predict the foreground label given only the representation of the background – better than through dataset-level correlations. However, their measurement method requires training two models – one to estimate dataset-level correlations and the other to estimate memorization. This multiple model setup becomes infeasible for large open-source models. In this work, we propose alter- native simple methods to estimate dataset-level correlations, and show that these can be used to approximate an off-the-shelf model’s memorization ability without any retraining. This enables, for the first time, the measurement of memorization in pre-trained open-source image representation and vision-language models. Our results show that different ways of measuring memorization yield very similar aggregate results. We also find that open-source models typically have lower aggregate memorization than similar models trained on a subset of the data. The code is available both for vision and vision language models."
  },
  {
    "paper_id": 14,
    "authors": "Yaron Lipman, Marton Havasi, Peter Holderrieth, Neta Shaul, Matt Le, Brian Karrer, Ricky Chen, David Lopez-Paz, Heli Ben Hamu, Itai Gat",
    "title": "Flow Matching Guide and Code",
    "paper_url": "https://ai.meta.com/research/publications/flow-matching-guide-and-code/",
    "pdf_link": "https://arxiv.org/pdf/2412.06264",
    "abstract": "Flow Matching (FM) is a recent framework for generative modeling that has achieved state-of-the-art performance across various domains, including image, video, audio, speech, and biological structures. This guide offers a comprehensive and self-contained review of FM, covering its mathematical foundations, design choices, and extensions. By also providing a PyTorch package featuring relevant examples (e.g., image and text generation), this work aims to serve as a resource for both novice and experienced researchers interested in understanding, applying and further developing FM."
  },
  {
    "paper_id": 15,
    "authors": "Itai Gat, Tal Remez, Felix Kreuk, Ricky Chen, Gabriel Synnaeve, Yossef (Yossi) Adi, Yaron Lipman, Neta Shaul",
    "title": "Discrete flow matching",
    "paper_url": "https://ai.meta.com/research/publications/discrete-flow-matching/",
    "pdf_link": "https://arxiv.org/pdf/2407.15595",
    "abstract": "Despite Flow Matching and diffusion models having emerged as powerful generative paradigms for continuous variables such as images and videos, their application to high-dimensional discrete data, such as language, is still limited. In this work, we present Discrete Flow Matching, a novel discrete flow paradigm designed specifically for generating discrete data. Discrete Flow Matching offers several key contributions: (i) it works with a general family of probability paths interpolating between source and target distributions; (ii) it allows for a generic formula for sampling from these probability paths using learned posteriors such as the probability denoiser and noise-prediction; (iii) practically, focusing on specific probability paths defined with different schedulers improves generative perplexity compared to previous discrete diffusion and flow models; and (iv) by scaling Discrete Flow Matching models up to 1.7B parameters, we reach 6.7% Pass@1 and 13.4% Pass@10 on HumanEval and 6.7% Pass@1 and 20.6% Pass@10 on 1-shot MBPP coding benchmarks. Our approach is capable of generating high-quality discrete data in a non-autoregressive fashion, significantly closing the gap between autoregressive models and discrete flow models."
  },
  {
    "paper_id": 16,
    "authors": "Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, Tri Dao",
    "title": "FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision",
    "paper_url": "https://ai.meta.com/research/publications/flashattention-3-fast-and-accurate-attention-with-asynchrony-and-low-precision/",
    "pdf_link": "https://arxiv.org/pdf/2407.08608",
    "abstract": "Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35% utilization on the H100 GPU. We develop three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. We demonstrate that our method, FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0X with BF16 reaching up to 840 TFLOPs/s (85% utilization), and with FP8 reaching 1.3 PFLOPs/s. We validate that FP8 FlashAttention-3 achieves 2.6X lower numerical error than a baseline FP8 attention."
  },
  {
    "paper_id": 17,
    "authors": "Jianfeng Chi, Ujjwal Karn, Hongyuan Zhan, Eric Smith, Javier Rando, Yiming Zhang, Kate Plawiak, Zacharie Delpierre Coudert, Kartikeya Upasani, Mahesh Pasupuleti",
    "title": "Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding Conversations",
    "paper_url": "https://ai.meta.com/research/publications/llama-guard-3-vision-safeguarding-human-ai-image-understanding-conversations/",
    "pdf_link": "https://arxiv.org/pdf/2411.10414",
    "abstract": "We introduce Llama Guard 3 Vision, a multimodal LLM-based safeguard for human-AI conversations that involves image understanding: it can be used to safeguard content for both multimodal LLM inputs (prompt classification) and outputs (response classification). Unlike the previous text-only Llama Guard versions (Inan et al., 2023; Llama Team, 2024b,a), it is specifically designed to support image reasoning use cases and is optimized to detect harmful multimodal (text and image) prompts and text responses to these prompts. Llama Guard 3 Vision is fine-tuned on Llama 3.2-Vision and demonstrates strong performance on the internal benchmarks using the MLCommons taxonomy. We also test its robustness against adversarial attacks. We believe that Llama Guard 3 Vision serves as a good starting point to build more capable and robust content moderation tools for human-AI conversation with multimodal capabilities."
  },
  {
    "paper_id": 18,
    "authors": "Igor Fedorov, Kate Plawiak, Lemeng Wu, Tarek Elgamal, Naveen Suda, Eric Smith, Hongyuan Zhan, Jianfeng Chi, Yuriy Hulovatyy, Kimish Patel, Zechun Liu, Yangyang Shi, Tijmen Blankevoort, Mahesh Pasupuleti, Bilge Soran, Zacharie Delpierre Coudert, Rachad Alao, Raghuraman Krishnamoorthi, Vikas Chandra",
    "title": "Llama Guard 3-1B-INT4: Compact and Efficient Safeguard for Human-AI Conversations",
    "paper_url": "https://ai.meta.com/research/publications/llama-guard-3-1b-int4-compact-and-efficient-safeguard-for-human-ai-conversations/",
    "pdf_link": "https://arxiv.org/pdf/2411.17713",
    "abstract": "This paper presents Llama Guard 3-1B-INT4, a compact and efficient Llama Guard model, which has been open-sourced to the community during Meta Connect 2024. We demonstrate that Llama Guard 3-1B-INT4 can be deployed on resource-constrained devices, achieving a throughput of at least 30 tokens per second and a time-to-first-token of 2.5 seconds or less on a commodity Android mobile CPU. Notably, our experiments show that Llama Guard 3-1B-INT4 attains comparable or superior safety moderation scores to its larger counterpart, Llama Guard 3-1B, despite being approximately 7 times smaller in size (440MB)."
  },
  {
    "paper_id": 19,
    "authors": "Shehzaad Dhuliawala, Ilia Kulikov, Ping Yu, Asli Celikyilmaz, Jason Weston, Sainbayar Sukhbaatar, Jack Lanchantin",
    "title": "Adaptive Decoding via Latent Preference Optimization",
    "paper_url": "https://ai.meta.com/research/publications/adaptive-decoding-via-latent-preference-optimization/",
    "pdf_link": "https://arxiv.org/pdf/2411.09661",
    "abstract": "During language model decoding, it is known that using higher temperature sampling gives more creative responses, while lower temperatures are more factually accurate. However, such models are commonly applied to general instruction following, which involves both creative and fact seeking tasks, using a single fixed temperature across all examples and tokens. In this work, we introduce Adaptive Decoding, a layer added to the model to select the sampling temperature dynamically at inference time, at either the token or example level, in order to optimize performance. To learn its parameters we introduce Latent Preference Optimization (LPO) a general approach to train discrete latent variables such as choices of temperature. Our method outperforms all fixed decoding temperatures across a range of tasks that require different temperatures, including UltraFeedback, Creative Story Writing, and GSM8K."
  },
  {
    "paper_id": 20,
    "authors": "Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, Xujie Si",
    "title": "A Survey on Deep Learning for Theorem Proving",
    "paper_url": "https://ai.meta.com/research/publications/a-survey-on-deep-learning-for-theorem-proving/",
    "pdf_link": "https://arxiv.org/pdf/2404.09939",
    "abstract": "Theorem proving is a fundamental aspect of mathematics, spanning from informal reasoning in natural language to rigorous derivations in formal systems. In recent years, the advancement of deep learning, especially the emergence of large language models, has sparked a notable surge of research exploring these techniques to enhance the process of theorem proving. This paper presents a comprehensive survey of deep learning for theorem proving by offering (i) a thorough review of existing approaches across various tasks such as autoformalization, premise selection, proofstep generation, and proof search; (ii) an extensive summary of curated datasets and strategies for synthetic data generation; (iii) a detailed analysis of evaluation metrics and the performance of state-of-the-art methods; and (iv) a critical discussion on the persistent challenges and the promising avenues for future exploration. Our survey aims to serve as a foundational reference for deep learning approaches in theorem proving, inspiring and catalyzing further research endeavors in this rapidly growing field. A curated list of papers is available at this https://github.com/zhaoyu-li/DL4TP."
  },
  {
    "paper_id": 21,
    "authors": "Sherry Xue, Romy Luo, Changan Chen, Kristen Grauman",
    "title": "HOI-Swap: Swapping Objects in Videos with Hand-Object Interaction Awareness",
    "paper_url": "https://ai.meta.com/research/publications/hoi-swap-swapping-objects-in-videos-with-hand-object-interaction-awareness/",
    "pdf_link": "https://arxiv.org/pdf/2406.07754",
    "abstract": "We study the problem of precisely swapping objects in videos, with a focus on those interacted with by hands, given one user-provided reference object image. Despite the great advancements that diffusion models have made in video editing recently, these models often fall short in handling the intricacies of hand-object interactions (HOI), failing to produce realistic edits---especially when object swapping results in object shape or functionality changes. To bridge this gap, we present HOI-Swap, a novel diffusion-based video editing framework trained in a self-supervised manner.  Designed in two stages, the first stage focuses on object swapping in a single frame with HOI awareness; the model learns to adjust the interaction patterns, such as the hand grasp, based on changes in the object's properties. The second stage extends the single-frame edit across the entire sequence; we achieve controllable motion alignment with the original video by: (1) warping a new sequence from the stage-I edited frame based on sampled motion points and (2) conditioning video generation on the warped sequence. Comprehensive qualitative and quantitative evaluations demonstrate that HOI-Swap significantly outperforms existing methods, delivering high-quality video edits with realistic HOIs."
  },
  {
    "paper_id": 22,
    "authors": "Aaron Defazio, Alice Yang, Harsh Mehta, Konstantin Mishchenko, Ahmed Khaled, Ashok Cutkosky",
    "title": "The Road Less Scheduled",
    "paper_url": "https://ai.meta.com/research/publications/the-road-less-scheduled/",
    "pdf_link": "https://arxiv.org/pdf/2405.15682",
    "abstract": "Existing learning rate schedules that do not require specification of the optimization stopping step T are greatly out-performed by learning rate schedules that depend on T. We propose an approach that avoids the need for this stopping time by eschewing the use of schedules entirely, while exhibiting state-of-the-art performance compared to schedules across a wide family of problems ranging from convex problems to large-scale deep learning problems. Our Schedule-Free approach introduces no additional hyper-parameters over standard optimizers with momentum. Our method is a direct consequence of a new theory we develop that unifies scheduling and iterate averaging. Schedule-Free AdamW is the core algorithm behind our winning entry to the MLCommons 2024 AlgoPerf Algorithmic Efficiency Challenge Self-Tuning track."
  },
  {
    "paper_id": 23,
    "authors": "Mike Lambeta, Tingfan Wu, Ali Sengül, Victoria Rose Most, Nolan Black, Kevin Sawyer, Romeo Mercado, Haozhi Qi, Alexander Sohn, Byron Taylor, Norb Tydingco, Gregg Kammerer, Dave Stroud, Jake Khatha, Kurt Jenkins, Kyle Most, Neal Stein, Ricardo Chavira, Thomas Craven-Bartle, Eric Sanchez, Yitian Ding, Jitendra Malik, Roberto Calandra",
    "title": "Digitizing Touch with an Artificial Multimodal Fingertip",
    "paper_url": "https://ai.meta.com/research/publications/digitizing-touch-with-an-artificial-multimodal-fingertip/",
    "pdf_link": "https://arxiv.org/pdf/2411.02479",
    "abstract": "Touch is a crucial sensing modality that provides rich information about object properties and interactions with the physical environment. Humans and robots both benefit from using touch to perceive and interact with the surrounding environment (Johansson and Flanagan, 2009; Li et al., 2020; Calandra et al., 2017). However, no existing systems provide rich, multi-modal digital touch-sensing capabilities through a hemispherical compliant embodiment. Here, we describe several conceptual and technological innovations to improve the digitization of touch. These advances are embodied in an artificial finger-shaped sensor with advanced sensing capabilities. Significantly, this fingertip contains high-resolution sensors (≈8.3 million taxels) that respond to omnidirectional touch, capture multi- modal signals, and use on-device artificial intelligence to process the data in real time. Evaluations show that the artificial fingertip can resolve spatial features as small as 7 um, sense normal and shear forces with a resolution of 1.01 mN and 1.27 mN, respectively, perceive vibrations up to 10 kHz, sense heat, and even sense odor. Furthermore, it embeds an on-device AI neural network accelerator that acts as a peripheral nervous system on a robot and mimics the reflex arc found in humans. These results demonstrate the possibility of digitizing touch with superhuman performance. The implications are profound, and we anticipate potential applications in robotics (industrial, medical, agricultural, and consumer-level), virtual reality and telepresence, prosthetics, and e-commerce. Toward digitizing touch at scale, we open-source a modular platform to facilitate future research on the nature of touch."
  },
  {
    "paper_id": 24,
    "authors": "Matthew Chang, Gunjan Chhablani, Alexander William Clegg, Mikael Dallaire Cote, Ruta Desai, Michal Hlavac, Vladimir Karashchuk, Jacob Krantz, Roozbeh Mottaghi, Priyam Parashar, Siddharth Patki, Ishita Prasad, Xavi Puig, Akshara Rai, Ram Ramrakhya, Daniel Tran, Joanne Truong, John Turner, Eric Undersander, Jimmy Yang",
    "title": "PARTNR: A Benchmark for Planning and Reasoning in Embodied Multi-agent Tasks",
    "paper_url": "https://ai.meta.com/research/publications/partnr-a-benchmark-for-planning-and-reasoning-in-embodied-multi-agent-tasks/",
    "pdf_link": "https://arxiv.org/pdf/2411.00081",
    "abstract": "We present a benchmark for Planning And Reasoning Tasks in humaN-Robot collaboration (PARTNR) designed to study human-robot coordination in household activities. PARTNR tasks exhibit characteristics of everyday tasks, such as spatial, temporal, and heterogeneous agent capability constraints. We employ a semi-automated task generation pipeline using Large Language Models (LLMs), incorporating simulation in the loop for grounding and verification. PARTNR stands as the largest benchmark of its kind, comprising 100,000 natural language tasks, spanning 60 houses and 5,819 unique objects. We analyze state-of-the-art LLMs on PARTNR tasks, across the axes of planning, perception and skill execution. The analysis reveals significant limitations in SoTA models, such as poor coordination and failures in task tracking and recovery from errors. When LLMs are paired with real humans, they require 1.5x as many steps as two humans collaborating and 1.1x more steps than a single human, underscoring the potential for improvement in these models. We further show that fine-tuning smaller LLMs with planning data can achieve performance on par with models 9 times larger, while being 8.6x faster at inference. Overall, PARTNR highlights significant challenges facing collaborative embodied agents and aims to drive research in this direction."
  },
  {
    "paper_id": 25,
    "authors": "Carolina Higuera, Akash Sharma, Krishna Bodduluri, Taosha Fan, Patrick Lancaster, Mrinal Kalakrishnan, Michael Kaess, Byron Boots, Mike Lambeta, Tingfan Wu, Mustafa Mukadam",
    "title": "Sparsh: Self-supervised touch representations for vision-based tactile sensing",
    "paper_url": "https://ai.meta.com/research/publications/sparsh-self-supervised-touch-representations-for-vision-based-tactile-sensing/",
    "pdf_link": "https://arxiv.org/pdf/2410.24090",
    "abstract": "In this work, we introduce general purpose touch representations for the increasingly accessible class of vision-based tactile sensors. Such sensors have led to many recent advances in robot manipulation as they markedly complement vision, yet solutions today often rely on task and sensor specific handcrafted perception models. Collecting real data at scale with task centric ground truth labels, like contact forces and slip, is a challenge further compounded by sensors of various form factor differing in aspects like lighting and gel markings. To tackle this we turn to self-supervised learning (SSL) that has demonstrated remarkable performance in computer vision. We present Sparsh, a family of SSL models that can support various vision-based tactile sensors, alleviating the need for custom labels through pre-training on 460k+ tactile images with masking and self-distillation in pixel and latent spaces. We also build TacBench, to facilitate standardized benchmarking across sensors and models, comprising of six tasks ranging from comprehending tactile properties to enabling physical perception and manipulation planning. In evaluations, we find that SSL pre-training for touch representation outperforms task and sensor-specific end-to-end training by 95.1% on average over TacBench, and Sparsh (DINO) and Sparsh (IJEPA) are the most competitive, indicating the merits of learning in latent space for tactile images."
  },
  {
    "paper_id": 26,
    "authors": "Movie Gen Team",
    "title": "Movie Gen: A Cast of Media Foundation Models",
    "paper_url": "https://ai.meta.com/research/publications/movie-gen-a-cast-of-media-foundation-models/",
    "pdf_link": "https://arxiv.org/pdf/2410.13720",
    "abstract": "We present Movie Gen, a cast of foundation models that generates high-quality, 1080p HD videos with different aspect ratios and synchronized audio. We also show additional capabilities such as precise instruction-based video editing and generation of personalized videos based on a user’s image. Our models set a new state-of-the-art on multiple tasks: text-to-video synthesis, video personalization, video editing, video-to-audio generation, and text-to-audio generation. Our largest video generation model is a 30B parameter transformer trained with a maximum context length of 73K video tokens, corresponding to a generated video of 16 seconds at 16 frames-per-second. We show multiple technical innovations and simplifications on the architecture, latent spaces, training objectives and recipes, data curation, evaluation protocols, parallelization techniques, and inference optimizations that allow us to reap the benefits of scaling pre-training data, model size, and training compute for training large scale media generation models. We hope this paper helps the research community to accelerate progress and innovation in media generation models. All videos from this paper are available at https://go.fb.me/MovieGenResearchVideos."
  },
  {
    "paper_id": 27,
    "authors": "Bandhav Veluri, Benjamin Peloquin, Bokai Yu, Hongyu Gong, Shyam Gollakota",
    "title": "Beyond Turn-Based Interfaces: Synchronous LLMs as Full-Duplex Dialogue Agents",
    "paper_url": "https://ai.meta.com/research/publications/beyond-turn-based-interfaces-synchronous-llms-as-full-duplex-dialogue-agents/",
    "pdf_link": "https://arxiv.org/pdf/2409.15594",
    "abstract": "Despite broad interest in modeling spoken dialogue agents, most approaches are inherently \"half-duplex\" -- restricted to turn-based interaction with responses requiring explicit prompting by the user or implicit tracking of interruption or silence events. Human dialogue, by contrast, is \"full-duplex\" allowing for rich synchronicity in the form of quick and dynamic turn-taking, overlapping speech, and backchanneling. Technically, the challenge of achieving full-duplex dialogue with LLMs lies in modeling synchrony as pre-trained LLMs do not have a sense of \"time\". To bridge this gap, we propose Synchronous LLMs for full-duplex spoken dialogue modeling. We design a novel mechanism to integrate time information into Llama3-8b so that they run synchronously with the real-world clock. We also introduce a training recipe that uses 212k hours of synthetic spoken dialogue data generated from text dialogue data to create a model that generates meaningful and natural spoken dialogue, with just 2k hours of real-world spoken dialogue data. Synchronous LLMs outperform state-of-the-art in dialogue meaningfulness while maintaining naturalness. Finally, we demonstrate the model's ability to participate in full-duplex dialogue by simulating interaction between two agents trained on different datasets, while considering Internet-scale latencies of up to 240 ms."
  },
  {
    "paper_id": 28,
    "authors": "David Dale, Marta R. Costa-jussa",
    "title": "BLASER 2.0: a metric for evaluation and quality estimation of massively multilingual speech and text translation",
    "paper_url": "https://ai.meta.com/research/publications/blaser-20-a-metric-for-evaluation-and-quality-estimation-of-massively-multilingual-speech-and-text-translation/",
    "pdf_link": "https://aclanthology.org/2024.findings-emnlp.943.pdf",
    "abstract": "We present BLASER 2.0, an automatic metric of machine translation quality which supports both speech and text modalities. Compared to its predecessor BLASER (Chen et al., 2023), BLASER 2.0 is based on better underlying text and speech representations that cover 202 text languages and 57 speech ones and extends the training data. BLASER 2.0 comes in two varieties: a reference-based and a reference-free (quality estimation) model. We demonstrate that the reference-free version is applicable not only at the dataset level, for evaluating the overall model performance, but also at the sentence level, for scoring individual translations. In particular, we show its applicability for detecting translation hallucinations and filtering training datasets to obtain more reliable translation models. The BLASER 2.0 models are publicly available at https://github.com/facebookresearch/sonar."
  },
  {
    "paper_id": 29,
    "authors": "Chenhao Fang, Derek Larson, Shitong Zhu, Sophie Zeng, Wendy Summer, Yanqing Peng, Yuriy Hulovatyy, Rajeev Rao, Gabriel Forgues, Arya Pudota, Alex Goncalves, Hervé Robert",
    "title": "Ingest-And-Ground: Dispelling Hallucinations from Continually-Pretrained LLMs with RAG",
    "paper_url": "https://ai.meta.com/research/publications/ingest-and-ground-dispelling-hallucinations-from-continually-pretrained-llms-with-rag/",
    "pdf_link": "https://arxiv.org/pdf/2410.02825",
    "abstract": "This paper presents new methods that have the potential to improve privacy process efficiency with LLM and RAG. To reduce hallucination, we continually pre-train the base LLM model with a privacy-specific knowledge base and then augment it with a semantic RAG layer. Our evaluations demonstrate that this approach enhances the model performance (as much as doubled metrics compared to out-of-box LLM) in handling privacy-related queries, by grounding responses with factual information which reduces inaccuracies."
  },
  {
    "paper_id": 30,
    "authors": "Belen Alastruey, Gerard I. Gállego, Marta R. Costa-jussa",
    "title": "Unveiling the Role of Pretraining in Direct Speech Translation",
    "paper_url": "https://ai.meta.com/research/publications/unveiling-the-role-of-pretraining-in-direct-speech-translation/",
    "pdf_link": "https://arxiv.org/pdf/2409.18044",
    "abstract": "Direct speech-to-text translation systems encounter an important drawback in data scarcity. A common solution consists on pretraining the encoder on automatic speech recognition, hence losing efficiency in the training process. In this study, we compare the training dynamics of a system using a pretrained encoder, the conventional approach, and one trained from scratch. We observe that, throughout the training, the randomly initialized model struggles to incorporate information from the speech inputs for its predictions. Hence, we hypothesize that this issue stems from the difficulty of effectively training an encoder for direct speech translation. While a model trained from scratch needs to learn acoustic and semantic modeling simultaneously, a pretrained one can just focus on the latter. Based on these findings, we propose a subtle change in the decoder cross-attention to integrate source information from earlier steps in training. We show that with this change, the model trained from scratch can achieve comparable performance to the pretrained one, while reducing the training time."
  },
  {
    "paper_id": 31,
    "authors": "Uriel Singer, Amit Zohar, Yuval Kirstain, Shelly Sheynin, Adam Polyak, Devi Parikh, Yaniv Taigman",
    "title": "Video Editing via Factorized Diffusion Distillation",
    "paper_url": "https://ai.meta.com/research/publications/video-editing-via-factorized-diffusion-distillation/",
    "pdf_link": "https://arxiv.org/pdf/2403.09334",
    "abstract": "We introduce Emu Video Edit (EVE), a model that establishes a new state-of-the art in video editing without relying on any supervised video editing data. To develop EVE we separately train an image editing adapter and a video generation adapter, and attach both to the same text-to-image model. Then, to align the adapters towards video editing we introduce a new unsupervised distillation procedure, Factorized Diffusion Distillation. This procedure distills knowledge from one or more teachers simultaneously, without any supervised data. We utilize this procedure to teach EVE to edit videos by jointly distilling knowledge to (i) precisely edit each individual frame from the image editing adapter, and (ii) ensure temporal consistency among the edited frames using the video generation adapter. Finally, to demonstrate the potential of our approach in unlocking other capabilities, we align additional combinations of adapters."
  },
  {
    "paper_id": 32,
    "authors": "Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Luke Zettlemoyer, Omer Levy, Xuezhe Ma",
    "title": "Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model",
    "paper_url": "https://ai.meta.com/research/publications/transfusion-predict-the-next-token-and-diffuse-images-with-one-multi-modal-model/",
    "pdf_link": "https://arxiv.org/pdf/2408.11039",
    "abstract": "We introduce Transfusion, a recipe for training a multi-modal model over discrete and continuous data.\nTransfusion combines the language modeling loss function (next token prediction) with diffusion to train a single transformer over mixed-modality sequences.\nWe pretrain multiple Transfusion models up to 7B parameters from scratch on a mixture of text and image data, establishing scaling laws with respect to a variety of uni- and cross-modal benchmarks.\nOur experiments show that Transfusion scales significantly better than quantizing images and training a language model over discrete image tokens.\nBy introducing modality-specific encoding and decoding layers, we can further improve the performance of Transfusion models, and even compress each image to just 16 patches.\nWe further demonstrate that scaling our Transfusion recipe to 7B parameters and 2T multi-modal tokens produces a model that can generate images and text on a par with similar scale diffusion models and language models, reaping the benefits of both worlds."
  },
  {
    "paper_id": 33,
    "authors": "Navonil Majumder, Chia-Yu Hung, Deepanway Ghosal, Wei-Ning Hsu, Rada Mihalcea, Soujanya Poria",
    "title": "Tango 2: Aligning Diffusion-based Text-to-Audio Generations through Direct Preference Optimization",
    "paper_url": "https://ai.meta.com/research/publications/tango-2-aligning-diffusion-based-text-to-audio-generations-through-direct-preference-optimization/",
    "pdf_link": "https://arxiv.org/pdf/2404.09956",
    "abstract": "Generative multimodal content is increasingly prevalent in much of the content creation arena, as it has the potential to allow artists and media personnel to create pre-production mockups by quickly bringing their ideas to life. The generation of audio from text prompts is an important aspect of such processes in the music and film industry. Many of the recent diffusion-based text-to-audio models focus on training increasingly sophisticated diffusion models on a large set of datasets of prompt-audio pairs. These models do not explicitly focus on the presence of concepts or events and their temporal ordering in the output audio with respect to the input prompt. Our hypothesis is focusing on how these aspects of audio generation could improve audio generation performance in the presence of limited data. As such, in this work, using an existing text-to-audio model Tango, we synthetically create a preference dataset where each prompt has a winner audio output and some loser audio outputs for the diffusion model to learn from. The loser outputs, in theory, have some concepts from the prompt missing or in an incorrect order. We fine-tune the publicly available Tango text-to-audio model using diffusion-DPO (direct preference optimization) loss on our preference dataset and show that it leads to improved audio output over Tango and AudioLDM2, in terms of both automatic- and manual-evaluation metrics."
  },
  {
    "paper_id": 34,
    "authors": "Ashish Shenoy, Yichao Lu, Srihari Jayakumar, Debojeet Chatterjee, Mohsen Moslehpour, Pierce Chuang, Abhay Harpale, Vikas Bhardwaj, Di Xu (SWE), Shicong Zhao, Ankit Ramchandani, Luna Dong, Anuj Kumar",
    "title": "Lumos : Empowering Multimodal LLMs with Scene Text Recognition",
    "paper_url": "https://ai.meta.com/research/publications/lumos-empowering-multimodal-llms-with-scene-text-recognition/",
    "pdf_link": "https://arxiv.org/pdf/2402.08017",
    "abstract": "We introduce Lumos, the first end-to-end multimodal question-answering system with text understanding capabilities. At the core of Lumos is a Scene Text Recognition (STR) component that extracts text from first person point-of-view images, the output of which is used to augment input to a Multimodal Large Language Model (MM-LLM). While building Lumos, we encountered numerous challenges related to STR quality, overall latency, and model inference. In this paper, we delve into those challenges, and discuss the system architecture, design choices, and modeling techniques employed to overcome these obstacles. We also provide a comprehensive evaluation for each component, showcasing high quality and efficiency."
  },
  {
    "paper_id": 35,
    "authors": "Zhihan Xiong, Maryam Fazel, Lin Xiao",
    "title": "Dual Approximation Policy Optimization",
    "paper_url": "https://ai.meta.com/research/publications/dual-approximation-policy-optimization/",
    "pdf_link": "https://arxiv.org/pdf/2410.01249",
    "abstract": "We propose Dual Approximation Policy Optimization (DAPO), a framework that incorporates general function approximation into policy mirror descent methods. In contrast to the popular approach of using the L2-norm to measure function approximation errors, DAPO uses the dual Bregman divergence induced by the mirror map for policy projection. This duality framework has both theoretical and practical implications: not only does it achieve fast linear convergence with general function approximation, but it also includes several well-known practical methods as special cases, immediately providing strong convergence guarantees."
  },
  {
    "paper_id": 36,
    "authors": "Kamalika Chaudhuri, Chuan Guo, Laurens van der Maaten, Saeed Mahloujifar, Mark Tygert",
    "title": "Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds",
    "paper_url": "https://ai.meta.com/research/publications/guarantees-of-confidentiality-via-hammersley-chapman-robbins-bounds/",
    "pdf_link": "https://arxiv.org/pdf/2404.02866",
    "abstract": "Protecting privacy during inference with deep neural networks is possible by adding Gaussian noise to the activations in the last layers prior to the final classifiers or other task-specific layers. The activations in such layers are known as \"features\" (or, less commonly, as \"embeddings\" or \"feature embeddings\"). The added noise helps prevent reconstruction of the inputs from the noisy features. Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise. Convenient, computationally tractable bounds are available from classic inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds. Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with the data sets, \"MNIST\" and \"CIFAR-10,\" which contain 10 classes each for image classification. The HCR bounds appear to be insufficient on their own to guarantee confidentiality of the inputs to inference with standard deep neural nets, \"ResNet-18\" and \"Swin-T,\" pre-trained on the data set, \"ImageNet-1000,\" which contains 1000 classes. Supplementing the addition of Gaussian noise to features with other methods for providing confidentiality may be warranted in the case of ImageNet. In all cases, the results reported here limit consideration to amounts of added noise that incur little degradation in the accuracy of classification from the noisy features. Thus, the added noise enhances confidentiality without much reduction in the accuracy on the task of image classification."
  },
  {
    "paper_id": 37,
    "authors": "Arman Zharmagambetov, Yuandong Tian, Aaron Ferber, Bistra Dilkina, Taoan Huang",
    "title": "Contrastive Predict-and-Search for Mixed Integer Linear Programs",
    "paper_url": "https://ai.meta.com/research/publications/contrastive-predict-and-search-for-mixed-integer-linear-programs/",
    "pdf_link": "https://raw.githubusercontent.com/mlresearch/v235/main/assets/huang24f/huang24f.pdf",
    "abstract": "Mixed integer linear programs (MILP) are flexible and powerful tools for modeling and solving many difficult real-world combinatorial optimization problems. In this paper, we propose a novel machine learning (ML)-based framework ConPaS that learns to predict solutions to MILPs with contrastive learning. For training, we collect high-quality solutions as positive samples. We also collect low-quality or infeasible solutions as negative samples using novel optimization-based or sampling approaches. We then learn to make discriminative predictions by contrasting the positive and negative samples. During testing, we predict and fix the assignments for a subset of integer variables and then solve the resulting reduced MILP to find high-quality solutions. Empirically, ConPaS achieves state-of-the-art results compared to other ML-based approaches in terms of the quality of and the speed at which solutions are found."
  },
  {
    "paper_id": 38,
    "authors": "Igor Tufanov, Karen Hambardzumyan, Javier Ferrando, Lena Voita",
    "title": "LM Transparency Tool: Interactive Tool for Analyzing Transformer Language Models",
    "paper_url": "https://ai.meta.com/research/publications/lm-transparency-tool-interactive-tool-for-analyzing-transformer-language-models/",
    "pdf_link": "https://arxiv.org/pdf/2404.07004",
    "abstract": "We present the LM Transparency Tool (LM-TT), an open-source interactive toolkit for analyzing the internal workings of Transformer-based language models. Differently from previously existing tools that focus on isolated parts of the decision-making process, our framework is designed to make the entire prediction process transparent, and allows tracing back model behavior from the top-layer representation to very fine-grained parts of the model. Specifically, it (i) shows the important part of the whole input-to-output information flow, (ii) allows attributing any changes done by a model block to individual attention heads and feed-forward neurons, (iii) allows interpreting the functions of those heads or neurons. A crucial part of this pipeline is showing the importance of specific model components at each step. As a result, we are able to look at the roles of model components only in cases where they are important for a prediction. Since knowing which components should be inspected is key for analyzing large models where the number of these components is extremely high, we believe our tool will greatly support the interpretability community both in research settings and in practical applications. The LM-TT codebase is available at https://github.com/facebookresearch/llm-transparency-tool."
  },
  {
    "paper_id": 39,
    "authors": "Marta R. Costa-jussa, Mariano Coria Meglioli, Pierre Andrews, David Dale, Kae Hansanti, Elahe Kalbassi, Christophe Ropers, Carleigh Wood",
    "title": "MuTox: Universal MUltilingual Audio-based TOXicity Dataset and Zero-shot Detector",
    "paper_url": "https://ai.meta.com/research/publications/mutox-universal-multilingual-audio-based-toxicity-dataset-and-zero-shot-detector/",
    "pdf_link": "https://arxiv.org/pdf/2401.05060",
    "abstract": "Research in toxicity detection in natural language\nprocessing for the speech modality\n(audio-based) is quite limited, particularly for\nlanguages other than English. To address these\nlimitations and lay the groundwork for truly\nmultilingual audio-based toxicity detection, we\nintroduce MuTox, the first highly multilingual\naudio-based dataset with toxicity labels which\ncovers 14 different linguistic families. The\ndataset comprises 20,000 audio utterances for\nEnglish and Spanish, and 4,000 for the other\n28 languages. To demonstrate the quality of\nthis dataset, we trained the MuTox audio-based\ntoxicity classifier, which enables zero-shot toxicity\ndetection across a wide range of languages.\nThis classifier performs on par with existing\ntext-based trainable classifiers, while expanding\nthe language coverage more than tenfold.\nWhen compared to a wordlist-based classifier\nthat covers a similar number of languages, Mu-\nTox improves F1-Score by an average of 100%.\nThis significant improvement underscores the\npotential of MuTox in advancing the field of\naudio-based toxicity detection."
  },
  {
    "paper_id": 40,
    "authors": "Nicola Cancedda",
    "title": "Spectral Filters, Dark Signals, and Attention Sinks",
    "paper_url": "https://ai.meta.com/research/publications/spectral-filters-dark-signals-and-attention-sinks/",
    "pdf_link": "https://arxiv.org/pdf/2402.09221",
    "abstract": "Projecting intermediate representations onto the vocabulary is an increasingly popular interpretation tool for transformer-based LLMs, also known as the logit lens [nostalgebraist 2020]. We propose a quantitative extension to this approach and define spectral filters on intermediate representations based on partitioning the singular vectors of the vocabulary embedding and unembedding matrices into bands. We find that the signals exchanged in the tail end of the spectrum, i.e. corresponding to the singular vectors with smallest singular values, are responsible for attention sinking [Xiao et al. 2023], of which we provide an explanation. We find that the negative log-likelihood of pretrained models can be kept low despite suppressing sizeable parts of the embedding spectrum in a layer-dependent way, as long as attention sinking is preserved. Finally, we discover that the representation of tokens that draw attention from many tokens have large projections on the tail end of the spectrum, and likely act as additional attention sinks."
  },
  {
    "paper_id": 41,
    "authors": "Emily Wenger, Eshika Saxena, Mohamed Malhou, Ellie Thieu, Kristin Lauter",
    "title": "Benchmarking Attacks on Learning with Errors",
    "paper_url": "https://ai.meta.com/research/publications/benchmarking-attacks-on-learning-with-errors/",
    "pdf_link": "https://arxiv.org/pdf/2408.00882v1",
    "abstract": "Lattice cryptography schemes based on the learning with errors (LWE) hardness assumption have been standardized by NIST for use as post-quantum cryptosystems, and by HomomorphicEncryption.org for performing encrypted computations on sensitive data. Thus, understanding their concrete security is critical. Most work on LWE security focuses on theoretical estimates of attack performance, which is important but may overlook attack nuances arising in real-world implementations. The sole existing concrete benchmarking effort, the Darmstadt Lattice Challenge, does not include benchmarks relevant to the standardized LWE parameter choices—such as small secret and small error distributions, and Ring-LWE (RLWE) and Module-LWE (MLWE) variants. To improve our understanding of concrete LWE security, we provide the first benchmarks for LWE secret recovery on standardized parameters, for small and low-weight (sparse) secrets. We evaluate four LWE attacks in these settings to serve as a baseline: the Search-LWE attacks uSVP, SALSA, and Cool&Cruel, and the DecisionLWE attack: Dual Hybrid Meet-in-the-Middle (MitM). We extend the SALSA and Cool&Cruel attacks in significant ways, and implement and scale up MitM attacks for the first time. For example, we recover hamming weight 9 − 11 binomial secrets for KYBER (kappa = 2) parameters in 28 − 36 hours with SALSA and Cool&Cruel, while we find that MitM can solve DecisionLWE instances for hamming weights up to 4 in under an hour for Kyber parameters, while uSVP attacks do not recover any secrets after running for more than 1100 hours. We also compare concrete performance against theoretical estimates. Finally, we open source the code to enable future research: https://github.com/facebookresearch/LWE-benchmarking"
  },
  {
    "paper_id": 42,
    "authors": "Arman Zharmagambetov, Yuandong Tian",
    "title": "GenCO: Generating Diverse Designs with Combinatorial Constraints",
    "paper_url": "https://ai.meta.com/research/publications/genco-generating-diverse-designs-with-combinatorial-constraints/",
    "pdf_link": "https://arxiv.org/pdf/2310.02442",
    "abstract": "Deep generative models like GAN and VAE have shown impressive results in generating unconstrained objects like images. However, many design settings arising in industrial design, material science, computer graphics and more require that the generated objects satisfy hard combinatorial constraints or meet objectives in addition to modeling a data distribution. To address this, we propose GenCO, a generative framework that guarantees constraint satisfaction throughout training by leveraging differentiable combinatorial solvers to enforce feasibility. GenCO imposes the generative loss on provably feasible solutions rather than intermediate soft solutions, meaning that the deep generative network can focus on ensuring the generated objects match the data distribution without having to also capture feasibility. This shift enables practitioners to enforce hard constraints on the generated outputs during end-to-end training, enabling assessments of their feasibility and introducing additional combinatorial loss components to deep generative training. We demonstrate the effectiveness of our approach on a variety of generative combinatorial tasks, including game level generation, map creation for path planning, and photonic device design, consistently demonstrating its capability to yield diverse, high-quality solutions that verifiably adhere to user-specified combinatorial properties."
  },
  {
    "paper_id": 43,
    "authors": "Ju-Chieh Chou, Wei-Ning Hsu, Karen Livescu, Arun Babu, Alexis Conneau, Alexei Baevski, Michael Auli",
    "title": "Toward Joint Language Modeling for Speech Units and Text",
    "paper_url": "https://ai.meta.com/research/publications/toward-joint-language-modeling-for-speech-units-and-text/",
    "pdf_link": "https://arxiv.org/pdf/2310.08715",
    "abstract": "Speech and text are two major forms of human language. The research community has been focusing on mapping speech to text or vice versa for many years. However, in the field of language modeling, very little effort has been made to model them jointly. In light of this, we explore joint language modeling for speech units and text. Specifically, we compare different speech tokenizers to transform continuous speech signals into discrete units and use different methods to construct mixed speech-text data. We introduce automatic metrics to evaluate how well the joint LM mixes speech and text. We also fine-tune the LM on downstream spoken language understanding (SLU) tasks with different modalities (speech or text) and test its performance to assess the model's learning of shared representations. Our results show that by mixing speech units and text with our proposed mixing techniques, the joint LM improves over a speech-only baseline on SLU tasks and shows zero-shot cross-modal transferability."
  },
  {
    "paper_id": 44,
    "authors": "Lucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Don Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, Madian Khabsa",
    "title": "The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants",
    "paper_url": "https://ai.meta.com/research/publications/the-belebele-benchmark-a-parallel-reading-comprehension-dataset-in-122-language-variants/",
    "pdf_link": "https://arxiv.org/pdf/2308.16884",
    "abstract": "We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the Flores-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and findings, notably that despite significant cross-lingual transfer in English-centric LLMs, much smaller MLMs pretrained on balanced multilingual data still understand far more languages. Overall, Belebele opens up new avenues for evaluating and analyzing the multilingual capabilities of NLP systems."
  },
  {
    "paper_id": 45,
    "authors": "Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chay Ryali, Tengyu Ma, Haitham Khedr, Roman Rädle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollar, Christoph Feichtenhofer",
    "title": "SAM 2: Segment Anything in Images and Videos",
    "paper_url": "https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/",
    "pdf_link": "https://arxiv.org/pdf/2408.00714",
    "abstract": "We present Segment Anything Model 2 (SAM 2 ), a foundation model towards solving promptable visual segmentation in images and videos. We build a data engine, which improves model and data via user interaction, to collect the largest video segmentation dataset to date. Our model is a simple transformer architecture with streaming memory for real-time video processing. SAM 2 trained on our data provides strong performance across a wide range of tasks. In video segmentation, we observe better accuracy, using 3x fewer interactions than prior approaches. In image segmentation, our model is more accurate and 6x faster than the Segment Anything Model (SAM). We believe that our data, model, and insights will serve as a significant milestone for video segmentation and related perception tasks. We are releasing a version of our model, the dataset and an interactive demo."
  },
  {
    "paper_id": 46,
    "authors": "Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Saketh Rambhatla, Mian Akbar Shah, Xi Yin, Devi Parikh, Ishan Misra",
    "title": "Factorizing Text-to-Video Generation by Explicit Image Conditioning",
    "paper_url": "https://ai.meta.com/research/publications/factorizing-text-to-video-generation-by-explicit-image-conditioning/",
    "pdf_link": "https://arxiv.org/pdf/2311.10709",
    "abstract": "We present Emu Video, a text-to-video generation model that factorizes the generation into two steps: first generating an image conditioned on the text, and then generating a video conditioned on the text and the generated image. We identify critical design decisions– adjusted noise schedules for diffusion, and multi-stage training– that enable us to directly generate high quality and high resolution videos, without requiring a deep cascade of models as in prior work. In human evaluations, our generated videos are strongly preferred in quality compared\nto all prior work–81% vs. Google’s Imagen Video, 90% vs. Nvidia’s PYOCO, and 96% vs. Meta’s Make-A-Video. Our model outperforms commercial solutions such as RunwayML’s Gen2 and Pika Labs. Finally, our factorizing approach naturally lends itself to animating images based on a user’s text prompt, where our generations are preferred 96% over prior work."
  },
  {
    "paper_id": 47,
    "authors": "Vlad Sobal, Mark Ibrahim, Randall Balestriero, Vivien Cabannes, Pietro Astolfi, Kyunghyun Cho, Yann LeCun",
    "title": "X-Sample Contrastive Loss: Improving Contrastive Learning with Sample Similarity Graphs",
    "paper_url": "https://ai.meta.com/research/publications/x-sample-contrastive-loss-improving-contrastive-learning-with-sample-similarity-graphs/",
    "pdf_link": "https://arxiv.org/pdf/2407.18134",
    "abstract": "Learning good representations involves capturing the diverse ways in which data samples relate. Contrastive loss—an objective matching related samples—underlies methods from self-supervised to multimodal learning. Contrastive losses, however, can be viewed more broadly as modifying a similarity graph to indicate how samples should relate in the embedding space. This view reveals a shortcoming in contrastive learning: the similarity graph is binary, as only one sample is the related positive sample. Crucially, similarities across samples are ignored. Based on this observation, we revise the standard contrastive loss to explicitly encode how a sample relates to others. We experiment with this new objective, called X-Sample Contrastive, to train vision models based on similarities in class or text caption descriptions. Our study spans three scales: ImageNet-1k with 1 million, CC3M with 3 million, and CC12M with 12 million samples. The representations learned via our objective outperform both contrastive self-supervised and vision-language models trained on the same data across a range of tasks. When training on CC12M, we outperform CLIP by 0.6% on both ImageNet and ImageNet Real. Our objective appears to work particularly well in lower-data regimes, with gains over CLIP of 16.8% on ImageNet and 18.1% on ImageNet Real when training with CC3M. Finally, our objective seems to encourage the model to learn representations that separate objects from their attributes and backgrounds, with gains of 3.3-5.6% over CLIP on ImageNet9. We hope the proposed solution takes a small step towards developing richer learning objectives for understanding sample relations in foundation models."
  },
  {
    "paper_id": 48,
    "authors": "Olga Golovneva, Zeyuan Allen-Zhu, Jason Weston, Sainbayar Sukhbaatar",
    "title": "Reverse Training to Nurse the Reversal Curse",
    "paper_url": "https://ai.meta.com/research/publications/reverse-training-to-nurse-the-reversal-curse/",
    "pdf_link": "https://arxiv.org/pdf/2403.13799",
    "abstract": "Large language models (LLMs) have a surprising failure: when trained on “A has a feature B”, they do not generalize to “B is a feature of A”, which is termed the Reversal Curse. Even when training with trillions of tokens this issue still appears due to Zipf’s law – hence even if we train on the entire internet. This work proposes an alternative training scheme, called reverse training, whereby all words are used twice, doubling the amount of available tokens. The LLM is trained in both forward and reverse directions by reversing training strings while preserving (i.e., not reversing) chosen substrings, such as entities. We show that data-matched reverse-trained models provide superior performance to standard models on standard tasks, and compute-matched reverse-trained models provide far superior performance on reversal tasks, helping resolve the reversal curse issue."
  },
  {
    "paper_id": 49,
    "authors": "Zecheng He, Bo Sun, Felix Xu, Haoyu Ma, Ankit Ramchandani, Vincent Cheung, Siddharth Shah, Anmol Kalia, Ning Zhang, Peizhao Zhang, Roshan Sumbaly, Peter Vajda, Animesh Sinha",
    "title": "Imagine yourself: Tuning-Free Personalized Image Generation",
    "paper_url": "https://ai.meta.com/research/publications/imagine-yourself-tuning-free-personalized-image-generation/",
    "pdf_link": "https://arxiv.org/pdf/2409.13346",
    "abstract": "Diffusion models have demonstrated remarkable efficacy across various image-to-image tasks. In this research, we introduce Imagine yourself, a state-of-the-art model designed for personalized image generation. Unlike conventional tuning-based personalization techniques, Imagine yourself operates as a tuning-free model, enabling all users to leverage a shared framework without individualized adjustments. Moreover, previous work met challenges balancing identity preservation, following complex prompts and preserving good visual quality, resulting in models having strong copy-paste effect of the reference images. Thus, they can hardly generate images following prompts that require significant changes to the reference image, e.g., changing facial expression, head and body poses, and the diversity of the generated images is low. To address these limitations, our proposed method introduces 1) a new synthetic paired data generation mechanism to encourage image diversity, 2) a fully parallel attention architecture with three text encoders and a fully trainable vision encoder to improve the text faithfulness, and 3) a novel coarse-to-fine multi-stage finetuning methodology that gradually pushes the boundary of visual quality. Our study demonstrates that Imagine yourself surpasses the state-of-the-art personalization model, exhibiting superior capabilities in identity preservation, visual quality, and text alignment. This model establishes a robust foundation for various personalization applications. Human evaluation results validate the model’s SOTA superiority across all aspects (identity preservation, text faithfulness, and visual appeal) compared to the previous personalization models."
  },
  {
    "paper_id": 50,
    "authors": "Llama team",
    "title": "The Llama 3 Herd of Models",
    "paper_url": "https://ai.meta.com/research/publications/the-llama-3-herd-of-models/",
    "pdf_link": "https://arxiv.org/pdf/2407.21783",
    "abstract": "Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage.\nOur largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development."
  },
  {
    "paper_id": 51,
    "authors": "Shengye Wan, Cyrus Nikolaidis, Daniel Song, David Molnar, James Crnkovich, Jayson Grace, Manish Bhatt, Sahana Chennabasappa, Spencer Whitman, Stephanie Ding, Vlad Ionescu, Yue Li, Joshua Saxe",
    "title": "CYBERSECEVAL 3: Advancing the Evaluation of Cybersecurity Risks and Capabilities in Large Language Models",
    "paper_url": "https://ai.meta.com/research/publications/cyberseceval-3-advancing-the-evaluation-of-cybersecurity-risks-and-capabilities-in-large-language-models/",
    "pdf_link": "https://arxiv.org/pdf/2408.01605",
    "abstract": "We are releasing a new suite of security benchmarks for LLMs, CYBERSECEVAL 3, to continue the conversation on empirically measuring LLM cybersecurity risks and capabilities. CYBERSECEVAL 3 assesses 8 different risks across two broad categories: risk to third parties, and risk to application developers and end users. Compared to previous work, we add new areas focused on offensive security capabilities: automated social engineering, scaling manual offensive cyber operations, and autonomous offensive cyber operations. In this paper we discuss applying these benchmarks to the Llama 3 models and a suite of contemporaneous state-of-the-art LLMs, enabling us to contextualize risks both with and without mitigations in place."
  },
  {
    "paper_id": 52,
    "authors": "Ouail Kitouni, Niklas Nolte, Samuel Pérez Díaz, Sokratis Trifinopoulos, Mike Williams",
    "title": "From Neurons to Neutrons: A Case Study in Mechanistic Interpretability",
    "paper_url": "https://ai.meta.com/research/publications/from-neurons-to-neutrons-a-case-study-in-mechanistic-interpretability/",
    "pdf_link": "https://arxiv.org/pdf/2405.17425",
    "abstract": "Mechanistic Interpretability (MI) promises a path\ntoward fully understanding how neural networks\nmake their predictions. Prior work demonstrates that even when trained to perform simple arithmetic, models can implement a variety of algorithms (sometimes concurrently) depending on initialization and hyperparameters.\nDoes this mean neuron-level interpretability techniques have limited applicability? We argue that\nhigh-dimensional neural networks can learn lowdimensional representations of their training data\nthat are useful beyond simply making good predictions. Such representations can be understood\nthrough the mechanistic interpretability lens and\nprovide insights that are surprisingly faithful to\nhuman-derived domain knowledge. This indicates\nthat such approaches to interpretability can be useful for deriving a new understanding of a problem\nfrom models trained to solve it. As a case study,\nwe extract nuclear physics concepts by studying\nmodels trained to reproduce nuclear data."
  },
  {
    "paper_id": 53,
    "authors": "Junlin Han, Filippos Kokkinos, Philip Torr",
    "title": "VFusion3D: Learning Scalable 3D Generative Models from Video Diffusion Models",
    "paper_url": "https://ai.meta.com/research/publications/vfusion3d-learning-scalable-3d-generative-models-from-video-diffusion-models/",
    "pdf_link": "https://arxiv.org/pdf/2403.12034",
    "abstract": "This paper presents a novel method for building scalable 3D generative models utilizing pre-trained video diffusion models. The primary obstacle in developing foundation 3D generative models is the limited availability of 3D data. Unlike images, texts, or videos, 3D data are not readily accessible and are difficult to acquire. This results in a significant disparity in scale compared to the vast quantities of other types of data.\nTo address this issue, we propose using a video diffusion model, trained with extensive volumes of text, images, and videos, as a knowledge source for 3D data. By unlocking its multi-view generative capabilities through fine-tuning, we generate a large-scale synthetic multi-view dataset to train a feed-forward 3D generative model. The proposed model, VFusion3D, trained on nearly 3M synthetic multi-view data, can generate a 3D asset from a single image in seconds and achieves superior performance when compared to current SOTA feed-forward 3D generative models, with users preferring our results over 90% of the time. The VFusion3D codebase is available at https://github.com/facebookresearch/vfusion3d."
  },
  {
    "paper_id": 54,
    "authors": "Antonio Orvieto, Lin Xiao",
    "title": "An Adaptive Stochastic Gradient Method with Non-negative Gauss-Newton Stepsizes",
    "paper_url": "https://ai.meta.com/research/publications/an-adaptive-stochastic-gradient-method-with-non-negative-gauss-newton-stepsizes/",
    "pdf_link": "https://arxiv.org/pdf/2407.04358",
    "abstract": "We consider the problem of minimizing the average of a large number of smooth but possibly non-convex functions. In the context of most machine learning applications, each loss function is non-negative and thus can be expressed as the composition of a square and its real-valued square root. This reformulation allows us to apply the Gauss-Newton method, or the Levenberg-Marquardt method when adding a quadratic regularization. The resulting algorithm, while being computationally as efficient as the vanilla stochastic gradient method, is highly adaptive and can automatically warmup and decay the effective stepsize while tracking the non-negative loss landscape. We provide a tight convergence analysis, leveraging new techniques, in the stochastic convex and non-convex settings. In particular, in the convex case, the method does not require access to the gradient Lipshitz constant for convergence, and is guaranteed to never diverge. The convergence rates and empirical evaluations compare favorably to the classical (stochastic) gradient method as well as to several other adaptive methods. (https://arxiv.org/abs/2407.04358)"
  },
  {
    "paper_id": 55,
    "authors": "Yawar Siddiqui, Tom Monnier, Filippos Kokkinos, Mahendra Kariya, Yanir Kleiman, Emilien Garreau, Oran Gafni, Natalia Neverova, Andrea Vedaldi, Roman Shapovalov, David Novotny",
    "title": "Meta 3D AssetGen: Text-to-Mesh Generation with High-Quality Geometry, Texture, and PBR Materials",
    "paper_url": "https://ai.meta.com/research/publications/meta-3d-assetgen-text-to-mesh-generation-with-high-quality-geometry-texture-and-pbr-materials/",
    "pdf_link": "https://arxiv.org/pdf/2407.02445",
    "abstract": "We present Meta 3D AssetGen (AssetGen), a significant advancement in text-to-3D generation which produces faithful, high-quality meshes with  texture and material control. Compared to works that bake shading in the 3D object's appearance, AssetGen outputs physically-based rendering (PBR) materials, supporting realistic relighting. AssetGen generates first several views of the object with factored shaded and albedo appearance channels, and then reconstructs colours, metalness and roughness in 3D, using a  deferred shading loss for efficient supervision. It also uses a sign-distance function to represent 3D shape more reliably and introduces a corresponding loss for direct shape supervision. This is implemented using fused kernels for high memory efficiency. After mesh extraction, a texture refinement transformer operating in UV space significantly improves sharpness and details. AssetGen achieves 17% improvement in Chamfer Distance and 40% in LPIPS over the best concurrent work for few-view reconstruction, and a human preference of 72% over the best industry competitors of comparable speed, including those that support PBR. Project page with generated assets: https://assetgen.github.io."
  },
  {
    "paper_id": 56,
    "authors": "Raphael Bensadoun, Tom Monnier, Yanir Kleiman, Filippos Kokkinos, Yawar Siddiqui, Mahendra Kariya, Omri Harosh, Roman Shapovalov, Emilien Garreau, Animesh Karnewar, Ang Cao, Idan Azuri, Iurii Makarov, Eric-Tuan Le, Antoine Toisoul, David Novotny, Oran Gafni, Natalia Neverova, Andrea Vedaldi",
    "title": "Meta 3D Gen",
    "paper_url": "https://ai.meta.com/research/publications/meta-3d-gen/",
    "pdf_link": "https://assetgen.github.io/static/AssetGen.pdf",
    "abstract": "We introduce Meta 3D Gen (3DGen), a new state-of-the-art, fast pipeline for text-to-3D asset generation. 3DGen offers 3D asset creation with high prompt fidelity and high-quality 3D shapes\nand textures in under a minute. It supports physically-based rendering (PBR), necessary for 3D asset relighting in real-world applications. Additionally, 3DGen supports generative retexturing of previously generated (or artist-created) 3D shapes using additional textual inputs provided by the user. 3DGen integrates key technical components, Meta 3D AssetGen and Meta 3D TextureGen, that we developed for text-to-3D and text-to-texture generation, respectively. By combining their strengths, 3DGen represents 3D objects simultaneously in three ways: in view space, in volumetric space, and in UV (or texture) space. The integration of these two techniques achieves a win rate of\n68% with respect to the single-stage model. We compare 3DGen to numerous industry baselines, and show that it outperforms them in terms of prompt fidelity and visual quality for complex textual prompts, while being significantly faster."
  },
  {
    "paper_id": 57,
    "authors": "Raphael Bensadoun, Yanir Kleiman, Idan Azuri, Omri Harosh, Andrea Vedaldi, Natalia Neverova, Oran Gafni",
    "title": "Meta 3D TextureGen: Fast and Consistent Texture Generation for 3D Objects",
    "paper_url": "https://ai.meta.com/research/publications/meta-3d-texturegen-fast-and-consistent-texture-generation-for-3d-objects/",
    "pdf_link": "https://arxiv.org/pdf/2407.02430",
    "abstract": "The recent availability and adaptability of text-to-image\nmodels has sparked a new era in many related domains that\nbenefit from the learned text priors as well as high-quality\nand fast generation capabilities, one of which is texture generation for 3D objects. Although recent texture generation\nmethods achieve impressive results by using text-to-image\nnetworks, the combination of global consistency, quality,\nand speed, which is crucial for advancing texture generation to real-world applications, remains elusive.\n\nTo that end, we introduce Meta 3D TextureGen: a new\nfeedforward method comprised of two sequential networks\naimed at generating high-quality and globally consistent\ntextures for arbitrary geometries of any complexity degree\nin less than 20 seconds. Our method achieves state-of-the-art results in quality and speed by conditioning a text-to-image model on 3D semantics in 2D space and fusing\nthem into a complete and high-resolution UV texture map,\nas demonstrated by extensive qualitative and quantitative\nevaluations. In addition, we introduce a texture enhancement network that is capable of up-scaling any texture by\nan arbitrary ratio, producing 4k pixel resolution textures."
  },
  {
    "paper_id": 58,
    "authors": "Andrei Lupu, Chris Lu, Robert Lange, Jakob Foerster",
    "title": "Behaviour Distillation",
    "paper_url": "https://ai.meta.com/research/publications/behaviour-distillation/",
    "pdf_link": "https://arxiv.org/pdf/2406.15042",
    "abstract": "Dataset distillation aims to condense large datasets into a small number of synthetic examples that can be used as drop-in replacements when training new models. It has applications to interpretability, neural architecture search, privacy, and continual learning. Despite strong successes in supervised domains, such methods have not yet been extended to reinforcement learning, where the lack of a fixed dataset renders most distillation methods unusable. Filling the gap, we formalize behaviour distillation, a setting that aims to discover and then condense the information required for training an expert policy into a synthetic dataset of state-action pairs, without access to expert data. We then introduce Hallucinating Datasets with Evolution Strategies (HaDES), a method for behaviour distillation that can discover datasets of just four state-action pairs which, under supervised learning, train agents to competitive performance levels in continuous control tasks. We show that these datasets generalize out of distribution to training policies with a wide range of architectures and hyperparameters. We also demonstrate application to a downstream task, namely training multi-task agents in a zero-shot fashion. Beyond behaviour distillation, HaDES provides significant improvements in neuroevolution for RL over previous approaches and achieves SoTA results on one standard supervised dataset distillation task. Finally, we show that visualizing the synthetic datasets can provide human-interpretable task insights."
  },
  {
    "paper_id": 59,
    "authors": "Chris Cummins, Volker Seeker, Dejan Grubisic, Baptiste Rozière, Jonas Gehring, Gabriel Synnaeve, Hugh Leather",
    "title": "Meta Large Language Model Compiler: Foundation Models of Compiler Optimization",
    "paper_url": "https://ai.meta.com/research/publications/meta-large-language-model-compiler-foundation-models-of-compiler-optimization/",
    "pdf_link": "https://arxiv.org/pdf/2407.02524",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, we introduce Meta Large Language Model Compiler (LLM Compiler), a suite of robust, openly available, pre-trained models specifically designed for code optimization tasks. Built on the foundation of Code Llama, LLM Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The model has been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and has undergone instruction fine-tuning to interpret compiler behavior. LLM Compiler is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. We also present fine-tuned versions of the model, demonstrating its enhanced capabilities in optimizing code size and disassembling from x86_64 and ARM assembly back into LLVM-IR. These achieve 77% of the optimising potential of an autotuning search, and 45% disassembly round trip (14% exact match). This release aims to provide a scalable, cost-effective foundation for further research and development in compiler optimization by both academic researchers and industry practitioners."
  },
  {
    "paper_id": 60,
    "authors": "Elena Voita, Javier Ferrando Monsonis, Christoforos Nalmpantis",
    "title": "Neurons in Large Language Models: Dead, N-gram, Positional",
    "paper_url": "https://ai.meta.com/research/publications/neurons-in-large-language-models-dead-n-gram-positional/",
    "pdf_link": "https://arxiv.org/pdf/2309.04827",
    "abstract": "We analyze a family of large language models in such a lightweight manner that can be done on a single GPU. Specifically, we focus on the OPT family of models ranging from 125m to 66b parameters and rely only on whether an FFN neuron is activated or not. First, we find that the early part of the network is sparse and represents many discrete features. Here, many neurons (more than 70% in some layers of the 66b model) are \"dead\", i.e. they never activate on a large collection of diverse data. At the same time, many of the alive neurons are reserved for discrete features and act as token and n-gram detectors. Interestingly, their corresponding FFN updates not only promote next token candidates as could be expected, but also explicitly focus on removing the information about triggering them tokens, i.e., current input. To the best of our knowledge, this is the first example of mechanisms specialized at removing (rather than adding) information from the residual stream. With scale, models become more sparse in a sense that they have more dead neurons and token detectors. Finally, some neurons are positional: them being activated or not depends largely (or solely) on position and less so (or not at all) on textual data. We find that smaller models have sets of neurons acting as position range indicators while larger models operate in a less explicit manner."
  },
  {
    "paper_id": 61,
    "authors": "Min-Jae Hwang, Ilia Kulikov, Benjamin Peloquin, Hongyu Gong, Peng-Jen Chen, Ann Lee",
    "title": "Textless Acoustic Model with Self-Supervised Distillation for Noise-Robust Expressive Speech-to-Speech Translation",
    "paper_url": "https://ai.meta.com/research/publications/textless-acoustic-model-with-self-supervised-distillation-for-noise-robust-expressive-speech-to-speech-translation/",
    "pdf_link": "https://arxiv.org/pdf/2406.02733",
    "abstract": "In this paper, we propose a textless acoustic model with a self-supervised distillation strategy for noise-robust expressive speech-to-speech translation (S2ST). Recently proposed expressive S2ST systems have achieved impressive expressivity preservation performances by cascading unit-to-speech (U2S) generator to the speech-to-unit translation model. However, these systems are vulnerable to the presence of noise in input speech, which is an assumption in real-world translation scenarios. To address this limitation, we propose a U2S generator that incorporates a distillation with no label (DINO) self-supervised training strategy into it's pretraining process. Because the proposed method captures noise-agnostic expressivity representation, it can generate qualified speech even in noisy environment. Objective and subjective evaluation results verified that the proposed method significantly improved the performance of the expressive S2ST system in noisy environments while maintaining competitive performance in clean environments."
  },
  {
    "paper_id": 62,
    "authors": "Weiyao Wang, Pierre Gleize, Hao Tang, Xingyu Chen, Kevin Liang, Matt Feiszli",
    "title": "ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization",
    "paper_url": "https://ai.meta.com/research/publications/icon-incremental-confidence-for-joint-pose-and-radiance-field-optimization/",
    "pdf_link": "https://arxiv.org/pdf/2401.08937",
    "abstract": "Neural Radiance Fields (NeRF) exhibit remarkable performance for Novel View Synthesis (NVS) given a set of 2D images. However, NeRF training requires accurate camera pose for each input view, typically obtained by Structure-from-Motion (SfM) pipelines. Recent works have attempted to relax this constraint, but they still often rely on decent initial poses which they can refine. Here we aim at removing the requirement for pose initialization. We present Incremental CONfidence (ICON), an optimization procedure for training NeRFs from 2D video frames. ICON only assumes smooth camera motion to estimate initial guess for poses. Further, ICON introduces \"confidence\": an adaptive measure of model quality used to dynamically reweight gradients. ICON relies on high-confidence poses to learn NeRF, and high-confidence 3D structure (as encoded by NeRF) to learn poses. We show that ICON, without prior pose initialization, achieves superior performance in both CO3D and HO3D versus methods which use SfM pose."
  },
  {
    "paper_id": 63,
    "authors": "Jiawei Ren, Frost Xu, Jerry Wu, Ziwei Liu, Tao Xiang, Antoine Toisoul",
    "title": "Move Anything with Layered Scene Diffusion",
    "paper_url": "https://ai.meta.com/research/publications/move-anything-with-layered-scene-diffusion/",
    "pdf_link": "https://arxiv.org/pdf/2404.07178",
    "abstract": "Diffusion models generate images with an unprecedented level of quality, but how can we freely rearrange image layouts? Recent works generate controllable scenes via learning spatially disentangled latent codes, but these methods do not apply to diffusion models due to their fixed forward process. In this work, we propose SceneDiffusion to optimize a layered scene representation during the diffusion sampling process. Our key insight is that spatial disentanglement can be obtained by jointly denoising scene renderings at different spatial layouts. Our generated scenes support a wide range of spatial editing operations, including moving, resizing, cloning, and layer-wise appearance editing operations, including object restyling and replacing. Moreover, a scene can be generated conditioned on a reference image, thus enabling object moving for in-the-wild images. Notably, this approach is training-free, compatible with general text-to-image diffusion models, and responsive in less than a second."
  },
  {
    "paper_id": 64,
    "authors": "Heli Ben-Hamu, Omri Puny, Itai Gat, Brian Karrer, Uriel Singer, Yaron Lipman",
    "title": "D-Flow: Differentiating through Flows for Controlled Generation",
    "paper_url": "https://ai.meta.com/research/publications/d-flow-differentiating-through-flows-for-controlled-generation/",
    "pdf_link": "https://arxiv.org/pdf/2402.14017",
    "abstract": "Taming the generation outcome of state of the art Diffusion and Flow-Matching (FM) models without having to re-train a task-specific model unlocks a powerful tool for solving inverse problems, conditional generation, and controlled generation in general. In this work we introduce D-Flow, a simple framework for controlling the generation process by differentiating through the flow, optimizing for the source (noise) point. We motivate this framework by our key observation stating that for Diffusion/FM models trained with Gaussian probability paths, differentiating through the generation process projects gradient on the data manifold, implicitly injecting the prior into the optimization process. We validate our framework on linear and non-linear controlled generation problems including: image and audio inverse problems and conditional molecule generation reaching state of the art performance across all."
  },
  {
    "paper_id": 65,
    "authors": "Neta Shaul, Uriel Singer, Ricky Chen, Matt Le, Ali Thabet, Albert Pumarola, Yaron Lipman",
    "title": "Bespoke Non-Stationary Solvers for Fast Sampling of Diffusion and Flow Models",
    "paper_url": "https://ai.meta.com/research/publications/bespoke-non-stationary-solvers-for-fast-sampling-of-diffusion-and-flow-models/",
    "pdf_link": "https://arxiv.org/pdf/2403.01329",
    "abstract": "This paper introduces Bespoke Non-Stationary (BNS) Solvers, a solver distillation approach to improve sample efficiency of Diffusion and Flow models. BNS solvers are based on a family of non-stationary solvers that provably subsumes existing numerical ODE solvers and consequently demonstrate considerable improvement in sample approximation (PSNR) over these baselines. Compared to model distillation, BNS solvers benefit from a tiny parameter space (<200 parameters), fast optimization (two orders of magnitude faster), maintain diversity of samples, and in contrast to previous solver distillation approaches nearly close the gap from standard distillation methods such as Progressive Distillation in the low-medium NFE regime. For example, BNS solver achieves 45 PSNR / 1.76 FID using 16 NFE in class-conditional ImageNet-64. We experimented with BNS solvers for conditional image generation, text-to-image generation, and text-2-audio generation showing significant improvement in sample approximation (PSNR) in all."
  },
  {
    "paper_id": 66,
    "authors": "Abhishek Sureddy, Dishant Padalia, Nandhinee Periyakaruppa, Oindrila Saha, Adina Williams, Adriana Romero Soriano, Megan Richards, Polina Kirichenko, Melissa Hall",
    "title": "Decomposed evaluations of geographic disparities in text-to-image models",
    "paper_url": "https://ai.meta.com/research/publications/decomposed-evaluations-of-geographic-disparities-in-text-to-image-models/",
    "pdf_link": "https://arxiv.org/pdf/2406.11988",
    "abstract": "Recent work has identified substantial disparities in generated images of different geographic regions, including stereotypical depictions of everyday objects like houses and cars. However, existing measures for these disparities have been limited to either human evaluations, which are time consuming and costly, or automatic metrics evaluating full images, which are unable to attribute these disparities to specific parts of the generated images. In this work, we introduce a new set of metrics, Decomposed Indicators of Disparities in Image Generation (Decomposed-DIG), that allows us to separately measure geographic disparities in the depiction of objects and backgrounds in generated images. Using Decomposed-DIG, we audit a widely used latent diffusion model and find that generated images depict objects with better realism than backgrounds and that backgrounds in generated images tend to contain larger regional disparities than objects. We use Decomposed-DIG to pinpoint specific examples of disparities, such as stereotypical background generation in Africa, struggling to generate modern vehicles in Africa, and unrealistically placing some objects in outdoor settings. Informed by our metric, we use a new prompting structure that enables a 52% worst-region improvement and a 20% average improvement in generated background diversity."
  },
  {
    "paper_id": 67,
    "authors": "Tom Sander, Yaodong Yu, Maziar Sanjabi, Alain Durmus, Yi Ma, Kamalika Chaudhuri, Chuan Guo",
    "title": "Differentially Private Representation Learning via Image Captioning",
    "paper_url": "https://ai.meta.com/research/publications/differentially-private-representation-learning-via-image-captioning/",
    "pdf_link": "https://arxiv.org/pdf/2403.02506",
    "abstract": "Differentially private (DP) machine learning is considered the gold-standard solution for training a model from sensitive data while still preserving privacy. However, a major barrier to achieving this ideal is its sub-optimal privacy-accuracy trade-off, which is particularly visible in DP representation learning. Specifically, it has been shown that under modest privacy budgets, most models learn representations that are not significantly better than hand-crafted features. In this work, we show that effective DP representation learning can be done via image captioning and scaling up to internet-scale multimodal datasets. Through a series of engineering tricks, we successfully train a DP image captioner (DP-Cap) on a 233M subset of LAION-2B from scratch using a reasonable amount of computation, and obtaining unprecedented high-quality image features that can be used in a variety of downstream vision and vision-language tasks. For example, under a privacy budget of epsilon=8 for the LAION dataset, a linear classifier trained on top of learned DP-Cap features attains 65.8% accuracy on ImageNet-1K, considerably improving the previous SOTA of 56.5%. Our work challenges the prevailing sentiment that high-utility DP representation learning cannot be achieved by training from scratch. Code is available at https://github.com/facebookresearch/dpcap."
  },
  {
    "paper_id": 68,
    "authors": "Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Scott Yih, Xilun Chen",
    "title": "How to Train Your DRAGON: Diverse Augmentation Towards Generalizable Dense Retrieval",
    "paper_url": "https://ai.meta.com/research/publications/how-to-train-your-dragon-diverse-augmentation-towards-generalizable-dense-retrieval/",
    "pdf_link": "https://arxiv.org/pdf/2302.07452",
    "abstract": "Various techniques have been developed in recent years to improve dense retrieval (DR), such as unsupervised contrastive learning and pseudo-query generation. Existing DRs, however, often suffer from effectiveness tradeoffs between supervised and zero-shot retrieval, which some argue was due to the limited model capacity. We contradict this hypothesis and show that a generalizable DR can be trained to achieve high accuracy in both supervised and zero-shot retrieval without increasing model size. In particular, we systematically examine the contrastive learning of DRs, under the framework of Data Augmentation (DA). Our study shows that common DA practices such as query augmentation with generative models and pseudo-relevance label creation using a cross-encoder, are often inefficient and sub-optimal. We hence propose a new DA approach with diverse queries and sources of supervision to progressively train a generalizable DR. As a result, DRAGON, our dense retriever trained with diverse augmentation, is the first BERT-base-sized DR to achieve state-of-the-art effectiveness in both supervised and zero-shot evaluations and even competes with models using more complex late interaction (ColBERTv2 and SPLADE++). The code is available at: https://github.com/facebookresearch/dpr-scale/tree/main/dragon"
  },
  {
    "paper_id": 69,
    "authors": "Yaodong Yu, Maziar Sanjabi, Yi Ma, Kamalika Chaudhuri, Chuan Guo",
    "title": "ViP: A Differentially Private Foundation Model for Computer Vision",
    "paper_url": "https://ai.meta.com/research/publications/vip-a-differentially-private-foundation-model-for-computer-vision/",
    "pdf_link": "https://arxiv.org/pdf/2306.08842",
    "abstract": "Artificial intelligence (AI) has seen a tremendous surge in capabilities thanks to the use of foundation models trained on internet-scale data. On the flip side, the uncurated nature of internet-scale data also poses significant privacy and legal risks, as they often contain personal information or copyrighted material that should not be trained on without permission. \nIn this work, we propose as a mitigation measure a recipe to train foundation vision models via self-supervised learning with differential privacy (DP) guarantee. We identify masked autoencoders as a suitable learning algorithm that aligns well with DP-SGD, and train ViP---a Vision transformer with differential Privacy---under a strict privacy budget of epsilon=8 on the LAION400M dataset. We evaluate the quality of representation learned by ViP using standard downstream vision tasks; in particular, ViP achieves a (non-private) linear probing accuracy of 55.7% on ImageNet, comparable to that of end-to-end trained AlexNet (trained and evaluated on ImageNet). Our result suggests that scaling to internet-scale data can be practical for private learning. Code and DP pre-trained models  are available at https://github.com/facebookresearch/ViP-MAE."
  },
  {
    "paper_id": 70,
    "authors": "Mostafa Elhoushi, Akshat Shrivastava, Diana Liskovich, Basil Hosmer, Bram Wasti, Liangzhen Lai, Bilge Acun, Ahmed Aly, Beidi Chen, Carole-Jean Wu, Ahmed Roman, Nas Mahmoud, Saurabh Agarwal",
    "title": "LayerSkip: Enabling Early Exit Inference and Self-Speculative Decoding",
    "paper_url": "https://ai.meta.com/research/publications/layerskip-enabling-early-exit-inference-and-self-speculative-decoding/",
    "pdf_link": "https://arxiv.org/pdf/2404.16710",
    "abstract": "We present LayerSkip, an end-to-end solution to speed-up inference of large language models (LLMs). First, during training we apply layer dropout, with low dropout rates for earlier layers and higher dropout rates for later layers, and an early exit loss where all transformer layers share the same exit. Second, during inference, we show that this training recipe increases the accuracy of early exit at earlier layers, without adding any auxiliary layers or modules to the model. Third, we present a novel self-speculative decoding solution where we exit at early layers and verify and correct with remaining layers of the model. Our proposed self-speculative decoding approach has less memory footprint than other speculative decoding approaches and benefits from shared compute and activations of the draft and verification stages. We run experiments on different Llama model sizes on different types of training: pretraining from scratch, continual pretraining, finetuning on specific data domain, and finetuning on specific task. We implement our inference solution and show speedups of up to 2.16x on summarization for CNN/DM documents, 1.82x on coding, and 2.0x on TOPv2 semantic parsing task. We open source our code at https://github.com/facebookresearch/LayerSkip."
  },
  {
    "paper_id": 71,
    "authors": "Ava Spataru, Eric Hambro, Lena Voita, Nicola Cancedda",
    "title": "Know When To Stop: A Study of Semantic Drift in Text Generation",
    "paper_url": "https://ai.meta.com/research/publications/know-when-to-stop-a-study-of-semantic-drift-in-text-generation/",
    "pdf_link": "https://arxiv.org/pdf/2404.05411",
    "abstract": "In this work, we explicitly show that modern LLMs tend to generate correct facts first, then \"drift away\" and generate incorrect facts later: this was occasionally observed but never properly measured. We develop a semantic drift score that measures the degree of separation between correct and incorrect facts in generated texts and confirm our hypothesis when generating Wikipedia-style biographies. This correct-then-incorrect generation pattern suggests that factual accuracy can be improved by knowing when to stop generation. Therefore, we explore the trade-off between information quantity and factual accuracy for several early stopping methods and manage to improve factuality by a large margin. We further show that reranking with semantic similarity can further improve these results, both compared to the baseline and when combined with early stopping. Finally, we try calling external API to bring the model back to the right generation path, but do not get positive results. Overall, our methods generalize and can be applied to any long-form text generation to produce more reliable information, by balancing trade-offs between factual accuracy, information quantity and computational cost."
  },
  {
    "paper_id": 72,
    "authors": "Iris Huijben, Matthijs Douze, Matthew Muckley, Ruud van Sloun, Jakob Verbeek",
    "title": "Residual Quantization with Implicit Neural Codebooks",
    "paper_url": "https://ai.meta.com/research/publications/residual-quantization-with-implicit-neural-codebooks/",
    "pdf_link": "https://arxiv.org/pdf/2401.14732",
    "abstract": "Vector quantization is a fundamental operation for data compression and vector search. To obtain high accuracy, multi-codebook methods  represent each vector using codewords across several codebooks. Residual quantization (RQ) is one such method, which iteratively quantizes the error of the previous step. While the error distribution is dependent on previously-selected codewords, this dependency is not accounted for in conventional RQ as it uses a fixed codebook per quantization step. In this paper, we propose QINCo, a neural RQ variant that constructs specialized codebooks per step that depend on the approximation of the vector from previous steps. Experiments show that QINCo outperforms state-of-the-art methods by a large margin on several datasets and code sizes. For example, QINCo achieves better nearest-neighbor search accuracy using 12-byte codes than the state-of-the-art UNQ using 16 bytes on the BigANN1M and Deep1M datasets."
  },
  {
    "paper_id": 73,
    "authors": "Carole-Jean Wu, Bilge Acun, Ramya Raghavendra, Kim Hazelwood",
    "title": "Beyond Efficiency: Scaling AI Sustainably",
    "paper_url": "https://ai.meta.com/research/publications/beyond-efficiency-scaling-ai-sustainably/",
    "pdf_link": "https://arxiv.org/pdf/2406.05303",
    "abstract": "Barroso's seminal contributions in energy- proportional warehouse-scale computing launched an era where modern datacenters have become more energy efficient and cost effective than ever before. At the same time, modern AI applications have driven ever-increasing demands in computing, highlighting the importance of optimizing efficiency across the entire deep learning model development cycle. This paper characterizes the carbon impact of AI, including both operational carbon emissions from training and inference as well as embodied carbon emissions from datacenter construction and hardware manufacturing. We highlight key efficiency optimization opportunities for cutting-edge AI technologies, from deep learning recommendation models to multi-modal generative AI tasks. To scale AI sustainably, we must also go beyond efficiency and optimize across the life cycle of computing infrastructures, from hardware manufacturing to datacenter operations and end-of-life processing for the hardware."
  },
  {
    "paper_id": 74,
    "authors": "Robin San Romin, Pierre Fernandez, Hady Elsahar, Alexandre Deffosez, Teddy Furon, Tuan Tran",
    "title": "Proactive Detection of Voice Cloning with Localized Watermarking",
    "paper_url": "https://ai.meta.com/research/publications/proactive-detection-of-voice-cloning-with-localized-watermarking/",
    "pdf_link": "https://arxiv.org/pdf/2401.17264",
    "abstract": "In the rapidly evolving field of speech generative models, there is a pressing need to ensure audio authenticity against the risks of voice cloning. We present AudioSeal, the first audio watermarking technique designed specifically for localized detection of AI-generated speech. AudioSeal employs a generator / detector architecture trained jointly with a localization loss to enable localized watermark detection up to the sample level, and a novel perceptual loss inspired by auditory masking, that enables AudioSeal to achieve better imperceptibility. AudioSeal achieves state-of-the-art performance in terms of robustness to real life audio manipulations and imperceptibility based on automatic and human evaluation metrics. Additionally, AudioSeal is designed with a fast, single-pass detector, that significantly surpasses existing models in speed, achieving detection up to two orders of magnitude faster, making it ideal for large-scale and real-time applications. Code is available at https://github.com/facebookresearch/audioseal"
  },
  {
    "paper_id": 75,
    "authors": "Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, Christian Rupprecht, Daniel Cramers, Peter Vajda, Jialiang Wang",
    "title": "Cache Me if You Can: Accelerating Diffusion Models through Block Caching",
    "paper_url": "https://ai.meta.com/research/publications/cache-me-if-you-can-accelerating-diffusion-models-through-block-caching/",
    "pdf_link": "https://arxiv.org/pdf/2312.03209",
    "abstract": "Diffusion models have recently revolutionized the field of image synthesis due to their ability to generate photorealistic images. However, one of the major drawbacks of diffusion models is that the image generation process is costly. A large image-to-image network has to be applied many times to iteratively refine an image from random noise. While many recent works propose techniques to reduce the number of required steps, they generally treat the underlying denoising network as a black box. In this work, we investigate the behavior of the layers within the network and find that 1) the layers' output changes smoothly over time, 2) the layers show distinct patterns of change, and 3) the change from step to step is often very small. We hypothesize that many layer computations in the denoising network are redundant. Leveraging this, we introduce block caching, in which we reuse outputs from layer blocks of previous steps to speed up inference. Furthermore, we propose a technique to automatically determine caching schedules based on each block's changes over timesteps. In our experiments, we show through FID, human evaluation and qualitative analysis that Block Caching allows to generate images with higher visual quality at the same computational cost. We demonstrate this for different state-of-the-art models (LDM and EMU) and solvers (DDIM and DPM)."
  },
  {
    "paper_id": 76,
    "authors": "Florian Bordes, Richard Pang, Anurag Ajay, Alexander C. Li, Adrien Bardes, Suzanne Petryk, Oscar Mañas, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman, Mark Ibrahim, Melissa Hall, Yunyang Xiong, Jonathan Lebensold, Candace Ross, Srihari Jayakumar, Chuan Guo, Diane Bouchacourt, Haider Al-Tahan, Karthik Padthe, Vasu Sharma, Hu Xu, Ellen Tan, Megan Richards, Samuel Lavoie, Pietro Astolfi, Reyhane Askari, Jun Chen, Kushal Tirumala, Rim Assouel, Mazda Moayeri, Arjang Talattof, Kamalika Chaudhuri, Zechun Liu, Xilun Chen, Quentin Garrido, Karen Ullrich, Aishwarya Agrawal, Kate Saenko, Asli Celikyilmaz, Vikas Chandra",
    "title": "An Introduction to Vision-Language Modeling",
    "paper_url": "https://ai.meta.com/research/publications/an-introduction-to-vision-language-modeling/",
    "pdf_link": "https://arxiv.org/pdf/2405.17247",
    "abstract": "Following the recent popularity of Large Language Models (LLMs), several attempts have been made to extend them to the visual domain. From having a visual assistant that could guide us through unfamiliar environments to generative models that produce images using only a high-level text description, the vision-language model (VLM) applications will significantly impact our relationship with technology. However, there are many challenges that need to be addressed to improve the reliability of those models. While language is discrete, vision evolves in a much higher dimensional space in which concepts cannot always be easily discretized. To better understand the mechanics behind mapping vision to language, we present this introduction to VLMs which we hope will help anyone who would like to enter the field. First, we introduce what VLMs are, how they work, and how to train them. Then, we present and discuss approaches to evaluate VLMs. Although this work primarily focuses on mapping images to language, we also discuss extending VLMs to videos."
  },
  {
    "paper_id": 77,
    "authors": "Saurabh Agarwal, Bilge Acun, Basil Hosmer, Mostafa Elhoushi, Yejin Lee, Carole-Jean Wu",
    "title": "CHAI: Clustered Head Attention for Efficient LLM Inference",
    "paper_url": "https://ai.meta.com/research/publications/chai-clustered-head-attention-for-efficient-llm-inference/",
    "pdf_link": "https://arxiv.org/pdf/2403.08058",
    "abstract": "Large Language Models (LLMs) with hundreds of billions of parameters have transformed the field of machine learning. However, serving these models at inference time is both compute and memory intensive, where a single request can require multiple GPUs and tens of Gigabytes of memory. Multi-Head Attention is one of the key components of LLMs, which can account for over 50% of LLMs memory and compute requirement. We observe that there is a high amount of redundancy across heads on which tokens they pay attention to. Based on this insight, we propose Clustered Head Attention ( CHAI ). CHAI combines heads with a high amount of correlation for self-attention at runtime, thus reducing both memory and compute. In our experiments, we show that CHAI is able to reduce the memory requirements for storing K,V cache by up to 21.4% and inference time latency by up to 1.73× without any fine-tuning required. CHAI achieves this with a maximum 3.2% deviation in accuracy across 3 different models (i.e. OPT-66B, LLAMA-7B, LLAMA-33B) and 5 different evaluation datasets."
  },
  {
    "paper_id": 78,
    "authors": "Zhe Liu",
    "title": "DOC-RAG: ASR Language Model Personalization with Domain-Distributed Co-occurrence Retrieval Augmentation",
    "paper_url": "https://ai.meta.com/research/publications/doc-rag-asr-language-model-personalization-with-domain-distributed-co-occurrence-retrieval-augmentation/",
    "pdf_link": "https://aclanthology.org/2024.lrec-main.457.pdf",
    "abstract": "We propose DOC-RAG - Domain-distributed Co-occurrence Retrieval Augmentation for ASR language model personalization aiming to improve the automatic speech recognition of rare word patterns in unseen domains. Our approach involves contrastively training a document retrieval module to rank external knowledge domains based on their semantic similarity with respect to the input query. We further use n-gram co-occurrence distribution to recognize rare word patterns associated with specific domains and we aggregate the next word probability distribution based on the relative importance of different domains. Extensive experiments on three user-specific speech-to-text tasks for meetings, TED talks, and financial earnings calls show that DOC-RAG significantly outperforms strong baselines with an 8-15% improvement in terms of perplexity and a 4-7% reduction in terms of Word Error Rates in various settings."
  },
  {
    "paper_id": 79,
    "authors": "Hwanwoo Kim, Xin Zhang, Jiwei Zhao, Qinglong Tian",
    "title": "ReTaSA: A Nonparametric Functional Estimation Approach for Addressing Continuous Target Shift",
    "paper_url": "https://ai.meta.com/research/publications/retasa-a-nonparametric-functional-estimation-approach-for-addressing-continuous-target-shift/",
    "pdf_link": "https://arxiv.org/pdf/2401.16410",
    "abstract": "The presence of distribution shifts poses a significant challenge for deploying modern machine learning models in real-world applications. This work focuses on the target shift problem in a regression setting (Zhang et al., 2013; Nguyen et al., 2016). More specifically, the target variable y (also known as the response variable), which is continuous, has different marginal distributions in the training source and testing domain, while the conditional distribution of features x given y remains the same. While most literature focuses on classification tasks with finite target space, the regression problem has an infinite dimensional target space, which makes many of the existing methods inapplicable. In this work, we show that the continuous target shift problem can be addressed by estimating the importance weight function from an ill-posed integral equation. We propose a nonparametric regularized approach named ReTaSA to solve the ill-posed integral equation and provide theoretical justification for the estimated importance weight function. The effectiveness of the proposed method has been demonstrated with extensive numerical studies on synthetic and real-world datasets."
  },
  {
    "paper_id": 80,
    "authors": "Gregoire Mialon, Yann LeCun, Thomas Scialom, Clémentine Fourrier, Thomas Wolf",
    "title": "GAIA: a benchmark for general AI assistants",
    "paper_url": "https://ai.meta.com/research/publications/gaia-a-benchmark-for-general-ai-assistants/",
    "pdf_link": "https://arxiv.org/pdf/2311.12983",
    "abstract": "We introduce GAIA, a benchmark for General AI Assistants that, if solved, would represent a milestone in AI research. GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency. GAIA questions are conceptually simple for humans yet challenging for most advanced AIs: we show that human respondents obtain 92% vs. 15% for GPT-4 equipped with plugins. This notable performance disparity contrasts with the recent trend of LLMs outperforming humans on tasks requiring professional skills in e.g. law or chemistry. GAIA’s philosophy departs from the current trend in AI benchmarks suggesting to target tasks that are ever more difficult for humans. We posit that the advent of Artificial General Intelligence (AGI) hinges on a system’s capability to exhibit similar robustness as the average human does on such questions. Using GAIA’s methodology, we devise 466 questions and their answer. We release our questions while retaining answers to 300 of them to power a leader-board accessible at https://huggingface.co/gaia-benchmark."
  },
  {
    "paper_id": 81,
    "authors": "Haoyue Tang, Tian Xie",
    "title": "Solving General Noisy Inverse Problem via Posterior Sampling: A Policy Gradient Viewpoint",
    "paper_url": "https://ai.meta.com/research/publications/solving-general-noisy-inverse-problem-via-posterior-sampling-a-policy-gradient-viewpoint/",
    "pdf_link": "https://arxiv.org/pdf/2403.10585",
    "abstract": "Solving image inverse problems (e.g., super- resolution and inpainting) requires generat- ing a high fidelity image that matches the given input (the low-resolution image or the masked image). By using the input image as guidance, we can leverage a pretrained diffu- sion generative model to solve a wide range of image inverse tasks without task specific model fine-tuning. To precisely estimate the guidance score function of the input image, we propose Diffusion Policy Gradient (DPG), a tractable computation method by viewing the intermediate noisy images as policies and the target image as the states selected by the policy. Experiments show that our method is robust to both Gaussian and Poisson noise degradation on multiple linear and non-linear inverse tasks, resulting into a higher image restoration quality on FFHQ, ImageNet and LSUN datasets."
  },
  {
    "paper_id": 82,
    "authors": "Ben Newman, Christopher Paxton, Kris Kitani, Henny Admoni",
    "title": "Bootstrapping Linear Models for Fast Online Adaptation in Human-Agent Collaboration",
    "paper_url": "https://ai.meta.com/research/publications/bootstrapping-linear-models-for-fast-online-adaptation-in-human-agent-collaboration/",
    "pdf_link": "https://arxiv.org/pdf/2404.10733",
    "abstract": "Agents that assist people need to have well-initialized policies that can adapt quickly to align with their partners' reward functions. Initializing policies to maximize performance with unknown partners can be achieved by bootstrapping nonlinear models using imitation learning over large, offline datasets. Such policies can require prohibitive computation to fine-tune in-situ and therefore may miss critical run-time information about a partner's reward function as expressed through their immediate behavior. In contrast, online logistic regression using low-capacity models performs rapid inference and fine-tuning updates and thus can make effective use of immediate in-task behavior for reward function alignment. However, these low-capacity models cannot be bootstrapped as effectively by offline datasets and thus have poor initializations. We propose BLR-HAC, Bootstrapped Logistic Regression for Human Agent Collaboration, which bootstraps large nonlinear models to learn the parameters of a low-capacity model which then uses online logistic regression for updates during collaboration. We test BLR-HAC in a simulated surface rearrangement task and demonstrate that it achieves higher zero-shot accuracy than shallow methods and takes far less computation to adapt online while still achieving similar performance to fine-tuned, large nonlinear models. For code, please see our project page https://sites.google.com/view/blr-hac."
  },
  {
    "paper_id": 83,
    "authors": "Mikayel Samvelyan, Minqi Jiang, Davide Paglieri, Jack Parker-Holder, Tim Rocktäschel",
    "title": "Multi-Agent Diagnostics for Robustness via Illuminated Diversity",
    "paper_url": "https://ai.meta.com/research/publications/multi-agent-diagnostics-for-robustness-via-illuminated-diversity/",
    "pdf_link": "https://arxiv.org/pdf/2401.13460",
    "abstract": "In the rapidly advancing field of multi-agent systems, ensuring robustness in unfamiliar and adversarial settings is crucial. Notwithstanding their outstanding performance in familiar environments, these systems often falter in new situations due to overfitting during the training phase. This is especially pronounced in settings where both cooperative and competitive behaviours are present, encapsulating a dual nature of overfitting and generalisation challenges. To address this issue, we present Multi-Agent Diagnostics for Robustness via Illuminated Diversity (MADRID), a novel approach for generating diverse adversarial scenarios that expose strategic vulnerabilities in pre-trained multi-agent policies. Leveraging the concepts from open-ended learning, MADRID navigates the vast space of adversarial settings, employing a target policy's regret to gauge the vulnerabilities of these settings. We evaluate the effectiveness of MADRID on the 11vs11 version of Google Research Football, one of the most complex environments for multi-agent reinforcement learning. Specifically, we employ MADRID for generating a diverse array of adversarial settings for TiZero, the state-of-the-art approach which \"masters\" the game through 45 days of training on a large-scale distributed infrastructure. We expose key shortcomings in TiZero's tactical decision-making, underlining the crucial importance of rigorous evaluation in multi-agent systems."
  },
  {
    "paper_id": 84,
    "authors": "Jamie Tolan, Eric Yang, Ben Nosarzewski, Guillaume Couairon, Huy V. Vo, John Brandt, Justine Spore, Sayantan Majumdar, Daniel Haziza, Janaki Vamaraju, Theo Moutakanni, Piotr Bojanowski, Tracy Johns, Brian White, Tobias Tiecke, Camille Couprie, Edward Saenz",
    "title": "Very high resolution canopy height maps from RGB imagery using self-supervised vision transformer and convolutional decoder trained on Aerial Lidar",
    "paper_url": "https://ai.meta.com/research/publications/very-high-resolution-canopy-height-maps-from-rgb-imagery-using-self-supervised-vision-transformer-and-convolutional-decoder-trained-on-aerial-lidar/",
    "pdf_link": "https://arxiv.org/pdf/2304.07213",
    "abstract": "Vegetation structure mapping is critical for understanding the global carbon cycle and monitoring nature-based approaches to climate adaptation and mitigation. Repeated measurements of these data allow for the observation of deforestation or degradation of existing forests, natural forest regeneration, and the implementation of sustainable agricultural practices like agroforestry. Assessments of tree canopy height and crown projected area at a high spatial resolution are also important for monitoring carbon fluxes and assessing tree-based land uses, since forest structures can be highly spatially heterogeneous, especially in agroforestry systems. Very high resolution satellite imagery (less than one meter (1 m) Ground Sample Distance) makes it possible to extract information at the tree level while allowing monitoring at a very large scale. This paper presents the first high-resolution canopy height map concurrently produced for multiple sub-national jurisdictions. Specifically, we produce very high resolution canopy height maps for the states of California and São Paulo, a significant improvement in resolution over the ten meter (10 m) resolution of previous Sentinel / GEDI based worldwide maps of canopy height. The maps are generated by the extraction of features from a self-supervised model trained on Maxar imagery from 2017 to 2020, and the training of a dense prediction decoder against aerial lidar maps. We also introduce a post-processing step using a convolutional network trained on GEDI observations. We evaluate the proposed maps with set-aside validation lidar data as well as by comparing with other remotely sensed maps and field-collected data, and find our model produces an average Mean Absolute Error (MAE) of 2.8 m and Mean Error (ME) of 0.6 m."
  },
  {
    "paper_id": 85,
    "authors": "Sachit Menon, Ishan Misra, Rohit Girdhar",
    "title": "Generating Illustrated Instructions",
    "paper_url": "https://ai.meta.com/research/publications/generating-illustrated-instructions/",
    "pdf_link": "https://arxiv.org/pdf/2312.04552",
    "abstract": "We introduce the new task of generating Illustrated Instructions, i.e., visual instructions customized to a user's needs. We identify desiderata unique to this task, and formalize it through a suite of automatic and human evaluation metrics, designed to measure the validity, consistency, and efficacy of the generations. We combine the power of large language models (LLMs) together with strong text-to-image generation diffusion models to propose a simple approach called StackedDiffusion, which generates such illustrated instructions given text as input. The resulting model strongly outperforms baseline approaches and state-of-the-art multimodal LLMs; and in 30% of cases, users even prefer it to human-generated articles. Most notably, it enables various new and exciting applications far beyond what static articles on the web can provide, such as personalized instructions complete with intermediate steps and pictures in response to a user's individual situation."
  },
  {
    "paper_id": 86,
    "authors": "Vasu Sharma *, Karthik Padthe *, Newsha Ardalani, Kushal Tirumala, Russ Howes, Hu Xu, Bernie Huang, Daniel Li (FAIR), Armen Aghajanyan, Gargi Ghosh, Luke Zettlemoyer",
    "title": "Text Quality-Based Pruning for Efficient Training of Language Models",
    "paper_url": "https://ai.meta.com/research/publications/text-quality-based-pruning-for-efficient-training-of-language-models/",
    "pdf_link": "https://arxiv.org/pdf/2405.01582",
    "abstract": "In recent times training Language Models (LMs) have relied on computationally heavy training over massive\ndatasets which makes this training process extremely laborious. In this paper we propose a novel method for\nnumerically evaluating text quality in large unlabelled NLP datasets in a model agnostic manner to assign the text\ninstances a \"quality score\". By proposing the text quality metric, the paper establishes a framework to identify\nand eliminate low-quality text instances, leading to improved training efficiency for LM models. Experimental\nresults over multiple models and datasets demonstrate the efficacy of this approach, showcasing substantial\ngains in training effectiveness and highlighting the potential for resource-efficient LM training. For example, we\nobserve an absolute accuracy improvement of 0.9% averaged over 14 downstream evaluation tasks for multiple LM\nmodels while using 40% lesser data and training 42% faster when training on the OpenWebText dataset and 0.8%\naverage absolute accuracy improvement while using 20% lesser data and training 21% faster on the Wikipedia dataset."
  },
  {
    "paper_id": 87,
    "authors": "GenAI Cybersec Team, Manish Bhatt, Sahana Chennabasappa, Yue Li, Cyrus Nikolaidis, Daniel Song, Shengye Wan, Faizan Ahmad, Cornelius Aschermann, Yaohui Chen, Dhaval Kapil, David Molnar, Spencer Whitman, Joshua Saxe",
    "title": "CYBERSECEVAL 2: A Wide-Ranging Cybersecurity Evaluation Suite for Large Language Models",
    "paper_url": "https://ai.meta.com/research/publications/cyberseceval-2-a-wide-ranging-cybersecurity-evaluation-suite-for-large-language-models/",
    "pdf_link": "https://arxiv.org/pdf/2404.13161",
    "abstract": "Large language models (LLMs) introduce new security risks, but there are few comprehensive evaluation suites to measure and reduce these risks. We present CYBERSECEVAL 2, a novel benchmark to quantify LLM security risks and capabilities. We introduce two new areas for testing: prompt injection and code interpreter abuse. We evaluated multiple state of the art (SOTA) LLMs, including GPT-4, Mistral, Meta Llama 3 70B-Instruct, and Code Llama. Our results show conditioning away risk of attack remains an unsolved problem; for example, all tested models showed between 25% and 50% successful prompt injection tests. Our code is open source and can be used to evaluate other LLMs.\nWe further introduce the safety-utility tradeoff : conditioning an LLM to reject unsafe prompts can cause the LLM to falsely reject answering benign prompts, which lowers utility. We propose quantifying this tradeoff using False Refusal Rate (FRR). As an illustration, we introduce a novel test set to quantify FRR for cyberattack helpfulness risk. We find many LLMs able to successfully comply with “borderline” benign requests while still rejecting most unsafe requests.\nFinally, we quantify the utility of LLMs for automating a core cybersecurity task, that of exploiting software vulnerabilities. This is important because the offensive capabilities of LLMs are of intense interest; we quantify this by creating novel test sets for four representative problems. We find that models with coding capabilities perform better than those without, but that further work is needed for LLMs to become proficient at exploit generation. Our code is open source and can be used to evaluate other LLMs."
  },
  {
    "paper_id": 88,
    "authors": "Jonas Kohler, Albert Pumarola, Edgar Schoenfeld, Artsiom Sanakoyeu, Roshan Sumbaly, Peter Vajda, Ali Thabet",
    "title": "Imagine Flash: Accelerating Emu Diffusion Models with Backward Distillation",
    "paper_url": "https://ai.meta.com/research/publications/imagine-flash-accelerating-emu-diffusion-models-with-backward-distillation/",
    "pdf_link": "https://arxiv.org/pdf/2405.05224",
    "abstract": "Diffusion models are a powerful generative framework, but come with expensive inference. Existing acceleration methods often compromise image quality or fail under complex conditioning when operating in an extremely low-step regime. In this work, we propose a novel distillation framework tailored to enable high-fidelity, diverse sample generation using just one to three steps. Our approach comprises three key components: (i) Backward Distillation, which mitigates training-inference discrepancies by calibrating the student on its own backward trajectory; (ii) Shifted Reconstruction Loss that dynamically adapts knowledge transfer based on the current time step; and (iii) Noise Correction, an inference time technique that enhances sample quality by addressing singularities in noise prediction. Through extensive experiments, we demonstrate that our method outperforms existing competitors in quantitative metrics and human evaluations. Remarkably, it achieves performance comparable to the teacher model using only three denoising steps, enabling efficient high-quality generation."
  },
  {
    "paper_id": 89,
    "authors": "Yun Wang (Speech), Arthur Hinsvark, Qing He, Shun Zhang, Wonjune Kang",
    "title": "Multi-task Learning for Front-end Text Processing in TTS",
    "paper_url": "https://ai.meta.com/research/publications/multi-task-learning-for-front-end-text-processing-in-tts/",
    "pdf_link": "https://arxiv.org/pdf/2401.06321",
    "abstract": "We propose a multi-task learning (MTL) model for jointly performing three tasks that are commonly solved in a text-to-speech (TTS) front-end: text normalization (TN), part-of-speech (POS) tagging, and homograph disambiguation (HD). Our framework utilizes a tree-like structure with a trunk that learns shared representations, followed by separate task-specific heads. We further incorporate a pre-trained language model to utilize its built-in lexical and contextual knowledge, and study how to best use its embeddings so as to most effectively benefit our multi-task model. Through task-wise ablations, we show that our full model trained on all three tasks achieves the strongest overall performance compared to models trained on individual or sub-combinations of tasks, confirming the advantages of our MTL framework. Finally, we introduce a new HD dataset containing a balanced number of sentences in diverse contexts for a variety of homographs and their pronunciations. We demonstrate that incorporating this dataset into training significantly improves HD performance over only using a commonly used, but imbalanced, pre-existing dataset."
  },
  {
    "paper_id": 90,
    "authors": "Heng-Jui Chang, Ning Dong (AI), Ruslan Mavlyutov, Sravya Popuri, Andy Chung",
    "title": "CoLLD: Contrastive Layer-to-Layer Distillation for Compressing Multilingual Pre-Trained Speech Encoders",
    "paper_url": "https://ai.meta.com/research/publications/colld-contrastive-layer-to-layer-distillation-for-compressing-multilingual-pre-trained-speech-encoders/",
    "pdf_link": "https://arxiv.org/pdf/2309.07707",
    "abstract": "Large-scale self-supervised pre-trained speech encoders outperform conventional approaches in speech recognition and translation tasks.  Due to the high cost of developing these large models, building new encoders for new tasks and deploying them to on-device applications are infeasible.  Prior studies propose model compression methods to address this issue, but those works focus on smaller models and less realistic tasks.  Thus, we propose Contrastive Layer-to-layer Distillation (CoLLD), a novel knowledge distillation method to compress pre-trained speech encoders by leveraging masked prediction and contrastive learning to train student models to copy the behavior of a large teacher model.  CoLLD outperforms prior methods and closes the gap between small and large models on multilingual speech-to-text translation and recognition benchmarks."
  },
  {
    "paper_id": 91,
    "authors": "Suyu Ge, Chunting Zhou, Rui Hou, Madian Khabsa, Yi-Chia Wang, Qifan Wang, Jiawei Han, Yuning Mao",
    "title": "MART: Improving LLM Safety with Multi-round Automatic Red-Teaming",
    "paper_url": "https://ai.meta.com/research/publications/mart-improving-llm-safety-with-multi-round-automatic-red-teaming/",
    "pdf_link": "https://arxiv.org/pdf/2311.07689",
    "abstract": "Red-teaming is a common practice for mitigating unsafe behaviors in Large Language Models (LLMs), which involves thoroughly assessing LLMs to identify potential flaws and addressing them with responsible and accurate responses.\nWhile effective, manual red-teaming is costly, and existing automatic red-teaming typically discovers safety risks without addressing them.\nIn this paper, we propose a Multi-round Automatic Red-Teaming (MART) method, which incorporates both automatic adversarial prompt writing and safe response generation, significantly increasing red-teaming scalability and the safety of the target LLM.\nSpecifically, an adversarial LLM and a target LLM interplay with each other in an iterative manner, where the adversarial LLM aims to generate challenging prompts that elicit unsafe responses from the target LLM, while the target LLM is fine-tuned with safety aligned data on these adversarial prompts.\nIn each round, the adversarial LLM crafts better attacks on the updated target LLM, while the target LLM also improves itself through safety fine-tuning.\nOn adversarial prompt benchmarks, the violation rate of an LLM with limited safety alignment reduces up to 84.7% after 4 rounds of MART, achieving comparable performance to LLMs with extensive adversarial prompt writing.\nNotably, model helpfulness on non-adversarial prompts remains stable throughout iterations, indicating the target LLM maintains strong performance on instruction following."
  },
  {
    "paper_id": 92,
    "authors": "Jonathan Lebensold, Maziar Sanjabi, Pietro Astolfi, Adriana Romero Soriano, Kamalika Chaudhuri, Mike Rabbat, Chuan Guo",
    "title": "DP-RDM: Adapting Diffusion Models to Private Domains Without Fine-Tuning",
    "paper_url": "https://ai.meta.com/research/publications/dp-rdm-adapting-diffusion-models-to-private-domains-without-fine-tuning/",
    "pdf_link": "https://arxiv.org/pdf/2403.14421",
    "abstract": "Text-to-image diffusion models have been shown to suffer from sample-level memorization, possibly reproducing near-perfect replica of images that they are trained on, which may be undesirable. To remedy this issue, we develop the first differentially private (DP) retrieval-augmented generation algorithm that is capable of generating high-quality image samples while providing provable privacy guarantees. Specifically, we assume access to a text-to-image diffusion model trained on a small amount of public data, and design a DP retrieval mechanism to augment the text prompt with samples retrieved from a private retrieval dataset. Our differentially private retrieval-augmented diffusion model (DP-RDM) requires no fine-tuning on the retrieval dataset to adapt to another domain, and can use state-of-the-art generative models to generate high-quality image samples while satisfying rigorous DP guarantees. For instance, when evaluated on MS-COCO, our DP-RDM can generate samples with a privacy budget of ϵ=10, while providing a 3.5 point improvement in FID compared to public-only retrieval for up to 10,000 queries."
  },
  {
    "paper_id": 93,
    "authors": "Francois Porcher, Camille Couprie, Marc Szafraniec, Jakob Verbeek",
    "title": "Better (pseudo-)labels for semi-supervised instance segmentation",
    "paper_url": "https://ai.meta.com/research/publications/better-pseudo-labels-for-semi-supervised-instance-segmentation/",
    "pdf_link": "https://arxiv.org/pdf/2403.11675",
    "abstract": "Despite the  availability of large datasets for tasks like image classification and image-text alignment,  labeled data for more complex  recognition tasks, such as detection and  segmentation, is less abundant. In particular, for  instance segmentation  annotations are time-consuming to produce, and  the distribution of instances is often highly skewed across classes. While semi-supervised teacher-student distillation methods show promise in leveraging vast amounts of unlabeled data, they suffer from miscalibration, resulting in overconfidence in frequently represented classes and underconfidence in rarer ones. Additionally, these methods encounter difficulties in efficiently learning from a limited set of examples. We introduce a dual-strategy to enhance the teacher model's training process, substantially improving the performance on few-shot learning. Secondly, we propose a calibration correction mechanism that that enables the student model to correct the teacher's calibration errors. Using our approach, we observed marked improvements over a state-of-the-art  supervised baseline performance on the LVIS dataset, with an increase of 2.8% in average precision (AP) and  10.3% gain in AP for rare classes."
  },
  {
    "paper_id": 94,
    "authors": "Anas Mahmoud, Mostafa Elhoushi, Amro Abbas, Yu Yang, Newsha Ardalani, Hugh Leather, Ari Morcos",
    "title": "Sieve: Multimodal Dataset Pruning Using Image Captioning Models",
    "paper_url": "https://ai.meta.com/research/publications/sieve-multimodal-dataset-pruning-using-image-captioning-models/",
    "pdf_link": "https://arxiv.org/pdf/2310.02110",
    "abstract": "Vision-Language Models (VLMs) are pretrained on large, diverse, and noisy web-crawled datasets. This underscores the critical need for dataset pruning, as the quality of these datasets is strongly correlated with the performance of VLMs on downstream tasks. Using CLIPScore from a pretrained model to only train models using highly-aligned samples is one of the most successful methods for pruning. We argue that this approach suffers from multiple limitations including: false positives and negatives due to CLIP's pretraining on noisy labels. We propose a pruning signal, Sieve, that employs synthetic captions generated by image-captioning models pretrained on small, diverse, and well-aligned image-text pairs to evaluate the alignment of noisy image-text pairs. To bridge the gap between the limited diversity of generated captions and the high diversity of alternative text (alt-text), we estimate the semantic textual similarity in the embedding space of a language model pretrained on unlabeled text corpus. Using DataComp, a multimodal dataset filtering benchmark, when evaluating on 38 downstream tasks, our pruning approach, surpasses CLIPScore by 2.6% and 1.7% on medium and large scale respectively. In addition, on retrieval tasks, Sieve leads to a significant improvement of 2.7% and 4.5% on medium and large scale respectively. We open source our code at https://github.com/facebookresearch/SIEVE."
  },
  {
    "paper_id": 95,
    "authors": "Patrick Lancaster, Nicklas Hansen, Aravind Rajeswaran, Vikash Kumar",
    "title": "MoDem-V2: Visuo-Motor World Models for Real-World Robot Manipulation",
    "paper_url": "https://ai.meta.com/research/publications/modem-v2-visuo-motor-world-models-for-real-world-robot-manipulation/",
    "pdf_link": "https://arxiv.org/pdf/2309.14236",
    "abstract": "Robotic systems that aspire to operate in uninstrumented real-world environments must perceive the world directly via onboard sensing. Vision-based learning systems aim to eliminate the need for environment instrumentation by building an implicit understanding of the world based on raw pixels, but navigating the contact-rich high-dimensional search space from solely sparse visual reward signals significantly exacerbates the challenge of exploration.  The applicability of such systems is thus typically restricted to simulated or heavily engineered environments since agent exploration in the real-world without the guidance of explicit state estimation and dense rewards can lead to unsafe behavior and safety faults that are catastrophic. In this study, we isolate the root causes behind these limitations to develop a system, called MoDem-V2, capable of learning contact-rich manipulation directly in the uninstrumented real world. Building on the latest algorithmic advancements in model-based reinforcement learning (MBRL), demo-bootstrapping, and effective exploration, MoDem-V2 can acquire contact-rich dexterous manipulation skills directly in the real world. We identify key ingredients for leveraging demonstrations in model learning while respecting real-world safety considerations -- exploration centering, agency handover, and actor-critic ensembles. We empirically demonstrate the contribution of these ingredients in four complex visuo-motor manipulation problems in both simulation and the real world. To the best of our knowledge, our work presents the first successful system for demonstration-augmented visual MBRL trained directly in the real world. Visit https://sites.google.com/view/modem-v2 for videos and more details."
  },
  {
    "paper_id": 96,
    "authors": "Judy Ye, Abhinav Gupta, Kris Kitani, Shubham Tulsiani",
    "title": "G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and Grasp Synthesis",
    "paper_url": "https://ai.meta.com/research/publications/g-hop-generative-hand-object-prior-for-interaction-reconstruction-and-grasp-synthesis/",
    "pdf_link": "https://arxiv.org/pdf/2404.12383",
    "abstract": "We propose G-HOP, a denoising diffusion based generative prior for hand-object interactions that allows modeling both the 3D object and a human hand, conditioned on the object category. To learn a 3D spatial diffusion model that can capture this joint distribution, we represent the human hand via a skeletal distance field to obtain a representation aligned with the (latent) signed distance field for the object. We show that this hand-object prior can then serve as generic guidance to facilitate other tasks like reconstruction from interaction clip and human grasp synthesis. We believe that our model, trained by aggregating several diverse real-world interaction datasets spanning across 155 categories, represents a first approach that allows jointly generating both hand and object. Our empirical evaluations demonstrate the benefit of this joint prior in video-based reconstruction and human grasp synthesis, outperforming current task-specific baselines."
  },
  {
    "paper_id": 97,
    "authors": "Vitoria Barin Pacela, Kartik Ahuja, Simon Lacoste-Julien, Pascal Vincent",
    "title": "On the Identifiability of Quantized Factors",
    "paper_url": "https://ai.meta.com/research/publications/on-the-identifiability-of-quantized-factors/",
    "pdf_link": "https://arxiv.org/pdf/2306.16334",
    "abstract": "Disentanglement aims to recover meaningful latent ground-truth factors from the observed distribution solely, and is formalized through the theory of identifiability. The identifiability of independent latent factors has been proven to be impossible in the unsupervised i.i.d. setting under a general nonlinear map from factors to observations. In this work, however, we demonstrate that it is possible to recover quantized latent factors under a generic nonlinear diffeomorphism. We only assume that the latent factors have independent discontinuities in their density, without requiring the factors to be statistically independent. We introduce this novel form of identifiability, termed quantized factor identifiability, and provide a comprehensive proof of the recovery of the quantized factors."
  },
  {
    "paper_id": 98,
    "authors": "Prajjwal Bhargava, Rohan Chitnis, Alborz Geramifard, Shagun Sodhani, Amy Zhang",
    "title": "When should we prefer Decision Transformers for Offline Reinforcement Learning?",
    "paper_url": "https://ai.meta.com/research/publications/when-should-we-prefer-decision-transformers-for-offline-reinforcement-learning/",
    "pdf_link": "https://arxiv.org/pdf/2305.14550",
    "abstract": "Offline reinforcement learning (RL) allows agents to learn effective, return-maximizing policies from a static dataset. Three popular algorithms for offline RL are Conservative Q-Learning (CQL), Behavior Cloning (BC), and Decision Transformer (DT), from the class of Q-Learning, Imitation Learning, and Sequence Modeling respectively. A key open question is: which algorithm is preferred under what conditions? We study this question empirically by exploring the performance of these algorithms across the commonly used D4RL and Robomimic benchmarks. We design targeted experiments to understand their behavior concerning data suboptimality, task complexity, and stochasticity. Our key findings are: (1) DT requires more data than CQL to learn competitive policies but is more robust; (2) DT is a substantially better choice than both CQL and BC in sparse-reward and low-quality data settings; (3) DT and BC are preferable as task horizon increases, or when data is obtained from human demonstrators; and (4) CQL excels in situations characterized by the combination of high stochasticity and low data quality. We also investigate architectural choices and scaling trends for DT on Atari and D4RL and make design/scaling recommendations. We find that scaling the amount of data for DT by 5x gives a 2.5x average score improvement on Atari."
  },
  {
    "paper_id": 99,
    "authors": "Armen Avetisyan, Chris Xie, Henry Howard-Jenkins, Tsun-Yi Yang, Samir Aroudj, Suvam Patra, Fuyang Zhang, Duncan Frost, Luke Holland, Campbell Orme, Jakob Julian Engel, Edward Miller, Richard Newcombe, Vasileios Balntas",
    "title": "SceneScript: Reconstructing Scenes With An Autoregressive Structured Language Model",
    "paper_url": "https://ai.meta.com/research/publications/scenescript-reconstructing-scenes-with-an-autoregressive-structured-language-model/",
    "pdf_link": "https://arxiv.org/pdf/2403.13064",
    "abstract": "We introduce SceneScript, a method that directly produces\nfull scene models as a sequence of structured language commands using an autoregressive, token-based approach. Our proposed scene representation is inspired by recent successes in transformers & LLMs, and departs from more traditional methods which commonly describe scenes as meshes, voxel grids, point clouds or radiance fields. Our method infers the set of structured language commands directly from encoded visual data using a scene language encoder-decoder architecture. To train SceneScript, we generate and release a large-scale synthetic dataset called Aria Synthetic Environments consisting of 100k high-quality indoor scenes, with photorealistic and ground-truth annotated renders of egocentric scene walkthroughs. Our method gives state-of-the art results in architectural layout estimation, and competitive results in 3D object detection. Lastly, we explore an advantage for SceneScript, which is the ability to readily adapt to new commands via simple additions to the structured language, which we illustrate for tasks such as coarse 3D object part reconstruction."
  },
  {
    "paper_id": 100,
    "authors": "Marlene Careil, Matthew Muckley, Jakob Verbeek, Stephane Lathuiliere",
    "title": "Towards image compression with perfect realism at ultra-low bitrates",
    "paper_url": "https://ai.meta.com/research/publications/towards-image-compression-with-perfect-realism-at-ultra-low-bitrates/",
    "pdf_link": "https://arxiv.org/pdf/2310.10325",
    "abstract": "Image codecs are typically optimized to trade-off bitrate vs. distortion metrics. At low bitrates, this leads to compression artefacts which are easily perceptible, even when training with perceptual or adversarial losses. To improve image quality and remove dependency on the bitrate we propose to decode with iterative diffusion models. We condition the decoding process on a vector-quantized image representation, as well as a global image description to provide additional context. We dub our model `PerCo'' for ``perceptual compression'', and compare it to state-of-the-art codecs at rates from 0.1 down to 0.003 bits per pixel. The latter rate is more than an order of magnitude smaller than those considered in most prior work, compressing a 512x768 Kodak image with less than 153 bytes. Despite this ultra-low bitrate, our approach maintains the ability to reconstruct realistic images. We find that our model leads to reconstructions with state-of-the-art visual quality as measured by FID and KID. As predicted by rate-distortion-perception theory, visual quality is less dependent on the bitrate than previous methods."
  },
  {
    "paper_id": 101,
    "authors": "Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, Yuandong Tian",
    "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection",
    "paper_url": "https://ai.meta.com/research/publications/galore-memory-efficient-llm-training-by-gradient-low-rank-projection/",
    "pdf_link": "https://arxiv.org/pdf/2403.03507",
    "abstract": "Training Large Language Models (LLMs) presents significant memory challenges, predominantly due to the growing size of weights and optimizer states. Common memory-reduction approaches, such as low-rank adaptation (LoRA), add a trainable low-rank matrix to the frozen pre-trained weight in each layer, reducing trainable parameters and optimizer states. However, such approaches typically underperform training with full-rank weights in both pre-training and fine-tuning stages since they limit the parameter search to a low-rank subspace and alter the training dynamics, and further, may require full-rank warm start. In this work, we propose Gradient Low-Rank Projection (GaLore), a training strategy that allows full-parameter learning but is more memory-efficient than common low-rank adaptation methods such as LoRA. Our approach reduces memory usage by up to 65.5% in optimizer states while maintaining both efficiency and performance for pre-training on LLaMA 1B and 7B architectures with C4 dataset with up to 19.7B tokens, and on fine-tuning RoBERTa on GLUE tasks. Our 8-bit GaLore further reduces optimizer memory by up to 82.5% and total training memory by 63.3%, compared to a BF16 baseline. Notably, we demonstrate, for the first time, the feasibility of pre-training a 7B model on consumer GPUs with 24GB memory (e.g., NVIDIA RTX 4090) without model parallel, checkpointing, or offloading strategies."
  },
  {
    "paper_id": 102,
    "authors": "Xavi Puig, Eric Undersander, Andrew Szot, Mikael Dallaire Cote, Jimmy Yang, Ruslan Partsey, Ruta Desai, Alexander William Clegg, Tiffany Min, Vladimír Vondruš, Theo Gervet, Vincent-Pierre Berges, Oleksandr Maksymets, Zsolt Kira, Mrinal Kalakrishnan, Jitendra Malik, Devendra Singh Chaplot, Unnat Jain, Dhruv Batra, Akshara Rai, Roozbeh Mottaghi",
    "title": "Habitat 3.0: A Co-Habitat for Humans, Avatars and Robots",
    "paper_url": "https://ai.meta.com/research/publications/habitat-30-a-co-habitat-for-humans-avatars-and-robots/",
    "pdf_link": "https://arxiv.org/pdf/2310.13724",
    "abstract": "We present Habitat 3.0: a simulation platform for studying collaborative human-robot tasks in home environments. Habitat 3.0 offers contributions across three dimensions: (1) Accurate humanoid simulation: addressing challenges in modeling complex deformable bodies and diversity in appearance and motion, all while ensuring high simulation speed. (2) Human-in-the-loop infrastructure: enabling real human interaction with simulated robots via mouse/keyboard or a VR interface, facilitating evaluation of robot policies with human input. (3) Collaborative tasks: studying two collaborative tasks, Social Navigation and Social Rearrangement. Social Navigation investigates a robot's ability to locate and follow humanoid avatars in unseen environments, whereas Social Rearrangement addresses collaboration between a humanoid and robot while rearranging a scene. These contributions allow us to study end-to-end learned and heuristic baselines for human-robot collaboration in-depth, as well as evaluate them with humans in the loop. Our experiments demonstrate that learned robot policies lead to efficient task completion when collaborating with unseen humanoid agents and human partners that might exhibit behaviors that the robot has not seen before. Additionally, we observe emergent behaviors during collaborative task execution, such as the robot yielding space when obstructing a humanoid agent, thereby allowing the effective completion of the task by the humanoid agent. Furthermore, our experiments using the human-in-the-loop tool demonstrate that our automated evaluation with humanoids can provide an indication of the relative ordering of different policies when evaluated with real human collaborators. Habitat 3.0 unlocks interesting new features in simulators for Embodied AI, and we hope it paves the way for a new frontier of embodied human-AI interaction capabilities."
  },
  {
    "paper_id": 103,
    "authors": "Maxime Wabartha, Joelle Pineau",
    "title": "PIECEWISE LINEAR PARAMETRIZATION OF POLICIES: TOWARDS INTERPRETABLE DEEP REINFORCEMENT LEARNING",
    "paper_url": "https://ai.meta.com/research/publications/piecewise-linear-parametrization-of-policies-towards-interpretable-deep-reinforcement-learning/",
    "pdf_link": "https://openreview.net/pdf?id=hOMVq57Ce0",
    "abstract": "Learning inherently interpretable policies is a central challenge in the path to developing autonomous agents that humans can trust. Linear policies can justify their decisions while interacting in a dynamic environment, but their reduced expressivity prevents them from solving hard tasks. Instead, we argue for the use\nof piecewise-linear policies. We carefully study to what extent they can retain the interpretable properties of linear policies while reaching competitive performance with neural baselines. In particular, we propose the HyperCombinator (HC), a piecewise-linear neural architecture expressing a policy with a controllably small number of sub-policies. Each sub-policy is linear with respect to interpretable features, shedding light on the decision process of the agent without requiring an additional explanation model. We evaluate HC policies in control and navigation\nexperiments, visualize the improved interpretability of the agent and highlight its trade-off with performance. Moreover, we validate that the restricted model class that the HyperCombinator belongs to is compatible with the algorithmic constraints of various reinforcement learning algorithms."
  },
  {
    "paper_id": 104,
    "authors": "Alex Liu, Matt Le, Apoorv Vyas, Bowen Shi, Andros Tjandra, Wei-Ning Hsu",
    "title": "Generative Pre-training for Speech with Flow Matching",
    "paper_url": "https://ai.meta.com/research/publications/generative-pre-training-for-speech-with-flow-matching/",
    "pdf_link": "https://arxiv.org/pdf/2310.16338",
    "abstract": "Generative models have gained more and more attention in recent years for their remarkable success in tasks that required estimating and sampling data distribution to generate high-fidelity synthetic data. In speech, text-to-speech synthesis and neural vocoder are good examples here generative models have shined. While generative models have been applied to different applications in speech, there exists no general-purpose generative model that models speech directly. In this work, we take a step toward this direction by showing a single pre-trained\ngenerative model can be adapted to different downstream tasks with strong performance. Specificall, we pre-trained a generative model, named SpeechFlow, on 60k hours of untranscribed speech with Flow Matching and masked conditions. Experiment results show the pre-trained generative model can be fine-tuned with task-specific data to match or surpass existing expert models on speech enhancement, separation, and synthesis. Our work suggested a foundational model for generation tasks in speech can be built with generative pre-training."
  },
  {
    "paper_id": 105,
    "authors": "Yingyi Ma, Zhe Liu, Ozlem Kalinli",
    "title": "Correction Focused Language Model Training for Speech Recognition",
    "paper_url": "https://ai.meta.com/research/publications/correction-focused-language-model-training-for-speech-recognition/",
    "pdf_link": "https://arxiv.org/pdf/2310.11003",
    "abstract": "Language models (LMs) have been commonly adopted to boost the performance of automatic speech recognition (ASR) particularly in domain adaptation tasks. Conventional way of LM training treats all the words in corpora equally, resulting in suboptimal improvements in ASR performance. In this work, we introduce a novel correction focused LM training approach which aims to prioritize ASR fallible words. The word-level ASR fallibility score, representing the likeli- hood of ASR mis-recognition, is defined and shaped as a prior word distribution to guide the LM training. To enable correction focused training with text-only corpora, large language models (LLMs) are employed as fallibility score predictors and text generators through multi-task fine-tuning. Experimental results for domain adaptation tasks demonstrate the effectiveness of our proposed method. Com- pared with conventional LMs, correction focused training achieves up to relatively 5.5% word error rate (WER) reduction in sufficient text scenarios. In insufficient text scenarios, LM training with LLM- generated text achieves up to relatively 13% WER reduction, while correction focused training further obtains up to relatively 6% WER reduction."
  },
  {
    "paper_id": 106,
    "authors": "Timo Schick, Jane Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom",
    "title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
    "paper_url": "https://ai.meta.com/research/publications/toolformer-language-models-can-teach-themselves-to-use-tools/",
    "pdf_link": "https://arxiv.org/pdf/2302.04761",
    "abstract": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller specialized models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of\nboth worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q&A system, a search engine, a translation\nsystem, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities."
  },
  {
    "paper_id": 107,
    "authors": "Tom Sander, Pierre Fernandez, Alain Durmus, Matthijs Douze, Teddy Furon",
    "title": "Watermarking Makes Language Models Radioactive",
    "paper_url": "https://ai.meta.com/research/publications/watermarking-makes-language-models-radioactive/",
    "pdf_link": "https://arxiv.org/pdf/2402.14904",
    "abstract": "This paper investigates the radioactivity of LLM-generated texts, i.e. whether it is possible to detect that such input was used as training data. Conventional methods like membership inference can carry out this detection with some level of accuracy. We show that watermarked training data leaves traces easier to detect and much more reliable than membership inference. We link the contamination level to the watermark robustness, its proportion in the training set, and the fine-tuning process. We notably demonstrate that training on watermarked synthetic instructions can be detected with high confidence (p-value < 10−5) even when as little as 5% of training text is watermarked. Thus, LLM\nwatermarking, originally designed for detecting machine-generated text, gives the ability to easily identify if the outputs of a watermarked LLM were used to fine-tune another LLM."
  },
  {
    "paper_id": 108,
    "authors": "Danny Deng, Hongkuan Zhou, Hanqing Zeng, Yinglong Xia, Chris Leung (AI), Jianbo Li, Rajgopal Kannan, Viktor Prasanna",
    "title": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph Representation Learning",
    "paper_url": "https://ai.meta.com/research/publications/taser-temporal-adaptive-sampling-for-fast-and-accurate-dynamic-graph-representation-learning/",
    "pdf_link": "https://arxiv.org/pdf/2402.05396",
    "abstract": "Temporal Graph Neural Networks (TGNNs) have demonstrated state-of-the-art performance in various high-impact applications, including fraud detection and content recommendation. Despite the success of TGNNs, they are prone to the prevalent noise found in real-world dynamic graphs like time-deprecated links and skewed interaction distribution. The noise causes two critical issues that significantly compromise the accuracy of TGNNs: (1) models are supervised by inferior interactions, and (2) noisy input induces high variance in the aggregated messages. However, current TGNN denoising techniques do not consider the diverse and dynamic noise pattern of each node. In addition, they also suffer from the excessive mini-batch generation overheads caused by traversing more neighbors. We believe the remedy for fast and accurate TGNNs lies in temporal adaptive sampling. In this work, we propose TASER, the first adaptive sampling method for TGNNs optimized for accuracy, efficiency, and scalability. TASER adapts its mini-batch selection based on training dynamics and temporal neighbor selection based on the contextual, structural, and temporal properties of past interactions. To alleviate the bottleneck in mini-batch generation, TASER implements a pure GPU-based temporal neighbor finder and a dedicated GPU feature cache. We evaluate the performance of TASER using two state-of-the-art backbone TGNNs. On five popular datasets, TASER outperforms the corresponding baselines by an average of 2.3% in Mean Reciprocal Rank (MRR) while achieving an average of 5.1× speedup in training time."
  },
  {
    "paper_id": 109,
    "authors": "Adrien Bardes, Quentin Garrido, Xinlei Chen, Michael Rabbat, Yann LeCun, Mido Assran, Nicolas Ballas, Jean Ponce",
    "title": "Revisiting Feature Prediction for Learning Visual Representations from Video",
    "paper_url": "https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/",
    "pdf_link": "https://arxiv.org/pdf/2404.08471",
    "abstract": "This paper explores feature prediction as a stand-alone objective for unsupervised learning from video and introduces V-JEPA, a collection of vision models trained solely using a feature prediction objective, without the use of pretrained image encoders, text, negative examples, reconstruction, or other sources of supervision. The models are trained on 2 million videos collected from public datasets and are evaluated on downstream image and video tasks. Our results show that learning by predicting video features leads to versatile visual representations that perform well on both motion and appearance-based tasks, without adaption of the model’s parameters; e.g., using a frozen backbone, our largest model, a ViT-H/16 trained only on videos, obtains 81.9% on Kinetics-400, 72.2% on Something-Something-v2, and 77.9% on ImageNet1K."
  },
  {
    "paper_id": 110,
    "authors": "Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, Natalia Neverova, Andrea Vedaldi, Oran Gafni, Filippos Kokkinos",
    "title": "IM-3D: Iterative Multiview Diffusion and Reconstruction for High-Quality 3D Generation",
    "paper_url": "https://ai.meta.com/research/publications/im-3d-iterative-multiview-diffusion-and-reconstruction-for-high-quality-3d-generation/",
    "pdf_link": "https://arxiv.org/pdf/2402.08682",
    "abstract": "Most text-to-3D generators build upon off-the-shelf text-to-image models trained on billions of images. They use variants of Score Distillation Sampling (SDS), which is slow, somewhat unstable, and prone to artifacts. A mitigation is to fine-tune the 2D generator to be multi-view aware, which can help distillation or can be combined with reconstruction networks to output 3D objects directly. In this paper, we further explore the design space of text-to-3D models. We significantly improve multi-view generation by considering video instead of image generators. Combined with a 3D reconstruction algorithm which, by using Gaussian splatting, can optimize a robust image-based loss, we directly produce high-quality 3D outputs from the generated views. Our new method, IM-3D, reduces the number of evaluations of the 2D generator network 10-100x, resulting in a much more efficient pipeline, better quality, fewer geometric inconsistencies, and higher yield of usable 3D assets."
  },
  {
    "paper_id": 111,
    "authors": "Cathy Li, Jana Sotakova, François Charton, Kristin Lauter, Emily Wenger, Evrard Garcelon, Mohamed Mahlou",
    "title": "Salsa Picante: A Machine Learning Attack On LWE with Binary Secrets",
    "paper_url": "https://ai.meta.com/research/publications/salsa-picante-a-machine-learning-attack-on-lwe-with-binary-secrets/",
    "pdf_link": "https://proceedings.neurips.cc/paper_files/paper/2023/file/a75db7d2ee1e4bee8fb819979b0a6cad-Paper-Conference.pdf",
    "abstract": "Learning With Errors (LWE) is a hard math problem underpinning\nmany proposed post-quantum cryptographic (PQC) systems. The\nonly PQC Key Exchange Mechanism (KEM) standardized by NIST [13] is based on module LWE, and current publicly available PQ Homomorphic Encryption (HE) libraries are based on ring LWE [2]. The security of LWE-based PQ cryptosystems is critical, but certain implementation choices could weaken them. One such choice is sparse binary secrets, desirable for PQ HE schemes for efficiency reasons. Prior work Salsa [51] demonstrated a machine learning based attack on LWE with sparse binary secrets in small dimensions (𝑛 ≤ 128) and low Hamming weights (ℎ ≤ 4). However, this attack assumes access to millions of eavesdropped LWE samples and fails\nat higher Hamming weights or dimensions. We present Picante, an enhanced machine learning attack on LWE with sparse binary secrets, which recovers secrets in much larger dimensions (up to 𝑛 = 350) and with larger Hamming weights (roughly 𝑛/10, and up to ℎ = 60 for 𝑛 = 350). We achieve this dramatic improvement via a novel preprocessing step, which allows us to generate training data from a linear number of eavesdropped LWE samples (4𝑛) and changes the distribution of the data to improve transformer training. We also improve the secret recovery methods of Salsa and introduce a novel cross-attention recovery mechanism allowing us to read off the secret directly from the trained models. While Picante does not threaten NIST’s proposed LWE standards, it demonstrates significant improvement over Salsa and could scale further, highlighting the need for future investigation into machine learning attacks on LWE with sparse binary secrets."
  },
  {
    "paper_id": 112,
    "authors": "Remy Sabathier, Niloy Mitra, David Novotny",
    "title": "Animal Avatars: Reconstructing Animatable 3D Animals from Casual Videos",
    "paper_url": "https://ai.meta.com/research/publications/animal-avatars-reconstructing-animatable-3d-animals-from-casual-videos/",
    "pdf_link": "https://scontent-lax3-2.xx.fbcdn.net/v/t39.2365-6/457131838_1554104768542850_5261081335022143517_n.pdf?_nc_cat=107&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=Aa70VeEzlnwQ7kNvgFSedrg&_nc_zt=14&_nc_ht=scontent-lax3-2.xx&_nc_gid=AMX4U1DUzEDwe94qlBYpYRm&oh=00_AYDFKxFovi4HknUHQKXZC-D-GJeFb-NcZaRIZDAJ3X1B2w&oe=67950364",
    "abstract": "We present a method to build animatable dog avatars from monocular videos.\nThis is challenging as animals display a range of (unpredictable) non-rigid movements and have a variety of appearance details (e.g., fur, spots, tails).\nWe develop an approach that links the video frames via a 4D solution that jointly solves for animal's pose variation, and its appearance (in a canonical pose).\nTo this end, we significantly improve the quality of template-based shape fitting by endowing the SMAL parametric model with Continuous Surface Embeddings~(CSE), which brings image-to-mesh reprojection constaints that are denser, and thus stronger, than the previously used sparse semantic keypoint correspondences.\nTo model appearance, we propose a novel implicit duplex-mesh texture that is defined in the canonical pose, but can be deformed using SMAL pose coefficients and later rendered to enforce a photometric compatibility with the input video frames.\nOn the challenging CoP3D and APTv2 datasets, we demonstrate superior results (both in terms of pose estimates and predicted appearance) over existing template-free (RAC) and template-based approaches (BARC, BITE)."
  },
  {
    "paper_id": 113,
    "authors": "Felix Xu, Di Lin, Jianjun Zhao, Jianlang Chen, Lei Ma, Qing Guo, Wei Feng, Xuhong Ren",
    "title": "LRR: Language-Driven Resamplable Continuous Representation against Adversarial Tracking Attacks",
    "paper_url": "https://ai.meta.com/research/publications/lrr-language-driven-resamplable-continuous-representation-against-adversarial-tracking-attacks/",
    "pdf_link": "https://arxiv.org/pdf/2404.06247",
    "abstract": "Visual object tracking plays a critical role in visual-based autonomous systems, as it aims to estimate the position and size of the object of interest within a live video. Despite significant progress made in this field, state-of-the-art (SOTA) trackers often fail when faced with adversarial perturbations in the incoming frames. This can lead to significant robustness and security issues when these trackers are deployed in the real world. To achieve high accuracy on both clean and adversarial data, we propose building a spatial-temporal implicit representation using the semantic text guidance of the object of interest extracted from the language-image model (i.e., CLIP). This novel representation enables us to reconstruct incoming frames to maintain semantics and appearance consistent with the object of interest and its clean counterparts. As a result, our proposed method successfully defends against different SOTA adversarial tracking attacks while maintaining high accuracy on clean data. In particular, our method significantly increases tracking accuracy under adversarial attacks with around 90% relative improvement on UAV123, which is close to the accuracy on clean data."
  },
  {
    "paper_id": 114,
    "authors": "Shuxiao Chen, Qinqing Zheng, Qi Long, Weijie Su",
    "title": "Minimax Estimation for Personalized Federated Learning: An Alternative between FedAvg and Local Training?",
    "paper_url": "https://ai.meta.com/research/publications/minimax-estimation-for-personalized-federated-learning-an-alternative-between-fedavg-and-local-training/",
    "pdf_link": "https://www.jmlr.org/papers/volume24/21-0224/21-0224.pdf",
    "abstract": "A widely recognized difficulty in federated learning arises from the statistical heterogeneity among clients: local datasets often originate from distinct yet not entirely unrelated probability distributions, and personalization is, therefore, necessary to achieve optimal results from each individual's perspective. In this paper, we show how the excess risks of personalized federated learning using a smooth, strongly convex loss depend on data heterogeneity from a minimax point of view, with a focus on the FedAvg algorithm (McMahan et al., 2017) and pure local training (i.e., clients solve empirical risk minimization problems on their local datasets without any communication). Our main result reveals an approximate alternative between these two baseline algorithms for federated learning: the former algorithm is minimax rate optimal over a collection of instances when data heterogeneity is small, whereas the latter is minimax rate optimal when data heterogeneity is large, and the threshold is sharp up to a constant.\n\nAs an implication, our results show that from a worst-case point of view, a dichotomous strategy that makes a choice between the two baseline algorithms is rate-optimal. Another implication is that the popular FedAvg following by local finetuning strategy is also minimax optimal under additional regularity conditions. Our analysis relies on a new notion of algorithmic stability that takes into account the nature of federated learning."
  },
  {
    "paper_id": 115,
    "authors": "Pauline Bennet, Abdourahman Khaireh Walieh, Peter Wiecha, Olivier Teytaud, Antoine Moreau",
    "title": "PyMoosh : a comprehensive numerical toolkit for computing the optical properties of multilayered structures",
    "paper_url": "https://ai.meta.com/research/publications/pymoosh-a-comprehensive-numerical-toolkit-for-computing-the-optical-properties-of-multilayered-structures/",
    "pdf_link": "https://arxiv.org/pdf/2309.00654",
    "abstract": "We present PyMoosh, a Python-based simulation library designed to provide a comprehensive set of numerical tools allowing to compute essentially all optical characteristics of multilayered structures, ranging from reflectance and transmittance to guided modes and photovoltaic efficiency. PyMoosh is designed not just for research purposes, but also for use-cases in education. To this end, we have invested significant effort in ensuring user-friendliness and simplicity of the interface. PyMoosh has been developed in line with the principles of Open Science and taking into account the fact that multilayered structures are increasingly being used as a testing ground for optimization and deep learning approaches. We provide in this paper the theoretical basis at the core of PyMoosh, an overview of its capabilities, as well as a comparison between the different numerical methods implemented in terms of speed and stability. We are convinced such a versatile tool will be useful for the community in many ways."
  },
  {
    "paper_id": 116,
    "authors": "Olivier Teytaud, Abdourahman Khaireh Walieh, Antoine Moreau, Pauline Bennet, Peter Wiecha",
    "title": "A newcomer's guide to deep learning for inverse design in nano-photonics",
    "paper_url": "https://ai.meta.com/research/publications/a-newcomers-guide-to-deep-learning-for-inverse-design-in-nano-photonics/",
    "pdf_link": "https://arxiv.org/pdf/2307.08618",
    "abstract": "Nanophotonic devices manipulate light at sub-wavelength scales, enabling tasks such as light concentration, routing, and filtering. Designing these devices is a challenging task. Traditionally, solving this problem has relied on computationally expensive, iterative methods. In recent years, deep learning techniques have emerged as promising tools for tackling the inverse design of nanophotonic devices. While several review articles have provided an overview of the progress in this rapidly evolving field, there is a need for a comprehensive tutorial that specifically targets newcomers without prior experience in deep learning. Our goal is to address this gap and provide practical guidance for applying deep learning to individual scientific problems. We introduce the fundamental concepts of deep learning and critically discuss the potential benefits it offers for various inverse design problems in nanophotonics. We present a suggested workflow and detailed, practical design guidelines to help newcomers navigate the challenges they may encounter. By following our guide, newcomers can avoid frustrating roadblocks commonly experienced when venturing into deep learning for the first time. In a second part, we explore different iterative and direct deep learning-based techniques for inverse design, and evaluate their respective advantages and limitations. To enhance understanding and facilitate implementation, we supplement the manuscript with detailed Python notebook examples, illustrating each step of the discussed processes. While our tutorial primarily focuses on researchers in (nano-)photonics, it is also relevant for those working with deep learning in other research domains. We aim at providing a solid starting point to empower researchers to leverage the potential of deep learning in their scientific pursuits."
  },
  {
    "paper_id": 117,
    "authors": "Less Wright, Adnan Hoque",
    "title": "Accelerating a Triton Fused Kernel for W4A16 Quantized Inference with SplitK Work Decomposition",
    "paper_url": "https://ai.meta.com/research/publications/accelerating-a-triton-fused-kernel-for-w4a16-quantized-inference-with-splitk-work-decomposition/",
    "pdf_link": "https://arxiv.org/pdf/2402.00025",
    "abstract": "We propose an implementation of an efficient fused matrix multiplication kernel for W4A16 quantized inference, where we perform dequantization and GEMM in a fused kernel using a SplitK work decomposition. \nOur implementation shows improvement for the type of skinny matrix-matrix multiplications found in foundation model inference workloads. In particular, this paper surveys the type of matrix multiplication between a skinny activation matrix and a square weight matrix. Our results show an average of 65\\% speed improvement on A100, and an average of 124\\% speed improvement on H100 (with a peak of 295\\%)  for a range of matrix dimensions including those found in a llama-style model, where m < n = k."
  },
  {
    "paper_id": 118,
    "authors": "Geng Ji, Wentao Jiang, Jiang Li, Fahmid Morshed Fahid, Zhengxing Chen, Yinghua Li, Jun Xiao, Chongxi Bao, Zheqing (Bill) Zhu",
    "title": "Learning to bid and rank together in recommendation systems",
    "paper_url": "https://ai.meta.com/research/publications/learning-to-bid-and-rank-together-in-recommendation-systems/",
    "pdf_link": "https://link.springer.com/content/pdf/10.1007/s10994-023-06444-4.pdf",
    "abstract": "Many Internet applications adopt real-time bidding mechanisms to ensure different services (types of content) are shown to the users through fair competitions. The service offering the highest bid price gets the content slot to present a list of items in its candidate pool. Through user interactions with the recommended items, the service obtains the desired engagement activities. We propose a contextual-bandit framework to jointly optimize the price to bid for the slot and the order to rank its candidates for a given service in this type of recommendation systems. Our method can take as input any feature that describes the user and the candidates, including the outputs of other machine learning models. We train reinforcement learning policies using deep neural networks, and compute top-K Gaussian propensity scores to exclude the variance in the gradients caused by randomness unrelated to the reward. This setup further facilitates us to automatically find accurate reward functions that trade off between budget spending and user engagements. In online A/B experiments on two major services of Facebook Home Feed, Groups You Should Join and Friend Requests, our method statistically significantly boosted the number of groups joined by 14.7%, the number of friend requests accepted by 7.0%, and the number of daily active Facebook users by about 1 million, against strong hand-tuned baselines that have been iterated in production over years."
  }
]