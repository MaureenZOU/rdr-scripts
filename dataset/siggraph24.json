[
  {
    "paper_id": "papers_443",
    "authors": "Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, Shenghua Gao",
    "title": "2D Gaussian Splatting for Geometrically Accurate Radiance Fields",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657428",
    "pdf_link": null,
    "abstract": "3D Gaussian Splatting (3DGS) has recently revolutionized radiance field reconstruction, achieving high quality novel view synthesis and fast rendering speed. However, 3DGS fails to accurately represent surfaces due to the multi-view inconsistent nature of 3D Gaussians. We present 2D Gaussian Splatting (2DGS), a novel approach to model and reconstruct geometrically accurate radiance fields from multi-view images. Our key idea is to collapse the 3D volume into a set of 2D oriented planar Gaussian disks. Unlike 3D Gaussians, 2D Gaussians provide view-consistent geometry while modeling surfaces intrinsically. To accurately recover thin surfaces and achieve stable optimization, we introduce a perspective-accurate 2D splatting process utilizing ray-splat intersection and rasterization. Additionally, we incorporate depth distortion and normal consistency terms to further enhance the quality of the reconstructions. We demonstrate that our differentiable renderer allows for noise-free and detailed geometry reconstruction while maintaining competitive appearance quality, fast training speed, and real-time rendering. Project page at https://surfsplatting.github.io.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_721",
    "authors": "Shengjie Ma, Yanlin Weng, Tianjia Shao, Kun Zhou",
    "title": "3D Gaussian Blendshapes for Head Avatar Animation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657462",
    "pdf_link": null,
    "abstract": "We introduce 3D Gaussian blendshapes for modeling photorealistic head avatars. Taking a monocular video as input, we learn a base head model of neutral expression, along with a group of expression blendshapes, each of which corresponds to a basis expression in classical parametric face models. Both the neutral model and expression blendshapes are represented as 3D Gaussians, which contain a few properties to depict the avatar appearance. The avatar model of an arbitrary expression can be effectively generated by combining the neutral model and expression blendshapes through linear blending of Gaussians with the expression coefficients. High-fidelity head avatar animations can be synthesized in real time using Gaussian splatting. Compared to state-of-the-art methods, our Gaussian blendshape representation better captures high-frequency details exhibited in input video, and achieves superior rendering performance.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_659",
    "authors": "Keyang Ye, Qiming Hou, Kun Zhou",
    "title": "3D Gaussian Splatting With Deferred Reflection",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657456",
    "pdf_link": null,
    "abstract": "The advent of neural and Gaussian-based radiance field methods have achieved great success in the field of novel view synthesis. However, specular reflection remains non-trivial, as the high frequency radiance field is notoriously difficult to fit stably and accurately. We present a deferred shading method to effectively render specular reflection with Gaussian splatting. The key challenge comes from the environment map reflection model, which requires accurate surface normal while simultaneously bottlenecks normal estimation with discontinuous gradients. We leverage the per-pixel reflection gradients generated by deferred shading to bridge the optimization process of neighboring Gaussians, allowing nearly correct normal estimations to gradually propagate and eventually spread over all reflective objects. Our method significantly outperforms state-of-the-art techniques and concurrent work in synthesizing high-quality specular reflection effects, demonstrating a consistent improvement of peak signal-to-noise ratio (PSNR) for both synthetic and real-world scenes, while running at a frame rate almost identical to vanilla Gaussian splatting.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_591",
    "authors": "Emilie Yu, Fanny Chevalier, Karan Singh, Adrien Bousseau",
    "title": "3D-Layers: Bringing Layer-based Color Editing to VR Painting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658183",
    "pdf_link": null,
    "abstract": "The ability to represent artworks as stacks of layers is fundamental to modern graphics design, as it allows artists to easily separate visual elements, edit them in isolation, and blend them to achieve rich visual effects. Despite their ubiquity in 2D painting software, layers have not yet made their way to VR painting, where users paint strokes directly in 3D space by gesturing a 6-degrees-of-freedom controller. But while the concept of a stack of 2D layers was inspired by real-world layers in cell animation, what should 3D layers be? We propose to define 3D-Layers as groups of 3D strokes, and we distinguish the ones that represent 3D geometry from the ones that represent color modifications of the geometry. We call the former substrate layers and the latter appearance layers. Strokes in appearance layers modify the color of the substrate strokes they intersect. Thanks to this distinction, artists can define sequences of color modifications as stacks of appearance layers, and edit each layer independently to finely control the final color of the substrate. We have integrated 3D-Layers into a VR painting application and we evaluate its flexibility and expressiveness by conducting a usability study with experienced VR artists.",
    "scholar_publication": "ACM Transactions on Graphics …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_336",
    "authors": "Changwoon Choi, Jaeah Lee, Jaesik Park, Young Min Kim",
    "title": "3Doodle: Compact Abstraction of Objects With 3D Strokes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658156",
    "pdf_link": null,
    "abstract": "While free-hand sketching has long served as an efficient representation to convey characteristics of an object, they are often subjective, deviating significantly from realistic representations. Moreover, sketches are not consistent for arbitrary viewpoints, making it hard to catch 3D shapes. We propose 3Dooole, generating descriptive and view-consistent sketch images given multi-view images of the target object. Our method is based on the idea that a set of 3D strokes can efficiently represent 3D structural information and render view-consistent 2D sketches. We express 2D sketches as a union of view-independent and view-dependent components. 3D cubic Bézier curves indicate view-independent 3D feature lines, while contours of superquadrics express a smooth outline of the volume of varying viewpoints. Our pipeline directly optimizes the parameters of 3D stroke primitives to minimize perceptual losses in a fully differentiable manner. The resulting sparse set of 3D strokes can be rendered as abstract sketches containing essential 3D characteristic shapes of various objects. We demonstrate that 3Doodle can faithfully express concepts of the original images compared with recent sketch generation approaches.1",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_726",
    "authors": "Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, Baoquan Chen",
    "title": "4D-Rotor Gaussian Splatting: Towards Efficient Novel View Synthesis for Dynamic Scenes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657463",
    "pdf_link": null,
    "abstract": "We consider the problem of novel-view synthesis (NVS) for dynamic scenes. Recent neural approaches have accomplished exceptional NVS results for static 3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior efforts often encode dynamics by learning a canonical space plus implicit or explicit deformation fields, which struggle in challenging scenarios like sudden movements or generating high-fidelity renderings. In this paper, we introduce 4D Gaussian Splatting (4DRotorGS), a novel method that represents dynamic scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D Gaussian Splatting in static scenes [Kerbl et al. 2023]. We model dynamics at each timestamp by temporally slicing the 4D Gaussians, which naturally compose dynamic 3D Gaussians and can be seamlessly projected into images. As an explicit spatial-temporal representation, 4DRotorGS demonstrates powerful capabilities for modeling complicated dynamics and fine details—especially for scenes with abrupt motions. We further implement our temporal slicing and splatting techniques in a highly optimized CUDA acceleration framework, achieving real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and 583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions showcase the superior efficiency and effectiveness of 4DRotorGS, which consistently outperforms existing methods both quantitatively and qualitatively.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_430",
    "authors": "Kaiwen Jiang, Yang Fu, Mukund Varma T, Yash Belhe, Xiaolong Wang, Hao Su, Ravi Ramamoorthi",
    "title": "A Construct-optimize Approach to Sparse View Synthesis Without Camera Pose",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657427",
    "pdf_link": null,
    "abstract": "Novel view synthesis from a sparse set of input images is a challenging problem of great practical interest, especially when camera poses are absent or inaccurate. Direct optimization of camera poses and usage of estimated depths in neural radiance field algorithms usually do not produce good results because of the coupling between poses and depths, and inaccuracies in monocular depth estimation. In this paper, we leverage the recent 3D Gaussian splatting method to develop a novel construct-and-optimize method for sparse view synthesis without camera poses. Specifically, we construct a solution progressively by using monocular depth and projecting pixels back into the 3D world. During construction, we optimize the solution by detecting 2D correspondences between training views and the corresponding rendered images. We develop a unified differentiable pipeline for camera registration and adjustment of both camera poses and depths, followed by back-projection. We also introduce a novel notion of an expected surface in Gaussian splatting, which is critical to our optimization. These steps enable a coarse solution, which can then be low-pass filtered and refined using standard optimization methods. We demonstrate results on the Tanks and Temples and Static Hikes datasets with as few as three widely-spaced views, showing significantly better quality than competing methods, including those with approximate camera pose information. Moreover, our results improve with more views and outperform previous InstantNGP and Gaussian Splatting algorithms even when using half the dataset.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_693",
    "authors": "Zihan Yu, Lifan Wu, Zhiqian Zhou, Shuang Zhao",
    "title": "A Differential Monte Carlo Solver for the Poisson Equation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657460",
    "pdf_link": null,
    "abstract": "The Poisson equation is an important partial differential equation (PDE) with numerous applications in physics, engineering, and computer graphics. Conventional solutions to the Poisson equation require discretizing the domain or its boundary, which can be very expensive for domains with detailed geometries. To overcome this challenge, a family of grid-free Monte Carlo solutions has recently been developed. By utilizing walk-on-sphere (WoS) processes, these techniques are capable of efficiently solving the Poisson equation over complex domains. In this paper, we introduce a general technique that differentiates solutions to the Poisson equation with Dirichlet boundary conditions. Specifically, we devise a new boundary-integral formulation for the derivatives with respect to arbitrary parameters including shapes of the domain. Further, we develop an efficient walk-on-spheres technique based on our new formulation—including a new approach to estimate normal derivatives of the solution field. We demonstrate the effectiveness of our technique over baseline methods using several synthetic examples.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_124",
    "authors": "Shusen Liu, Xiaowei He, Yuzhong Guo, Yue Chang, Wencheng Wang, Shusen Liu",
    "title": "A Dual-Particle Approach for Incompressible SPH Fluids",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3649888",
    "pdf_link": null,
    "abstract": "Tensile instability is one of the major obstacles to particle methods in fluid simulation, which would cause particles to clump in pairs under tension and prevent fluid simulation to generate small-scale thin features. To address this issue, previous particle methods either use a background pressure or a finite difference scheme to alleviate the particle clustering artifacts, yet still fail to produce small-scale thin features in free-surface flows. In this article, we propose a dual-particle approach for simulating incompressible fluids. Our approach involves incorporating supplementary virtual particles designed to capture and store particle pressures. These pressure samples undergo systematic redistribution at each time step, grounded in the initial positions of the fluid particles. By doing so, we effectively reduce tensile instability in standard SPH by narrowing down the unstable regions for particles experiencing tensile stress. As a result, we can accurately simulate free-surface flows with rich small-scale thin features, such as droplets, streamlines, and sheets, as demonstrated by experimental results.",
    "scholar_publication": "ACM Transactions on Graphics, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_610",
    "authors": "Xuan Li, Minchen Li, Xuchen Han, Huamin Wang, Yin Yang, Chenfanfu Jiang",
    "title": "A Dynamic Duo of Finite Elements and Material Points",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657449",
    "pdf_link": null,
    "abstract": "This paper presents a novel method to couple Finite Element Methods (FEM), typically employed for modeling Lagrangian solids such as flesh, cloth, hair, and rigid bodies, with Material Point Methods (MPM), which are well-suited for simulating materials undergoing substantial deformation and topology change, including Newtonian/non-Newtonian fluid, granular materials, and fracturing materials. The challenge of coupling these diverse methods arises from their contrasting computational needs: implicit FEM integration is often favored to enjoy stability and large timesteps, while explicit MPM integration benefits from its allowance for efficient GPU optimization and flexibility of applying different plasticity models, which only allows for moderate timesteps. To bridge this gap, a mixed implicit-explicit time integration (IMEX) approach is proposed, utilizing principles from time splitting for partial differential equations and optimization-based time integrators. This method adopts incremental potential contact (IPC) to define a variational frictional contact model between the two materials, serving as the primary coupling mechanism. Our method enables implicit FEM and explicit MPM to coexist with significantly different timestep sizes while preserving two-way coupling. Experimental results demonstrate the potential of our method as a strong foundation for future exploration and enhancement in the field of multi-material simulation.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_125",
    "authors": "Leticia Mattos Da Silva, Oded Stein, Justin Solomon, Leticia Mattos Da Silva",
    "title": "A Framework for Solving Parabolic Partial Differential Equations on Discrete Domains",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3666087",
    "pdf_link": null,
    "abstract": "We introduce a framework for solving a class of parabolic partial differential equations on triangle mesh surfaces, including the Hamilton-Jacobi equation and the Fokker-Planck equation. PDE in this class often have nonlinear or stiff terms that cannot be resolved with standard methods on curved triangle meshes. To address this challenge, we leverage a splitting integrator combined with a convex optimization step to solve these PDE. Our machinery can be used to compute entropic approximation of optimal transport distances on geometric domains, overcoming the numerical limitations of the state-of-the-art method. In addition, we demonstrate the versatility of our method on a number of linear and nonlinear PDE that appear in diffusion and front propagation tasks in geometry processing.",
    "scholar_publication": "ACM Transactions on Graphics, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_392",
    "authors": "Shlomi Steinberg, Ravi Ramamoorthi, Benedikt Bitterli, Arshiya Mollazainali, Eugene d'Eon, Matt Pharr",
    "title": "A Free-space Diffraction BSDF",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658166",
    "pdf_link": null,
    "abstract": "… to produce free-space diffractions, … diffraction lobes, and, in contrast to any existing method, are able to handle complex, real-world geometry. This work serves to connect free-space …",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1025",
    "authors": "Simon Lucas, Mickaël Ribardière, Romain Pacanowski, Pascal Barla",
    "title": "A Fully-correlated Anisotropic Micrograin BSDF Model",
    "paper_url": "https://hal.science/hal-04567402/",
    "pdf_link": null,
    "abstract": "We introduce an improved version of the micrograin BSDF model [Lucas et al. 2023] for the rendering of anisotropic porous layers. Our approach leverages the properties of micrograins to take into account the correlation between their height and normal, as well as the correlation between the light and view directions. This allows us to derive an exact analytical expression for the Geometrical Attenuation Factor (GAF), summarizing shadowing and masking inside the porous layer. This fully-correlated GAF is then used to define appropriate mixing weights to blend the BSDFs of the porous and base layers. Furthermore, by generalizing the micrograins shape to anisotropy, combined with their fully-correlated GAF, our improved BSDF model produces effects specific to porous layers such as retro-reflection visible on dust layers at grazing angles or height and color correlation that can be found on rusty materials. Finally, we demonstrate very close matches between our BSDF model and light transport simulations realized with explicit instances of micrograins, thus validating our model.",
    "scholar_publication": "ACM Transactions on …, 2024 - hal.science"
  },
  {
    "paper_id": "papers_942",
    "authors": "Nicole Feng, Keenan Crane",
    "title": "A Heat Method for Generalized Signed Distance",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658220",
    "pdf_link": null,
    "abstract": "… a method for approximating the signed distance function (SDF) of geometry corrupted by holes, noise, or self-intersections. The method … of the heat method for geodesic distance, which …",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_367",
    "authors": "Bernhard Kerbl, Andreas Meuleman, Georgios Kopanas, Michael Wimmer, Alexandre Lanvin, George Drettakis",
    "title": "A Hierarchical 3D Gaussian Representation for Real-time Rendering of Very Large Scenes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658160",
    "pdf_link": null,
    "abstract": "Novel view synthesis has seen major advances in recent years, with 3D Gaussian splatting offering an excellent level of visual quality, fast training and real-time rendering. However, the resources needed for training and rendering inevitably limit the size of the captured scenes that can be represented with good visual quality. We introduce a hierarchy of 3D Gaussians that preserves visual quality for very large scenes, while offering an efficient Level-of-Detail (LOD) solution for efficient rendering of distant content with effective level selection and smooth transitions between levels. We introduce a divide-and-conquer approach that allows us to train very large scenes in independent chunks. We consolidate the chunks into a hierarchy that can be optimized to further improve visual quality of Gaussians merged into intermediate nodes. Very large captures typically have sparse coverage of the scene, presenting many challenges to the original 3D Gaussian splatting training method; we adapt and regularize training to account for these issues. We present a complete solution, that enables real-time rendering of very large scenes and can adapt to available resources thanks to our LOD method. We show results for captured scenes with up to tens of thousands of images with a simple and affordable rig, covering trajectories of up to several kilometers and lasting up to one hour.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_453",
    "authors": "Craig Gotsman, Kai Hormann",
    "title": "A Linear Method to Consistently Orient Normals of a 3D Point Cloud",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657429",
    "pdf_link": null,
    "abstract": "Correctly and consistently orienting a set of normal vectors associated with a point cloud sampled from a surface in 3D is a difficult procedure necessary for further downstream processing of sampled 3D geometry, such as surface reconstruction and registration. It is difficult because correct orientation cannot be achieved without global considerations of the entire point cloud. We present an algorithm to orient a given set of normals of a 3D point cloud of size N, whose main computational component is the least-squares solution of an O(N) linear system, mostly sparse, derived from the classical Stokes’ theorem. We show experimentally that our method can successfully orient sets of normals computed locally from point clouds containing a moderate amount of noise, representing also 3D surfaces with non-smooth features (such as corners and edges), in a fraction of the time required by state-of-the-art methods.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_198",
    "authors": "Yushan Han, Yizhou Chen, Carmichael Ong, Jingyu Chen, Jennifer Hicks, Joseph Teran",
    "title": "A Neural Network Model for Efficient Musculoskeletal-driven Skin Deformation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658135",
    "pdf_link": null,
    "abstract": "We present a comprehensive neural network to model the deformation of human soft tissues including muscle, tendon, fat and skin. Our approach provides kinematic and active correctives to linear blend skinning [Magnenat-Thalmann et al. 1989] that enhance the realism of soft tissue deformation at modest computational cost. Our network accounts for deformations induced by changes in the underlying skeletal joint state as well as the active contractile state of relevant muscles. Training is done to approximate quasistatic equilibria produced from physics-based simulation of hyperelastic soft tissues in close contact. We use a layered approach to equilibrium data generation where deformation of muscle is computed first, followed by an inner skin/fascia layer, and lastly a fat layer between the fascia and outer skin. We show that a simple network model which decouples the dependence on skeletal kinematics and muscle activation state can produce compelling behaviors with modest training data burden. Active contraction of muscles is estimated using inverse dynamics where muscle moment arms are accurately predicted using the neural network to model kinematic musculotendon geometry. Results demonstrate the ability to accurately replicate compelling musculoskeletal and skin deformation behaviors over a representative range of motions, including the effects of added weights in body building motions.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_427",
    "authors": "Junqiu Zhu, Christophe Hery, Lukas Bode, Carlos Aliaga, Adrian Jarabo, Ling-Qi Yan, Matt Jen-Yuan Chiang",
    "title": "A Realistic Multi-scale Surface-based Cloth Appearance Model",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657426",
    "pdf_link": null,
    "abstract": "Surface-based cloth appearance models have been rapidly advancing, shifting from detail-less BRDFs to modern per-point shading models with accurate spatially-varying reflection, transmission, and so on. However, the increased complexity has brought about realism-performance trade-offs: from closeup, rendered cloth can be highly inaccurate due to the missing, unaffordable parallax effects; from far away, significant amount of noise will show up since every point can be shaded differently inside a pixel’s footprint. In this paper, we aim at eliminating the trade-off with a realistic multi-scale surface-based cloth appearance model. We propose a comprehensive micro-scale model focusing on correct parallax effects, and a practical meso-scale integration scheme, emphasizing efficiency while losslessly preserving accurate highlights and self-shadowing. We further improve its performance using our novel Clustered Control Variates (CCV) and Summed-Area Table (SAT) integration scheme, and its practicality using an efficient Clustered Principal Component Analysis (C-PCA) compression method. As a result, our multi-scale model achieves a 30 × acceleration compared to the state-of-the-art, is able to represent a variety of realistic cloth appearance, and can be potentially applied in real-time applications.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_881",
    "authors": "Hsueh-Ti Derek Liu, Maneesh Agrawala, Cem Yuksel, Tim Omernick, Vinith Misra, Stefano Corazza, Morgan McGuire, Victor Zordan",
    "title": "A Unified Differentiable Boolean Operator With Fuzzy Logic",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657484",
    "pdf_link": null,
    "abstract": "This paper presents a unified differentiable boolean operator for implicit solid shape modeling using Constructive Solid Geometry (CSG). Traditional CSG relies on min, max operators to perform boolean operations on implicit shapes. But because these boolean operators are discontinuous and discrete in the choice of operations, this makes optimization over the CSG representation challenging. Drawing inspiration from fuzzy logic, we present a unified boolean operator that outputs a continuous function and is differentiable with respect to operator types. This enables optimization of both the primitives and the boolean operations employed in CSG with continuous optimization techniques, such as gradient descent. We further demonstrate that such a continuous boolean operator allows the modeling of both sharp mechanical objects and smooth organic shapes with the same framework. Our proposed boolean operator opens up new possibilities for future research toward fully continuous CSG optimization.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_389",
    "authors": "Ningxiao Tao, Liangwang Ruan, Yitong Deng, Bo Zhu, Bin Wang, Baoquan Chen",
    "title": "A Vortex Particle-on-mesh Method for Soap Film Simulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658165",
    "pdf_link": null,
    "abstract": "This paper introduces a novel physically-based vortex fluid model for films, aimed at accurately simulating cascading vortical structures on deforming thin films. Central to our approach is a novel mechanism decomposing the film's tangential velocity into circulation and dilatation components. These components are then evolved using a hybrid particle-mesh method, enabling the effective reconstruction of three-dimensional tangential velocities and seamlessly integrating surfactant and thickness dynamics into a unified framework. By coupling with its normal component and surface-tension model, our method is particularly adept at depicting complex interactions between in-plane vortices and out-of-plane physical phenomena, such as gravity, surfactant dynamics, and solid boundary, leading to highly realistic simulations of complex thin-film dynamics, achieving an unprecedented level of vortical details and physical realism.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_606",
    "authors": "Mohamad Qadri, Kevin Zhang, Akshay Hinduja, Michael Kaess, Adithya Pediredla, Christopher Metzler",
    "title": "AONeuS: A Neural Rendering Framework for Acoustic-optical Sensor Fusion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657446",
    "pdf_link": null,
    "abstract": "Underwater perception and 3D surface reconstruction are challenging problems with broad applications in construction, security, marine archaeology, and environmental monitoring. Treacherous operating conditions, fragile surroundings, and limited navigation control often dictate that submersibles restrict their range of motion and, thus, the baseline over which they can capture measurements. In the context of 3D scene reconstruction, it is well-known that smaller baselines make reconstruction more challenging. Our work develops a physics-based multimodal acoustic-optical neural surface reconstruction framework (AONeuS) capable of effectively integrating high-resolution RGB measurements with low-resolution depth-resolved imaging sonar measurements. By fusing these complementary modalities, our framework can reconstruct accurate high-resolution 3D surfaces from measurements captured over heavily-restricted baselines. Through extensive simulations and in-lab experiments, we demonstrate that AONeuS dramatically outperforms recent RGB-only and sonar-only inverse-differentiable-rendering–based surface reconstruction methods.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_477",
    "authors": "Daniel Jiménez Navarro, Xi Peng, Yunxiang Zhang, Karol Myszkowski, Hans-Peter Seidel, Qi Sun, Ana Serrano",
    "title": "Accelerating Saccadic Response Through Spatial and Temporal Cross-modal Misalignments",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657432",
    "pdf_link": null,
    "abstract": "Human senses and perception are our mechanisms to explore the external world. In this context, visual saccades –rapid and coordinated eye movements– serve as a primary tool for awareness of our surroundings. Typically, our perception is not limited to visual stimuli alone but is enriched by cross-modal interactions, such as the combination of sight and hearing. In this work, we investigate the temporal and spatial relationship of these interactions, focusing on how auditory cues that precede visual stimuli influence saccadic latency –the time that it takes for the eyes to react and start moving towards a visual target. Our research, conducted within a virtual reality environment, reveals that auditory cues preceding visual information can significantly accelerate saccadic responses, but this effect plateaus beyond certain temporal thresholds. Additionally, while the spatial positioning of visual stimuli influences the speed of these eye movements, as reported in previous research, we find that the location of auditory cues with respect to their corresponding visual stimulus does not have a comparable effect. To validate our findings, we implement two practical applications: first, a basketball training task set in a more realistic environment with complex audiovisual signals, and second, an interactive farm game that explores previously untested values of our key factors. Lastly, we discuss various potential applications where our model could be beneficial.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_894",
    "authors": "Yiwen Ju, Xingyi Du, Qingnan Zhou, Nathan Carr, Tao Ju",
    "title": "Adaptive Grid Generation for Discretizing Implicit Complexes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658215",
    "pdf_link": null,
    "abstract": "We present a method for generating a simplicial (e.g., triangular or tetrahedral) grid to enable adaptive discretization of implicit shapes defined by a vector function. Such shapes, which we call implicit complexes, are generalizations of implicit surfaces and useful for representing non-smooth and non-manifold structures. While adaptive grid generation has been extensively studied for polygonizing implicit surfaces, few methods are designed for implicit complexes. Our method can generate adaptive grids for several implicit complexes, including arrangements of implicit surfaces, CSG shapes, material interfaces, and curve networks. Importantly, our method adapts the grid to the geometry of not only the implicit surfaces but also their lower-dimensional intersections. We demonstrate how our method enables efficient and detail-preserving discretization of non-trivial implicit shapes.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_221",
    "authors": "Masaaki Miki, Toby Mitchell",
    "title": "Alignment Conditions for NURBS-based Design of Mixed Tension-compression Grid Shells",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658142",
    "pdf_link": null,
    "abstract": "In architecture, shapes of surfaces that can withstand gravity with no bending action are considered ideal for shell structures. Those shells have special geometries through which they can stream gravitational force toward the ground via stresses strictly tangent to the surface, making them highly efficient. The process of finding these special forms is called form-finding. Recently, [Miki and Mitchell 2022] presented a method to reliably produce mixed tension-compression continuum shells, a type of shells known to be especially difficult to form-find. The key to this method was to use the concept of the Airy stress function to derive a valid bending-free shell shape by iterating on both the shell shape and the Airy stress function; this turns a problem that is over-constrained in general into a problem with many solutions. In [Miki and Mitchell 2022], it was proposed that the method could also be used to design grid shells by tracing curves on a continuum shell such that the resulting grid has bars that are both bending-free and form flat panels, a property useful for construction of real grid shells made of glass and steel. However, this special type of grid is not guaranteed to exist in general on a mixed-tension compression shell, even when the shell is in bending-free equilibrium [Miki and Mitchell 2023]. Additional conditions must be imposed on the shell shape to guarantee the existence of simultaneously bending-free and conjugate grid directions. The current study resolves the existence issue by adding alignment conditions. We consider several practical curve alignment conditions: alignment with the lines of curvature of the shell, approximate alignment with a bidirectional set of user-prescribed guide curves, and exact alignment with a single direction of user-prescribed guide curves. We report that the variable projection method originally used to solve the form-finding problem in the work of [Miki and Mitchell 2022] can be successfully extended to solve the newly introduced alignment conditions, and conclude with results for several practical design examples. To our knowledge, this is the first method that can take a user-input grid and find a \"nearby\" grid that is both flat-panelled and in bending-free equilibrium for the general case of mixed tension-compression grid shells.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_138",
    "authors": "Xingyu Ni, Ruicheng Wang, Bin Wang, Baoquan Chen",
    "title": "An Induce-on-Boundary Magnetostatic Solver for Grid-based Ferrofluids",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658124",
    "pdf_link": null,
    "abstract": "This paper introduces a novel Induce-on-Boundary (IoB) solver designed to address the magnetostatic governing equations of ferrofluids. The IoB solver is based on a single-layer potential and utilizes only the surface point cloud of the object, offering a lightweight, fast, and accurate solution for calculating magnetic fields. Compared to existing methods, it eliminates the need for complex linear system solvers and maintains minimal computational complexities. Moreover, it can be seamlessly integrated into conventional fluid simulators without compromising boundary conditions. Through extensive theoretical analysis and experiments, we validate both the convergence and scalability of the IoB solver, achieving state-of-the-art performance. Additionally, a straightforward coupling approach is proposed and executed to showcase the solver's effectiveness when integrated into a grid-based fluid simulation pipeline, allowing for realistic simulations of representative ferrofluid instabilities.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_202",
    "authors": "Zheng Gu, Shiyuan Yang, Jing Liao, Jing Huo, Yang Gao",
    "title": "Analogist: Out-of-the-box Visual In-context Learning With Image Diffusion Model",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658136",
    "pdf_link": null,
    "abstract": "Visual In-Context Learning (ICL) has emerged as a promising research area due to its capability to accomplish various tasks with limited example pairs through analogical reasoning. However, training-based visual ICL has limitations in its ability to generalize to unseen tasks and requires the collection of a diverse task dataset. On the other hand, existing methods in the inference-based visual ICL category solely rely on textual prompts, which fail to capture fine-grained contextual information from given examples and can be time-consuming when converting from images to text prompts. To address these challenges, we propose Analogist, a novel inference-based visual ICL approach that exploits both visual and textual prompting techniques using a text-to-image diffusion model pretrained for image inpainting. For visual prompting, we propose a self-attention cloning (SAC) method to guide the fine-grained structural-level analogy between image examples. For textual prompting, we leverage GPT-4V's visual reasoning capability to efficiently generate text prompts and introduce a cross-attention masking (CAM) operation to enhance the accuracy of semantic-level analogy guided by text prompts. Our method is out-of-the-box and does not require fine-tuning or optimization. It is also generic and flexible, enabling a wide range of visual tasks to be performed in an in-context manner. Extensive experiments demonstrate the superiority of our method over existing approaches, both qualitatively and quantitatively. Our project webpage is available at https://analogist2d.github.io.",
    "scholar_publication": "ACM Transactions on Graphics …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_205",
    "authors": "Arjun Teh, Ioannis Gkioulekas, Matthew O'Toole",
    "title": "Aperture-aware Lens Design",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657398",
    "pdf_link": null,
    "abstract": "… Additionally, we consider only geometric optics and focus on sequential lens design: that is, … lens, transmit once through all lens surfaces in order, and reach a sensor plane after the lens…",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_849",
    "authors": "Song Zhang, Daqi Lin, Markus Kettunen, Chris Wyman, Cem Yuksel",
    "title": "Area ReSTIR: Resampling for Real-time Defocus and Antialiasing",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658210",
    "pdf_link": null,
    "abstract": "Recent advancements in spatiotemporal reservoir resampling (ReSTIR) leverage sample reuse from neighbors to efficiently evaluate the path integral. Like rasterization, ReSTIR methods implicitly assume a pinhole camera and evaluate the light arriving at a pixel through a single predetermined subpixel location at a time (e.g., the pixel center). This prevents efficient path reuse in and near pixels with high-frequency details. We introduce Area ReSTIR, extending ReSTIR reservoirs to also integrate each pixel's 4D ray space, including 2D areas on the film and lens. We design novel subpixel-tracking temporal reuse and shift mappings that maximize resampling quality in such regions. This robustifies ReSTIR against high-frequency content, letting us importance sample subpixel and lens coordinates and efficiently render antialiasing and depth of field.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1240",
    "authors": "Yitong Jin, Zhiping Qiu, Yi Shi, Shuangpeng Sun, Chongwu Wang, Donghao Pan, Jiachen Zhao, Zhenghao Liang, Yuan Wang, Xiaobing Li, Feng Yu, Tao Yu, Qionghai Dai",
    "title": "Audio Matters Too! Enhancing Markerless Motion Capture With Audio Signals for String Performance Capture",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658235",
    "pdf_link": null,
    "abstract": "In this paper, we touch on the problem of markerless multi-modal human motion capture especially for string performance capture which involves inherently subtle hand-string contacts and intricate movements. To fulfill this goal, we first collect a dataset, named String Performance Dataset (SPD), featuring cello and violin performances. The dataset includes videos captured from up to 23 different views, audio signals, and detailed 3D motion annotations of the body, hands, instrument, and bow. Moreover, to acquire the detailed motion annotations, we propose an audio-guided multi-modal motion capture framework that explicitly incorporates hand-string contacts detected from the audio signals for solving detailed hand poses. This framework serves as a baseline for string performance capture in a completely markerless manner without imposing any external devices on performers, eliminating the potential of introducing distortion in such delicate movements. We argue that the movements of performers, particularly the sound-producing gestures, contain subtle information often elusive to visual methods but can be inferred and retrieved from audio cues. Consequently, we refine the vision-based motion capture results through our innovative audio-guided approach, simultaneously clarifying the contact relationship between the performer and the instrument, as deduced from the audio. We validate the proposed framework and conduct ablation studies to demonstrate its efficacy. Our results outperform current state-of-the-art vision-based algorithms, underscoring the feasibility of augmenting visual motion capture with audio modality. To the best of our knowledge, SPD is the first dataset for musical instrument performance, covering fine-grained hand motion details in a multi-modal, large-scale collection. It holds significant implications and guidance for string instrument pedagogy, animation, and virtual concerts, as well as for both musical performance analysis and generation. Our code and SPD dataset are available at https://github.com/Yitongishere/string_performance.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_150",
    "authors": "Chen Liu, Weiwei Xu, Yin Yang, Huamin Wang",
    "title": "Automatic Digital Garment Initialization From Sewing Patterns",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658128",
    "pdf_link": null,
    "abstract": "The rapid advancement of digital fashion and generative AI technology calls for an automated approach to transform digital sewing patterns into well-fitted garments on human avatars. When given a sewing pattern with its associated sewing relationships, the primary challenge is to establish an initial arrangement of sewing pieces that is free from folding and intersections. This setup enables a physics-based simulator to seamlessly stitch them into a digital garment, avoiding undesirable local minima. To achieve this, we harness AI classification, heuristics, and numerical optimization. This has led to the development of an innovative hybrid system that minimizes the need for user intervention in the initialization of garment pieces. The seeding process of our system involves the training of a classification network for selecting seed pieces, followed by solving an optimization problem to determine their positions and shapes. Subsequently, an iterative selection-arrangement procedure automates the selection of pattern pieces and employs a phased initialization approach to mitigate local minima associated with numerical optimization. Our experiments confirm the reliability, efficiency, and scalability of our system when handling intricate garments with multiple layers and numerous pieces. According to our findings, 68 percent of garments can be initialized with zero user intervention, while the remaining garments can be easily corrected through user operations.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_842",
    "authors": "Jean-Marc THIERY, Elie Michel, Jiong Chen",
    "title": "Biharmonic Coordinates and Their Derivatives for Triangular 3D Cages",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658208",
    "pdf_link": null,
    "abstract": "As a natural extension to the harmonic coordinates, the biharmonic coordinates have been found superior for planar shape and image manipulation with an enriched deformation space. However, the 3D biharmonic coordinates and their derivatives have remained unexplored. In this work, we derive closed-form expressions for biharmonic coordinates and their derivatives for 3D triangular cages. The core of our derivation lies in computing the closed-form expressions for the integral of the Euclidean distance over a triangle and its derivatives. The derived 3D biharmonic coordinates not only fill a missing component in methods of generalized barycentric coordinates but also pave the way for various interesting applications in practice, including producing a family of biharmonic deformations, solving variational shape deformations, and even unlocking the closed-form expressions for recently-introduced Somigliana coordinates for both fast and accurate evaluations.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_260",
    "authors": "Yuehao Wang, Chaoyi Wang, Bingchen Gong, Tianfan Xue",
    "title": "Bilateral Guided Radiance Field Processing",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658148",
    "pdf_link": null,
    "abstract": "Neural Radiance Fields (NeRF) achieves unprecedented performance in synthesizing novel view synthesis, utilizing multi-view consistency. When capturing multiple inputs, image signal processing (ISP) in modern cameras will independently enhance them, including exposure adjustment, color correction, local tone mapping, etc. While these processings greatly improve image quality, they often break the multi-view consistency assumption, leading to \"floaters\" in the reconstructed radiance fields. To address this concern without compromising visual aesthetics, we aim to first disentangle the enhancement by ISP at the NeRF training stage and re-apply user-desired enhancements to the reconstructed radiance fields at the finishing stage. Furthermore, to make the re-applied enhancements consistent between novel views, we need to perform imaging signal processing in 3D space (i.e. \"3D ISP\"). For this goal, we adopt the bilateral grid, a locally-affine model, as a generalized representation of ISP processing. Specifically, we optimize per-view 3D bilateral grids with radiance fields to approximate the effects of camera pipelines for each input view. To achieve user-adjustable 3D finishing, we propose to learn a low-rank 4D bilateral grid from a given single view edit, lifting photo enhancements to the whole 3D scene. We demonstrate our approach can boost the visual quality of novel view synthesis by effectively removing floaters and performing enhancements from user retouching. The source code and our data are available at: https://bilarfpro.github.io.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_169",
    "authors": "Christian Reiser, Stephan Garbin, Pratul Srinivasan, Dor Verbin, Richard Szeliski, Ben Mildenhall, Jonathan Barron, Peter Hedman, Andreas Geiger",
    "title": "Binary Opacity Grids: Capturing Fine Geometric Detail for Mesh-based View Synthesis",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658130",
    "pdf_link": null,
    "abstract": "While surface-based view synthesis algorithms are appealing due to their low computational requirements, they often struggle to reproduce thin structures. In contrast, more expensive methods that model the scene's geometry as a volumetric density field (e.g. NeRF) excel at reconstructing fine geometric detail. However, density fields often represent geometry in a \"fuzzy\" manner, which hinders exact localization of the surface. In this work, we modify density fields to encourage them to converge towards surfaces, without compromising their ability to reconstruct thin structures. First, we employ a discrete opacity grid representation instead of a continuous density field, which allows opacity values to discontinuously transition from zero to one at the surface. Second, we anti-alias by casting multiple rays per pixel, which allows occlusion boundaries and subpixel structures to be modelled without using semi-transparent voxels. Third, we minimize the binary entropy of the opacity values, which facilitates the extraction of surface geometry by encouraging opacity values to binarize towards the end of training. Lastly, we develop a fusion-based meshing strategy followed by mesh simplification and appearance model fitting. The compact meshes produced by our model can be rendered in real-time on mobile devices and achieve significantly higher view synthesis quality compared to existing mesh-based approaches. Our interactive webdemo is available at https://binary-opacity-grid.github.io.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_657",
    "authors": "Zhennan Wu, Yang Li, Han Yan, Taizhang Shang, Weixuan Sun, Senbo Wang, Ruikai Cui, Weizhe Liu, Hiroyuki Sato, Hongdong Li, Pan Ji",
    "title": "BlockFusion: Expandable 3D Scene Generation Using Latent Tri-plane Extrapolation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658188",
    "pdf_link": null,
    "abstract": "We present BlockFusion, a diffusion-based model that generates 3D scenes as unit blocks and seamlessly incorporates new blocks to extend the scene. BlockFusion is trained using datasets of 3D blocks that are randomly cropped from complete 3D scene meshes. Through per-block fitting, all training blocks are converted into the hybrid neural fields: with a tri-plane containing the geometry features, followed by a Multi-layer Perceptron (MLP) for decoding the signed distance values. A variational auto-encoder is employed to compress the tri-planes into the latent tri-plane space, on which the denoising diffusion process is performed. Diffusion applied to the latent representations allows for high-quality and diverse 3D scene generation. To expand a scene during generation, one needs only to append empty blocks to overlap with the current scene and extrapolate existing latent tri-planes to populate new blocks. The extrapolation is done by conditioning the generation process with the feature samples from the overlapping tri-planes during the denoising iterations. Latent tri-plane extrapolation produces semantically and geometrically meaningful transitions that harmoniously blend with the existing scene. A 2D layout conditioning mechanism is used to control the placement and arrangement of scene elements. Experimental results indicate that BlockFusion is capable of generating diverse, geometrically consistent and unbounded large 3D scenes with unprecedented high-quality shapes in both indoor and outdoor scenarios.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_497",
    "authors": "Xingchang Huang, Corentin Salaun, Cristina Vasconcelos, Christian Theobalt, Cengiz Oztireli, Gurprit Singh",
    "title": "Blue Noise for Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657435",
    "pdf_link": null,
    "abstract": "… We compare Gaussian noise and Gaussian blue noise by training a diffusion model (IADB) up to some time steps with certain noise magnitude (eg, 30%). During the testing phase, …",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_337",
    "authors": "Chih-Hai Su, Chih-Yao Hu, Shr-Ruei Tsai, Jie-Ying Lee, Chin-Yang Lin, Yu-Lun Liu",
    "title": "BoostMVSNeRFs: Boosting MVS-based NeRFs to Generalizable View Synthesis in Large-scale Scenes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657416",
    "pdf_link": null,
    "abstract": "While Neural Radiance Fields (NeRFs) have demonstrated exceptional quality, their protracted training duration remains a limitation. Generalizable and MVS-based NeRFs, although capable of mitigating training time, often incur tradeoffs in quality. This paper presents a novel approach called BoostMVSNeRFs to enhance the rendering quality of MVS-based NeRFs in large-scale scenes. We first identify limitations in MVS-based NeRF methods, such as restricted viewport coverage and artifacts due to limited input views. Then, we address these limitations by proposing a new method that selects and combines multiple cost volumes during volume rendering. Our method does not require training and can adapt to any MVS-based NeRF methods in a feed-forward fashion to improve rendering quality. Furthermore, our approach is also end-to-end trainable, allowing fine-tuning on specific scenes. We demonstrate the effectiveness of our method through experiments on large-scale datasets, showing significant rendering quality improvements in large-scale scenes and unbounded outdoor scenarios. We release the source code of BoostMVSNeRFs at https://su-terry.github.io/BoostMVSNeRFs.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_162",
    "authors": "Xiang Xu, Joseph Lambourne, Pradeep Jayaraman, Zhengqing Wang, Karl Willis, Yasutaka Furukawa",
    "title": "BrepGen: A B-rep Generative Diffusion Model With Structured Latent Geometry",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658129",
    "pdf_link": null,
    "abstract": "This paper presents BrepGen, a diffusion-based generative approach that directly outputs a Boundary representation (B-rep) Computer-Aided Design (CAD) model. BrepGen represents a B-rep model as a novel structured latent geometry in a hierarchical tree. With the root node representing a whole CAD solid, each element of a B-rep model (i.e., a face, an edge, or a vertex) progressively turns into a child-node from top to bottom. B-rep geometry information goes into the nodes as the global bounding box of each primitive along with a latent code describing the local geometric shape. The B-rep topology information is implicitly represented by node duplication. When two faces share an edge, the edge curve will appear twice in the tree, and a T-junction vertex with three incident edges appears six times in the tree with identical node features. Starting from the root and progressing to the leaf, BrepGen employs Transformer-based diffusion models to sequentially denoise node features while duplicated nodes are detected and merged, recovering the B-Rep topology information. Extensive experiments show that BrepGen advances the task of CAD B-rep generation, surpassing existing methods on various benchmarks. Results on our newly collected furniture dataset further showcase its exceptional capability in generating complicated geometry. While previous methods were limited to generating simple prismatic shapes, BrepGen incorporates free-form and doubly-curved surfaces for the first time. Additional applications of BrepGen include CAD autocomplete and design interpolation. The code, pretrained models, and dataset are available at https://github.com/samxuxiang/BrepGen.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_242",
    "authors": "Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, Jingyi Yu",
    "title": "CLAY: A Controllable Large-scale Generative Model for Creating High-quality 3D Assets",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658146",
    "pdf_link": null,
    "abstract": "In the realm of digital creativity, our potential to craft intricate 3D worlds from imagination is often hampered by the limitations of existing digital tools, which demand extensive expertise and efforts. To narrow this disparity, we introduce CLAY, a 3D geometry and material generator designed to effortlessly transform human imagination into intricate 3D digital structures. CLAY supports classic text or image inputs as well as 3D-aware controls from diverse primitives (multi-view images, voxels, bounding boxes, point clouds, implicit representations, etc). At its core is a large-scale generative model composed of a multi-resolution Variational Autoencoder (VAE) and a minimalistic latent Diffusion Transformer (DiT), to extract rich 3D priors directly from a diverse range of 3D geometries. Specifically, it adopts neural fields to represent continuous and complete surfaces and uses a geometry generative module with pure transformer blocks in latent space. We present a progressive training scheme to train CLAY on an ultra large 3D model dataset obtained through a carefully designed processing pipeline, resulting in a 3D native geometry generator with 1.5 billion parameters. For appearance generation, CLAY sets out to produce physically-based rendering (PBR) textures by employing a multi-view material diffusion model that can generate 2K resolution textures with diffuse, roughness, and metallic modalities. We demonstrate using CLAY for a range of controllable 3D asset creations, from sketchy conceptual designs to production ready assets with intricate details. Even first time users can easily use CLAY to bring their vivid 3D imaginations to life, unleashing unlimited creativity.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_292",
    "authors": "Jingyu Hu, Ka-Hei Hui, Zhengzhe Liu, Hao Zhang, Chi-Wing Fu",
    "title": "CNS-Edit: 3D Shape Editing via Coupled Neural Shape Optimization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657412",
    "pdf_link": null,
    "abstract": "This paper introduces a new approach based on a coupled representation and a neural volume optimization to implicitly perform 3D shape editing in latent space. This work has three innovations. First, we design the coupled neural shape (CNS) representation for supporting 3D shape editing. This representation includes a latent code, which captures high-level global semantics of the shape, and a 3D neural feature volume, which provides a spatial context to associate with the local shape changes given by the editing. Second, we formulate the coupled neural shape optimization procedure to co-optimize the two coupled components in the representation subject to the editing operation. Last, we offer various 3D shape editing operators, i.e., copy, resize, delete, and drag, and derive each into an objective for guiding the CNS optimization, such that we can iteratively co-optimize the latent code and neural feature volume to match the editing target. With our approach, we can achieve a rich variety of editing results that are not only aware of the shape semantics but are also not easy to achieve by existing approaches. Both quantitative and qualitative evaluations demonstrate the strong capabilities of our approach over the state-of-the-art solutions.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_356",
    "authors": "Rui Xu, Longdu Liu, Ningna Wang, Shuangmin Chen, Shiqing Xin, Xiaohu Guo, Zichun Zhong, Taku Komura, Wenping Wang, Changhe Tu",
    "title": "CWF: Consolidating Weak Features in High-quality Mesh Simplification",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658159",
    "pdf_link": null,
    "abstract": "In mesh simplification, common requirements like accuracy, triangle quality, and feature alignment are often considered as a trade-off. Existing algorithms concentrate on just one or a few specific aspects of these requirements. For example, the well-known Quadric Error Metrics (QEM) approach [Garland and Heckbert 1997] prioritizes accuracy and can preserve strong feature lines/points as well, but falls short in ensuring high triangle quality and may degrade weak features that are not as distinctive as strong ones. In this paper, we propose a smooth functional that simultaneously considers all of these requirements. The functional comprises a normal anisotropy term and a Centroidal Voronoi Tessellation (CVT) [Du et al. 1999] energy term, with the variables being a set of movable points lying on the surface. The former inherits the spirit of QEM but operates in a continuous setting, while the latter encourages even point distribution, allowing various surface metrics. We further introduce a decaying weight to automatically balance the two terms. We selected 100 CAD models from the ABC dataset [Koch et al. 2019], along with 21 organic models, to compare the existing mesh simplification algorithms with ours. Experimental results reveal an important observation: the introduction of a decaying weight effectively reduces the conflict between the two terms and enables the alignment of weak features. This distinctive feature sets our approach apart from most existing mesh simplification methods and demonstrates significant potential in shape understanding. Please refer to the teaser figure for illustration.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_637",
    "authors": "Gianpaolo Palma, Narges Pourjafarian, Jürgen Steimle, Paolo Cignoni",
    "title": "Capacitive Touch Sensing on General 3D Surfaces",
    "paper_url": "https://iris.cnr.it/bitstream/20.500.14243/486641/1/paper_compressed%20%281%29.pdf",
    "pdf_link": null,
    "abstract": "The rapid advancement of capacitive touch sensing technology has revolutionized how we interact with digital devices and interfaces. While capacitive touch sensing has been widely implemented on flat surfaces such as touchscreens, the growing demand for more intuitive and immersive user experiences has led to exploring touch sensing on 3D surfaces. Capacitive touch sensing on complex 3D surfaces offers new possibilities for interaction and expands the range of objects that can be transformed into interactive interfaces. However, extending high-resolution, multi-touch sensing to 3D",
    "scholar_publication": "ACM Trans. Graph, 2024 - iris.cnr.it"
  },
  {
    "paper_id": "papers_847",
    "authors": "Sebastian Starke, Paul Starke, Taku Komura, Yuting Ye",
    "title": "Categorical Codebook Matching for Embodied Character Controllers",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658209",
    "pdf_link": null,
    "abstract": "Translating motions from a real user onto a virtual embodied avatar is a key challenge for character animation in the metaverse. In this work, we present a novel generative framework that enables mapping from a set of sparse sensor signals to a full body avatar motion in real-time while faithfully preserving the motion context of the user. In contrast to existing techniques that require training a motion prior and its mapping from control to motion separately, our framework is able to learn the motion manifold as well as how to sample from it at the same time in an end-to-end manner. To achieve that, we introduce a technique called codebook matching which matches the probability distribution between two categorical codebooks for the inputs and outputs for synthesizing the character motions. We demonstrate this technique can successfully handle ambiguity in motion generation and produce high quality character controllers from unstructured motion capture data. Our method is especially useful for interactive applications like virtual reality or video games where high accuracy and responsiveness are needed.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_916",
    "authors": "Hao-Yang Peng, Jia-Peng Zhang, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu",
    "title": "CharacterGen: Efficient 3D Character Generation From Single Images With Multi-view Pose Canonicalization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658217",
    "pdf_link": null,
    "abstract": "BNRist, Department of Computer Science and Technology, Tsinghua University, China In the field of digital content creation, generating high-quality 3D characters from single images is challenging, especially given the complexities of various body poses and the issues of self-occlusion and pose ambiguity. In this paper, we present CharacterGen, a framework developed to efficiently generate 3D characters. CharacterGen introduces a streamlined generation pipeline along with an image-conditioned multi-view diffusion model. This model effectively calibrates input poses to a canonical form while retaining key attributes of the input image, thereby addressing the challenges posed by diverse poses. A transformer-based, generalizable sparse-view reconstruction model is the other core component of our approach, facilitating the creation of detailed 3D models from multi-view images. We also adopt a texture-back-projection strategy to produce high-quality texture maps. Additionally, we have curated a dataset of anime characters, rendered in multiple poses and views, to train and evaluate our model. Our approach has been thoroughly evaluated through quantitative and qualitative experiments, showing its proficiency in generating 3D characters with high-quality shapes and textures, ready for downstream applications such as rigging and animation.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_423",
    "authors": "Wenqi Dong, Bangbang Yang, Lin Ma, Xiao Liu, Liyuan Cui, Hujun Bao, Yuewen Ma, Zhaopeng Cui",
    "title": "Coin3D: Controllable and Interactive 3D Assets Generation With Proxy-guided Conditioning",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657425",
    "pdf_link": null,
    "abstract": "As humans, we aspire to create media content that is both freely willed and readily controlled. Thanks to the prominent development of generative techniques, we now can easily utilize 2D diffusion methods to synthesize images controlled by raw sketch or designated human poses, and even progressively edit/regenerate local regions with masked inpainting. However, similar workflows in 3D modeling tasks are still unavailable due to the lack of controllability and efficiency in 3D generation. In this paper, we present a novel controllable and interactive 3D assets modeling framework, named Coin3D. Coin3D allows users to control the 3D generation using a coarse geometry proxy assembled from basic shapes, and introduces an interactive generation workflow to support seamless local part editing while delivering responsive 3D object previewing within a few seconds. To this end, we develop several techniques, including the 3D adapter that applies volumetric coarse shape control to the diffusion model, proxy-bounded editing strategy for precise part editing, progressive volume cache to support responsive preview, and volume-SDS to ensure consistent mesh reconstruction. Extensive experiments of interactive generation and editing on diverse shape proxies demonstrate that our method achieves superior controllability and flexibility in the 3D assets generation task. Code and data are available on the project webpage: https://zju3dv.github.io/coin3d/.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_236",
    "authors": "Rafal K. Mantiuk, Param Hanji, Maliha Ashraf, Yuta Asano, Alexandre Chapiro",
    "title": "ColorVideoVDP: A Visual Difference Predictor for Image, Video, and Display Distortions",
    "paper_url": "https://arxiv.org/abs/2401.11485",
    "pdf_link": null,
    "abstract": "ColorVideoVDP is a video and image quality metric that models spatial and temporal aspects of vision, for both luminance and color. The metric is built on novel psychophysical models of chromatic spatiotemporal contrast sensitivity and cross-channel contrast masking. It accounts for the viewing conditions, geometric, and photometric characteristics of the display. It was trained to predict common video streaming distortions (eg video compression, rescaling, and transmission errors), and also 8 new distortion types related to AR/VR …",
    "scholar_publication": "arXiv preprint arXiv …, 2024 - arxiv.org"
  },
  {
    "paper_id": "papers_841",
    "authors": "Ladislav Kavan, John Doublestein, Martin Prazak, Matthew Cioffi, Doug Roble",
    "title": "Compressed Skinning for Facial Blendshapes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657477",
    "pdf_link": null,
    "abstract": "We present a new method to bake classical facial animation blendshapes into a fast linear blend skinning representation. Previous work explored skinning decomposition methods that approximate general animated meshes using a dense set of bone transformations; these optimizers typically alternate between optimizing for the bone transformations and the skinning weights. We depart from this alternating scheme and propose a new approach based on proximal algorithms, which effectively means adding a projection step to the popular Adam optimizer. This approach is very flexible and allows us to quickly experiment with various additional constraints and/or loss functions. Specifically, we depart from the classical skinning paradigms and restrict the transformation coefficients to contain only about 90% non-zeros, while achieving similar accuracy and visual quality as the state-of-the-art. The sparse storage enables our method to deliver significant savings in terms of both memory and run-time speed. We include a compact implementation of our new skinning decomposition method in PyTorch, which is easy to experiment with and modify to related problems.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_140",
    "authors": "Yingying Ren, Julian Panetta, Seiichi Suzuki, Uday Kusupati, Florin Isvoranu, Mark Pauly",
    "title": "Computational Homogenization for Inverse Design of Surface-based Inflatables",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658125",
    "pdf_link": null,
    "abstract": "Surface-based inflatables are composed of two thin layers of nearly inextensible sheet material joined together along carefully selected fusing curves. During inflation, pressure forces separate the two sheets to maximize the enclosed volume. The fusing curves restrict this expansion, leading to a spatially varying in-plane contraction and hence metric frustration. The inflated structure settles into a 3D equilibrium that balances pressure forces with the internal elastic forces of the sheets. We present a computational framework for analyzing and designing surface-based inflatable structures with arbitrary fusing patterns. Our approach employs numerical homogenization to characterize the behavior of parametric families of periodic inflatable patch geometries, which can then be combined to tessellate the sheet with smoothly varying patterns. We propose a novel parametrization of the underlying deformation space that allows accurate, efficient, and systematical analysis of the stretching and bending behavior of inflated patches with potentially open boundaries. We apply our homogenization algorithm to create a database of geometrically diverse fusing patterns spanning a wide range of material properties and deformation characteristics. This database is employed in an inverse design algorithm that solves for fusing curves to best approximate a given input target surface. Local patches are selected and blended to form a global network of curves based on a geometric flattening algorithm. These fusing curves are then further optimized to minimize the distance of the deployed structure to target surface. We show that this approach offers greater flexibility to approximate given target geometries compared to previous work while significantly improving structural performance.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1120",
    "authors": "Amy Zhu, Yuxuan Mei, Benjamin Jones, Zachary Tatlock, Adriana Schulz",
    "title": "Computational Illusion Knitting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658231",
    "pdf_link": null,
    "abstract": "… Illusion-knit fabrics reveal distinct patterns or images … However, past work in computational 3D knitting does not … for exploring illusion knitting in the context of computational design and …",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_105",
    "authors": "Elad Richardson, Kfir Goldberg, Yuval Alaluf, Daniel Cohen-Or, Yuval Alaluf",
    "title": "ConceptLab: Creative Concept Generation using VLM-Guided Diffusion Prior Constraints",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3659578",
    "pdf_link": null,
    "abstract": "Recent text-to-image generative models have enabled us to transform our words into vibrant, captivating imagery. The surge of personalization techniques that has followed has also allowed us to imagine unique concepts in new scenes. However, an intriguing question remains: How can we generate a new, imaginary concept that has never been seen before? In this article, we present the task of creative text-to-image generation, where we seek to generate new members of a broad category (e.g., generating a pet that differs from all existing pets). We leverage the under-studied Diffusion Prior models and show that the creative generation problem can be formulated as an optimization process over the output space of the diffusion prior, resulting in a set of “prior constraints.” To keep our generated concept from converging into existing members, we incorporate a question-answering Vision-Language Model that adaptively adds new constraints to the optimization problem, encouraging the model to discover increasingly more unique creations. Finally, we show that our prior constraints can also serve as a strong mixing mechanism allowing us to create hybrids between generated concepts, introducing even more flexibility into the creative process.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_183",
    "authors": "Zhimin Fan, Pengcheng Shi, Mufan Guo, Ruoyu Fu, Yanwen Guo, Jie Guo",
    "title": "Conditional Mixture Path Guiding for Differentiable Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658133",
    "pdf_link": null,
    "abstract": "The efficiency of inverse optimization in physically based differentiable rendering heavily depends on the variance of Monte Carlo estimation. Despite recent advancements emphasizing the necessity of tailored differential sampling strategies, the general approaches remain unexplored. In this paper, we investigate the interplay between local sampling decisions and the estimation of light path derivatives. Considering that modern differentiable rendering algorithms share the same path for estimating differential radiance and ordinary radiance, we demonstrate that conventional guiding approaches, conditioned solely on the last vertex, cannot attain this density. Instead, a mixture of different sampling distributions is required, where the weights are conditioned on all the previously sampled vertices in the path. To embody our theory, we implement a conditional mixture path guiding that explicitly computes optimal weights on the fly. Furthermore, we show how to perform positivization to eliminate sign variance and extend to scenes with millions of parameters. To the best of our knowledge, this is the first generic framework for applying path guiding to differentiable rendering. Extensive experiments demonstrate that our method achieves nearly one order of magnitude improvements over state-of-the-art methods in terms of variance reduction in gradient estimation and errors of inverse optimization. The implementation of our proposed method is available at https://github.com/mollnn/conditional-mixture.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_831",
    "authors": "Weizhou Liu, Xingce Wang, Haichuan Zhao, Xingfei Xue, Zhongke Wu, Xuequan Lu, Ying He",
    "title": "Consistent Point Orientation for Manifold Surfaces via Boundary Integration",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657475",
    "pdf_link": null,
    "abstract": "This paper introduces a new approach for generating globally consistent normals for point clouds sampled from manifold surfaces. Given that the generalized winding number (GWN) field generated by a point cloud with globally consistent normals is a solution to a PDE with jump boundary conditions and possesses harmonic properties, and the Dirichlet energy of the GWN field can be defined as an integral over the boundary surface, we formulate a boundary energy derived from the Dirichlet energy of the GWN. Taking as input a point cloud with randomly oriented normals, we optimize this energy to restore the global harmonicity of the GWN field, thereby recovering the globally consistent normals. Experiments show that our method outperforms state-of-the-art approaches, exhibiting enhanced robustness to noise, outliers, complex topologies, and thin structures. Our code can be found at https://github.com/liuweizhou319/BIM.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_723",
    "authors": "Octave Crespel, Emile Hohnadel, Thibaut Metivet, Florence Bertails-Descoubes",
    "title": "Contact Detection Between Curved Fibres: High Order Makes a Difference",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658191",
    "pdf_link": null,
    "abstract": "Computer Graphics has a long history in the design of effective algorithms for handling contact and friction between solid objects. For the sake of simplicity and versatility, most methods rely on low-order primitives such as line segments or triangles, both for the detection and the response stages. In this paper we carefully analyse, in the case of fibre systems, the impact of such choices on the retrieved contact forces. We highlight the presence of artifacts in the force response that are tightly related to the low-order geometry used for contact detection. Our analysis draws upon thorough comparisons between the high-order super-helix model and the low-order discrete elastic rod model. These reveal that when coupled to a low-order, segment-based detection scheme, both models yield spurious jumps in the contact force profile. Moreover, these artifacts are shown to be all the more visible as the geometry of fibres at contact is curved. In order to remove such artifacts we develop an accurate high-order detection scheme between two smooth curves, which relies on an efficient adaptive pruning strategy. We use this algorithm to detect contact between super-helices at high precision, allowing us to recover, in the range of wavy to highly curly fibres, much smoother force profiles during sliding motion than with a classical segment-based strategy. Furthermore, we show that our approach offers better scaling properties in terms of efficiency vs. precision compared to segment-based approaches, making it attractive for applications where accurate and reliable forces are desired. Finally, we demonstrate the robustness and accuracy of our fully high-order approach on a challenging hair combing scenario.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_258",
    "authors": "Artur Grigorev, Giorgio Becherini, Michael Black, Otmar Hilliges, Bernhard Thomaszewski",
    "title": "ContourCraft: Learning to Resolve Intersections in Neural Multi-garment Simulations",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657408",
    "pdf_link": null,
    "abstract": "Learning-based approaches to cloth simulation have started to show their potential in recent years. However, handling collisions and intersections in neural simulations remains a largely unsolved problem. In this work, we present ContourCraft, a learning-based solution for handling intersections in neural cloth simulations. Unlike conventional approaches that critically rely on intersection-free inputs, ContourCraft robustly recovers from intersections introduced through missed collisions, self-penetrating bodies, or errors in manually designed multi-layer outfits. The technical core of ContourCraft is a novel intersection contour loss that penalizes interpenetrations and encourages rapid resolution thereof. We integrate our intersection loss with a collision-avoiding repulsion objective into a neural cloth simulation method based on graph neural networks (GNNs). We demonstrate our method’s ability across a challenging set of diverse multi-layer outfits under dynamic human motions. Our extensive analysis indicates that ContourCraft significantly improves collision handling for learned simulation and produces visually compelling results.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_828",
    "authors": "Guilherme Gomes Haetinger, Jingwei Tang, Raphael Ortiz, Paul Kanyuk, Vinicius Azevedo",
    "title": "Controllable Neural Style Transfer for Dynamic Meshes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657474",
    "pdf_link": null,
    "abstract": "In recent years, animation movies are shifting from realistic representations to more stylized depictions that support unique design languages. To favor that, recent works implemented a Neural Style Transfer (NST) pipeline that supports the stylization of 3D assets by 2D images. In this paper we propose a novel mesh stylization technique that improves previous NST works in several ways. First, we replace the standard Gram-Matrix style loss by a Neural Neighbor formulation that enables sharper and artifact-free results. To support large mesh deformations, we reparametrize the optimized mesh positions through an implicit formulation based on the Laplace-Beltrami operator that better captures silhouette gradients that are common in inverse differentiable rendering setups. This reparametrization is coupled with a coarse-to-fine stylization setup, which enables deformations that can change large structures of the mesh. We provide artistic control through a novel method that enables directional and temporal control over synthesized styles by a guiding vector field. Lastly, we improve the previous time-coherency schemes and develop an efficient regularization that controls volume changes during the stylization process. These improvements enable high quality mesh stylizations that can create unique looks for both simulations and 3D assets.",
    "scholar_publication": "Acm siggraph 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_400",
    "authors": "Jiahao Ge, Mingjun Zhou, Wenrui Bao, Hao Xu, Chi-Wing Fu",
    "title": "Creating LEGO Figurines From Single Images",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658167",
    "pdf_link": null,
    "abstract": "This paper presents a computational pipeline for creating personalized, physical LEGO®1 figurines from user-input portrait photos. The generated figurine is an assembly of coherently-connected LEGO® bricks detailed with uv-printed decals, capturing prominent features such as hairstyle, clothing style, and garment color, and also intricate details such as logos, text, and patterns. This task is non-trivial, due to the substantial domain gap between unconstrained user photos and the stylistically-consistent LEGO® figurine models. To ensure assemble-ability by LEGO® bricks while capturing prominent features and intricate details, we design a three-stage pipeline: (i) we formulate a CLIP-guided retrieval approach to connect the domains of user photos and LEGO® figurines, then output physically-assemble-able LEGO® figurines with decals excluded; (ii) we then synthesize decals on the figurines via a symmetric U-Nets architecture conditioned on appearance features extracted from user photos; and (iii) we next reproject and uv-print the decals on associated LEGO® bricks for physical model production. We evaluate the effectiveness of our method against eight hundred expert-designed figurines, using a comprehensive set of metrics, which include a novel GPT-4V-based evaluation metric, demonstrating superior performance of our method in visual quality and resemblance to input photos. Also, we show our method's robustness by generating LEGO® figurines from diverse inputs and physically fabricating and assembling several of them.",
    "scholar_publication": "ACM Transactions on Graphics …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_752",
    "authors": "Shree Nayar, Jeremy Klotz, Nikhil Nanda, Mikhail Fridberg",
    "title": "Cricket: A Self-powered Chirping Pixel",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658196",
    "pdf_link": null,
    "abstract": "We present a sensor that can measure light and wirelessly communicate the measurement, without the need for an external power source or a battery. Our sensor, called cricket, harvests energy from incident light. It is asleep for most of the time and transmits a short and strong radio frequency chirp when its harvested energy reaches a specific level. The carrier frequency of each cricket is fixed and reveals its identity, and the duration between consecutive chirps is a measure of the incident light level. We have characterized the radiometric response function, signal-to-noise ratio and dynamic range of cricket. We have experimentally verified that cricket can be miniaturized at the expense of increasing the duration between chirps. We show that a cube with a cricket on each of its sides can be used to estimate the centroid of any complex illumination, which has value in applications such as solar tracking. We also demonstrate the use of crickets for creating untethered sensor arrays that can produce video and control lighting for energy conservation. Finally, we modified cricket's circuit to develop battery-free electronic sunglasses that can instantly adapt to environmental illumination.",
    "scholar_publication": "ACM Transactions on Graphics …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_417",
    "authors": "Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar Averbuch-Elor, Daniel Cohen-Or",
    "title": "Cross-image Attention for Zero-shot Appearance Transfer",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657423",
    "pdf_link": null,
    "abstract": "Recent advancements in text-to-image generative models have demonstrated a remarkable ability to capture a deep semantic understanding of images. In this work, we leverage this semantic knowledge to transfer the visual appearance between objects that share similar semantics but may differ significantly in shape. To achieve this, we build upon the self-attention layers of these generative models and introduce a cross-image attention mechanism that implicitly establishes semantic correspondences across images. Specifically, given a pair of images — one depicting the target structure and the other specifying the desired appearance — our cross-image attention combines the queries corresponding to the structure image with the keys and values of the appearance image. This operation, when applied during the denoising process, leverages the established semantic correspondences to generate an image combining the desired structure and appearance. In addition, to improve the output image quality, we harness three mechanisms that either manipulate the noisy latent codes or the model’s internal representations throughout the denoising process. Importantly, our approach is zero-shot, requiring no optimization or training. Experiments show that our method is effective across a wide range of object categories and is robust to variations in shape, size, and viewpoint between the two input images.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_238",
    "authors": "Etienne Corman",
    "title": "Curvature-driven Conformal Deformations",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658145",
    "pdf_link": null,
    "abstract": "… such as surface fairing, deformation, and approximation using … , we concentrate on conformal deformations, leveraging the … a connection between conformal deformations and changes …",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_211",
    "authors": "Colin Groth, Marcus Magnor, Steve Grogorick, Martin Eisemann, Piotr Didyk",
    "title": "Cybersickness Reduction via Gaze-contingent Image Deformation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658138",
    "pdf_link": null,
    "abstract": "Virtual reality has ushered in a revolutionary era of immersive content perception. However, a persistent challenge in dynamic environments is the occurrence of cybersickness arising from a conflict between visual and vestibular cues. Prior techniques have demonstrated that limiting illusory self-motion, so-called vection, by blurring the peripheral part of images, introducing tunnel vision, or altering the camera path can effectively reduce the problem. Unfortunately, these methods often alter the user's experience with visible changes to the content. In this paper, we propose a new technique for reducing vection and combating cybersickness by subtly lowering the screen-space speed of objects in the user's peripheral vision. The method is motivated by our hypothesis that small modifications to the objects' velocity in the periphery and geometrical distortions in the peripheral vision can remain unnoticeable yet lead to reduced vection. This paper describes the experiments supporting this hypothesis and derives its limits. Furthermore, we present a method that exploits these findings by introducing subtle, screen-space geometrical distortions to animation frames to counteract the motion contributing to vection. We implement the method as a realtime post-processing step that can be integrated into existing rendering frameworks. The final validation of the technique and comparison to an alternative approach confirms its effectiveness in reducing cybersickness.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_266",
    "authors": "Jorge Alejandro Amador Herrera, Jonathan Klein, Daoming Liu, Wojtek Pałubicki, Sören Pirk, Dominik L. Michels",
    "title": "Cyclogenesis: Simulating Hurricanes and Tornadoes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658149",
    "pdf_link": null,
    "abstract": "Cyclones are large-scale phenomena that result from complex heat and water transfer processes in the atmosphere, as well as from the interaction of multiple hydrometeors, i.e., water and ice particles. When cyclones make landfall, they are considered natural disasters and spawn dread and awe alike. We propose a physically-based approach to describe the 3D development of cyclones in a visually convincing and physically plausible manner. Our approach allows us to capture large-scale heat and water continuity, turbulent microphysical dynamics of hydrometeors, and mesoscale cyclonic processes within the planetary boundary layer. Modeling these processes enables us to simulate multiple hurricane and tornado phenomena. We evaluate our simulations quantitatively by comparing to real data from storm soundings and observations of hurricane landfall from climatology research. Additionally, qualitative comparisons to previous methods are performed to validate the different parts of our scheme. In summary, our model simulates cyclogenesis in a comprehensive way that allows us to interactively render animations of some of the most complex weather events.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1282",
    "authors": "Zhiqin Chen, Qimin Chen, Hang Zhou, Hao Zhang",
    "title": "DAE-Net: Deforming Auto-Encoder for Fine-grained Shape Co-segmentation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657528",
    "pdf_link": null,
    "abstract": "We present an unsupervised 3D shape co-segmentation method which learns a set of deformable part templates from a shape collection. To accommodate structural variations in the collection, our network composes each shape by a selected subset of template parts which are affine-transformed. To maximize the expressive power of the part templates, we introduce a per-part deformation network to enable the modeling of diverse parts with substantial geometry variations, while imposing constraints on the deformation capacity to ensure fidelity to the originally represented parts. We also propose a training scheme to effectively overcome local minima. Architecturally, our network is a branched autoencoder, with a CNN encoder taking a voxel shape as input and producing per-part transformation matrices, latent codes, and part existence scores, and the decoder outputting point occupancies to define the reconstruction loss. Our network, coined DAE-Net for Deforming Auto-Encoder, can achieve unsupervised 3D shape co-segmentation that yields fine-grained, compact, and meaningful parts that are consistent across diverse shapes. We conduct extensive experiments on the ShapeNet Part dataset, DFAUST, and an animal subset of Objaverse to show superior performance over prior methods. Code and data are available at https://github.com/czq142857/DAE-Net.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_101",
    "authors": "Haipeng Li, Hai Jiang, Ao Luo, Ping Tan, Haoqiang Fan, Bing Zeng, Shuaicheng Liu, Haipeng Li",
    "title": "DMHomo: Learning Homography with Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3652207",
    "pdf_link": null,
    "abstract": "Supervised homography estimation methods face a challenge due to the lack of adequate labeled training data. To address this issue, we propose DMHomo, a diffusion model-based framework for supervised homography learning. This framework generates image pairs with accurate labels, realistic image content, and realistic interval motion, ensuring that they satisfy adequate pairs. We utilize unlabeled image pairs with pseudo labels such as homography and dominant plane masks, computed from existing methods, to train a diffusion model that generates a supervised training dataset. To further enhance performance, we introduce a new probabilistic mask loss, which identifies outlier regions through supervised training, and an iterative mechanism to optimize the generative and homography models successively. Our experimental results demonstrate that DMHomo effectively overcomes the scarcity of qualified datasets in supervised homography learning and improves generalization to real-world scenes. The code and dataset are available at GitHub (https://github.com/lhaippp/DMHomo).",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_121",
    "authors": "Adrien Peytavie, James Gain, Eric Guérin, Oscar Argudo, Eric Galin, Adrien Peytavie",
    "title": "DeadWood: Including disturbance and decay in the depiction of digital nature",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641816",
    "pdf_link": null,
    "abstract": "The creation of truly believable simulated natural environments remains an unsolved problem in Computer Graphics. This is, in part, due to a lack of visual variety. In nature, apart from variation due to abiotic and biotic growth factors, a significant role is played by disturbance events, such as fires, windstorms, disease, and death and decay processes, which give rise to both standing dead trees (snags) and downed woody debris (logs). For instance, snags constitute on average 10% of unmanaged forests by basal area, and logs account for 2 times this quantity.While previous systems have incorporated individual elements of disturbance (e.g., forest fires) and decay (e.g., the formation of humus), there has been no unifying treatment, perhaps because of the challenge of matching simulation results with generated geometric models.In this paper, we present a framework that combines an ecosystem simulation, which explicitly incorporates disturbance events and decay processes, with a model realization process, which balances the uniqueness arising from life history with the need for instancing due to memory constraints. We tested our hypothesis concerning the visual impact of disturbance and decay with a two-alternative forced-choice experiment (n = 116). Our findings are that the presence of dead wood in various forms, as snags or logs, significantly improves the believability of natural scenes, while, surprisingly, general variation in the number of model instances, with up to 8 models per species, and a focus on disturbance events, does not.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_123",
    "authors": "Rohan Sawhney, Daqi Lin, Markus Kettunen, Benedikt Bitterli, Ravi Ramamoorthi, Chris Wyman, Matt Pharr, Rohan Sawhney",
    "title": "Decorrelating ReSTIR Samplers via MCMC Mutations",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3629166",
    "pdf_link": null,
    "abstract": "Monte Carlo rendering algorithms often utilize correlations between pixels to improve efficiency and enhance image quality. For real-time applications in particular, repeated reservoir resampling offers a powerful framework to reuse samples both spatially in an image and temporally across multiple frames. While such techniques achieve equal-error up to 100× faster for real-time direct lighting [Bitterli et al. ] and global illumination [Ouyang et al. ; Lin et al. ], they are still far from optimal. For instance, spatiotemporal resampling often introduces noticeable correlation artifacts, while reservoirs holding more than one sample suffer from impoverishment in the form of duplicate samples. We demonstrate how interleaving Markov Chain Monte Carlo (MCMC) mutations with reservoir resampling helps alleviate these issues, especially in scenes with glossy materials and difficult-to-sample lighting. Moreover, our approach does not introduce any bias, and in practice, we find considerable improvement in image quality with just a single mutation per reservoir sample in each frame.",
    "scholar_publication": "ACM transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_570",
    "authors": "Haonan Zhang, Jie Guo, Jiawei Zhang, Haoyu Qin, Zesen Feng, Ming Yang, Yanwe Guo",
    "title": "Deep Fourier-based Arbitrary-scale Super-resolution for Real-time Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657439",
    "pdf_link": null,
    "abstract": "As a prevailing tool for effectively reducing rendering costs in many graphical applications, frame super-resolution has seen important progress in recent years. However, most of prior works designed for rendering contents face a common limitation: once a model is trained, it can only afford a single fixed scale. In this paper, we attempt to eliminate this limitation by supporting arbitrary-scale super-resolution for a trained neural model. The key is a Fourier-based implicit neural representation which maps arbitrary and naturally coordinates in the high-resolution spatial domain to valid pixel values. By observing that high-resolution G-buffers possess similar spectrum to high-resolution rendered frames, we design a High-Frequency Fourier Mapping (HFFM) module to recover fine details from low-resolution inputs, without introducing noticeable artifacts. A Low-Frequency Residual Learning (LFRL) strategy is adopted to preserve low-frequency structures and ensure low biasedness caused by network inference. Moreover, different rendering contents are well separated by our spatial-temporal masks derived from G-buffers and motion vectors. Several light-weight designs to the neural network guarantee the real-time performance on a wide range of scenes.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1024",
    "authors": "Jaesung Rim, Junyong Lee, Heemin Yang, Sunghyun Cho",
    "title": "Deep Hybrid Camera Deblurring for Smartphone Cameras",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657507",
    "pdf_link": null,
    "abstract": "Mobile cameras, despite their significant advancements, still have difficulty in low-light imaging due to compact sensors and lenses, leading to longer exposures and motion blur. Traditional blind deconvolution methods and learning-based deblurring methods can be potential solutions to remove blur. However, achieving practical performance still remains a challenge. To address this, we propose a learning-based deblurring framework for smartphones, utilizing wide and ultra-wide cameras as a hybrid camera system. We simultaneously capture a long-exposure wide image and short-exposure burst ultra-wide images, and utilize the burst images to deblur the wide image. To fully exploit burst ultra-wide images, we present HCDeblur, a practical deblurring framework that includes novel deblurring networks, HC-DNet and HC-FNet. HC-DNet utilizes motion information extracted from burst images to deblur a wide image, and HC-FNet leverages burst images as reference images to further enhance a deblurred output. For training and evaluating the proposed method, we introduce the HCBlur dataset, which consists of synthetic and real-world datasets. Our experiments demonstrate that HCDeblur achieves state-of-the-art deblurring quality. Codes and datasets are available at https://cg.postech.ac.kr/research/HCDeblur.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_763",
    "authors": "Chuan Yan, Yong Li, Deepali Aneja, Matthew Fisher, Edgar Simo-Serra, Yotam Gingold",
    "title": "Deep Sketch Vectorization via Implicit Surface Extraction",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658197",
    "pdf_link": null,
    "abstract": "We introduce an algorithm for sketch vectorization with state-of-the-art accuracy and capable of handling complex sketches. We approach sketch vectorization as a surface extraction task from an unsigned distance field, which is implemented using a two-stage neural network and a dual contouring domain post processing algorithm. The first stage consists of extracting unsigned distance fields from an input raster image. The second stage consists of an improved neural dual contouring network more robust to noisy input and more sensitive to line geometry. To address the issue of under-sampling inherent in grid-based surface extraction approaches, we explicitly predict undersampling and keypoint maps. These are used in our post-processing algorithm to resolve sharp features and multi-way junctions. The keypoint and undersampling maps are naturally controllable, which we demonstrate in an interactive topology refinement interface. Our proposed approach produces far more accurate vectorizations on complex input than previous approaches with efficient running time.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_197",
    "authors": "Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, Xin Tong",
    "title": "DiLightNet: Fine-grained Lighting Control for Diffusion-based Image Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657396",
    "pdf_link": null,
    "abstract": "This paper presents a novel method for exerting fine-grained lighting control during text-driven diffusion-based image generation. While existing diffusion models already have the ability to generate images under any lighting condition, without additional guidance these models tend to correlate image content and lighting. Moreover, text prompts lack the necessary expressional power to describe detailed lighting setups. To provide the content creator with fine-grained control over the lighting during image generation, we augment the text-prompt with detailed lighting information in the form of radiance hints, i.e., visualizations of the scene geometry with a homogeneous canonical material under the target lighting. However, the scene geometry needed to produce the radiance hints is unknown. Our key observation is that we only need to guide the diffusion process, hence exact radiance hints are not necessary; we only need to point the diffusion model in the right direction. Based on this observation, we introduce a three stage method for controlling the lighting during image generation. In the first stage, we leverage a standard pretrained diffusion model to generate a provisional image under uncontrolled lighting. Next, in the second stage, we resynthesize and refine the foreground object in the generated image by passing the target lighting to a refined diffusion model, named DiLightNet, using radiance hints computed on a coarse shape of the foreground object inferred from the provisional image. To retain the texture details, we multiply the radiance hints with a neural encoding of the provisional synthesized image before passing it to DiLightNet. Finally, in the third stage, we resynthesize the background to be consistent with the lighting on the foreground object. We demonstrate and validate our lighting controlled diffusion model on a variety of text prompts and lighting conditions.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1268",
    "authors": "Daoyi Gao, David Rozenberszki, Stefan Leutenegger, Angela Dai",
    "title": "DiffCAD: Weakly-supervised Probabilistic CAD Model Retrieval and Alignment From an RGB Image",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658236",
    "pdf_link": null,
    "abstract": "Perceiving 3D structures from RGB images based on CAD model primitives can enable an effective, efficient 3D object-based representation of scenes. However, current approaches rely on supervision from expensive yet imperfect annotations of CAD models associated with real images, and encounter challenges due to the inherent ambiguities in the task - both in depth-scale ambiguity in monocular perception, as well as inexact matches of CAD database models to real observations. We thus propose DiffCAD, the first weakly-supervised probabilistic approach to CAD retrieval and alignment from an RGB image. We learn a probabilistic model through diffusion, modeling likely distributions of shape, pose, and scale of CAD objects in an image. This enables multi-hypothesis generation of different plausible CAD reconstructions, requiring only a few hypotheses to characterize ambiguities in depth/scale and inexact shape matches. Our approach is trained only on synthetic data, leveraging monocular depth and mask estimates to enable robust zero-shot adaptation to various real target domains. Despite being trained solely on synthetic data, our multi-hypothesis approach can even surpass the supervised state-of-the-art on the Scan2CAD dataset by 5.9% with 8 hypotheses.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_980",
    "authors": "Zhiyao Sun, Tian Lv, Sheng Ye, Matthieu Lin, Jenny Sheng, Yu-Hui Wen, Minjing Yu, Yong-Jin Liu",
    "title": "DiffPoseTalk: Speech-driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658221",
    "pdf_link": null,
    "abstract": "The generation of stylistic 3D facial animations driven by speech presents a significant challenge as it requires learning a many-to-many mapping between speech, style, and the corresponding natural facial motion. However, existing methods either employ a deterministic model for speech-to-motion mapping or encode the style using a one-hot encoding scheme. Notably, the one-hot encoding approach fails to capture the complexity of the style and thus limits generalization ability. In this paper, we propose DiffPoseTalk, a generative framework based on the diffusion model combined with a style encoder that extracts style embeddings from short reference videos. During inference, we employ classifier-free guidance to guide the generation process based on the speech and style. In particular, our style includes the generation of head poses, thereby enhancing user perception. Additionally, we address the shortage of scanned 3D talking face data by training our model on reconstructed 3DMM parameters from a high-quality, in-the-wild audio-visual dataset. Extensive experiments and user study demonstrate that our approach outperforms state-of-the-art methods. The code and dataset are at https://diffposetalk.github.io.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_922",
    "authors": "Xutong Jin, Chenxi Xu, Ruohan Gao, Jiajun Wu, Guoping Wang, Sheng Li",
    "title": "DiffSound: Differentiable Modal Sound Rendering and Inverse Rendering for Diverse Inference Tasks",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657493",
    "pdf_link": null,
    "abstract": "Accurately estimating and simulating the physical properties of objects from real-world sound recordings is of great practical importance in the fields of vision, graphics, and robotics. However, the progress in these directions has been limited—prior differentiable rigid or soft body simulation techniques cannot be directly applied to modal sound synthesis due to the high sampling rate of audio, while previous audio synthesizers often do not fully model the accurate physical properties of the sounding objects. We propose DiffSound, a differentiable sound rendering framework for physics-based modal sound synthesis, which is based on an implicit shape representation, a new high-order finite element analysis module, and a differentiable audio synthesizer. Our framework can solve a wide range of inverse problems thanks to the differentiability of the entire pipeline, including physical parameter estimation, geometric shape reasoning, and impact position prediction. Experimental results demonstrate the effectiveness of our approach, highlighting its ability to accurately reproduce the target sound in a physics-based manner. DiffSound serves as a valuable tool for various sound synthesis and analysis applications.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_133",
    "authors": "Yue Li, Logan Numerow, Bernhard Thomaszewski, Stelian Coros",
    "title": "Differentiable Geodesic Distance for Intrinsic Minimization on Triangle Meshes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658122",
    "pdf_link": null,
    "abstract": "Computing intrinsic distances on discrete surfaces is at the heart of many minimization problems in geometry processing and beyond. Solving these problems is extremely challenging as it demands the computation of on-surface distances along with their derivatives. We present a novel approach for intrinsic minimization of distance-based objectives defined on triangle meshes. Using a variational formulation of shortest-path geodesics, we compute first and second-order distance derivatives based on the implicit function theorem, thus opening the door to efficient Newton-type minimization solvers. We demonstrate our differentiable geodesic distance framework on a wide range of examples, including geodesic networks and membranes on surfaces of arbitrary genus, two-way coupling between hosting surface and embedded system, differentiable geodesic Voronoi diagrams, and efficient computation of Karcher means on complex shapes. Our analysis shows that second-order descent methods based on our differentiable geodesics outperform existing first-order and quasi-Newton methods by large margins.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_319",
    "authors": "Logan Numerow, Yue Li, Stelian Coros, Bernhard Thomaszewski",
    "title": "Differentiable Voronoi Diagrams for Simulation of Cell-based Mechanical Systems",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658152",
    "pdf_link": null,
    "abstract": "Navigating topological transitions in cellular mechanical systems is a significant challenge for existing simulation methods. While abstract models lack predictive capabilities at the cellular level, explicit network representations struggle with topology changes, and per-cell representations are computationally too demanding for large-scale simulations. To address these challenges, we propose a novel cell-centered approach based on differentiable Voronoi diagrams. Representing each cell with a Voronoi site, our method defines shape and topology of the interface network implicitly. In this way, we substantially reduce the number of problem variables, eliminate the need for explicit contact handling, and ensure continuous geometry changes during topological transitions. Closed-form derivatives of network positions facilitate simulation with Newton-type methods for a wide range of per-cell energies. Finally, we extend our differentiable Voronoi diagrams to enable coupling with arbitrary rigid and deformable boundaries. We apply our approach to a diverse set of examples, highlighting splitting and merging of cells as well as neighborhood changes. We illustrate applications to inverse problems by matching soap foam simulations to real-world images. Comparative analysis with explicit cell models reveals that our method achieves qualitatively comparable results at significantly faster computation times.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_118",
    "authors": "Zizhou Huang, Davi Colli Tozoni, Arvi Gjoka, Zachary Ferguson, Teseo Schneider, Daniele Panozzo, Denis Zorin, Zizhou Huang",
    "title": "Differentiable solver for time-dependent deformation problems with contact",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3657648",
    "pdf_link": null,
    "abstract": "We introduce a general differentiable solver for time-dependent deformation problems with contact and friction. Our approach uses a finite element discretization with a high-order time integrator coupled with the recently proposed incremental potential contact method for handling contact and friction forces to solve ODE- and PDE-constrained optimization problems on scenes with complex geometry. It supports static and dynamic problems and differentiation with respect to all physical parameters involved in the physical problem description, which include shape, material parameters, friction parameters, and initial conditions. Our analytically derived adjoint formulation is efficient, with a small overhead (typically less than 10% for nonlinear problems) over the forward simulation, and shares many similarities with the forward problem, allowing the reuse of large parts of existing forward simulator code. We implement our approach on top of the open-source PolyFEM library and demonstrate the applicability of our solver to shape design, initial condition optimization, and material estimation on both simulated results and physical validations.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_978",
    "authors": "Ryan Burgert, Xiang Li, Abe Leite, Kanchana Ranasinghe, Michael Ryoo",
    "title": "Diffusion Illusions: Hiding Images in Plain Sight",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657500",
    "pdf_link": null,
    "abstract": "We explore the problem of computationally generating special images that produce multi-arrangement optical illusions when physically arranged and viewed in a certain way, which we call ‘prime’ images. First, we propose a formal definition for this problem. Next, we introduce Diffusion Illusions, the first comprehensive pipeline designed to automatically generate a wide range of these multi-arrangement illusions. Specifically, we both adapt the existing ‘score distillation loss’ and propose a new ‘dream target loss’ to optimize a group of differentially parametrized prime images, using a frozen text-to-image diffusion model. We study three types of illusions, each where the prime images are arranged in different ways and optimized using the aforementioned losses such that images derived from them align with user-chosen text prompts or images. We conduct comprehensive experiments on these illusions and verify the effectiveness of our proposed method qualitatively and quantitatively. Additionally, we showcase the successful physical fabrication of our illusions — as they are all designed to work in the real world. Code and examples are publicly available at our interactive project website: https://diffusionillusion.github.io/",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_677",
    "authors": "Anita Hu, Nishkrit Desai, Hassan Abu Alhaija, Seung Wook Kim, Maria Shugrina",
    "title": "Diffusion Texture Painting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657458",
    "pdf_link": null,
    "abstract": "… diffusion models (DMs) for interactive texture painting … texture painting systems, our method allows artists to paint with any complex image texture, and in contrast with traditional texture …",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_874",
    "authors": "Shiyuan Yang, Liang Hou, Haibin Huang, Chongyang Ma, Pengfei Wan, DI Zhang, Xiaodong Chen, Jing Liao",
    "title": "Direct-a-Video: Customized Video Generation With User-directed Camera Movement and Object Motion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657481",
    "pdf_link": null,
    "abstract": "Recent text-to-video diffusion models have achieved impressive progress. In practice, users often desire the ability to control object motion and camera movement independently for customized video creation. However, current methods lack the focus on separately controlling object motion and camera movement in a decoupled manner, which limits the controllability and flexibility of text-to-video models. In this paper, we introduce Direct-a-Video, a system that allows users to independently specify motions for multiple objects as well as camera’s pan and zoom movements, as if directing a video. We propose a simple yet effective strategy for the decoupled control of object motion and camera movement. Object motion is controlled through spatial cross-attention modulation using the model’s inherent priors, requiring no additional optimization. For camera movement, we introduce new temporal cross-attention layers to interpret quantitative camera movement parameters. We further employ an augmentation-based approach to train these layers in a self-supervised manner on a small-scale dataset, eliminating the need for explicit motion annotation. Both components operate independently, allowing individual or combined control, and can generalize to open-domain scenarios. Extensive experiments demonstrate the superiority and effectiveness of our method. Project page and code are available at https://direct-a-video.github.io/.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_204",
    "authors": "Minyoung Kim, Rawan Alghofaili, Changyang Li, Lap-Fai Yu",
    "title": "Dragon's Path: Synthesizing User-centered Flying Creature Animation Paths for Outdoor Augmented Reality Experiences",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657397",
    "pdf_link": null,
    "abstract": "Advances in augmented reality promise to deliver highly immersive storytelling experiences by animating virtual characters naturally in the real world. However, creating such realistic animated content for viewing in augmented reality is non-trivial and challenging. In this paper, we present a novel approach to automatically generate user-centered flying creature animation paths for outdoor augmented reality experiences. Given a sequence of storyline actions, our approach finds suitable locations for the character to perform its actions via a location compatibility predictor trained with user preferences, synthesizing a corresponding animation path optimized with respect to the user’s perspective. We applied our approach to synthesize user-centered augmented reality experiences based on different storyline actions and environments. We also conducted user study experiments to validate the efficacy of our approach for synthesizing desirable augmented reality experiences.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_836",
    "authors": "Xiang Li, Lei Meng, Lei Wu, Manyi Li, Xiangxu Meng",
    "title": "DreamFont3D: Personalized Text-to-3D Artistic Font Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657476",
    "pdf_link": null,
    "abstract": "Text-to-3D artistic font generation aims to assist users for innovative and customized 3D font design by exploring novel concepts and styles. Despite of the advances in the text-to-3D tasks for general objects or scenes, the additional challenge of 3D font generation is to preserve the geometric structures of strokes in an appropriate extent, which determines the generation quality in terms of the recognizability and the local effect control of the 3D fonts. This paper presents a novel approach for text-to-3D artistic font generation, named DreamFont3D, which utilizes multi-view font masks and layout conditions to constrain the 3D font structure and local font effects. Specifically, to enhance the recognizability of 3D fonts, we propose the multi-view mask constraint (MC) to optimize the differentiable 3D representation while preserving the font structure. We also present a progressive mask weighting (MW) module to ensure a trade-off between the text-guided stylization of font effects and the mask-guided preservation of font structure. For precise control over local font effects, we design the multi-view attention modulation (AM) that guides the visual concepts to appear in specific regions according to the provided layout conditions. Compared with existing text-to-3D methods, DreamFont3D shows its own superiority in the consistency between font effects and text prompts, the recognizability, and the localization of font effects. Code and data at https://moonlight03.github.io/DreamFont3D/.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_444",
    "authors": "Yuqing Zhang, Yuan Liu, Zhiyu Xie, Lei Yang, Zhongyuan Liu, Mengzhou Yang, Runze Zhang, Qilong Kou, Cheng Lin, Wenping Wang, Xiaogang Jin",
    "title": "DreamMat: High-quality PBR Material Generation With Geometry- and Light-aware Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658170",
    "pdf_link": null,
    "abstract": "Recent advancements in 2D diffusion models allow appearance generation on untextured raw meshes. These methods create RGB textures by distilling a 2D diffusion model, which often contains unwanted baked-in shading effects and results in unrealistic rendering effects in the downstream applications. Generating Physically Based Rendering (PBR) materials instead of just RGB textures would be a promising solution. However, directly distilling the PBR material parameters from 2D diffusion models still suffers from incorrect material decomposition, such as baked-in shading effects in albedo. We introduce DreamMat, an innovative approach to resolve the aforementioned problem, to generate high-quality PBR materials from text descriptions. We find out that the main reason for the incorrect material distillation is that large-scale 2D diffusion models are only trained to generate final shading colors, resulting in insufficient constraints on material decomposition during distillation. To tackle this problem, we first finetune a new light-aware 2D diffusion model to condition on a given lighting environment and generate the shading results on this specific lighting condition. Then, by applying the same environment lights in the material distillation, DreamMat can generate high-quality PBR materials that are not only consistent with the given geometry but also free from any baked-in shading effects in albedo. Extensive experiments demonstrate that the materials produced through our methods exhibit greater visual appeal to users and achieve significantly superior rendering quality compared to baseline methods, which are preferable for downstream tasks such as game and film production.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_252",
    "authors": "Kai He, Kaixin Yao, Qixuan Zhang, Jingyi Yu, Lingjie Liu, Lan Xu",
    "title": "DressCode: Autoregressively Sewing and Generating Garments From Text Guidance",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658147",
    "pdf_link": null,
    "abstract": "Apparel's significant role in human appearance underscores the importance of garment digitalization for digital human creation. Recent advances in 3D content creation are pivotal for digital human creation. Nonetheless, garment generation from text guidance is still nascent. We introduce a text-driven 3D garment generation framework, DressCode, which aims to democratize design for novices and offer immense potential in fashion design, virtual try-on, and digital human creation. We first introduce SewingGPT, a GPT-based architecture integrating cross-attention with text-conditioned embedding to generate sewing patterns with text guidance. We then tailor a pre-trained Stable Diffusion to generate tile-based Physically-based Rendering (PBR) textures for the garments. By leveraging a large language model, our framework generates CG-friendly garments through natural language interaction. It also facilitates pattern completion and texture editing, streamlining the design process through user-friendly interaction. This framework fosters innovation by allowing creators to freely experiment with designs and incorporate unique elements into their work. With comprehensive evaluations and comparisons with other state-of-the-art methods, our method showcases superior quality and alignment with input prompts. User studies further validate our high-quality rendering results, highlighting its practical utility and potential in production settings. Our project page is https://IHe-KaiI.github.io/DressCode/.",
    "scholar_publication": "ACM Transactions on Graphics …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_982",
    "authors": "Sai Raj Kishore Perla, Yizhi Wang, Ali Mahdavi-Amiri, Hao Zhang",
    "title": "EASI-Tex: Edge-Aware Mesh Texturing from Single Image",
    "paper_url": "https://summit.sfu.ca/item/38211",
    "pdf_link": null,
    "abstract": "We present a novel approach for single-image guided 3D mesh texturing, which employs an image diffusion model with judicious conditioning to transfer textures from a single image to a given 3D shape in a consistent manner. We condition a pre-trained Stable Diffusion generator with edges describing the mesh through ControlNet, and features extracted from the input image using IP-Adapter to produce textures that respect the underlying geometry of the mesh and the input texture without any optimization or training. However, we also introduce Image Inversion, a novel technique to quickly personalize the diffusion model using a single-image, for cases where the pre-trained IP-Adapter falls short in capturing all the details from the input image faithfully. Experimental results showcase the efficiency and effectiveness of our single-image guided edge-aware 3D mesh texturing approach, EASI-Tex, in preserving the details of the input texture on diverse 3D objects while respecting their geometry.",
    "scholar_publication": "2024 - summit.sfu.ca"
  },
  {
    "paper_id": "papers_892",
    "authors": "Aryamaan Jain, Bedrich Benes, Guillaume Cordonnier",
    "title": "Efficient Debris-flow Simulation for Steep Terrain Erosion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658213",
    "pdf_link": null,
    "abstract": "Erosion simulation is a common approach used for generating and authoring mountainous terrains. While water is considered the primary erosion factor, its simulation fails to capture steep slopes near the ridges. In these low-drainage areas, erosion is often approximated with slope-reducing erosion, which yields unrealistically uniform slopes. However, geomorphology observed that another process dominates the low-drainage areas: erosion by debris flow, which is a mixture of mud and rocks triggered by strong climatic events. We propose a new method to capture the interactions between debris flow and fluvial erosion thanks to a new mathematical formulation for debris flow erosion derived from geomorphology and a unified GPU algorithm for erosion and deposition. In particular, we observe that sediment and debris deposition tend to intersect river paths, which motivates the design of a new, approximate flow routing algorithm on the GPU to estimate the water path out of these newly formed depressions. We demonstrate that debris flow carves distinct patterns in the form of erosive scars on steep slopes and cones of deposited debris competing with fluvial erosion downstream.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_643",
    "authors": "Marcelo Martins, Lucas Morais, Rafael Torchelsen, Luciana Nedel, Anderson Maciel",
    "title": "Efficient Position-based Deformable Colon Modeling for Endoscopic Procedures Simulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657454",
    "pdf_link": null,
    "abstract": "Current endoscopy simulators oversimplify navigation and interaction within tubular anatomical structures to maintain interactive frame rates, neglecting the intricate dynamics of permanent contact between the organ and the medical tool. Traditional algorithms fail to represent the complexities of long, slender, deformable tools like endoscopes and hollow organs, such as the human colon, and their interaction. In this paper, we address longstanding challenges hindering the realism of surgery simulators, explicitly focusing on these structures. One of the main components we introduce is a new model for the overall shape of the organ, which is challenging to retain due to the complex surroundings inside the abdomen. Our approach uses eXtended Position-Based Dynamics (XPBD) with a Cosserat rod constraint combined with a mesh of tetrahedrons to retain the colon’s shape. We also introduce a novel contact detection algorithm for tubular structures, allowing for real-time performance. This comprehensive representation captures global deformations and local features, significantly enhancing simulation fidelity compared to previous works. Results showcase that navigating the endoscope through our simulated colon seemingly mirrors real-world operations. Additionally, we use real-patient data to generate the colon model, resulting in a highly realistic virtual colonoscopy simulation. Integrating efficient simulation techniques with practical medical applications arguably advances surgery simulation realism.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_571",
    "authors": "Junwei Zhou, Duowen Chen, Molin Deng, Yitong Deng, Yuchen Sun, Sinan Wang, Shiying Xiong, Bo Zhu",
    "title": "Eulerian-Lagrangian Fluid Simulation on Particle Flow Maps",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658180",
    "pdf_link": null,
    "abstract": "We propose a novel Particle Flow Map (PFM) method to enable accurate long-range advection for incompressible fluid simulation. The foundation of our method is the observation that a particle trajectory generated in a forward simulation naturally embodies a perfect flow map. Centered on this concept, we have developed an Eulerian-Lagrangian framework comprising four essential components: Lagrangian particles for a natural and precise representation of bidirectional flow maps; a dual-scale map representation to accommodate the mapping of various flow quantities; a particle-to-grid interpolation scheme for accurate quantity transfer from particles to grid nodes; and a hybrid impulse-based solver to enforce incompressibility on the grid. The efficacy of PFM has been demonstrated through various simulation scenarios, highlighting the evolution of complex vortical structures and the details of turbulent flows. Notably, compared to NFM, PFM reduces computing time by up to 49 times and memory consumption by up to 41%, while enhancing vorticity preservation as evidenced in various tests like leapfrog, vortex tube, and turbulent flow.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1014",
    "authors": "Shijun Liang, Haofei Wang, Feng Lu",
    "title": "EyeIR: Single Eye Image Inverse Rendering in the Wild",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657506",
    "pdf_link": null,
    "abstract": "We propose a method to decompose a single eye region image in the wild into albedo, shading, specular, normal and illumination. This inverse rendering problem is particularly challenging due to inherent ambiguities and complex properties of the natural eye region. To address this problem, first we construct a synthetic eye region dataset with rich diversity. Then we propose a synthetic to real adaptation framework to leverage the supervision signals from synthetic data to guide the direction of self-supervised learning. We design region-aware self-supervised losses based on image formation and eye region intrinsic properties, which can refine each predicted component by mutual learning and reduce the artifacts caused by ambiguities of natural eye images. Particularly, we address the demanding problem of specularity removal in the eye region. We show high-quality inverse rendering results of our method and demonstrate its use for a number of applications.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_293",
    "authors": "Aviv Segall, Jing Ren, Amir Vaxman, Olga Sorkine-Hornung",
    "title": "Fabric Tessellation: Realizing Freeform Surfaces by Smocking",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658151",
    "pdf_link": null,
    "abstract": "We present a novel method for realizing freeform surfaces with pieces of flat fabric, where curvature is created by stitching together points on the fabric using a technique known as smocking. Smocking is renowned for producing intricate geometric textures with voluminous pleats. However, it has been mostly used to realize flat shapes or manually designed, limited classes of curved surfaces. Our method combines the computation of directional fields with continuous optimization of a Tangram graph in the plane, which together allow us to realize surfaces of arbitrary topology and curvature with smocking patterns of diverse symmetries. Given a target surface and the desired smocking pattern, our method outputs a corresponding 2D smocking pattern that can be fabricated by sewing specified points together. The resulting textile fabrication approximates the target shape and exhibits visually pleasing pleats. We validate our method through physical fabrication of various smocked examples.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_642",
    "authors": "Kenji Tojo, Ariel Shamir, Bernd Bickel, Nobuyuki Umetani",
    "title": "Fabricable 3D Wire Art",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657453",
    "pdf_link": null,
    "abstract": "… a number of wire-art shapes from various input types, including 3D models, images, text, and combinations thereof. In addition, we show examples of fabricated wire art using our 3D-…",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_969",
    "authors": "Nagabhushan Somraj, Kapil Choudhary, Sai Harsha Mupparaju, Rajiv Soundararajan",
    "title": "Factorized Motion Fields for Fast Sparse Input Dynamic View Synthesis",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657498",
    "pdf_link": null,
    "abstract": "Designing a 3D representation of a dynamic scene for fast optimization and rendering is a challenging task. While recent explicit representations enable fast learning and rendering of dynamic radiance fields, they require a dense set of input viewpoints. In this work, we focus on learning a fast representation for dynamic radiance fields with sparse input viewpoints. However, the optimization with sparse input is under-constrained and necessitates the use of motion priors to constrain the learning. Existing fast dynamic scene models do not explicitly model the motion, making them difficult to be constrained with motion priors. We design an explicit motion model as a factorized 4D representation that is fast and can exploit the spatio-temporal correlation of the motion field. We then introduce reliable flow priors including a combination of sparse flow priors across cameras and dense flow priors within cameras to regularize our motion model. Our model is fast, compact and achieves very good performance on popular multi-view dynamic scene datasets with sparse input viewpoints. The source code for our model can be found on our project page: https://nagabhushansn95.github.io/publications/2024/RF-DeRF.html.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_901",
    "authors": "Zeqi Gu, Ethan Yang, Abe Davis",
    "title": "Filter-Guided Diffusion for Controllable Image Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657489",
    "pdf_link": null,
    "abstract": "Recent advances in diffusion-based generative models have shown incredible promise for zero shot image-to-image translation and editing. Most of these approaches work by combining or replacing network-specific features used in the generation of new images with those taken from the inversion of some guide image. Methods of this type are considered the current state-of-the-art in training-free approaches, but have some notable limitations: they tend to be costly in runtime and memory, and often depend on deterministic sampling that limits variation in generated results. We propose Filter-Guided Diffusion (FGD), an alternative approach that leverages fast filtering operations during the diffusion process to support finer control over the strength and frequencies of guidance and can work with non-deterministic samplers to produce greater variety. With its efficiency, FGD can be sampled over multiple seeds and hyperparameters in less time than a single run of other SOTA methods to produce superior results based on structural and semantic metrics. We conduct extensive quantitative and qualitative experiments to evaluate the performance of FGD in translation tasks and also demonstrate its potential in localized editing when used with masks.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_525",
    "authors": "Juan Sebastian Montes Maestre, Yinwei Du, Ronan Hinchet, Stelian Coros, Bernhard Thomaszewski",
    "title": "FlexScale: Modeling and Characterization of Flexible Scaled Sheets",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658175",
    "pdf_link": null,
    "abstract": "We present a computational approach for modeling the mechanical behavior of flexible scaled sheet materials---3D-printed hard scales embedded in a soft substrate. Balancing strength and flexibility, these structured materials find applications in protective gear, soft robotics, and 3D-printed fashion. To unlock their full potential, however, we must unravel the complex relation between scale pattern and mechanical properties. To address this problem, we propose a contact-aware homogenization approach that distills native-level simulation data into a novel macromechanical model. This macro-model combines piecewise-quadratic uniaxial fits with polar interpolation using circular harmonics, allowing for efficient simulation of large-scale patterns. We apply our approach to explore the space of isohedral scale patterns, revealing a diverse range of anisotropic and nonlinear material behaviors. Through an extensive set of experiments, we show that our models reproduce various scale-level effects while offering good qualitative agreement with physical prototypes on the macro-level.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_323",
    "authors": "Setareh Cohan, Guy Tevet, Daniele Reda, Xue Bin Peng, Michiel van de Panne",
    "title": "Flexible Motion In-betweening With Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657414",
    "pdf_link": null,
    "abstract": "Motion in-betweening, a fundamental task in character animation, consists of generating motion sequences that plausibly interpolate user-provided keyframe constraints. It has long been recognized as a labor-intensive and challenging process. We investigate the potential of diffusion models in generating diverse human motions guided by keyframes. Unlike previous inbetweening methods, we propose a simple unified model capable of generating precise and diverse motions that conform to a flexible range of user-specified spatial constraints, as well as text conditioning. To this end, we propose Conditional Motion Diffusion In-betweening (CondMDI) which allows for arbitrary dense-or-sparse keyframe placement and partial keyframe constraints while generating high-quality motions that are diverse and coherent with the given keyframes. We evaluate the performance of CondMDI on the text-conditioned HumanML3D dataset and demonstrate the versatility and efficacy of diffusion models for keyframe in-betweening. We further explore the use of guidance and imputation-based approaches for inference-time keyframing and compare CondMDI against these methods.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_754",
    "authors": "Yixin Chen, David Levin, Timothy Langlois",
    "title": "Fluid Control With Laplacian Eigenfunctions",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657468",
    "pdf_link": null,
    "abstract": "Physics-based fluid control has long been a challenging problem in balancing efficiency and accuracy. We introduce a novel physics-based fluid control pipeline using Laplacian Eigenfluids. Utilizing the adjoint method with our provided analytical gradient expressions, the derivative computation of the control problem is efficient and easy to formulate. We demonstrate that our method is fast enough to support real-time fluid simulation, editing, control, and optimal animation generation. Our pipeline naturally supports multi-resolution and frequency control of fluid simulations. The effectiveness and efficiency of our fluid control pipeline are validated through a variety of 2D examples and comparisons.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_124",
    "authors": "Dario Seyb, Eugene d'Eon, Benedikt Bitterli, Wojciech Jarosz",
    "title": "From Microfacets to Participating Media: A Unified Theory of Light Transport With Stochastic Geometry",
    "paper_url": "https://par.nsf.gov/biblio/10519654",
    "pdf_link": null,
    "abstract": "Stochastic geometry models have enjoyed immense success in graphics for modeling interactions of light with complex phenomena such as participating media, rough surfaces, fibers, and more. Although each of these models operates on the same principle of replacing intricate geometry by a random process and deriving the average light transport across all instances thereof, they are each tailored to one specific application and are fundamentally distinct. Each type of stochastic geometry present in the scene is firmly encapsulated in its own appearance model, with its own statistics and light transport average, and no cross-talk between different models or deterministic and stochastic geometry is possible. In this paper, we derive a theory of light transport on stochastic implicit surfaces, a geometry model capable of expressing deterministic geometry, microfacet surfaces, participating media, and an exciting new continuum in between containing aggregate appearance, non-classical media, and more. Our model naturally supports spatial correlations, missing from most existing stochastic models. Our theory paves the way for tractable rendering of scenes in which all geometry is described by the same stochastic model, while leaving ample future work for developing efficient sampling and rendering algorithms.",
    "scholar_publication": "ACM transactions on graphics, 2024 - par.nsf.gov"
  },
  {
    "paper_id": "papers_325",
    "authors": "Dmitrii Petrov, Pradyumn Goyal, Vikas Thamizharasan, Vladimir Kim, Matheus Gadelha, Melinos Averkiou, Siddhartha Chaudhuri, Evangelos Kalogerakis",
    "title": "GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657415",
    "pdf_link": null,
    "abstract": "We introduce GEM3D 1 – a new deep, topology-aware generative model of 3D shapes. The key ingredient of our method is a neural skeleton-based representation encoding information on both shape topology and geometry. Through a denoising diffusion probabilistic model, our method first generates skeleton-based representations following the Medial Axis Transform (MAT), then generates surfaces through a skeleton-driven neural implicit formulation. The neural implicit takes into account the topological and geometric information stored in the generated skeleton representations to yield surfaces that are more topologically and geometrically accurate compared to previous neural field formulations. We discuss applications of our method in shape synthesis and point cloud reconstruction tasks, and evaluate our method both qualitatively and quantitatively. We demonstrate significantly more faithful surface reconstruction and diverse shape generation results compared to the state-of-the-art, also involving challenging scenarios of reconstructing and synthesizing structurally complex, high-genus shape surfaces from Thingi10K and ShapeNet.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_102",
    "authors": "Kemeng Huang, Floyd Chitalu, Huancheng Lin, Taku Komura, Kemeng Huang",
    "title": "GIPC: Fast and stable Gauss-Newton optimization of IPC barrier energy",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3643028",
    "pdf_link": null,
    "abstract": "Barrier functions are crucial for maintaining an intersection- and inversion-free simulation trajectory but existing methods, which directly use distance can restrict implementation design and performance. We present an approach to rewriting the barrier function for arriving at an efficient and robust approximation of its Hessian. The key idea is to formulate a simplicial geometric measure of contact using mesh boundary elements, from which analytic eigensystems are derived and enhanced with filtering and stiffening terms that ensure robustness with respect to the convergence of a Project-Newton solver. A further advantage of our rewriting of the barrier function is that it naturally caters to the notorious case of nearly parallel edge-edge contacts for which we also present a novel analytic eigensystem. Our approach is thus well suited for standard second-order unconstrained optimization strategies for resolving contacts, minimizing nonlinear nonconvex functions where the Hessian may be indefinite. The efficiency of our eigensystems alone yields a 3× speedup over the standard Incremental Potential Contact (IPC) barrier formulation. We further apply our analytic proxy eigensystems to produce an entirely GPU-based implementation of IPC with significant further acceleration.",
    "scholar_publication": "ACM Transactions on Graphics, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_343",
    "authors": "Shen Ciao, Zhongyue Guan, Qianxi Liu, Li-Yi Wei, Zeyu Wang",
    "title": "GPU-accelerated Rendering of Vector Brush Strokes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657418",
    "pdf_link": null,
    "abstract": "This paper introduces novel GPU-based rendering techniques for digital painting and animation to bridge the gap between raster and vector stroke representations. We propose efficient rendering methods for vanilla, stamp, and airbrush strokes that integrate the expressiveness of raster-based textures with the ease of real-time editing. Based on our stroke representation, we implement an open-source prototype drawing system with a vector fill feature, demonstrating that our techniques can enhance the expressiveness, efficiency, and edibility of digital drawing. Our work can serve as a foundation for future research on vector-based and GPU-accelerated rendering techniques in industrial-level brush engines.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_341",
    "authors": "Boming Zhao, Yuan Li, Ziyu Sun, Lin Zeng, Yujun Shen, Rui Ma, Yinda Zhang, Hujun Bao, Zhaopeng Cui",
    "title": "GaussianPrediction: Dynamic 3D Gaussian Prediction for Motion Extrapolation and Free View Synthesis",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657417",
    "pdf_link": null,
    "abstract": "Forecasting future scenarios in dynamic environments is essential for intelligent decision-making and navigation, a challenge yet to be fully realized in computer vision and robotics. Traditional approaches like video prediction and novel-view synthesis either lack the ability to forecast from arbitrary viewpoints or to predict temporal dynamics. In this paper, we introduce GaussianPrediction, a novel framework that empowers 3D Gaussian representations with dynamic scene modeling and future scenario synthesis in dynamic environments. GaussianPrediction can forecast future states from any viewpoint, using video observations of dynamic scenes. To this end, we first propose a 3D Gaussian canonical space with deformation modeling to capture the appearance and geometry of dynamic scenes, and integrate the lifecycle property into Gaussians for irreversible deformations. To make the prediction feasible and efficient, a concentric motion distillation approach is developed by distilling the scene motion with key points. Finally, a Graph Convolutional Network is employed to predict the motions of key points, enabling the rendering of photorealistic images of future scenarios. Our framework shows outstanding performance on both synthetic and real-world datasets, demonstrating its efficacy in predicting and rendering future environments. Code is available on the project webpage: https://zju3dv.github.io/gaussian-prediction.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_626",
    "authors": "Noam Aigerman, Thibault Groueix",
    "title": "Generative Escher Meshes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657452",
    "pdf_link": null,
    "abstract": "… generative method for producing perfectly-repeating, periodic, tile-able 2D imagery, such as the one seen on floors, mosaics, ceramics, and the work of MC Escher… a 2D mesh, yielding a …",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_378",
    "authors": "Yousuf Soliman, Marcel Padilla, Oliver Gross, Felix Knöppel, Ulrich Pinkall, Peter Schröder",
    "title": "Going With the Flow",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658164",
    "pdf_link": null,
    "abstract": "… We give the shape-changing body in a flow its full second-order treatment. For dissipative loss … The Lagrangian of a body going with the flow is the total kinetic energy of the fluid body …",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_119",
    "authors": "Jia-Mu Sun, Jie Yang, Kaichun Mo, Yukun Lai, Leonidas Guibas, Lin Gao, Jia-Mu Sun",
    "title": "HAISOR: Human-Aware Indoor Scene Optimization via Deep Reinforcement Learning",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3632947",
    "pdf_link": null,
    "abstract": "3D scene synthesis facilitates and benefits many real-world applications. Most scene generators focus on making indoor scenes plausible via learning from training data and leveraging extra constraints such as adjacency and symmetry. Although the generated 3D scenes are mostly plausible with visually realistic layouts, they can be functionally unsuitable for human users to navigate and interact with furniture. Our key observation is that human activity plays a critical role and sufficient free space is essential for human-scene interactions. This is exactly where many existing synthesized scenes fail—the seemingly correct layouts are often not fit for living. To tackle this, we present a human-aware optimization framework Haisor for 3D indoor scene arrangement via reinforcement learning, which aims to find an action sequence to optimize the indoor scene layout automatically. Based on the hierarchical scene graph representation, an optimal action sequence is predicted and performed via Deep Q-Learning with Monte Carlo Tree Search (MCTS), where MCTS is our key feature to search for the optimal solution in long-term sequences and large action space. Multiple human-aware rewards are designed as our core criteria of human-scene interaction, aiming to identify the next smart action by leveraging powerful reinforcement learning. Our framework is optimized end-to-end by giving the indoor scenes with part-level furniture layout including part mobility information. Furthermore, our methodology is extensible and allows utilizing different reward designs to achieve personalized indoor scene synthesis. Extensive experiments demonstrate that our approach optimizes the layout of 3D indoor scenes in a human-aware manner, which is more realistic and plausible than original state-of-the-art generator results, and our approach produces superior smart actions, outperforming alternative baselines.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_109",
    "authors": "Kartik Teotia, Byrasandra Ramalinga Reddy Mallikarjun, Xingang Pan, Hyeongwoo Kim, Pablo Garrido, Mohamed Elgharib, Christian Theobalt, Kartik Teotia",
    "title": "HQ3DAvatar: High Quality Implicit 3D Head Avatar",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3649889",
    "pdf_link": null,
    "abstract": "Multi-view volumetric rendering techniques have recently shown great potential in modeling and synthesizing high-quality head avatars. A common approach to capture full head dynamic performances is to track the underlying geometry using a mesh-based template or 3D cube-based graphics primitives. While these model-based approaches achieve promising results, they often fail to learn complex geometric details such as the mouth interior, hair, and topological changes over time. This article presents a novel approach to building highly photorealistic digital head avatars. Our method learns a canonical space via an implicit function parameterized by a neural network. It leverages multiresolution hash encoding in the learned feature space, allowing for high quality, faster training, and high-resolution rendering. At test time, our method is driven by a monocular RGB video. Here, an image encoder extracts face-specific features that also condition the learnable canonical space. This encourages deformation-dependent texture variations during training. We also propose a novel optical flow-based loss that ensures correspondences in the learned canonical space, thus encouraging artifact-free and temporally consistent renderings. We show results on challenging facial expressions and show free-viewpoint renderings at interactive real-time rates for a resolution of 480x270. Our method outperforms related approaches both visually and numerically. We will release our multiple-identity dataset to encourage further research.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1010",
    "authors": "Haoyu Hu, Xin-yu Yi, Zhe Cao, Jun-Hai Yong, Feng Xu",
    "title": "Hand-Object Interaction Controller (HOIC): Deep Reinforcement Learning for Reconstructing Interactions With Physics",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657505",
    "pdf_link": null,
    "abstract": "Hand manipulating objects is an important interaction motion in our daily activities. We faithfully reconstruct this motion with a single RGBD camera by a novel deep reinforcement learning method to leverage physics. Firstly, we propose object compensation control which establishes direct object control to make the network training more stable. Meanwhile, by leveraging the compensation force and torque, we seamlessly upgrade the simple point contact model to a more physical-plausible surface contact model, further improving the reconstruction accuracy and physical correctness. Experiments indicate that without involving any heuristic physical rules, this work still successfully involves physics in the reconstruction of hand-object interactions which are complex motions hard to imitate with deep reinforcement learning. Our code and data are available at https://github.com/hu-hy17/HOIC.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1092",
    "authors": "Hongyu Liu, Xuan Wang, Ziyu Wan, Yujun Shen, Yibing Song, Jing Liao, Qifeng Chen",
    "title": "HeadArtist: Text-conditioned 3D Head Generation With Self Score Distillation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657512",
    "pdf_link": null,
    "abstract": "We present HeadArtist for 3D head generation following human-language descriptions. With a landmark-guided ControlNet serving as a generative prior, we come up with an efficient pipeline that optimizes a parameterized 3D head model under the supervision of the prior distillation itself. We call such a process self score distillation (SSD). In detail, given a sampled camera pose, we first render an image and its corresponding landmarks from the head model, and add some particular level of noise onto the image. The noisy image, landmarks, and text condition are then fed into a frozen ControlNet twice for noise prediction. We conduct two predictions via the same ControlNet structure but with different classifier-free guidance (CFG) weights. The difference between these two predicted results directs how the rendered image can better match the text of interest. Experimental results show that our approach produces high-quality 3D head sculptures with rich geometry and photo-realistic appearance, which significantly outperforms state-of-the-art methods. We also show that our pipeline supports editing operations on the generated heads, including both geometry deformation and appearance change. Project page:https://kumapowerliu.github.io/HeadArtist.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_589",
    "authors": "Pinxuan Dai, Jiamin Xu, Wenxiang Xie, Xinguo Liu, Huamin Wang, Weiwei Xu",
    "title": "High-quality Surface Reconstruction Using Gaussian Surfels",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657441",
    "pdf_link": null,
    "abstract": "We propose a novel point-based representation, Gaussian surfels, to combine the advantages of the flexible optimization procedure in 3D Gaussian points and the surface alignment property of surfels. This is achieved by directly setting the z-scale of 3D Gaussian points to 0, effectively flattening the original 3D ellipsoid into a 2D ellipse. Such a design provides clear guidance to the optimizer. By treating the local z-axis as the normal direction, it greatly improves optimization stability and surface alignment. While the derivatives to the local z-axis computed from the covariance matrix are zero in this setting, we design a self-supervised normal-depth consistency loss to remedy this issue. Monocular normal priors and foreground masks are incorporated to enhance the quality of the reconstruction, mitigating issues related to highlights and background. We propose a volumetric cutting method to aggregate the information of Gaussian surfels so as to remove erroneous points in depth maps generated by alpha blending. Finally, we apply screened Poisson reconstruction method to the fused depth maps to extract the surface mesh. Experimental results show that our method demonstrates superior performance in surface reconstruction compared to state-of-the-art neural volume rendering and point-based rendering methods.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_410",
    "authors": "Dongyeon Kim, Seung-Woo Nam, Suyeon Choi, Jong-Mo Seo, Gordon Wetzstein, Yoonchan Jeong",
    "title": "Holographic Parallax Improves 3D Perceptual Realism",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658168",
    "pdf_link": null,
    "abstract": "Holographic near-eye displays are a promising technology to solve long-standing challenges in virtual and augmented reality display systems. Over the last few years, many different computer-generated holography (CGH) algorithms have been proposed that are supervised by different types of target content, such as 2.5D RGB-depth maps, 3D focal stacks, and 4D light fields. It is unclear, however, what the perceptual implications are of the choice of algorithm and target content type. In this work, we build a perceptual testbed of a full-color, high-quality holographic near-eye display. Under natural viewing conditions, we examine the effects of various CGH supervision formats and conduct user studies to assess their perceptual impacts on 3D realism. Our results indicate that CGH algorithms designed for specific viewpoints exhibit noticeable deficiencies in achieving 3D realism. In contrast, holograms incorporating parallax cues consistently outperform other formats across different viewing conditions, including the center of the eyebox. This finding is particularly interesting and suggests that the inclusion of parallax cues in CGH rendering plays a crucial role in enhancing the overall quality of the holographic experience. This work represents an initial stride towards delivering a perceptually realistic 3D experience with holographic near-eye displays.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_249",
    "authors": "Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Pengfei Wan, Di Zhang, Yufan Liu, Weiming Hu, Zhengjun Zha, Haibin Huang, Chongyang Ma",
    "title": "I2V-Adapter: A General Image-to-Video Adapter for Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657407",
    "pdf_link": null,
    "abstract": "Text-guided image-to-video (I2V) generation aims to generate a coherent video that preserves the identity of the input image and semantically aligns with the input prompt. Existing methods typically augment pretrained text-to-video (T2V) models by either concatenating the image with noised video frames channel-wise before being fed into the model or injecting the image embedding produced by pretrained image encoders in cross-attention modules. However, the former approach often necessitates altering the …",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_116",
    "authors": "Stefan Rhys Jeske, Lukas Westhofen, Fabian Löschner, José Antonio Fernández-Fernández, Jan Bender, Stefan Rhys Jeske",
    "title": "Implicit Surface Tension for SPH Fluid Simulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3631936",
    "pdf_link": null,
    "abstract": "The numerical simulation of surface tension is an active area of research in many different fields of application and has been attempted using a wide range of methods. Our contribution is the derivation and implementation of an implicit cohesion force based approach for the simulation of surface tension effects using the Smoothed Particle Hydrodynamics (SPH) method. We define a continuous formulation inspired by the properties of surface tension at the molecular scale which is spatially discretized using SPH. An adapted variant of the linearized backward Euler method is used for time discretization, which we also strongly couple with an implicit viscosity model. Finally, we extend our formulation with adhesion forces for interfaces with rigid objects. Existing SPH approaches for surface tension in computer graphics are mostly based on explicit time integration, thereby lacking in stability for challenging settings. We compare our implicit surface tension method to these approaches and further evaluate our model on a wider variety of complex scenarios, showcasing its efficacy and versatility. Among others, these include but are not limited to simulations of a water crown, a dripping faucet, and a droplet toy.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_573",
    "authors": "Jingping Wang, Tingrui Zhang, Qixuan Zhang, Chuxiao Zeng, Jingyi Yu, Chao Xu, Lan Xu, Fei Gao",
    "title": "Implicit Swept Volume SDF: Enabling Continuous Collision-free Trajectory Generation for Arbitrary Shapes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658181",
    "pdf_link": null,
    "abstract": "In the field of trajectory generation for objects, ensuring continuous collision-free motion remains a huge challenge, especially for non-convex geometries and complex environments. Previous methods either oversimplify object shapes, which results in a sacrifice of feasible space or rely on discrete sampling, which suffers from the \"tunnel effect\". To address these limitations, we propose a novel hierarchical trajectory generation pipeline, which utilizes the Swept Volume Signed Distance Field (SVSDF) to guide trajectory optimization for Continuous Collision Avoidance (CCA). Our interdisciplinary approach, blending techniques from graphics and robotics, exhibits outstanding effectiveness in solving this problem. We formulate the computation of the SVSDF as a Generalized Semi-Infinite Programming model, and we solve for the numerical solutions at query points implicitly, thereby eliminating the need for explicit reconstruction of the surface. Our algorithm has been validated in a variety of complex scenarios and applies to robots of various dynamics, including both rigid and deformable shapes. It demonstrates exceptional universality and superior CCA performance compared to typical algorithms. The code will be released at https://github.com/ZJU-FAST-Lab/Implicit-SVSDF-Planner for the benefit of the community.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_108",
    "authors": "Yash Belhe, Bing Xu, Sai Praveen Bangaru, Ravi Ramamoorthi, Tzu-Mao Li, Yash Belhe",
    "title": "Importance Sampling BRDF Derivatives",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3648611",
    "pdf_link": null,
    "abstract": "… We propose effective importance sampling of derivatives of not only anisotropic GGX and … We propose new importance sampling techniques for sampling derivatives of BRDFs, and they …",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_120",
    "authors": "Dmitry Sokolov, Vladimir Garanzha, Igor Kaporin, Liudmila Kudryavtseva, François Protais, Dmitry Sokolov",
    "title": "In the Quest for Scale-Optimal Mappings",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3627102",
    "pdf_link": null,
    "abstract": "Optimal mapping is one of the longest-standing problems in computational mathematics. It is natural to measure the relative curve length error under map to assess its quality. The maximum of such error is called the quasi-isometry constant, and its minimization is a nontrivial max-norm optimization problem. We present a physics-based quasi-isometric stiffening (QIS) algorithm for the max-norm minimization of hyperelastic distortion.QIS perfectly equidistributes distortion over the entire domain for the ground-truth test (unit hemisphere flattening) and, when it is not possible, tends to create zones where all cells have the same distortion. Such zones correspond to fragments of elastic material that became rigid under stiffening, reaching the deformation limit. As such, maps built by QIS are related to the de Boor equidistribution principle, which asks for an integral of a certain error indicator function to be the same over each mesh cell.Under certain assumptions on the minimization toolbox, we prove that our method can build, in a finite number of steps, a deformation whose maximum distortion is arbitrarily close to the (unknown) minimum. We performed extensive testing: on more than 10,000 domains QIS was reliably better than the competing methods. In summary, we reliably build 2D and 3D mesh deformations with the smallest known distortion estimates for very stiff problems.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_218",
    "authors": "Yi Shi, Jingbo Wang, Xuekun Jiang, Bingkun Lin, Bo Dai, Xue Bin Peng",
    "title": "Interactive Character Control With Auto-regressive Motion Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658140",
    "pdf_link": null,
    "abstract": "Real-time character control is an essential component for interactive experiences, with a broad range of applications, including physics simulations, video games, and virtual reality. The success of diffusion models for image synthesis has led to the use of these models for motion synthesis. However, the majority of these motion diffusion models are primarily designed for offline applications, where space-time models are used to synthesize an entire sequence of frames simultaneously with a pre-specified length. To enable real-time motion synthesis with diffusion model that allows time-varying controls, we propose A-MDM (Auto-regressive Motion Diffusion Model). Our conditional diffusion model takes an initial pose as input, and auto-regressively generates successive motion frames conditioned on the previous frame. Despite its streamlined network architecture, which uses simple MLPs, our framework is capable of generating diverse, long-horizon, and high-fidelity motion sequences. Furthermore, we introduce a suite of techniques for incorporating interactive controls into A-MDM, such as task-oriented sampling, in-painting, and hierarchical reinforcement learning (See Figure 1). These techniques enable a pre-trained A-MDM to be efficiently adapted for a variety of new downstream tasks. We conduct a comprehensive suite of experiments to demonstrate the effectiveness of A-MDM, and compare its performance against state-of-the-art auto-regressive methods.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1074",
    "authors": "Michael A. Hopkins, Georg Wiedebach, Kyle Cesare, Jared Bishop, Espen Knoop, Moritz Bächer",
    "title": "Interactive Design of Stylized Walking Gaits for Robotic Characters",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658227",
    "pdf_link": null,
    "abstract": "Procedural animation has seen widespread use in the design of expressive walking gaits for virtual characters. While similar tools could breathe life into robotic characters, existing techniques are largely unaware of the kinematic and dynamic constraints imposed by physical robots. In this paper, we propose a system for the artist-directed authoring of stylized bipedal walking gaits, tailored for execution on robotic characters. The artist interfaces with an interactive editing tool that generates the desired character motion in realtime, either on the physical or simulated robot, using a model-based control stack. Each walking style is encoded as a set of sample parameters which are translated into whole-body reference trajectories using the proposed procedural animation technique. In order to generalize the stylized gait over a continuous range of input velocities, we employ a phase-space blending strategy that interpolates a set of example walk cycles authored by the animator while preserving contact constraints. To demonstrate the utility of our approach, we animate gaits for a custom, free-walking robotic character, and show, with two additional in-simulation examples, how our procedural animation technique generalizes to bipeds with different degrees of freedom, proportions, and mass distributions.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_829",
    "authors": "Bosheng Li, Nikolas Schwarz, Wojtek Palubicki, Soeren Pirk, Bedrich Benes",
    "title": "Interactive Invigoration: Volumetric Modeling of Trees With Strands",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658206",
    "pdf_link": null,
    "abstract": "Generating realistic models of trees and plants is a complex problem because of the vast variety of shapes trees can form. Procedural modeling algorithms are popular for defining branching structures and steadily increasing their expressive power by considering more biological findings. Most existing methods focus on defining the branching structure of trees based on skeletal graphs, while the surface mesh of branches is most commonly defined as simple cylinders. One critical open problem is defining and controlling the complex details observed in real trees. This paper aims to advance tree modeling by proposing a strand-based volumetric representation for tree models. Strands are fixed-size volumetric pipes that define the branching structure. By leveraging strands, our approach captures the lateral development of trees. We combine the strands with a novel branch development formulation that allows us to locally inject vigor and reshape the tree model. Moreover, we define a set of editing operators for tree primary and lateral development that enables users to interactively generate complex tree models with unprecedented detail with minimal effort.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_740",
    "authors": "Alexa Schor, Theodore Kim",
    "title": "Into the Portal: Directable Fractal Self-Similarity",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657466",
    "pdf_link": null,
    "abstract": "We present a novel, directable method for introducing fractal self-similarity into arbitrary shapes. Our method allows a user to directly specify the locations of self-similarities in a Julia set, and is general enough to reproduce other well-known fractals such as the Koch snowflake. Ours is the first algorithm to enable this level of general artistic control while also maintaining the character of the original fractal shape. We introduce the notion of placing “portals” in the iteration space of a dynamical system, bridging the aesthetics of iterated maps with the fine-grained control of iterated function systems (IFS). Our method is effective in both 2D and 3D.",
    "scholar_publication": "ACM SIGGRAPH 2024 conference papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_110",
    "authors": "Christian Careaga, Yagiz Aksoy, Christian Careaga",
    "title": "Intrinsic Image Decomposition via Ordinal Shading",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3630750",
    "pdf_link": null,
    "abstract": "Intrinsic decomposition is a fundamental mid-level vision problem that plays a crucial role in various inverse rendering and computational photography pipelines. Generating highly accurate intrinsic decompositions is an inherently under-constrained task that requires precisely estimating continuous-valued shading and albedo. In this work, we achieve high-resolution intrinsic decomposition by breaking the problem into two parts. First, we present a dense ordinal shading formulation using a shift- and scale-invariant loss in order to estimate ordinal shading cues without restricting the predictions to obey the intrinsic model. We then combine low- and high-resolution ordinal estimations using a second network to generate a shading estimate with both global coherency and local details. We encourage the model to learn an accurate decomposition by computing losses on the estimated shading as well as the albedo implied by the intrinsic model. We develop a straightforward method for generating dense pseudo ground truth using our model’s predictions and multi-illumination data, enabling generalization to in-the-wild imagery. We present exhaustive qualitative and quantitative analysis of our predicted intrinsic components against state-of-the-art methods. Finally, we demonstrate the real-world applicability of our estimations by performing otherwise difficult editing tasks such as recoloring and relighting.",
    "scholar_publication": "ACM Transactions on Graphics, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_795",
    "authors": "Jundan Luo, Duygu Ceylan, Jae Shin Yoon, Nanxuan Zhao, Julien Philip, Anna Frühstück, Wenbin Li, Christian Richardt, Tuanfeng Wang",
    "title": "IntrinsicDiffusion: Joint Intrinsic Layers From Latent Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657472",
    "pdf_link": null,
    "abstract": "Reasoning about the intrinsic properties of an image, such as albedo, illumination, and surface geometry, is a long-standing problem with many applications in image editing and compositing. Existing solutions to this ill-posed problem either heavily rely on manually designed priors or learn priors from limited datasets that lack diversity. Hence, they fall short in generalizing to in-the-wild test scenarios. In this paper, we show that a large-scale text-to-image generation model trained on a massive amount of visual data can implicitly learn intrinsic image priors. In particular, we introduce a novel conditioning mechanism built on top of a pre-trained foundational image generation model to jointly predict multiple intrinsic modalities from an input image. We demonstrate that predicting different modalities in a collaborative manner improves the overall quality. This design also enables mixing datasets with annotations of only a subset of the modalities during training, contributing to the generalizability of our approach. Our method achieves state-of-the-art performance in intrinsic image decomposition, both qualitatively and quantitatively. We also demonstrate downstream image editing applications, such as relighting and retexturing.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_859",
    "authors": "Xiaochen Zhao, Jingxiang Sun, Lizhen Wang, Jinli Suo, Yebin Liu",
    "title": "InvertAvatar: Incremental GAN Inversion for Generalized Head Avatars",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657478",
    "pdf_link": null,
    "abstract": "While high fidelity and efficiency are central to the creation of digital head avatars, recent methods relying on 2D or 3D generative models often experience limitations such as shape distortion, expression inaccuracy, and identity flickering. Additionally, existing one-shot inversion techniques fail to fully leverage multiple input images for detailed feature extraction. We propose a novel framework, Incremental 3D GAN Inversion, that enhances avatar reconstruction performance using an algorithm designed to increase the fidelity from multiple frames, resulting in improved reconstruction quality proportional to frame count. Our method introduces a unique animatable 3D GAN prior with two crucial modifications for enhanced expression controllability alongside an innovative neural texture encoder that categorizes texture feature spaces based on UV parameterization. Differentiating from traditional techniques, our architecture emphasizes pixel-aligned image-to-image translation, mitigating the need to learn correspondences between observation and canonical spaces. Furthermore, we incorporate ConvGRU-based recurrent networks for temporal data aggregation from multiple frames, boosting geometry and texture detail reconstruction. The proposed paradigm demonstrates state-of-the-art performance on one-shot and few-shot avatar animation tasks. The code will be available at https://github.com/XChenZ/invertAvatar.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_607",
    "authors": "Purvi Goel, Kuan-Chieh Wang, C. Karen Liu, Kayvon Fatahalian",
    "title": "Iterative Motion Editing With Natural Language",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657447",
    "pdf_link": null,
    "abstract": "Text-to-motion diffusion models can generate realistic animations from text prompts, but do not support fine-grained motion editing controls. In this paper, we present a method for using natural language to iteratively specify local edits to existing character animations, a task that is common in most computer animation workflows. Our key idea is to represent a space of motion edits using a set of kinematic motion editing operators (MEOs) whose effects on the source motion is well-aligned with user expectations. We provide an algorithm that leverages pre-existing language models to translate textual descriptions of motion edits into source code for programs that define and execute sequences of MEOs on a source animation. We execute MEOs by first translating them into keyframe constraints, and then use diffusion-based motion models to generate output motions that respect these constraints. Through a user study and quantitative evaluation, we demonstrate that our system can perform motion edits that respect the animator’s editing intent, remain faithful to the original animation (it edits the original animation, but does not dramatically change it), and yield realistic character animation results.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_104",
    "authors": "Haoran Mo, Chengying Gao, Ruomei Wang, Haoran Mo",
    "title": "Joint Stroke Tracing and Correspondence for 2D Animation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3649890",
    "pdf_link": null,
    "abstract": "To alleviate human labor in redrawing keyframes with ordered vector strokes for automatic inbetweening, we for the first time propose a joint stroke tracing and correspondence approach. Given consecutive raster keyframes along with a single vector image of the starting frame as a guidance, the approach generates vector drawings for the remaining keyframes while ensuring one-to-one stroke correspondence. Our framework trained on clean line drawings generalizes to rough sketches, and the generated results can be imported into inbetweening systems to produce inbetween sequences. Hence, the method is compatible with standard 2D animation workflow. An adaptive spatial transformation module (ASTM) is introduced to handle non-rigid motions and stroke distortion. We collect a dataset for training with 10k+ pairs of raster frames and their vector drawings with stroke correspondence. Comprehensive validations on real clean and rough animated frames manifest the effectiveness of our method and superiority to existing methods.",
    "scholar_publication": "ACM Transactions on Graphics, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_563",
    "authors": "Wei Li, Kui Wu, Mathieu Desbrun",
    "title": "Kinetic Simulation of Turbulent Multifluid Flows",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658178",
    "pdf_link": null,
    "abstract": "Despite its visual appeal, the simulation of separated multiphase flows (i.e., streams of fluids separated by interfaces) faces numerous challenges in accurately reproducing complex behaviors such as guggling, wetting, or bubbling. These difficulties are especially pronounced for high Reynolds numbers and large density variations between fluids, most likely explaining why they have received comparatively little attention in Computer Graphics compared to single- or two-phase flows. In this paper, we present a full LBM solver for multifluid simulation. We derive a conservative phase field model with which the spatial presence of each fluid or phase is encoded to allow for the simulation of miscible, immiscible and even partially-miscible fluids, while the temporal evolution of the phases is performed using a D3Q7 lattice-Boltzmann discretization. The velocity field, handled through the recent high-order moment-encoded LBM (HOME-LBM) framework to minimize its memory footprint, is simulated via a velocity-based distribution stored on a D3Q27 or D3Q19 discretization to offer accuracy and stability to large density ratios even in turbulent scenarios, while coupling with the phases through pressure, viscosity, and interfacial forces is achieved by leveraging the diffuse encoding of interfaces. The resulting solver addresses a number of limitations of kinetic methods in both computational fluid dynamics and computer graphics: it offers a fast, accurate, and low-memory fluid solver enabling efficient turbulent multiphase simulations free of the typical oscillatory pressure behavior near boundaries. We present several numerical benchmarks, examples and comparisons of multiphase flows to demonstrate our solver's visual complexity, accuracy, and realism.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_415",
    "authors": "Haowen Sun, Ruikun Zheng, Haibin Huang, Chongyang Ma, Hui Huang, Ruizhen Hu",
    "title": "LGTM: Local-to-Global Text-driven Human Motion Diffusion Model",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657422",
    "pdf_link": null,
    "abstract": "In this paper, we introduce LGTM, a novel Local-to-Global pipeline for Text-to-Motion generation. LGTM utilizes a diffusion-based architecture and aims to address the challenge of accurately translating textual descriptions into semantically coherent human motion in computer animation. Specifically, traditional methods often struggle with semantic discrepancies, particularly in aligning specific motions to the correct body parts. To address this issue, we propose a two-stage pipeline to overcome this challenge: it first employs large language models (LLMs) to decompose global motion descriptions into part-specific narratives, which are then processed by independent body-part motion encoders to ensure precise local semantic alignment. Finally, an attention-based full-body optimizer refines the motion generation results and guarantees the overall coherence. Our experiments demonstrate that LGTM gains significant improvements in generating locally accurate, semantically-aligned human motion, marking a notable advancement in text-to-motion applications. Code and data for this paper are available at https://github.com/L-Sun/LGTM",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1263",
    "authors": "Shariq Bhat, Niloy Mitra, Peter Wonka",
    "title": "LOOSECONTROL: Lifting ControlNet for Generalized Depth Conditioning",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657525",
    "pdf_link": null,
    "abstract": "We present LooseControl to allow generalized depth conditioning for diffusion-based image generation. ControlNet, the SOTA for depth conditioned image generation, produces remarkable results but relies on having access to detailed depth maps for guidance. Creating such exact depth maps, in many scenarios, is challenging. This paper introduces a generalized version of depth conditioning that enables new content creation workflows. Specifically, we allow (C1) scene boundary control for loosely specifying scenes with only …",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1106",
    "authors": "Zhiqi Li, Barnabás Börcsök, Duowen Chen, Yutong Sun, Bo Zhu, Greg Turk",
    "title": "Lagrangian Covector Fluid With Free Surface",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657514",
    "pdf_link": null,
    "abstract": "This paper introduces a novel Lagrangian fluid solver based on covector flow maps. We aim to address the challenges of establishing a robust flow-map solver for incompressible fluids under complex boundary conditions. Our key idea is to use particle trajectories to establish precise flow maps and tailor path integrals of physical quantities along these trajectories to reformulate the Poisson problem during the projection step. We devise a decoupling mechanism based on path-integral identities from flow-map theory. This mechanism integrates long-range flow maps for the main fluid body into a short-range projection framework, ensuring a robust treatment of free boundaries. We show that our method can effectively transform a long-range projection problem with integral boundaries into a Poisson problem with standard boundary conditions — specifically, zero Dirichlet on the free surface and zero Neumann on solid boundaries. This transformation significantly enhances robustness and accuracy, extending the applicability of flow-map methods to complex free-surface problems.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_122",
    "authors": "Jae Joong Lee, Bosheng Li, Bedrich Benes, Jae Joong Lee",
    "title": "Latent L-systems: Transformer-based Tree Generator",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3627101",
    "pdf_link": null,
    "abstract": "We show how a Transformer can encode hierarchical tree-like string structures by introducing a new deep learning-based framework for generating 3D biological tree models represented as Lindenmayer system (L-system) strings. L-systems are string-rewriting procedural systems that encode tree topology and geometry. L-systems are efficient, but creating the production rules is one of the most critical problems precluding their usage in practice. We substitute the procedural rules creation with a deep neural model. Instead of writing the rules, we train a deep neural model that produces the output strings. We train our model on 155k tree geometries that are encoded as L-strings, de-parameterized, and converted to a hierarchy of linear sequences corresponding to branches. An end-to-end deep learning model with an attention mechanism then learns the distributions of geometric operations and branches from the input, effectively replacing the L-system rewriting rule generation. The trained deep model generates new L-strings representing 3D tree models in the same way L-systems do by providing the starting string. Our model allows for the generation of a wide variety of new trees, and the deep model agrees with the input by 93.7% in branching angles, 97.2% in branch lengths, and 92.3% in an extracted list of geometric features. We also validate the generated trees using perceptual metrics showing 97% agreement with input geometric models.",
    "scholar_publication": "ACM Transactions on Graphics, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_999",
    "authors": "Siyou Lin, Zhe Li, Zhaoqi Su, Zerong Zheng, Hongwen Zhang, Yebin Liu",
    "title": "LayGA: Layered Gaussian Avatars for Animatable Clothing Transfer",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657501",
    "pdf_link": null,
    "abstract": "Animatable clothing transfer, aiming at dressing and animating garments across characters, is a challenging problem. Most human avatar works entangle the representations of the human body and clothing together, which leads to difficulties for virtual try-on across identities. What’s worse, the entangled representations usually fail to exactly track the sliding motion of garments. To overcome these limitations, we present Layered Gaussian Avatars (LayGA), a new representation that formulates body and clothing as two separate layers for photorealistic animatable clothing transfer from multi-view videos. Our representation is built upon the Gaussian map-based avatar for its excellent representation power of garment details. However, the Gaussian map produces unstructured 3D Gaussians distributed around the actual surface. The absence of a smooth explicit surface raises challenges in accurate garment tracking and collision handling between body and garments. Therefore, we propose two-stage training involving single-layer reconstruction and multi-layer fitting. In the single-layer reconstruction stage, we propose a series of geometric constraints to reconstruct smooth surfaces and simultaneously obtain the segmentation between body and clothing. Next, in the multi-layer fitting stage, we train two separate models to represent body and clothing and utilize the reconstructed clothing geometries as 3D supervision for more accurate garment tracking. Furthermore, we propose geometry and rendering layers for both high-quality geometric reconstruction and high-fidelity rendering. Overall, the proposed LayGA realizes photorealistic animations and virtual try-on, and outperforms other baseline methods. Our project page is https://jsnln.github.io/layga/index.html.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_715",
    "authors": "Krzysztof Wolski, Adarsh Djeacoumar, Alireza Javanmardi, Hans-Peter Seidel, Christian Theobalt, Guillaume Cordonnier, Karol Myszkowski, George Drettakis, Xingang Pan, Thomas Leimkühler",
    "title": "Learning Images Across Scales Using Adversarial Training",
    "paper_url": "https://hal.science/hal-04593775/",
    "pdf_link": null,
    "abstract": "The real world exhibits rich structure and detail across many scales of observation. It is difficult, however, to capture and represent a broad spectrum of scales using ordinary images. We devise a novel paradigm for learning a representation that captures an orders-of-magnitude variety of scales from an unstructured collection of ordinary images. We treat this collection as a distribution of scale-space slices to be learned using adversarial training, and additionally enforce coherency across slices. Our approach relies on a multiscale generator with carefully injected procedural frequency content, which allows to interactively explore the emerging continuous scale space. Training across vastly different scales poses challenges regarding stability, which we tackle using a supervision scheme that involves careful sampling of scales. We show that our generator can be used as a multiscale generative model, and for reconstructions of scale spaces from unstructured patches. Significantly outperforming the state of the art, we demonstrate zoom-in factors of up to 256x at high quality and scale consistency.",
    "scholar_publication": "ACM Transactions on …, 2024 - hal.science"
  },
  {
    "paper_id": "paperstog_111",
    "authors": "Hang Zhao, Zherong Pan, Yang Yu, Kai Xu, Hang Zhao",
    "title": "Learning Physically Realizable Skills for Online Packing of General 3D Shapes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3603544",
    "pdf_link": null,
    "abstract": "We study the problem of learning online packing skills for irregular 3D shapes, which is arguably the most challenging setting of bin packing problems. The goal is to consecutively move a sequence of 3D objects with arbitrary shapes into a designated container with only partial observations of the object sequence. We take physical realizability into account, involving physics dynamics and constraints of a placement. The packing policy should understand the 3D geometry of the object to be packed and make effective decisions to accommodate it in the container in a physically realizable way. We propose a Reinforcement Learning (RL) pipeline to learn the policy. The complex irregular geometry and imperfect object placement together lead to huge solution space. Direct training in such space is prohibitively data intensive. We instead propose a theoretically provable method for candidate action generation to reduce the action space of RL and the learning burden. A parameterized policy is then learned to select the best placement from the candidates. Equipped with an efficient method of asynchronous RL acceleration and a data preparation process of simulation-ready training sequences, a mature packing policy can be trained in a physics-based environment within 48 hours. Through extensive evaluation on a variety of real-life shape datasets and comparisons with state-of-the-art baselines, we demonstrate that our method outperforms the best-performing baseline on all datasets by at least 12.8% in terms of packing utility. We also release our datasets and source code to support further research in this direction.",
    "scholar_publication": "ACM Transactions on Graphics, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_664",
    "authors": "Lingchen Yang, Gaspard Zoss, Prashanth Chandran, Markus Gross, Barbara Solenthaler, Eftychios Sifakis, Derek Bradley",
    "title": "Learning a Generalized Physical Face Model From Data",
    "paper_url": "https://arxiv.org/abs/2402.19477",
    "pdf_link": null,
    "abstract": "Physically-based simulation is a powerful approach for 3D facial animation as the resulting deformations are governed by physical constraints, allowing to easily resolve self-collisions, respond to external forces and perform realistic anatomy edits. Today's methods are data-driven, where the actuations for finite elements are inferred from captured skin geometry. Unfortunately, these approaches have not been widely adopted due to the complexity of initializing the material space and learning the deformation model for each character separately, which often requires a skilled artist followed by lengthy network training. In this work, we aim to make physics-based facial animation more accessible by proposing a generalized physical face model that we learn from a large 3D face dataset. Once trained, our model can be quickly fit to any unseen identity and produce a ready-to-animate physical face model automatically. Fitting is as easy as providing a single 3D face scan, or even a single face image. After fitting, we offer intuitive animation controls, as well as the ability to retarget animations across characters. All the while, the resulting animations allow for physical effects like collision avoidance, gravity, paralysis, bone reshaping and more.",
    "scholar_publication": "arXiv preprint arXiv …, 2024 - arxiv.org"
  },
  {
    "paper_id": "papers_767",
    "authors": "David R. Palmer, Albert Chern, Justin M. Solomon",
    "title": "Lifting Directional Fields to Minimal Sections",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658198",
    "pdf_link": null,
    "abstract": "Directional fields, including unit vector, line, and cross fields, are essential tools in the geometry processing toolkit. The topology of directional fields is characterized by their singularities. While singularities play an important role in downstream applications such as meshing, existing methods for computing directional fields either require them to be specified in advance, ignore them altogether, or treat them as zeros of a relaxed field. While fields are ill-defined at their singularities, the graphs of directional fields with singularities are well-defined surfaces in a circle bundle. By lifting optimization of fields to optimization over their graphs, we can exploit a natural convex relaxation to a minimal section problem over the space of currents in the bundle. This relaxation treats singularities as first-class citizens, expressing the relationship between fields and singularities as an explicit boundary condition. As curvature frustrates finite element discretization of the bundle, we devise a hybrid spectral method for representing and optimizing minimal sections. Our method supports field optimization on both flat and curved domains and enables more precise control over singularity placement.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_114",
    "authors": "Mohit Gupta, Jian Wang, Karl Bayer, Shree Nayar, Jian Wang",
    "title": "Light Codes for Fast Two-Way Human-Centric Visual Communication",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3617682",
    "pdf_link": null,
    "abstract": "Visual codes, such as QR codes, are widely used in several applications for conveying information to users. However, user interactions based on spatial codes (e.g., displaying codes on phone screens for exchanging contact information) are often tedious, time consuming, and prone to errors due to image corruptions such as noise, blur, saturation, and perspective distortions. We propose Light Codes (LICO), a novel method for fast and fluid exchange of information among users. Light codes are based on transmitting and receiving temporal codes (instead of spatial) using compact and low-cost transceiver devices. The resulting approach enables seamless and near instantaneous exchange of short messages among users with minimal physical and cognitive effort. We design novel coding techniques, hardware prototypes, and applications that are optimized for human-centric communication, and facilitate fast and fluid user-to-user interactions in various challenging conditions, including a range of distances, motion, and ambient illumination. We evaluate the performance of the proposed methods both via quantitative analysis and user study based comparisons with several existing approaches including display-camera links, Bluetooth, and near-field communication, which show strong preference toward Light Codes in various real-world application scenarios.",
    "scholar_publication": "ACM Transactions on Graphics, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_1110",
    "authors": "Haocheng Ren, Yuchi Huo, Yifan Peng, Hongtao Sheng, Weidong Xue, Hongxiang Huang, Jingzhen Lan, Rui Wang, Hujun Bao",
    "title": "LightFormer: Light-oriented Global Neural Rendering in Dynamic Scene",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658229",
    "pdf_link": null,
    "abstract": "The generation of global illumination in real time has been a long-standing challenge in the graphics community, particularly in dynamic scenes with complex illumination. Recent neural rendering techniques have shown great promise by utilizing neural networks to represent the illumination of scenes and then decoding the final radiance. However, incorporating object parameters into the representation may limit their effectiveness in handling fully dynamic scenes. This work presents a neural rendering approach, dubbed LightFormer, that can generate realistic global illumination for fully dynamic scenes, including dynamic lighting, materials, cameras, and animated objects, in real time. Inspired by classic many-lights methods, the proposed approach focuses on the neural representation of light sources in the scene rather than the entire scene, leading to the overall better generalizability. The neural prediction is achieved by leveraging the virtual point lights and shading clues for each light. Specifically, two stages are explored. In the light encoding stage, each light generates a set of virtual point lights in the scene, which are then encoded into an implicit neural light representation, along with screen-space shading clues like visibility. In the light gathering stage, a pixel-light attention mechanism composites all light representations for each shading point. Given the geometry and material representation, in tandem with the composed light representations of all lights, a lightweight neural network predicts the final radiance. Experimental results demonstrate that the proposed LightFormer can yield reasonable and realistic global illumination in fully dynamic scenes with real-time performance.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_773",
    "authors": "Jiong Chen, Florian Schaefer, Mathieu Desbrun",
    "title": "Lightning-fast Method of Fundamental Solutions",
    "paper_url": "https://hal.science/hal-04589038/",
    "pdf_link": null,
    "abstract": "The method of fundamental solutions (MFS) and its associated boundary element method (BEM) have gained popularity in computer graphics due to the reduced dimensionality they offer: for three-dimensional linear problems, they only require variables on the domain boundary to solve and evaluate the solution throughout space, making them a valuable tool in a wide variety of applications. However, MFS and BEM have poor computational scalability and huge memory requirements for large-scale problems, limiting their applicability and efficiency in practice. By leveraging connections with Gaussian Processes and exploiting the sparse structure of the inverses of boundary integral matrices, we introduce a variational preconditioner that can be computed via a sparse inverse-Cholesky factorization in a massively parallel manner. We show that applying our preconditioner to the Preconditioned Conjugate Gradient algorithm greatly improves the efficiency of MFS or BEM solves, up to four orders of magnitude in our series of tests.",
    "scholar_publication": "ACM Transactions on Graphics, 2024 - hal.science"
  },
  {
    "paper_id": "papers_771",
    "authors": "Pramod Rao, Gereon Fox, Abhimitra Meka, Mallikarjun B R, Fangneng Zhan, Tim Weyrich, Bernd Bickel, Hanspeter Pfister, Wojciech Matusik, Mohamed Elgharib, Christian Theobalt",
    "title": "Lite2Relight: 3D-aware Single Image Portrait Relighting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657470",
    "pdf_link": null,
    "abstract": "Achieving photorealistic 3D view synthesis and relighting of human portraits is pivotal for advancing AR/VR applications. Existing methodologies in portrait relighting demonstrate substantial limitations in terms of generalization and 3D consistency, coupled with inaccuracies in physically realistic lighting and identity preservation. Furthermore, personalization from a single view is difficult to achieve and often requires multiview images during the testing phase or involves slow optimization processes. This paper introduces Lite2Relight , a novel technique that can predict 3D consistent head poses of portraits while performing physically plausible light editing at interactive speed. Our method uniquely extends the generative capabilities and efficient volumetric representation of EG3D, leveraging a lightstage dataset to implicitly disentangle face reflectance and perform relighting under target HDRI environment maps. By utilizing a pre-trained geometry-aware encoder and a feature alignment module, we map input images into a relightable 3D space, enhancing them with a strong face geometry and reflectance prior. Through extensive quantitative and qualitative evaluations, we show that our method outperforms the state-of-the-art methods in terms of efficacy, photorealism, and practical application. This includes producing 3D-consistent results of the full head, including hair, eyes, and expressions. Lite2Relight paves the way for large-scale adoption of photorealistic portrait editing in various domains, offering a robust, interactive solution to a previously constrained problem.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_106",
    "authors": "Raphaël Charrondière, Sébastien Neukirch, Florence Bertails-Descoubes, Sébastien Neukirch, Florence Bertails-Descoubes",
    "title": "MERCI: Mixed curvature-based elements for computing equilibria of thin elastic ribbons",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3674502",
    "pdf_link": null,
    "abstract": "Thin elastic ribbons represent a class of intermediary objects lying in-between thin elastic plates and thin elastic rods. Although the two latter families of thin structures have received much interest from the Computer Graphics community over the last decades, ribbons have seldom been considered and modelled numerically so far, in spite of a growing number of applications in Computer Design. In this article, starting from the reduced developable ribbon models [Sadowsky ; Wunderlich ] recently popularised in Soft Matter Physics, we propose a both accurate and efficient algorithm for computing the statics of thin elastic ribbons. Inspired by the super-clothoid model for thin elastic rods, our method relies on compact ribbon elements whose normal curvature varies linearly with respect to arc length s, while their geodesic torsion is quadratic in s. In contrast, however, for the sake of efficiency, our algorithm avoids building a fully reduced kinematic chain and instead treats each element independently, gluing them only at the final solving stage through well-chosen bilateral constraints.Thanks to this mixed variational strategy, which yields a banded Hessian, our algorithm recovers the linear complexity of low-order models while preserving the high-order convergence of curvature-based models. As a result, our approach is scalable to a large number of elements, and suitable for various boundary conditions and unilateral contact constraints, making it possible to handle challenging scenarios such as confined buckling experiments or Möbius bands with contact. Remarkably, our mixed algorithm proves an order of magnitude faster compared to Discrete Elastic Ribbon models of the literature while achieving, in a few seconds only, high accuracy levels that remain out of reach for such low-order models. Additionally, our numerical model can incorporate various ribbon energies, including the RibExt model for quasi-developable ribbons recently introduced in Physics [Audoly and Neukirch ], which allows to transition smoothly between a rectangular Kirchhoff rod and a (developable) Sadowsky ribbon. Our numerical scheme is carefully validated against demanding experiments of the Physics literature, which demonstrates its accuracy, efficiency, robustness, and versatility.Our Merci code is publicly available at https://gitlab.inria.fr/elan-public-code/merci for the sake of reproducibility and future benchmarking.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_229",
    "authors": "Xin-Yang Zheng, Hao Pan, Yu-Xiao Guo, Xin Tong, Yang Liu",
    "title": "MVD^2: Efficient Multiview 3D Reconstruction for Multiview Diffusion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657403",
    "pdf_link": null,
    "abstract": "Multiview diffusion (MVD) has emerged as a prominent 3D generation technique, acclaimed for its generalizability, quality, and efficiency. MVD models finetune image diffusion models with 3D data to generate multiple views of a 3D object from an image or text prompt, followed by a multiview 3D reconstruction process. However, the sparsity of views and inconsistent details in the generated multiview images pose challenges for 3D reconstruction. We present MVD2, an efficient 3D reconstruction method tailored for MVD images. MVD2 integrates multiview image features into a 3D feature volume, then transforms this volume into a textureless 3D mesh, onto which the MVD images are mapped as textures. It employs a simple-yet-efficient view-dependent training scheme to mitigate discrepancies between MVD images and ground-truth views of 3D shapes, effectively improving 3D generation quality and robustness. MVD2 is trained with 3D collections and MVD images, and the trained MVD2 efficiently reconstructs 3D meshes from multiview images within one second and exhibits great model generalizability in dealing with images generated by various MVD methods. Our code and the pretrained model are available at: https://zhengxinyang.github.io/projects/MVD_square.html.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1007",
    "authors": "Shangzhan Zhang, Sida Peng, Tao Xu, Yuanbo Yang, Tianrun Chen, Nan Xue, Yujun Shen, Hujun Bao, Ruizhen Hu, Xiaowei Zhou",
    "title": "MaPa: Text-driven Photorealistic Material Painting for 3D Shapes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657504",
    "pdf_link": null,
    "abstract": "This paper aims to generate materials for 3D meshes from text descriptions. Unlike existing methods that synthesize texture maps, we propose to generate segment-wise procedural material graphs as the appearance representation, which supports high-quality rendering and provides substantial flexibility in editing. Instead of relying on extensive paired data, i.e., 3D meshes with material graphs and corresponding text descriptions, to train a material graph generative model, we propose to leverage the pre-trained 2D diffusion model as a bridge to connect the text and material graphs. Specifically, our approach decomposes a shape into a set of segments and designs a segment-controlled diffusion model to synthesize 2D images that are aligned with mesh parts. Based on generated images, we initialize parameters of material graphs and fine-tune them through the differentiable rendering module to produce materials in accordance with the textual description. Extensive experiments demonstrate the superior performance of our framework in photorealism, resolution, and editability over existing methods.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1160",
    "authors": "Zhixiang Wang, Baiang Li, Jian Wang, Yu-Lun Liu, Jinwei Gu, Yung-Yu Chuang, Shin'ichi Satoh",
    "title": "Matting by Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657519",
    "pdf_link": null,
    "abstract": "This paper introduces an innovative approach for image matting that redefines the traditional regression-based task as a generative modeling challenge. Our method harnesses the …",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_294",
    "authors": "Qingcheng Zhao, Pengyu Long, Qixuan Zhang, Dafei Qin, Han Liang, Longwen Zhang, Yingliang Zhang, Jingyi Yu, Lan Xu",
    "title": "Media2Face: Co-speech Facial Animation Generation With Multi-modality Guidance",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657413",
    "pdf_link": null,
    "abstract": "The synthesis of 3D facial animations from speech has garnered considerable attention. Due to the scarcity of high-quality 4D facial data and well-annotated abundant multi-modality labels, previous methods often suffer from limited realism and a lack of flexible conditioning. We address this challenge through a trilogy. We first introduce Generalized Neural Parametric Facial Asset (GNPFA), an efficient variational auto-encoder mapping facial geometry and images to a highly generalized expression latent space, decoupling expressions and identities. Then, we utilize GNPFA to extract high-quality expressions and accurate head poses from a large array of videos. This presents the M2F-D dataset, a large, diverse, and scan-level co-speech 3D facial animation dataset with well-annotated emotion and style labels. Finally, we propose Media2Face, a diffusion model in GNPFA latent space for co-speech facial animation generation, accepting rich multi-modality guidances from audio, text, and image. Extensive experiments demonstrate that our model not only achieves high fidelity in facial animation synthesis but also broadens the scope of expressiveness and style adaptability in 3D facial animation.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_146",
    "authors": "Ehsan Pajouheshgar, Yitao Xu, Alexander Mordvintsev, Eyvind Niklasson, Tong Zhang, Sabine Süsstrunk",
    "title": "Mesh Neural Cellular Automata",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658127",
    "pdf_link": null,
    "abstract": "… Lastly, Cellular Automata models utilize local update rules to … In this paper, we focus on and improve upon the Cellular-Automata… We build upon Neural Cellular Automata (NCA1), a …",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_943",
    "authors": "Jiří Minarčík, Sam Estep, Wode Ni, Keenan Crane",
    "title": "Minkowski Penalties: Robust Differentiable Constraint Enforcement for Vector Graphics",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657495",
    "pdf_link": null,
    "abstract": "This paper describes an optimization-based framework for finding arrangements of 2D shapes subject to pairwise constraints. Such arrangements naturally arise in tasks such as vector illustration and diagram generation, but enforcing these criteria robustly is surprisingly challenging. We approach this problem through the minimization of novel energetic penalties, derived from the signed distance function of the Minkowski difference between interacting shapes. This formulation provides useful gradients even when initialized from a wildly infeasible state, and, unlike many common collision penalties, can handle open curves that do not have a well-defined inside and outside. Moreover, it supports rich features beyond the basic no-overlap condition, such as tangency, containment, and precise padding, which are especially valuable in the vector illustration context. We develop closed-form expressions and efficient approximations of our penalty for standard vector graphics primitives, yielding efficient evaluation and easy implementation within existing automatic differentiation pipelines. The method has already been “battle-tested” as a component of public-facing open source software; we demonstrate the utility of the framework via examples from illustration, data visualization, diagram generation, and geometry processing.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_203",
    "authors": "Heyuan Yao, Zhenhua Song, Yuyang Zhou, Tenglong Ao, Baoquan Chen, Libin Liu",
    "title": "MoConVQ: Unified Physics-based Motion Control via Scalable Discrete Representations",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658137",
    "pdf_link": null,
    "abstract": "In this work, we present MoConVQ, a novel unified framework for physics-based motion control leveraging scalable discrete representations. Building upon vector quantized variational autoencoders (VQ-VAE) and model-based reinforcement learning, our approach effectively learns motion embeddings from a large, unstructured dataset spanning tens of hours of motion examples. The resultant motion representation not only captures diverse motion skills but also offers a robust and intuitive interface for various applications. We demonstrate the versatility of MoConVQ through several applications: universal tracking control from various motion sources, interactive character control with latent motion representations using supervised learning, physics-based motion generation from natural language descriptions using the GPT framework, and, most interestingly, seamless integration with large language models (LLMs) with in-context learning to tackle complex and abstract tasks.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_422",
    "authors": "Sipeng Yang, Qingchuan Zhu, Junhao Zhuge, Qiang Qiu, Chen Li, Yuzhong Yan, Huihui Xu, Ling-Qi Yan, Xiaogang Jin",
    "title": "Mob-FGSR: Frame Generation and Super Resolution for Mobile Real-time Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657424",
    "pdf_link": null,
    "abstract": "Recent advances in supersampling for frame generation and super-resolution improve real-time rendering performance significantly. However, because these methods rely heavily on the most recent features of high-end GPUs, they are impractical for mobile platforms, which are limited by lower GPU capabilities and a lack of dedicated optical flow estimation hardware. We propose Mob-FGSR, a novel lightweight supersampling framework tailored for mobile devices that integrates frame generation with super resolution to effectively improve real-time rendering performance. Our method introduces a splat-based motion vectors reconstruction method, which allows for accurate pixel-level motion estimation for both interpolation and extrapolation at desired times without the need for high-end GPUs or rendering data from generated frames. Subsequently, fast image generation models are designed to construct interpolated or extrapolated frames and improve resolution, providing users with a plethora of options. Our runtime models operate without the use of neural networks, ensuring their applicability to mobile devices. Extensive testing shows that our framework outperforms other lightweight solutions and rivals the performance of algorithms designed specifically for high-end GPUs. Our model’s minimal runtime is confirmed by on-device testing, demonstrating its potential to benefit a wide range of mobile real-time rendering applications. More information and an Android demo can be found at: https://mob-fgsr.github.io/.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_216",
    "authors": "Pengbin Tang, Ronan Hinchet, Roi Poranne, Bernhard Thomaszewski, Stelian Coros",
    "title": "Modal Folding: Discovering Smooth Folding Patterns for Sheet Materials Using Strain-space Modes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657401",
    "pdf_link": null,
    "abstract": "Folding can transform mundane objects such as napkins into stunning works of art. However, finding new folding transformations for sheet materials is a challenging problem that requires expertise and real-world experimentation. In this paper, we present Modal Folding—an automated approach for discovering energetically optimal folding transformations, i.e., large deformations that require little mechanical work. For small deformations, minimizing internal energy for fixed displacement magnitudes leads to the well-known elastic eigenmodes. While linear modes provide promising directions for bending, they cannot capture the rotational motion required for folding. To overcome this limitation, we introduce strain-space modes—nonlinear analogues of elastic eigenmodes that operate on per-element curvatures instead of vertices. Using strain-space modes to determine target curvatures for bending elements, we can generate complex nonlinear folding motions by simply minimizing the sheet’s internal energy. Our modal folding approach offers a systematic and automated way to create complex designs. We demonstrate the effectiveness of our method with simulation results for a range of shapes and materials, and validate our designs with physical prototypes.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_897",
    "authors": "Meng-Li Shih, Jia-Bin Huang, Changil Kim, Rajvi Shah, Johannes Kopf, Chen Gao",
    "title": "Modeling Ambient Scene Dynamics for Free-view Synthesis",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657488",
    "pdf_link": null,
    "abstract": "We introduce a novel method for dynamic free-view synthesis of an ambient scenes from a monocular capture bringing a immersive quality to the viewing experience. Our method builds upon the recent advancements in 3D Gaussian Splatting (3DGS) that can faithfully reconstruct complex static scenes. Previous attempts to extend 3DGS to represent dynamics have been confined to bounded scenes or require multi-camera captures, and often fail to generalize to unseen motions, limiting their practical application. Our approach overcomes these constraints by leveraging the periodicity of ambient motions to learn the motion trajectory model, coupled with careful regularization. We also propose important practical strategies to improve the visual quality of the baseline 3DGS static reconstructions and to improve memory efficiency critical for GPU-memory intensive learning. We demonstrate high-quality photorealistic novel view synthesis of several ambient natural scenes with intricate textures and fine structural elements. We show that our method significantly outperforms prior methods both qualitatively and quantitatively. Project page: https://ambientgaussian.github.io/",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_613",
    "authors": "Alexander Reshetov, David Hart",
    "title": "Modeling Hair Strands With Roving Capsules",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657450",
    "pdf_link": null,
    "abstract": "Hair strands can be modeled by sweeping spheres with varying radii along Bézier curves. We ray-trace such shapes by finding intersections of a given ray with a set of capsules dynamically defined at runtime. A substantial performance boost is achieved by systematically eliminating parts of the shape that are guaranteed not to intersect with the given ray. The new intersector is more than twice faster than the previously leading phantom algorithm [Reshetov and Luebke 2018]. This improvement results in a 30% overall performance increase, which includes traversal, shading, and the rendering system overhead. In addition, we derive a parametric form of the swept sphere shapes. This provides a deeper understanding of the properties of such objects compared to the offset surfaces obtained by sweeping circles orthogonal to a given curve. The complete WebGL implementation of our algorithm is available at https://www.shadertoy.com/view/4ffXWs.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1001",
    "authors": "Jean Jouve, Victor Romero, Rahul Narain, Laurence Boissieux, Theodore Kim, Florence Bertails-Descoubes",
    "title": "Modelling a Feather as a Strongly Anisotropic Elastic Shell",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657503",
    "pdf_link": null,
    "abstract": "Feathers exhibit a highly anisotropic behaviour, governed by their complex hierarchical microstructure composed of individual hairs (barbs) clamped onto a spine (rachis) and attached to each other through tiny hooks (barbules). Previous methods in computer graphics have approximated feathers as strips of cloth, thus failing to capture the particular macroscopic nonlinear behaviour of the feather surface (vane). To investigate the anisotropic properties of a feather vane, we design precise measurement protocols on real feather samples. Our experimental results suggest a linear strain-stress relationship of the feather membrane with orientation-dependent coefficients, as well as an extreme ratio of stiffnesses in the barb and barbule direction, of the order of 104. From these findings we build a simple continuum model for the feather vane, where the vane is represented as a three-parameter anisotropic elastic shell. However, implementing the model numerically reveals severe locking and ill-conditioning issues, due to the extreme stiffness ratio between the barb and the barbule directions. To resolve these issues, we align the mesh along the barb directions and replace the stiffest modes with an inextensibility constraint. We extensively validate our membrane model against real-world laboratory measurements, by using an intermediary microscale model that allows us to limit the number of required lab experiments. Finally, we enrich our membrane model with anisotropic bending, and show its practicality in graphics-like scenarios like a full feather and a larger-scale bird. Code and data for this paper are available at https://gitlab.inria.fr/elan-public-code/feather-shell/.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_975",
    "authors": "Yufan Chen, Lizhen Wang, Qijing Li, Hongjiang Xiao, Shengping Zhang, Hongxun Yao, Yebin Liu",
    "title": "MonoGaussianAvatar: Monocular Gaussian Point-based Head Avatar",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657499",
    "pdf_link": null,
    "abstract": "The ability to animate photo-realistic head avatars reconstructed from monocular portrait video sequences represents a crucial step in bridging the gap between the virtual and real worlds. Recent advancements in head avatar techniques, including explicit 3D morphable meshes (3DMM), point clouds, and neural implicit representation have been exploited for this ongoing research. However, 3DMM-based methods are constrained by their fixed topologies, point-based approaches suffer from a heavy training burden due to the extensive quantity of points involved, and the last ones suffer from limitations in deformation flexibility and rendering efficiency. In response to these challenges, we propose MonoGaussianAvatar (Monocular Gaussian Point-based Head Avatar), a novel approach that harnesses 3D Gaussian point representation coupled with a Gaussian deformation field to learn explicit head avatars from monocular portrait videos. We define our head avatars with Gaussian points characterized by adaptable shapes, enabling flexible topology. These points exhibit movement with a Gaussian deformation field in alignment with the target pose and expression of a person, facilitating efficient deformation. Additionally, the Gaussian points have controllable shape, size, color, and opacity combined with Gaussian splatting, allowing for efficient training and rendering. Experiments demonstrate the superior performance of our method, which achieves state-of-the-art results among previous methods. Code and data can be found at https://github.com/aipixel/MonoGaussianAvatar.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_965",
    "authors": "Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, Hongsheng Li",
    "title": "Motion-I2V: Consistent and Controllable Image-to-video Generation With Explicit Motion Modeling",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657497",
    "pdf_link": null,
    "abstract": "We introduce Motion-I2V, a novel framework for consistent and controllable text-guided image-to-video generation (I2V). In contrast to previous methods that directly learn the complicated image-to-video mapping, Motion-I2V factorizes I2V into two stages with explicit motion modeling. For the first stage, we propose a diffusion-based motion field predictor, which focuses on deducing the trajectories of the reference image’s pixels. For the second stage, we propose motion-augmented temporal attention to enhance the limited 1-D temporal attention in video latent diffusion models. This module can effectively propagate reference image features to synthesized frames with the guidance of predicted trajectories from the first stage. Compared with existing methods, Motion-I2V can generate more consistent videos even in the presence of large motion and viewpoint variation. By training a sparse trajectory ControlNet for the first stage, Motion-I2V can support users to precisely control motion trajectories and motion regions with sparse trajectory and region. This offers more controllability of the I2V process than solely relying on textual instructions. Additionally, Motion-I2V’s second stage naturally supports zero-shot video-to-video translation. Both qualitative and quantitative comparisons demonstrate the advantages of Motion-I2V over prior approaches in consistent and controllable image-to-video generation. Please see our project page at https://xiaoyushi97.github.io/Motion-I2V/.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1140",
    "authors": "Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, Ying Shan",
    "title": "MotionCtrl: A Unified and Flexible Motion Controller for Video Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657518",
    "pdf_link": null,
    "abstract": "Motions in a video primarily consist of camera motion, induced by camera movement, and object motion, resulting from object movement. Accurate control of both camera and object motion is essential for video generation. However, existing works either mainly focus on one type of motion or do not clearly distinguish between the two, limiting their control capabilities and diversity. Therefore, this paper presents MotionCtrl, a unified and flexible motion controller for video generation designed to effectively and independently control camera and object motion. The architecture and training strategy of MotionCtrl are carefully devised, taking into account the inherent properties of camera motion, object motion, and imperfect training data. Compared to previous methods, MotionCtrl offers three main advantages: 1) It effectively and independently controls camera motion and object motion, enabling more fine-grained motion control and facilitating flexible and diverse combinations of both types of motion. 2) Its motion conditions are determined by camera poses and trajectories, which are appearance-free and minimally impact the appearance or shape of objects in generated videos. 3) It is a relatively generalizable model that can adapt to a wide array of camera poses and trajectories once trained. Extensive qualitative and quantitative experiments have been conducted to demonstrate the superiority of MotionCtrl over existing methods. Project page: https://wzhouxiff.github.io/projects/MotionCtrl/ .",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1003",
    "authors": "Peter Heiss-Synak, Aleksei Kalinov, Malina Strugaru, Arian Etemadi, Huidong Yang, Chris Wojtan",
    "title": "Multi-material Mesh-based Surface Tracking With Implicit Topology Changes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658223",
    "pdf_link": null,
    "abstract": "We introduce a multi-material non-manifold mesh-based surface tracking algorithm that converts self-intersections into topological changes. Our algorithm generalizes prior work on manifold surface tracking with topological changes: it preserves surface features like mesh-based methods, and it robustly handles topological changes like level set methods. Our method also offers improved efficiency and robustness over the state of the art. We demonstrate the effectiveness of the approach on a range of examples, including complex soap film simulations with thousands of interacting bubbles, and boolean unions of non-manifold meshes consisting of millions of triangles.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_732",
    "authors": "Philippe Weier, Alexander Rath, Élie Michel, Iliyan Georgiev, Philipp Slusallek, Tamy Boubekeur",
    "title": "N-BVH: Neural Ray Queries With Bounding Volume Hierarchies",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657464",
    "pdf_link": null,
    "abstract": "Neural representations have shown spectacular ability to compress complex signals in a fraction of the raw data size. In 3D computer graphics, the bulk of a scene’s memory usage is due to polygons and textures, making them ideal candidates for neural compression. Here, the main challenge lies in finding good trade-offs between efficient compression and cheap inference while minimizing training time. In the context of rendering, we adopt a ray-centric approach to this problem and devise<? TeX N?> Math 1-BVH, a neural compression architecture designed to answer arbitrary ray queries in 3D. Our compact model is learned from the input geometry and substituted for it whenever a ray intersection is queried by a path-tracing engine. While prior neural compression methods have focused on point queries, ours proposes neural ray queries that integrate seamlessly into standard ray-tracing pipelines. At the core of our method, we employ an adaptive BVH-driven probing scheme to optimize the parameters of a multi-resolution hash grid, focusing its neural capacity on the sparse 3D occupancy swept by the original surfaces. As a result, our<? TeX N?> Math 2-BVH can serve accurate ray queries from a representation that is more than an order of magnitude more compact, providing faithful approximations of visibility, depth, and appearance attributes. The flexibility of our method allows us to combine and overlap neural and non-neural entities within the same 3D scene and extends to appearance level of detail.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1000",
    "authors": "Stavros Diolatzis, Tobias Zirr, Alexander Kuznetsov, Georgios Kopanas, Anton Kaplanyan",
    "title": "N-Dimensional Gaussians for Fitting of High Dimensional Functions",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657502",
    "pdf_link": null,
    "abstract": "In the wake of many new ML-inspired approaches for reconstructing and representing high-quality 3D content, recent hybrid and explicitly learned representations exhibit promising performance and quality characteristics. However, their scaling to higher dimensions is challenging, e.g. when accounting for dynamic content with respect to additional parameters such as material properties, illumination, or time. In this paper, we tackle these challenges for an explicit representations based on Gaussian mixture models. With our solutions, we arrive at efficient fitting of compact N-dimensional Gaussian mixtures and enable efficient evaluation at render time: For fast fitting and evaluation, we introduce a high-dimensional culling scheme that efficiently bounds N-D Gaussians, inspired by Locality Sensitive Hashing. For adaptive refinement yet compact representation, we introduce a loss-adaptive density control scheme that incrementally guides the use of additional capacity towards missing details. With these tools we can for the first time represent complex appearance that depends on many input dimensions beyond position or viewing angle within a compact, explicit representation optimized in minutes and rendered in milliseconds.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1115",
    "authors": "Yi Li, Benjamin Tag, Shaozhang Dai, Robert Crowther, Tim Dwyer, Pourang Irani, Barrett Ens",
    "title": "NICER: A New and Improved Consumed Endurance and Recovery Metric to Quantify Muscle Fatigue of Mid-air Interactions",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658230",
    "pdf_link": null,
    "abstract": "Natural gestures are crucial for mid-air interaction, but predicting and managing muscle fatigue is challenging. Existing torque-based models are limited in their ability to model above-shoulder interactions and to account for fatigue recovery. We introduce a new hybrid model, NICER, which combines a torque-based approach with a new term derived from the empirical measurement of muscle contraction and a recovery factor to account for decreasing fatigue during rest. We evaluated NICER in a mid-air selection task using two interaction methods with different degrees of perceived fatigue. Results show that NICER can accurately model above-shoulder interactions as well as reflect fatigue recovery during rest periods. Moreover, both interaction methods show a stronger correlation with subjective fatigue measurement (ρ = 0.978/0.976) than a previous model, Cumulative Fatigue (ρ = 0.966/0.923), confirming that NICER is a powerful analytical tool to predict fatigue across a variety of gesture-based interactive applications.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_861",
    "authors": "James Andrews",
    "title": "Navigation-driven Approximate Convex Decomposition",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657479",
    "pdf_link": null,
    "abstract": "Approximate convex decomposition – approximating a shape by a set of convex hulls – is a popular approach to creating efficient collision representations for games and simulations. Existing algorithms to construct such decompositions are typically driven by general surface- or volume-based error metrics that can’t ignore unreachable internal surfaces nor provide local control over the results. We introduce the problem of navigable approximate convex decomposition: First, define a navigable space for the input shape which other objects in the game or simulation must be able to move through, then find a decomposition which does not overlap that space. We show how to automatically find such navigable space, how to customize it, and we introduce an approximate convex decomposition algorithm that protects it. Our results demonstrate that this approach can generate decompositions that meet application requirements faster and with fewer convex hulls than previous methods, while providing a new level of flexibility in defining what those requirements are.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_112",
    "authors": "Chuankun Zheng, Yuchi Huo, Shaohua Mo, Zhihua Zhong, Zhizhen Wu, Wei Hua, Rui Wang, Hujun Bao, Chuankun Zheng",
    "title": "NeLT: Object-oriented Neural Light Transfer",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3596491",
    "pdf_link": null,
    "abstract": "… views, lighting, materials, and transformations. We first learn for each object a neural light transfer function (NeLT), an object-oriented function that reflects the light transfer between the …",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_232",
    "authors": "Jingwang Ling, Ruihan Yu, Feng Xu, Chun Du, Shuang Zhao",
    "title": "NeRF as a Non-distant Environment Emitter in Physics-based Inverse Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657404",
    "pdf_link": null,
    "abstract": "Physics-based inverse rendering enables joint optimization of shape, material, and lighting based on captured 2D images. To ensure accurate reconstruction, using a light model that closely resembles the captured environment is essential. Although the widely adopted distant environmental lighting model is adequate in many cases, we demonstrate that its inability to capture spatially varying illumination can lead to inaccurate reconstructions in many real-world inverse rendering scenarios. To address this limitation, we incorporate NeRF as a non-distant environment emitter into the inverse rendering pipeline. Additionally, we introduce an emitter importance sampling technique for NeRF to reduce the rendering variance. Through comparisons on both real and synthetic datasets, our results demonstrate that our NeRF-based emitter offers a more precise representation of scene lighting, thereby improving the accuracy of inverse rendering.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_476",
    "authors": "Qiujie Dong, Rui Xu, Pengfei Wang, Shuangmin Chen, Shiqing Xin, Xiaohong Jia, Wenping Wang, Changhe Tu",
    "title": "NeurCADRecon: Neural Representation for Reconstructing CAD Surfaces by Enforcing Zero Gaussian Curvature",
    "paper_url": "https://arxiv.org/abs/2404.13420",
    "pdf_link": null,
    "abstract": "Despite recent advances in reconstructing an organic model with the neural signed distance function (SDF), the high-fidelity reconstruction of a CAD model directly from low-quality unoriented point clouds remains a significant challenge. In this paper, we address this challenge based on the prior observation that the surface of a CAD model is generally composed of piecewise surface patches, each approximately developable even around the feature line. Our approach, named NeurCADRecon, is self-supervised, and its loss includes a developability term to encourage the Gaussian curvature toward 0 while ensuring fidelity to the input points. Noticing that the Gaussian curvature is non-zero at tip points, we introduce a double-trough curve to tolerate the existence of these tip points. Furthermore, we develop a dynamic sampling strategy to deal with situations where the given points are incomplete or too sparse. Since our resulting neural SDFs can clearly manifest sharp feature points/lines, one can easily extract the feature-aligned triangle mesh from the SDF and then decompose it into smooth surface patches, greatly reducing the difficulty of recovering the parametric CAD design. A comprehensive comparison with existing state-of-the-art methods shows the significant advantage of our approach in reconstructing faithful CAD shapes.",
    "scholar_publication": "arXiv preprint arXiv …, 2024 - arxiv.org"
  },
  {
    "paper_id": "papers_593",
    "authors": "Stephanie Wenxin Liu, Michael Fischer, Paul D. Yoo, Tobias Ritschel",
    "title": "Neural Bounding",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657442",
    "pdf_link": null,
    "abstract": "… Bounding volumes are an established concept in computer graphics and vision tasks but … the use of neural networks as bounding volumes. Our key observation is that bounding, which …",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_175",
    "authors": "Zilu Li, Guandao Yang, Qingqing Zhao, Xi Deng, Leonidas Guibas, Bharath Hariharan, Gordon Wetzstein",
    "title": "Neural Control Variates With Automatic Integration",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657395",
    "pdf_link": null,
    "abstract": "This paper presents a method to leverage arbitrary neural network architecture for control variates. Control variates are crucial in reducing the variance of Monte Carlo integration, but they hinge on finding a function that both correlates with the integrand and has a known analytical integral. Traditional approaches rely on heuristics to choose this function, which might not be expressive enough to correlate well with the integrand. Recent research alleviates this issue by modeling the integrands with a learnable parametric model, such as a neural network. However, the challenge remains in creating an expressive parametric model with a known analytical integral. This paper proposes a novel approach to construct learnable parametric control variates functions from arbitrary neural network architectures. Instead of using a network to approximate the integrand directly, we employ the network to approximate the anti-derivative of the integrand. This allows us to use automatic differentiation to create a function whose integration can be constructed by the antiderivative network. We apply our method to solve partial differential equations using the Walk-on-sphere algorithm [Sawhney and Crane 2020]. Our results indicate that this approach is unbiased using various network architectures and achieves lower variance than other control variate methods.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_377",
    "authors": "Felix Mujkanovic, Ntumba Elie Nsampi, Christian Theobalt, Hans-Peter Seidel, Thomas Leimkühler",
    "title": "Neural Gaussian Scale-space Fields",
    "paper_url": "https://arxiv.org/abs/2405.20980",
    "pdf_link": null,
    "abstract": "Gaussian scale spaces are a cornerstone of signal representation and processing, with applications in filtering, multiscale analysis, anti-aliasing, and many more. However, obtaining such a scale space is costly and cumbersome, in particular for continuous representations such as neural fields. We present an efficient and lightweight method to learn the fully continuous, anisotropic Gaussian scale space of an arbitrary signal. Based on Fourier feature modulation and Lipschitz bounding, our approach is trained self-supervised, i.e., training does not require any manual filtering. Our neural Gaussian scale-space fields faithfully capture multiscale representations across a broad range of modalities, and support a diverse set of applications. These include images, geometry, light-stage data, texture anti-aliasing, and multiscale optimization.",
    "scholar_publication": "arXiv preprint arXiv …, 2024 - arxiv.org"
  },
  {
    "paper_id": "papers_206",
    "authors": "Venkataram Edavamadathil Sivaram, Tzu-Mao Li, Ravi Ramamoorthi",
    "title": "Neural Geometry Fields for Meshes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657399",
    "pdf_link": null,
    "abstract": "… neural networks by displacing the patches. We then extract a traditional triangular mesh from a neural geometry field … We show that our representation excels in mesh compression, …",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_569",
    "authors": "Pranav Jain, Ziyin Qu, Peter Yichen Chen, Oded Stein",
    "title": "Neural Monte Carlo Fluid Simulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657438",
    "pdf_link": null,
    "abstract": "The idea of using a neural network to represent continuous vector fields (i.e., neural fields) has become popular for solving PDEs arising from physics simulations. Here, the classical spatial discretization (e.g., finite difference) of PDE solvers is replaced with a neural network that models a differentiable function, so the spatial gradients of the PDEs can be readily computed via autodifferentiation. When used in fluid simulation, however, neural fields fail to capture many important phenomena, such as the vortex shedding experienced in the von Kármán vortex street experiment. We present a novel neural network representation for fluid simulation that augments neural fields with explicitly enforced boundary conditions as well as a Monte Carlo pressure solver to get rid of all weakly enforced boundary conditions. Our method, the Neural Monte Carlo method (NMC), is completely mesh-free, i.e., it doesn’t depend on any grid-based discretization. While NMC does not achieve the state-of-the-art accuracy of the well-established grid-based methods, it significantly outperforms previous mesh-free neural fluid methods on fluid flows involving intricate boundaries and turbulence regimes.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_885",
    "authors": "Tao Liu, Tianyu Zhang, Yongxue Chen, Yuming Huang, Charlie C.L. Wang",
    "title": "Neural Slicer for Multi-axis 3D Printing",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658212",
    "pdf_link": null,
    "abstract": "We introduce a novel neural network-based computational pipeline as a representation-agnostic slicer for multi-axis 3D printing. This advanced slicer can work on models with diverse representations and intricate topology. The approach involves employing neural networks to establish a deformation mapping, defining a scalar field in the space surrounding an input model. Isosurfaces are subsequently extracted from this field to generate curved layers for 3D printing. Creating a differentiable pipeline enables us to optimize the mapping through loss functions directly defined on the field gradients as the local printing directions. New loss functions have been introduced to meet the manufacturing objectives of support-free and strength reinforcement. Our new computation pipeline relies less on the initial values of the field and can generate slicing results with significantly improved performance.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_283",
    "authors": "Xudong Feng, Huamin Wang, Yin Yang, Weiwei Xu",
    "title": "Neural-assisted Homogenization of Yarn-level Cloth",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657411",
    "pdf_link": null,
    "abstract": "Real-world fabrics, composed of threads and yarns, often display complex stress-strain relationships, making their homogenization a challenging task for fast simulation by continuum-based models. Consequently, existing homogenized yarn-level models frequently struggle with numerical stability without line search at large time steps, forcing a trade-off between model accuracy and stability. In this paper, we propose a neural-assisted homogenized constitutive model for simulating yarn-level cloth. Unlike analytic models, a …",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_652",
    "authors": "Yuxiang Cai, Jiaxiong Qiu, Zhong Li, Bo Ren",
    "title": "NeuralTO: Neural Reconstruction and View Synthesis of Translucent Objects",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658186",
    "pdf_link": null,
    "abstract": "Learning from multi-view images using neural implicit signed distance functions shows impressive performance on 3D Reconstruction of opaque objects. However, existing methods struggle to reconstruct accurate geometry when applied to translucent objects due to the non-negligible bias in their rendering function. To address the inaccuracies in the existing model, we have reparameterized the density function of the neural radiance field by incorporating an estimated constant extinction coefficient. This modification forms the basis of our innovative framework, which is geared towards highfidelity surface reconstruction and the novel-view synthesis of translucent objects. Our framework contains two stages. In the reconstruction stage, we introduce a novel weight function to achieve accurate surface geometry reconstruction. Following the recovery of geometry, the second phase involves learning the distinct scattering properties of the participating media to enhance rendering. A comprehensive dataset, comprising both synthetic and real translucent objects, has been built for conducting extensive experiments. Experiments reveal that our method outperforms existing approaches in terms of reconstruction and novel-view synthesis.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_115",
    "authors": "Doyub Kim, Minjae Lee, Ken Museth, Doyub Kim",
    "title": "NeuralVDB: High-resolution Sparse Volume Representation using Hierarchical Neural Networks",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641817",
    "pdf_link": null,
    "abstract": "We introduce NeuralVDB, which improves on an existing industry standard for efficient storage of sparse volumetric data, denoted VDB [Museth ], by leveraging recent advancements in machine learning. Our novel hybrid data structure can reduce the memory footprints of VDB volumes by orders of magnitude, while maintaining its flexibility and only incurring small (user-controlled) compression errors. Specifically, NeuralVDB replaces the lower nodes of a shallow and wide VDB tree structure with multiple hierarchical neural networks that separately encode topology and value information by means of neural classifiers and regressors respectively. This approach is proven to maximize the compression ratio while maintaining the spatial adaptivity offered by the higher-level VDB data structure. For sparse signed distance fields and density volumes, we have observed compression ratios on the order of 10× to more than 100× from already compressed VDB inputs, with little to no visual artifacts. Furthermore, NeuralVDB is shown to offer more effective compression performance compared to other neural representations such as Neural Geometric Level of Detail [Takikawa et al. ], Variable Bitrate Neural Fields [Takikawa et al. ], and Instant Neural Graphics Primitives [Müller et al. ]. Finally, we demonstrate how warm-starting from previous frames can accelerate training, i.e., compression, of animated volumes as well as improve temporal coherency of model inference, i.e., decompression.",
    "scholar_publication": "ACM Transactions on Graphics, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_261",
    "authors": "Zhengzhe Liu, Qing Liu, Chirui Chang, Jianming Zhang, Daniil Pakhomov, Haitian Zheng, Zhe Lin, Daniel Cohen-Or, Chi-Wing Fu",
    "title": "Object-level Scene Deocclusion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657409",
    "pdf_link": null,
    "abstract": "… We believe many more applications would require object-level deocclusion, and particularly, more research effort are needed to achieving fully automated deocclusion. …",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_739",
    "authors": "Arman Maesumi, Dylan Hu, Krishi Saripalli, Vladimir Kim, Matthew Fisher, Soeren Pirk, Daniel Ritchie",
    "title": "One Noise to Rule Them All: Learning a Unified Model of Spatially-Varying Noise Patterns",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658195",
    "pdf_link": null,
    "abstract": "Procedural noise is a fundamental component of computer graphics pipelines, offering a flexible way to generate textures that exhibit \"natural\" random variation. Many different types of noise exist, each produced by a separate algorithm. In this paper, we present a single generative model which can learn to generate multiple types of noise as well as blend between them. In addition, it is capable of producing spatially-varying noise blends despite not having access to such data for training. These features are enabled by training a denoising diffusion model using a novel combination of data augmentation and network conditioning techniques. Like procedural noise generators, the model's behavior is controllable via interpretable parameters plus a source of randomness. We use our model to produce a variety of visually compelling noise textures. We also present an application of our model to improving inverse procedural material design; using our model in place of fixed-type noise nodes in a procedural material graph results in higher-fidelity material reconstructions without needing to know the type of noise in advance. Open-sourced materials can be found at https://armanmaesumi.github.io/onenoise/",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_103",
    "authors": "Jiawei Huang, Akito Iizuka, Hajime Tanaka, Taku Komura, Yoshifumi Kitamura, Jiawei Huang",
    "title": "Online Neural Path Guiding with Normalized Anisotropic Spherical Gaussians",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3649310",
    "pdf_link": null,
    "abstract": "Importance sampling techniques significantly reduce variance in physically based rendering. In this article, we propose a novel online framework to learn the spatial-varying distribution of the full product of the rendering equation, with a single small neural network using stochastic ray samples. The learned distributions can be used to efficiently sample the full product of incident light. To accomplish this, we introduce a novel closed-form density model, called the Normalized Anisotropic Spherical Gaussian mixture, that can model a complex light field with a small number of parameters and that can be directly sampled. Our framework progressively renders and learns the distribution, without requiring any warm-up phases. With the compact and expressive representation of our density model, our framework can be implemented entirely on the GPU, allowing it to produce high-quality images with limited computational resources. The results show that our framework outperforms existing neural path guiding approaches and achieves comparable or even better performance than state-of-the-art online statistical path guiding techniques.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_142",
    "authors": "Kenneth Chen, Thomas Wan, Nathan Matsuda, Ajit Ninan, Alexandre Chapiro, Qi Sun",
    "title": "PEA-PODs: Perceptual Evaluation of Algorithms for Power Optimization in XR Displays",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658126",
    "pdf_link": null,
    "abstract": "Display power consumption is an emerging concern for untethered devices. This goes double for augmented and virtual extended reality (XR) displays, which target high refresh rates and high resolutions while conforming to an ergonomically light form factor. A number of image mapping techniques have been proposed to extend battery usage. However, there is currently no comprehensive quantitative understanding of how the power savings provided by these methods compare to their impact on visual quality. We set out to answer this question. To this end, we present a perceptual evaluation of algorithms (PEA) for power optimization in XR displays (PODs). Consolidating a portfolio of six power-saving display mapping approaches, we begin by performing a large-scale perceptual study to understand the impact of each method on perceived quality in the wild. This results in a unified quality score for each technique, scaled in just-objectionable-difference (JOD) units. In parallel, each technique is analyzed using hardware-accurate power models. The resulting JOD-to-Milliwatt transfer function provides a first-of-its-kind look into tradeoffs offered by display mapping techniques, and can be directly employed to make architectural decisions for power budgets on XR displays. Finally, we leverage our study data and power models to address important display power applications like the choice of display primary, power implications of eye tracking, and more1.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_875",
    "authors": "Anran Liu, Cheng Lin, Yuan Liu, Xiaoxiao Long, Zhiyang Dou, Hao-Xiang Guo, Ping Luo, Wenping Wang",
    "title": "Part123: Part-aware 3D Reconstruction From a Single-view Image",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657482",
    "pdf_link": null,
    "abstract": "Recently, the emergence of diffusion models has opened up new opportunities for single-view reconstruction. However, all the existing methods represent the target object as a closed mesh devoid of any structural information, thus neglecting the part-based structure, which is crucial for many downstream applications, of the reconstructed shape. Moreover, the generated meshes usually suffer from large noises, unsmooth surfaces, and blurry textures, making it challenging to obtain satisfactory part segments using 3D segmentation techniques. In this paper, we present Part123, a novel framework for part-aware 3D reconstruction from a single-view image. We first use diffusion models to generate multiview-consistent images from a given image, and then leverage Segment Anything Model (SAM), which demonstrates powerful generalization ability on arbitrary objects, to generate multiview segmentation masks. To effectively incorporate 2D part-based information into 3D reconstruction and handle inconsistency, we introduce contrastive learning into a neural rendering framework to learn a part-aware feature space based on the multiview segmentation masks. A clustering-based algorithm is also developed to automatically derive 3D part segmentation results from the reconstructed models. Experiments show that our method can generate 3D models with high-quality segmented parts on various objects. Compared to existing unstructured reconstruction methods, the part-aware 3D models from our method benefit some important applications, including feature-preserving reconstruction, primitive fitting, and 3D shape editing.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_827",
    "authors": "Siwei Zhou, Youngha Chang, Nobuhiko Mukai, Hiroaki Santo, Fumio Okura, Yasuyuki Matsushita, Shuang Zhao",
    "title": "Path-space Differentiable Rendering of Implicit Surfaces",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657473",
    "pdf_link": null,
    "abstract": "Physics-based differentiable rendering is a key ingredient for integrating forward rendering into probabilistic inference and machine learning pipelines. As a state-of-the-art formulation for differentiable rendering, differential path integrals have enabled the development of efficient Monte Carlo estimators for both interior and boundary integrals. Unfortunately, this formulation has been designed mostly for explicit geometries like polygonal meshes. In this paper, we generalize the theory of differential path integrals to support implicit geometries …",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_890",
    "authors": "Seungjae Lee, Seung-Woo Nam, Kevin Rio, Renate Landig, Hsien-Hui Cheng, Lu Lu, Barry Silverstein",
    "title": "Perceptual Evaluation of Steered Retinal Projection",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657486",
    "pdf_link": null,
    "abstract": "Steered retinal projection (SRP) is an emerging display technology that combines retinal projection and pupil steering to achieve exceptional light efficiency and a consistent viewing experience. Retinal projection enables most photons from a display projector to reach the retina, and pupil steering dynamically aligns the narrow viewing window of the retinal projection with the eye. While SRP holds considerable promise, its development has been stagnant due to a lack of understanding how human vision reacts to the dynamic steering movement of the viewing window. To delve into these areas, this study introduces the first SRP system testbed specifically designed for perceptual studies on the viewing experience of pupil steering. The testbed replicates the SRP viewing experience and offers the flexibility in adjusting several parameters including steering resolution, accuracy, and latency. We conducted two perceptual studies utilizing the testbed. The first study investigates the impact of saccadic suppression, a phenomenon that reduces visual sensitivity during rapid eye movements, on the SRP viewing experience. The second study explores the trade space between eye-tracking and pupil steering performance, providing insights into the optimal balance between these factors. Additionally, we introduce a numerical model to predict the detection probability for SRP artifacts considering the temporal characteristics of global luminance and the human vision system. This model enables a more comprehensive interpretation of user study and provides preliminary hardware requirements for SRP systems. The findings from this study offer invaluable research directions that may help determine component-level development milestones for SRP development, paving the way for the practical implementation of this promising technology.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_548",
    "authors": "Xinyu Yi, Yuxiao Zhou, Feng Xu",
    "title": "Physical Non-inertial Poser (PNP): Modeling Non-inertial Effects in Sparse-inertial Human Motion Capture",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657436",
    "pdf_link": null,
    "abstract": "Existing inertial motion capture techniques use the human root coordinate frame to estimate local poses and treat it as an inertial frame by default. We argue that when the root has linear acceleration or rotation, the root frame should be considered non-inertial theoretically. In this paper, we model the fictitious forces that are non-neglectable in a non-inertial frame by an auto-regressive estimator delicately designed following physics. With the fictitious forces, the force-related IMU measurement (accelerations) can be correctly compensated in the non-inertial frame and thus Newton’s laws of motion are satisfied. In this case, the relationship between the accelerations and body motions is deterministic and learnable, and we train a neural network to model it for better motion capture. Furthermore, to train the neural network with synthetic data, we develop an IMU synthesis by simulation strategy to better model the noise model of IMU hardware and allow parameter tuning to fit different hardware. This strategy not only establishes the network training with synthetic data but also enables calibration error modeling to handle bad motion capture calibration, increasing the robustness of the system. Code is available at https://xinyu-yi.github.io/PNP/.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1127",
    "authors": "Jianan Li, Tao Huang, Qingxu Zhu, Tien-Tsin Wong",
    "title": "Physics-based Scene Layout Generation From Human Motion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657517",
    "pdf_link": null,
    "abstract": "Creating scenes for captured motions that achieve realistic human-scene interaction is crucial for 3D animation in movies or video games. As character motion is often captured in a blue-screened studio without real furniture or objects in place, there may be a discrepancy between the planned motion and the captured one. This gives rise to the need for automatic scene layout generation to relieve the burdens of selecting and positioning furniture and objects. Previous approaches cannot avoid artifacts like penetration and floating due to the lack of physical constraints. Furthermore, some heavily rely on specific data to learn the contact affordances, restricting the generalization ability to different motions. In this work, we present a physics-based approach that simultaneously optimizes a scene layout generator and simulates a moving human in a physics simulator. To attain plausible and realistic interaction motions, our method explicitly introduces physical constraints. To automatically recover and generate the scene layout, we minimize the motion tracking errors to identify the objects that can afford interaction. We use reinforcement learning to perform a dual-optimization of both the character motion imitation controller and the scene layout generator. To facilitate the optimization, we reshape the tracking rewards and devise pose prior guidance obtained from our estimated pseudo-contact labels. We evaluate our method using motions from SAMP and PROX, and demonstrate physically plausible scene layout reconstruction compared with the previous kinematics-based method.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_880",
    "authors": "Yiming Wang, Siyu Tang, Mengyu Chu",
    "title": "Physics-informed Learning of Characteristic Trajectories for Smoke Reconstruction",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657483",
    "pdf_link": null,
    "abstract": "We delve into the physics-informed neural reconstruction of smoke and obstacles through sparse-view RGB videos, tackling challenges arising from limited observation of complex dynamics. Existing physics-informed neural networks often emphasize short-term physics constraints, leaving the proper preservation of long-term conservation less explored. We introduce Neural Characteristic Trajectory Fields, a novel representation utilizing Eulerian neural fields to implicitly model Lagrangian fluid trajectories. This topology-free, auto-differentiable representation facilitates efficient flow map calculations between arbitrary frames as well as efficient velocity extraction via auto-differentiation. Consequently, it enables end-to-end supervision covering long-term conservation and short-term physics priors. Building on the representation, we propose physics-informed trajectory learning and integration into NeRF-based scene reconstruction. We enable advanced obstacle handling through self-supervised scene decomposition and seamless integrated boundary constraints. Our results showcase the ability to overcome challenges like occlusion uncertainty, density-color ambiguity, and static-dynamic entanglements. Code and sample tests are at https://github.com/19reborn/PICT_smoke.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_376",
    "authors": "Yiqian Wu, Hao Xu, Xiangjun Tang, Xien Chen, Siyu Tang, Zhebin Zhang, Chen Li, Xiaogang Jin",
    "title": "Portrait3D: Text-guided High-quality 3D Portrait Generation Using Pyramid Representation and GANs Prior",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658162",
    "pdf_link": null,
    "abstract": "Existing neural rendering-based text-to-3D-portrait generation methods typically make use of human geometry prior and diffusion models to obtain guidance. However, relying solely on geometry information introduces issues such as the Janus problem, over-saturation, and over-smoothing. We present Portrait3D, a novel neural rendering-based framework with a novel joint geometry-appearance prior to achieve text-to-3D-portrait generation that overcomes the aforementioned issues. To accomplish this, we train a 3D portrait generator …",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_328",
    "authors": "Yizhou Chen, Yushan Han, Jingyu Chen, Zhan Zhang, Alex McAdams, Joseph Teran",
    "title": "Position-based Nonlinear Gauss-Seidel for Quasistatic Hyperelasticity",
    "paper_url": "https://arxiv.org/abs/2306.09021",
    "pdf_link": null,
    "abstract": "Position based dynamics is a powerful technique for simulating a variety of materials. Its primary strength is its robustness when run with limited computational budget. We develop a novel approach to address problems with PBD for quasistatic hyperelastic materials. Even though PBD is based on the projection of static constraints, PBD is best suited for dynamic simulations. This is particularly relevant since the efficient creation of large data sets of plausible, but not necessarily accurate elastic equilibria is of increasing importance with the emergence of quasistatic neural networks. Furthermore, PBD projects one constraint at a time. We show that ignoring the effects of neighboring constraints limits its convergence and stability properties. Recent works have shown that PBD can be related to the Gauss-Seidel approximation of a Lagrange multiplier formulation of backward Euler time stepping, where each constraint is solved/projected independently of the others in an iterative fashion. We show that a position-based, rather than constraint-based nonlinear Gauss-Seidel approach solves these problems. Our approach retains the essential PBD feature of stable behavior with constrained computational budgets, but also allows for convergent behavior with expanded budgets. We demonstrate the efficacy of our method on a variety of representative hyperelastic problems and show that both successive over relaxation (SOR) and Chebyshev acceleration can be easily applied.",
    "scholar_publication": "arXiv preprint arXiv:2306.09021, 2023 - arxiv.org"
  },
  {
    "paper_id": "papers_1066",
    "authors": "Arthur Firmino, Ravi Ramamoorthi, Jeppe Revall Frisvad, Henrik Wann Jensen",
    "title": "Practical Error Estimation for Denoised Monte Carlo Image Synthesis",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657511",
    "pdf_link": null,
    "abstract": "We present a practical global error estimation technique for Monte Carlo ray tracing combined with deep learning based denoising. Our method uses aggregated estimates of bias and variance to determine the squared error distribution of the pixels. Unlike unbiased estimates for classical Monte Carlo ray tracing, this distribution follows a noncentral chi-squared distribution, under reasonable assumptions. Based on this, we develop a stopping criterion for denoised Monte Carlo image synthesis that terminates rendering once a user specified error threshold has been achieved. Our results demonstrate that our error estimate and stopping criterion work well on a variety of scenes, and that we are able to achieve a given error threshold without the user specifying the number of samples needed.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_907",
    "authors": "Xing Shen, Runyuan Cai, Mengxiao Bi, Tangjie Lv",
    "title": "Preconditioned Nonlinear Conjugate Gradient Method for Real-time Interior-point Hyperelasticity",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657490",
    "pdf_link": null,
    "abstract": "The linear conjugate gradient method is widely used in physical simulation, particularly for solving large-scale linear systems derived from Newton’s method. The nonlinear conjugate gradient method generalizes the conjugate gradient method to nonlinear optimization, which is extensively utilized in solving practical large-scale unconstrained optimization problems. However, it is rarely discussed in physical simulation due to the requirement of multiple vector-vector dot products. Fortunately, with the advancement of GPU-parallel acceleration techniques, it is no longer a bottleneck. In this paper, we propose a Jacobi preconditioned nonlinear conjugate gradient method for elastic deformation using interior-point methods. Our method is straightforward, GPU-parallelizable, and exhibits fast convergence and robustness against large time steps. The employment of the barrier function in interior-point methods necessitates continuous collision detection per iteration to obtain a penetration-free step size, which is computationally expensive and challenging to parallelize on GPUs. To address this issue, we introduce a line search strategy that deduces an appropriate step size in a single pass, eliminating the need for additional collision detection. Furthermore, we simplify and accelerate the computations of Jacobi preconditioning and Hessian-vector product for hyperelasticity and barrier function. Our method can accurately simulate objects comprising over 100,000 tetrahedra in complex self-collision scenarios at real-time speeds.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_884",
    "authors": "Yi-Lu Chen, Mickaël Ly, Chris Wojtan",
    "title": "Primal-dual Non-smooth Friction for Rigid Body Animation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657485",
    "pdf_link": null,
    "abstract": "Current numerical algorithms for simulating friction fall in one of two camps: smooth solvers sacrifice the stable treatment of static friction in exchange for fast convergence, and non-smooth solvers accurately compute friction at convergence rates that are often prohibitive for large graphics applications. We introduce a novel bridge between these two ideas that computes static and dynamic friction stably and efficiently. Our key idea is to convert the highly constrained non-smooth problem into an unconstrained smooth problem using logarithmic barriers that converges to the exact solution as accuracy increases. We phrase the problem as an interior point primal-dual problem that can be solved efficiently with Newton iteration. We observe quadratic convergence despite the non-smooth nature of the original problem, and our method is well-suited for large systems of tightly packed objects with many contact points. We demonstrate the efficacy of our method with stable piles of grains and stacks of objects, complex granular flows, and robust interlocking assemblies of rigid bodies.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_893",
    "authors": "Jiayi Eris Zhang, Doug James, Danny M. Kaufman",
    "title": "Progressive Dynamics for Cloth and Shell Animation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658214",
    "pdf_link": null,
    "abstract": "We propose Progressive Dynamics, a coarse-to-fine, level-of-detail simulation method for the physics-based animation of complex frictionally contacting thin shell and cloth dynamics. Progressive Dynamics provides tight-matching consistency and progressive improvement across levels, with comparable quality and realism to high-fidelity, IPC-based shell simulations [Li et al. 2021] at finest resolutions. Together these features enable an efficient animation-design pipeline with predictive coarse-resolution previews providing rapid design iterations for a final, to-be-generated, high-resolution animation. In contrast, previously, to design such scenes with comparable dynamics would require prohibitively slow design iterations via repeated direct simulations on high-resolution meshes. We evaluate and demonstrate Progressive Dynamics's features over a wide range of challenging stress-tests, benchmarks, and animation design tasks. Here Progressive Dynamics efficiently computes consistent previews at costs comparable to coarsest-level direct simulations. Its matching progressive refinements across levels then generate rich, high-resolution animations with high-speed dynamics, impacts, and the complex detailing of the dynamic wrinkling, folding, and sliding of frictionally contacting thin shells and fabrics.",
    "scholar_publication": "ACM Transactions on Graphics …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_107",
    "authors": "Willi Menapace, Aliaksandr Siarohin, Stéphane Lathuilière, Panos Achlioptas, Vladislav Golyanik, Sergey Tulyakov, Elisa Ricci, Willi Menapace",
    "title": "Promptable Game Models: Text-Guided Game Simulation via Masked Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3635705",
    "pdf_link": null,
    "abstract": "Neural video game simulators emerged as powerful tools to generate and edit videos. Their idea is to represent games as the evolution of an environment’s state driven by the actions of its agents. While such a paradigm enables users to play a game action-by-action, its rigidity precludes more semantic forms of control. To overcome this limitation, we augment game models with prompts specified as a set of natural language actions and desired states. The result—a Promptable Game Model (PGM)—makes it possible for a user to play the game by prompting it with high- and low-level action sequences. Most captivatingly, our PGM unlocks the director’s mode, where the game is played by specifying goals for the agents in the form of a prompt. This requires learning “game AI,” encapsulated by our animation model, to navigate the scene using high-level constraints, play against an adversary, and devise a strategy to win a point. To render the resulting state, we use a compositional NeRF representation encapsulated in our synthesis model. To foster future research, we present newly collected, annotated and calibrated Tennis and Minecraft datasets. Our method significantly outperforms existing neural video game simulators in terms of rendering quality and unlocks applications beyond the capabilities of the current state-of-the-art. Our framework, data, and models are available at snap-research.github.io/promptable-game-models.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_560",
    "authors": "Zhongtian Zheng, Tongtong Wang, Qijia Feng, Zherong Pan, Xifeng Gao, Kui Wu",
    "title": "Proxy Asset Generation for Cloth Simulation in Games",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658177",
    "pdf_link": null,
    "abstract": "Simulating high-resolution cloth poses computational challenges in real-time applications. In the gaming industry, the proxy mesh technique offers an alternative, simulating a simplified low-resolution cloth geometry, proxy mesh. This proxy mesh's dynamics drive the detailed high-resolution geometry, visual mesh, through Linear Blended Skinning (LBS). However, generating a suitable proxy mesh with appropriate skinning weights from a given visual mesh is non-trivial, often requiring skilled artists several days for fine-tuning. This paper presents an automatic pipeline to convert an ill-conditioned highresolution visual mesh into a single-layer low-poly proxy mesh. Given that the input visual mesh may not be simulation-ready, our approach then simulates the proxy mesh based on specific use scenarios and optimizes the skinning weights, relying on differential skinning with several well-designed loss functions to ensure the skinned visual mesh appears plausible in the final simulation. We have tested our method on various challenging cloth models, demonstrating its robustness and effectiveness.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_913",
    "authors": "Fujia Su, Bingxuan Li, Qingyang Yin, Yanchen Zhang, Sheng Li",
    "title": "Proxy Tracing: Unbiased Reciprocal Estimation for Optimized Sampling in BDPT",
    "paper_url": "https://arxiv.org/abs/2503.23412",
    "pdf_link": null,
    "abstract": "Robust light transport algorithms, particularly bidirectional path tracing (BDPT), face significant challenges when dealing with specular or highly glossy involved paths. BDPT constructs the full path by connecting sub-paths traced individually from the light source and camera. However, it remains difficult to sample by connecting vertices on specular and glossy surfaces with narrow-lobed BSDF, as it poses severe constraints on sampling in the feasible direction. To address this issue, we propose a novel approach, called \\emph{proxy sampling}, that enables efficient sub-path connection of these challenging paths. When a low-contribution specular/glossy connection occurs, we drop out the problematic neighboring vertex next to this specular/glossy vertex from the original path, then retrace an alternative sub-path as a proxy to complement this incomplete path. This newly constructed complete path ensures that the connection adheres to the constraint of the narrow lobe within the BSDF of the specular/glossy surface. Unbiased reciprocal estimation is the key to our method to obtain a probability density function (PDF) reciprocal to ensure unbiased rendering. We derive the reciprocal estimation method and provide an efficiency-optimized setting for efficient sampling and connection. Our method provides a robust tool for substituting problematic paths with favorable alternatives while ensuring unbiasedness. We validate this approach in the probabilistic connections BDPT for addressing specular-involved difficult paths. Experimental results have proved the effectiveness and efficiency of our approach, showcasing high-performance rendering capabilities across diverse settings.",
    "scholar_publication": "arXiv preprint arXiv:2503.23412, 2025 - arxiv.org"
  },
  {
    "paper_id": "papers_616",
    "authors": "Yitian Liu, Zhouhui Lian",
    "title": "QT-Font: High-efficiency Font Synthesis via Quadtree-based Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657451",
    "pdf_link": null,
    "abstract": "Few-shot font generation (FFG) aims to streamline the manual aspects of the font design process. Existing models are capable of generating glyph images in the same style of a few input reference glyphs. However, mainly due to their inefficient glyph representations, these existing FFG methods are limited to generating low-resolution glyph images. To address this problem, we introduce QT-Font, an efficient quadtree-based diffusion model specifically designed for FFG. More specifically, we design a sparse quadtree-based glyph representation to reduce the complexity of the representation space, exhibiting linear complexity and uniqueness. Concurrently, to reduce computational complexity, we propose a U-net model based on the dual quadtree graph network and the discrete diffusion model. Furthermore, a content-aware pooling module is also adopted to lessen the computational demands of the diffusion process. Qualitative and quantitative experiments have been conducted to demonstrate that our QT-Font, compared to existing approaches, can generate high-resolution glyph images with superior quality and more visually pleasing details, meanwhile significantly reducing both parameter sizes and computational costs.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_469",
    "authors": "Victor Ostromoukhov, David Coeurjolly, Nicolas Bonneel, Jean-Claude Iehl",
    "title": "Quad-optimized Low-discrepancy Sequences",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657431",
    "pdf_link": null,
    "abstract": "The convergence of Monte Carlo integration is given by the uniformity of samples as well as the regularity of the integrand. Despite much effort dedicated to producing excellent, extremely uniform, sampling patterns, the Sobol’ sampler remains unchallenged in production rendering systems. This is not only due to its reasonable quality, but also because it allows for integration in (almost) arbitrary dimension, with arbitrary sample count, while actually producing sequences thus allowing for progressive rendering, with fast sample generation and small memory footprint. We improve over Sobol’ sequences in terms of sample uniformity in consecutive 2-d and 4-d projections, while providing similar practical benefits – sequences, high dimensionality, speed and compactness. We base our contribution on a base-3 Sobol’ construction, involving a search over irreducible polynomials and generator matrices, that produce (1, 4)-sequences or (2,4)-sequences in all consecutive quadruplets of dimensions, and (0, 2)-sequence in all consecutive pairs of dimensions. We provide these polynomials and matrices that may be used as a replacement of Joe & Kuo’s widely used ones, with computational overhead, for moderate-dimensional problems.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_604",
    "authors": "Zheng Zeng, Valentin Deschaintre, Iliyan Georgiev, Yannick Hold-Geoffroy, Yiwei Hu, Fujun Luan, Ling-Qi Yan, Miloš Hašan",
    "title": "RGB↔X: Image Decomposition and Synthesis Using Material- and Lighting-aware Diffusion Models",
    "paper_url": "https://arxiv.org/abs/2509.03680",
    "pdf_link": null,
    "abstract": "… In this paper, we build on CogVideoX [62], a latent video diffusion model trained on … RGB↔X: image decomposition and synthesis using material-and lighting-aware diffusion models. In …",
    "scholar_publication": "arXiv preprint arXiv …, 2025 - arxiv.org"
  },
  {
    "paper_id": "papers_648",
    "authors": "Zhexi Peng, Tianjia Shao, Yong Liu, Jingke Zhou, Yin Yang, Jingdong Wang, Kun Zhou",
    "title": "RTG-SLAM: Real-time 3D Reconstruction at Scale Using Gaussian Splatting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657455",
    "pdf_link": null,
    "abstract": "We present Real-time Gaussian SLAM (RTG-SLAM), a real-time 3D reconstruction system with an RGBD camera for large-scale environments using Gaussian splatting. The system features a compact Gaussian representation and a highly efficient on-the-fly Gaussian optimization scheme. We force each Gaussian to be either opaque or nearly transparent, with the opaque ones fitting the surface and dominant colors, and transparent ones fitting residual colors. By rendering depth in a different way from color rendering, we let a single opaque Gaussian well fit a local surface region without the need of multiple overlapping Gaussians, hence largely reducing the memory and computation cost. For on-the-fly Gaussian optimization, we explicitly add Gaussians for three types of pixels per frame: newly observed, with large color errors, and with large depth errors. We also categorize all Gaussians into stable and unstable ones, where the stable Gaussians are expected to well fit previously observed RGBD images and otherwise unstable. We only optimize the unstable Gaussians and only render the pixels occupied by unstable Gaussians. In this way, both the number of Gaussians to be optimized and pixels to be rendered are largely reduced, and the optimization can be done in real time. We show real-time reconstructions of a variety of large scenes. Compared with the state-of-the-art NeRF-based RGBD SLAM, our system achieves comparable high-quality reconstruction but with around twice the speed and half the memory cost, and shows superior performance in the realism of novel view synthesis and camera tracking accuracy.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1064",
    "authors": "David Borts, Erich Liang, Jipeng Sun, Tim Broedermann, David Brueggemann, Christos Sakaridis, Luc Van Gool, Andrea Ramazzina, Edoardo Palladin, Stefanie Walz, Mario Bijelic, Felix Heide",
    "title": "Radar Fields: Frequency-space Neural Scene Representations for FMCW Radar",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657510",
    "pdf_link": null,
    "abstract": "Neural fields have been broadly investigated as scene representations for the reproduction and novel generation of diverse outdoor scenes, including those autonomous vehicles and robots must handle. While successful approaches for RGB and LiDAR data exist, neural reconstruction methods for radar as a sensing modality have been largely unexplored. Operating at millimeter wavelengths, radar sensors are robust to scattering in fog and rain, and, as such, offer a complementary modality to active and passive optical sensing techniques. Moreover, existing radar sensors are highly cost-effective and deployed broadly in robots and vehicles that operate outdoors. We introduce Radar Fields –- a neural scene reconstruction method designed for active radar imagers. Our approach unites an explicit, physics-informed sensor model with an implicit neural geometry and reflectance model to directly synthesize raw radar measurements and extract scene occupancy. The proposed method does not rely on volume rendering. Instead, we learn fields in Fourier frequency space, supervised with raw radar data. We validate our method’s effectiveness across diverse outdoor scenarios, including urban scenes with dense vehicles and infrastructure, and harsh weather scenarios, where mm-wavelength sensing is favorable.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_786",
    "authors": "Mark Gillespie, Denise Yang, Mario Botsch, Keenan Crane",
    "title": "Ray Tracing Harmonic Functions",
    "paper_url": "https://ls7-gv.cs.tu-dortmund.de/publications/2024-harnack.supplementary.pdf",
    "pdf_link": null,
    "abstract": "… how to express these functions as the restrictions of harmonic polynomials of degree ℓ to the unit … To ensure that our spherical harmonics have unit 𝐿2-norm over the sphere, we use the …",
    "scholar_publication": "ACM Trans …, 2024 - ls7-gv.cs.tu-dortmund.de"
  },
  {
    "paper_id": "papers_1267",
    "authors": "Sergey Zakharov, Katherine Liu, Adrien Gaidon, Rares Ambrus",
    "title": "ReFiNe: Recursive Field Networks for Cross-modal Multi-scene Representation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657526",
    "pdf_link": null,
    "abstract": "The common trade-offs of state-of-the-art methods for multi-shape representation (a single model \"packing\" multiple objects) involve trading modeling accuracy against memory and storage. We show how to encode multiple shapes represented as continuous neural fields with a higher degree of precision than previously possible and with low memory usage. Key to our approach is a recursive hierarchical formulation that exploits object self-similarity, leading to a highly compressed and efficient shape latent space. Thanks to the recursive formulation, our method supports spatial and global-to-local latent feature fusion without needing to initialize and maintain auxiliary data structures, while still allowing for continuous field queries to enable applications such as raytracing. In experiments on a set of diverse datasets, we provide compelling qualitative results and demonstrate state-of-the-art multi-scene reconstruction and compression results with a single network per dataset.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_349",
    "authors": "Silvia Sellán, Yingying Ren, Christopher Batty, Oded Stein",
    "title": "Reach for the Arcs: Reconstructing Surfaces From SDFs via Tangent Points",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657419",
    "pdf_link": null,
    "abstract": "We introduce an algorithm to reconstruct a mesh from discrete samples of a shape’s Signed Distance Function (SDF). A simple geometric reinterpretation of the SDF lets us formulate the problem through a point cloud, from which a surface can be extracted with existing techniques. We extract all possible information from the SDF data, outperforming commonly used algorithms and imposing no topological or geometric restrictions.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_127",
    "authors": "Tizian Zeltner, Fabrice Rousselle, Andrea Weidlich, Petrik Clarberg, Jan Novák, Benedikt Bitterli, Alex Evans, Tomáš Davidovič, Simon Kallweit, Aaron Lefohn, Tizian Zeltner",
    "title": "Real-Time Neural Appearance Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3659577",
    "pdf_link": null,
    "abstract": "We present a complete system for real-time rendering of scenes with complex appearance previously reserved for offline use. This is achieved with a combination of algorithmic and system level innovations. Our appearance model utilizes learned hierarchical textures that are interpreted using neural decoders, which produce reflectance values and importance-sampled directions. To best utilize the modeling capacity of the decoders, we equip the decoders with two graphics priors. The first prior—transformation of directions into learned …",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1226",
    "authors": "Gaurav Bhokare, Eisen Montalvo, Elie Diaz, Cem Yuksel",
    "title": "Real-time Hair Rendering With Hair Meshes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657521",
    "pdf_link": null,
    "abstract": "Hair meshes are known to be effective for modeling and animating hair in computer graphics. We present how the hair mesh structure can be used for efficiently rendering strand-based hair models on the GPU with on-the-fly geometry generation that provides orders of magnitude reduction in storage and memory bandwidth. We use mesh shaders to carefully distribute the computation and a custom texture layout for offloading a part of the computation to the hardware texture units. We also present a set of procedural styling operations to achieve hair strand variations for a wide range of hairstyles and a consistent coordinate-frame generation approach to attach these variations to an animating/deforming hair mesh. Finally, we describe level-of-detail techniques for improving the performance of rendering distant hair models. Our results show an unprecedented level of performance with strand-based hair rendering, achieving hundreds of full hair models animated and rendered at real-time frame rates on a consumer GPU.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_957",
    "authors": "Xiang Chen, Lu Wang, Beibei Wang",
    "title": "Real-time Neural Woven Fabric Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657496",
    "pdf_link": null,
    "abstract": "Woven fabrics are widely used in applications of realistic rendering, where real-time capability is also essential. However, rendering realistic woven fabrics in real time is challenging due to their complex structure and optical appearance, which cause aliasing and noise without many samples. The core of this issue is a multi-scale representation of the fabric shading model, which allows for a fast range query. Some previous neural methods deal with the issue at the cost of training on each material, which limits their practicality. In this paper, we propose a lightweight neural network to represent different types of woven fabrics at different scales. Thanks to the regularity and repetitiveness of woven fabric patterns, our network can encode fabric patterns and parameters as a small latent vector, which is later interpreted by a small decoder, enabling the representation of different types of fabrics. By applying the pixel’s footprint as input, our network achieves multi-scale representation. Moreover, our network is fast and occupies little storage because of its lightweight structure. As a result, our method achieves rendering and editing woven fabrics at nearly 60 frames per second on an RTX 3090, showing a quality close to the ground truth and being free from visible aliasing and noise.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_810",
    "authors": "Haolin Lu, Wesley Chang, Trevor Hedstrom, Tzu-Mao Li",
    "title": "Real-time Path Guiding Using Bounding Voxel Sampling",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658203",
    "pdf_link": null,
    "abstract": "We propose a real-time path guiding method, Voxel Path Guiding (VXPG), that significantly improves fitting efficiency under limited sampling budget. Our key idea is to use a spatial irradiance voxel data structure across all shading points to guide the location of path vertices. For each frame, we first populate the voxel data structure with irradiance and geometry information. To sample from the data structure for a shading point, we need to select a voxel with high contribution to that point. To importance sample the voxels while …",
    "scholar_publication": "ACM Transactions on Graphics …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_559",
    "authors": "Jerry Hsu, Tongtong Wang, Zherong Pan, Xifeng Gao, Cem Yuksel, Kui Wu",
    "title": "Real-time Physically Guided Hair Interpolation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658176",
    "pdf_link": null,
    "abstract": "Strand-based hair simulations have recently become increasingly popular for a range of real-time applications. However, accurately simulating the full number of hair strands remains challenging. A commonly employed technique involves simulating a subset of guide hairs to capture the overall behavior of the hairstyle. Details are then enriched by interpolation using linear skinning. Hair interpolation enables fast real-time simulations but frequently leads to various artifacts during runtime. As the skinning weights are often pre-computed, substantial variations between the initial and deformed shapes of the hair can cause severe deviations in fine hair geometry. Straight hairs may become kinked, and curly hairs may become zigzags. This work introduces a novel physical-driven hair interpolation scheme that utilizes existing simulated guide hair data. Instead of directly operating on positions, we interpolate the internal forces from the guide hairs before efficiently reconstructing the rendered hairs based on their material model. We formulate our problem as a constraint satisfaction problem for which we present an efficient solution. Further practical considerations are addressed using regularization terms that regulate penetration avoidance and drift correction. We have tested various hairstyles to illustrate that our approach can generate visually plausible rendered hairs with only a few guide hairs and minimal computational overhead, amounting to only about 20% of conventional linear hair interpolation. This efficiency underscores the practical viability of our method for real-time applications.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_492",
    "authors": "Qiang Chen, Zhigang Deng, Feng Li, Yuming Fang, Tingsong Lu, Yang Tong, Yifan Zuo",
    "title": "Real-time Wing Deformation Simulations for Flying Insects",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657434",
    "pdf_link": null,
    "abstract": "Realistic simulation of the intricate wing deformations seen in flying insects not only deepens our comprehension of insect flight mechanics but also opens up numerous applications in fields such as computer animation and virtual reality. Despite its importance, this research area has been relatively underexplored due to the complex and diverse wing structures and the intricate patterns of deformation. This paper presents an efficient skeleton-driven model specifically designed to real-time simulate realistic wing deformations across a wide range of flying insects. Our approach begins with the construction of a virtual skeleton that accurately reflects the distinct morphological characteristics of individual insect species. This skeleton serves as the foundation for the simulation of the intricate deformation wave propagation often observed in wing deformations. To faithfully reproduce the bending effect seen in these deformations, we introduce both internal and external forces that act on the wing joints, drawing on periodic wing-beat motion and a simplified aerodynamics model. Additionally, we utilize mass-spring algorithms to simulate the inherent elasticity of the wings, helping to prevent excessive twisting. Through various simulation experiments, comparisons, and user studies, we demonstrate the effectiveness, robustness, and adaptability of our model.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1269",
    "authors": "Luming Tang, Nataniel Ruiz, Qinghao Chu, Yuanzhen Li, Aleksander Holynski, David E. Jacobs, Bharath Hariharan, Yael Pritch, Neal Wadhwa, Kfir Aberman, Michael Rubinstein",
    "title": "RealFill: Reference-driven Generation for Authentic Image Completion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658237",
    "pdf_link": null,
    "abstract": "Recent advances in generative imagery have brought forth outpainting and inpainting models that can produce high-quality, plausible image content in unknown regions. However, the content these models hallucinate is necessarily inauthentic, since they are unaware of the true scene. In this work, we propose RealFill, a novel generative approach for image completion that fills in missing regions of an image with the content that should have been there. RealFill is a generative inpainting model that is personalized using only a few reference images of a scene. These reference images do not have to be aligned with the target image, and can be taken with drastically varying viewpoints, lighting conditions, camera apertures, or image styles. Once personalized, RealFill is able to complete a target image with visually compelling contents that are faithful to the original scene. We evaluate RealFill on a new image completion benchmark that covers a set of diverse and challenging scenarios, and find that it outperforms existing approaches by a large margin. Project page: https://realfill.github.io.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_210",
    "authors": "Niklaus Houska, Cheryl Lau, Matthias Specht",
    "title": "Recompose Grammars for Procedural Architecture",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657400",
    "pdf_link": null,
    "abstract": "We present the novel grammar language Recomp for the procedural modeling of architecture. In grammar-based approaches, the procedural refinement process is based on shape subdivisions. This process of decomposition results in disconnected subparts, which not only restricts the geometric expressiveness but also limits the control over an appropriate shape granularity needed to coordinate design decisions. Recomp overcomes these limitations by extending grammar languages with the recomposition ability. Fundamental is the concept of rule inlining, allowing for the topological recomposition of edited subparts by collapsing a shape subtree into one single shape on which derivation can continue. This is completed with a versatile geometry tagging system, allowing authors to compile and transport context information at any level of detail and gain full control over the geometry independent of the structure of the shape tree. Through various examples, we demonstrate the power of Recomp in procedural layout and mass modeling, as well as its capabilities in facilitating context-sensitive design.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_503",
    "authors": "Josua Sassen, Henrik Schumacher, Martin Rumpf, Keenan Crane",
    "title": "Repulsive Shells",
    "paper_url": "https://hal.science/hal-04695470/",
    "pdf_link": null,
    "abstract": "… ), a geodesic in the space of elastic shells (center), and a geodesic in our repulsive shell space (bo om). Notice that the repulsive metric does not merely resolve local intersections near …",
    "scholar_publication": "ACM Transactions on …, 2024 - hal.science"
  },
  {
    "paper_id": "papers_217",
    "authors": "Junchen Liu, Wenbo Hu, Zhuo Yang, Jianteng Chen, Guoliang Wang, Xiaoxue Chen, Yantong Cai, Huan-ang Gao, Hao Zhao",
    "title": "Rip-NeRF: Anti-aliasing Radiance Fields With Ripmap-encoded Platonic Solids",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657402",
    "pdf_link": null,
    "abstract": "Despite significant advancements in Neural Radiance Fields (NeRFs), the renderings may still suffer from aliasing and blurring artifacts, since it remains a fundamental challenge to effectively and efficiently characterize anisotropic areas induced by the cone-casting procedure. This paper introduces a Ripmap-Encoded Platonic Solid representation to precisely and efficiently featurize 3D anisotropic areas, achieving high-fidelity anti-aliased renderings. Central to our approach are two key components: Platonic Solid Projection and Ripmap encoding. The Platonic Solid Projection factorizes the 3D space onto the unparalleled faces of a certain Platonic solid, such that the anisotropic 3D areas can be projected onto planes with distinguishable characterization. Meanwhile, each face of the Platonic solid is encoded by the Ripmap encoding, which is constructed by anisotropically pre-filtering a learnable feature grid, to enable featurzing the projected anisotropic areas both precisely and efficiently by the anisotropic area-sampling. Extensive experiments on both well-established synthetic datasets and a newly captured real-world dataset demonstrate that our Rip-NeRF attains state-of-the-art rendering quality, particularly excelling in the fine details of repetitive structures and textures, while maintaining relatively swift training times, as shown in Fig. 1. The source code and data for this paper are at https://github.com/JunchenLiu77/Rip-NeRF.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1104",
    "authors": "Jacob Spainhour, David Gunderman, Kenneth Weiss",
    "title": "Robust Containment Queries Over Collections of Rational Parametric Curves via Generalized Winding Numbers",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658228",
    "pdf_link": null,
    "abstract": "Point containment queries for regions bound by watertight geometric surfaces, i.e., closed and without self-intersections, can be evaluated straightforwardly with a number of well-studied algorithms. When this assumption on domain geometry is not met, such methods are either unusable, or prone to misclassifications that can lead to cascading errors in downstream applications. More robust point classification schemes based on generalized winding numbers have been proposed, as they are indifferent to these imperfections. However, existing algorithms are limited to point clouds and collections of linear elements. We extend this methodology to encompass more general curved shapes with an algorithm that evaluates the winding number scalar field over unstructured collections of rational parametric curves. In particular, we evaluate the winding number for each curve independently, making the derived containment query robust to how the curves are arranged. We ensure geometric fidelity in our queries by treating each curve as equivalent to an adaptively constructed polyline that provably has the same generalized winding number at the point of interest. Our algorithm is numerically stable for points that are arbitrarily close to the model, and explicitly treats points that are coincident with curves. We demonstrate the improvements in computational performance granted by this method over conventional techniques as well as the robustness induced by its application.",
    "scholar_publication": "ACM Transactions on Graphics …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_481",
    "authors": "Yifang Pan, Rishabh Agrawal, Karan Singh",
    "title": "S3: Speech, Script, and Scene Driven Head and Eye Animation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658172",
    "pdf_link": null,
    "abstract": "We present S3, a novel approach to generating expressive, animator-centric 3D head and eye animation of characters in conversation. Given speech audio, a Directorial script and a cinematographic 3D scene as input, we automatically output the animated 3D rotation of each character's head and eyes. S3 distills animation and psycho-linguistic insights into a novel modular framework for conversational gaze capturing: audio-driven rhythmic head motion; narrative script-driven emblematic head and eye gestures; and gaze trajectories computed from audio-driven gaze focus/aversion and 3D visual scene salience. Our evaluation is four-fold: we quantitatively validate our algorithm against ground truth data and baseline alternatives; we conduct a perceptual study showing our results to compare favourably to prior art; we present examples of animator control and critique of S3 output; and present a large number of compelling and varied animations of conversational gaze.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_661",
    "authors": "Jean Basset, Pierre Bénard, Pascal Barla",
    "title": "SMEAR: Stylized Motion Exaggeration With ARt-direction",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657457",
    "pdf_link": null,
    "abstract": "Smear frames are routinely used by artists for the expressive depiction of motion in animations. In this paper, we present an automatic, yet art-directable method for the generation of smear frames in 3D, with a focus on elongated in-betweens where an object is stretched along its trajectory. It takes as input a key-framed animation of a 3D mesh, and outputs a deformed version of this mesh for each frame of the animation, while providing for artistic refinement at the end of the animation process and prior to rendering. Our approach works in two steps. We first compute spatially and temporally coherent motion offsets that describe to which extent parts of the input mesh should be leading in front or trailing behind. We then describe a framework to stylize these motion offsets in order to produce elongated in-betweens at interactive rates, which we extend to the other two common smear frame effects: multiple in-betweens and motion lines. Novice users may rely on preset stylization functions for fast and easy prototyping, while more complex custom-made stylization functions may be designed by experienced artists through our geometry node implementation in Blender.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_727",
    "authors": "Daniel Duckworth, Peter Hedman, Christian Reiser, Peter Zhizhin, Jean-François Thibert, Mario Lučić, Richard Szeliski, Jonathan T. Barron",
    "title": "SMERF: Streamable Memory Efficient Radiance Fields for Real-time Large-scene Exploration",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658193",
    "pdf_link": null,
    "abstract": "Recent techniques for real-time view synthesis have rapidly advanced in fidelity and speed, and modern methods are capable of rendering near-photorealistic scenes at interactive frame rates. At the same time, a tension has arisen between explicit scene representations amenable to rasterization and neural fields built on ray marching, with state-of-the-art instances of the latter surpassing the former in quality while being prohibitively expensive for real-time applications. We introduce SMERF, a view synthesis approach that achieves state-of-the-art accuracy among real-time methods on large scenes with footprints up to 300 m2 at a volumetric resolution of 3.5 mm3. Our method is built upon two primary contributions: a hierarchical model partitioning scheme, which increases model capacity while constraining compute and memory consumption, and a distillation training strategy that simultaneously yields high fidelity and internal consistency. Our method enables full six degrees of freedom navigation in a web browser and renders in real-time on commodity smartphones and laptops. Extensive experiments show that our method exceeds the state-of-the-art in real-time novel view synthesis by 0.78 dB on standard benchmarks and 1.78 dB on large scenes, renders frames three orders of magnitude faster than state-of-the-art radiance field models, and achieves real-time performance across a wide variety of commodity devices, including smartphones. We encourage readers to explore these models interactively at our project website: https://smerf-3d.github.io.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1196",
    "authors": "Deqi Li, Shi-Sheng Huang, Hua Huang, Zhiyuan Lu, Xinran Duan",
    "title": "ST-4DGS: Spatial-Temporally Consistent 4D Gaussian Splatting for Efficient Dynamic Scene Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657520",
    "pdf_link": null,
    "abstract": "Dynamic scene rendering at any novel view continues to be a difficult but important task, especially for high-fidelity rendering quality with efficient rendering speed. The recent 3D Gaussian Splatting, i.e., 3DGS, shows great success for static scene rendering with impressive quality at a very efficient speed. However, the extension of 3DGS from static scene to dynamic 4DGS is still challenging, even for scenes with modest amounts of foreground object movement (such as a human moving an object). This paper proposes a novel spatial-temporally 4D Gaussian Splatting, i.e., ST-4DGS, which aims at the spatial-temporally persistent dynamic rendering quality and maintains real-time rendering efficiency. The key ideas of ST-4DGS are two novel mechanisms: (1) a novel spatial-temporal 4D Gaussian Splatting with a motion-aware shape regularization, and (2) a spatial-temporal joint density control mechanism. The proposed mechanisms efficiently prevent the compactness degeneration of the 4D Gaussian representation during dynamic scene learning, thus leading to spatial-temporally consistent dynamic rendering quality. With extensive evaluation on public datasets, our ST-4DGS can achieve much better dynamic rendering quality than previous approaches, such as 4DGS, HexPlane, K-Plane, 4K4D, etc, and in a more efficient rendering speed for persistent dynamic rendering. To our best knowledge, ST-4DGS is a new state-of-the-art 4D Gaussian Splatting for high-fidelity dynamic rendering, especially ensuring the spatial-temporally consistent rendering quality in scenes with modest movement. The code is available at https://github.com/wanglids/ST-4DGS.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_364",
    "authors": "Yuna Kwak, Eric Penner, Xuan Wang, Mohammad R. Saeedpour-Parizi, Olivier Mercier, Xiuyun Wu, Scott Murdison, Phillip Guan",
    "title": "Saccade-contingent Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657420",
    "pdf_link": null,
    "abstract": "… In our second experiment, we use these results to reduce the rendered image resolution … between saccade-contingent, reduced-resolution rendering and fullresolution rendering under …",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1255",
    "authors": "S. Mahdi H. Miangoleh, Mahesh Reddy, Yağız Aksoy",
    "title": "Scale-invariant Monocular Depth Estimation via SSI Depth",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657523",
    "pdf_link": null,
    "abstract": "Existing methods for scale-invariant monocular depth estimation (SI MDE) often struggle due to the complexity of the task, and limited and non-diverse datasets, hindering generalizability in real-world scenarios. This is while shift-and-scale-invariant (SSI) depth estimation, simplifying the task and enabling training with abundant stereo datasets achieves high performance. We present a novel approach that leverages SSI inputs to enhance SI depth estimation, streamlining the network's role and facilitating in-the-wild generalization for SI …",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_725",
    "authors": "Andrzej Kokosza, Helge Wrede, Daniel Gonzalez Esparza, Milosz Makowski, Daoming Liu, Dominik L. Michels, Soren Pirk, Wojtek Palubicki",
    "title": "Scintilla: Simulating Combustible Vegetation for Wildfires",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658192",
    "pdf_link": null,
    "abstract": "Wildfires are a complex physical phenomenon that involves the combustion of a variety of flammable materials ranging from fallen leaves and dried twigs to decomposing organic material and living flora. All these materials can potentially act as fuel with different properties that determine the progress and severity of a wildfire. In this paper, we propose a novel approach for simulating the dynamic interaction between the varying components of a wildfire, including processes of convection, combustion and heat transfer between vegetation, soil and atmosphere. We propose a novel representation of vegetation that includes detailed branch geometry, fuel moisture, and distribution of grass, fine fuel, and duff. Furthermore, we model the ignition, generation, and transport of fire by firebrands and embers. This allows simulating and rendering virtual 3D wildfires that realistically capture key aspects of the process, such as progressions from ground to crown fires, the impact of embers carried by wind, and the effects of fire barriers and other human intervention methods. We evaluate our approach through numerous experiments and based on comparisons to real-world wildfire data.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_787",
    "authors": "Ryan Capouellez, Denis Zorin",
    "title": "Seamless Parametrization in Penner Coordinates",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658202",
    "pdf_link": null,
    "abstract": "We introduce a conceptually simple and efficient algorithm for seamless parametrization, a key element in constructing quad layouts and texture charts on surfaces. More specifically, we consider the construction of parametrizations with prescribed holonomy signatures i.e., a set of angles at singularities, and rotations along homology loops, preserving which is essential for constructing parametrizations following an input field, as well as for user control of the parametrization structure. Our algorithm performs exceptionally well on a large dataset based on Thingi10k [Zhou and Jacobson 2016], (16156 meshes) as well as on a challenging smaller dataset of [Myles et al. 2014], converging, on average, in 9 iterations. Although the algorithm lacks a formal mathematical guarantee, presented empirical evidence and the connections between convex optimization and closely related algorithms, suggest that a similar formulation can be found for this algorithm in the future.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_117",
    "authors": "Francesco Banterle, Demetris Marnerides, Thomas Bashford-Rogers, Kurt Debattista, Francesco Banterle",
    "title": "Self-Supervised High Dynamic Range Imaging: What Can Be Learned from a Single 8-bit Video?",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3648570",
    "pdf_link": null,
    "abstract": "Recently, Deep Learning-based methods for inverse tone mapping standard dynamic range (SDR) images to obtain high dynamic range (HDR) images have become very popular. These methods manage to fill over-exposed areas convincingly both in terms of details and dynamic range. To be effective, deep learning-based methods need to learn from large datasets and transfer this knowledge to the network weights. In this work, we tackle this problem from a completely different perspective. What can we learn from a single SDR 8-bit …",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1257",
    "authors": "Lingyan Ruan, Mojtaba Bemana, Krzysztof Wolski, Martin Bálint, Hans-Peter Seidel, Karol Myszkowski, Bin Chen",
    "title": "Self-supervised Video Defocus Deblurring With Atlas Learning",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657524",
    "pdf_link": null,
    "abstract": "Misfocus is ubiquitous for almost all video producers, degrading video quality and often causing expensive delays and reshoots. Current autofocus (AF) systems are vulnerable to sudden disturbances such as subject movement or lighting changes commonly present in real-world and on-set conditions. Single image defocus deblurring methods are temporally unstable when applied to videos and cannot recover details obscured by temporally varying defocus blur. In this paper, we present an end-to-end solution that allows users to correct misfocus during post-processing. Our method generates and parameterizes defocused videos into sharp layered neural atlases and propagates consistent focus tracking back to the video frames. We introduce a novel differentiable disk blur layer for more accurate point spread function (PSF) simulation, coupled with a circle of confusion (COC) map estimation module with knowledge transferred from the current single image defocus deblurring (SIDD) networks. Our pipeline offers consistent, sharp video reconstruction and effective subject-focus correction and tracking directly on the generated atlases. Furthermore, by adopting our approach, we achieve comparable results to the state-of-the-art optical flow estimation approach from defocus videos.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_192",
    "authors": "Zeyi Zhang, Tenglong Ao, Yuyao Zhang, Qingzhe Gao, Chuan Lin, Baoquan Chen, Libin Liu",
    "title": "Semantic Gesticulator: Semantics-aware Co-speech Gesture Synthesis",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658134",
    "pdf_link": null,
    "abstract": "In this work, we present Semantic Gesticulator, a novel framework designed to synthesize realistic gestures accompanying speech with strong semantic correspondence. Semantically meaningful gestures are crucial for effective non-verbal communication, but such gestures often fall within the long tail of the distribution of natural human motion. The sparsity of these movements makes it challenging for deep learning-based systems, trained on moderately sized datasets, to capture the relationship between the movements and the corresponding speech semantics. To address this challenge, we develop a generative retrieval framework based on a large language model. This framework efficiently retrieves suitable semantic gesture candidates from a motion library in response to the input speech. To construct this motion library, we summarize a comprehensive list of commonly used semantic gestures based on findings in linguistics, and we collect a high-quality motion dataset encompassing both body and hand movements. We also design a novel GPT-based model with strong generalization capabilities to audio, capable of generating high-quality gestures that match the rhythm of speech. Furthermore, we propose a semantic alignment mechanism to efficiently align the retrieved semantic gestures with the GPT's output, ensuring the naturalness of the final animation. Our system demonstrates robustness in generating gestures that are rhythmically coherent and semantically explicit, as evidenced by a comprehensive collection of examples. User studies confirm the quality and human-likeness of our results, and show that our system outperforms state-of-the-art systems in terms of semantic appropriateness by a clear margin. We will release the code and dataset for academic research.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_401",
    "authors": "Uday Kusupati, Mathieu Gaillard, Jean-Marc Thiery, Adrien Kaiser",
    "title": "Semantic Shape Editing With Parametric Implicit Templates",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657421",
    "pdf_link": null,
    "abstract": "We propose a semantic shape editing method to edit 3D triangle meshes using parametric implicit surface templates, benefiting from the many advantages offered by analytical implicit representations, such as infinite resolution and boolean or blending operations. We propose first a template fitting method to optimize its parameters to best capture the input mesh. For subsequent template edits, our novel mesh deformation method allows tracking the template’s 0-set even when featuring anisotropic stretch and/or local volume change. We make few assumptions on the template implicit fields and only strictly require continuity. We demonstrate applications to interactive semantic shape editing and semantic mesh retargeting.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1275",
    "authors": "Zhipeng Bao, Yijun Li, Krishna Kumar Singh, Yu-Xiong Wang, Martial Hebert",
    "title": "Separate-and-Enhance: Compositional Finetuning for Text-to-image Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657527",
    "pdf_link": null,
    "abstract": "Despite recent significant strides achieved by diffusion-based Text-to-Image (T2I) models, current systems are still less capable of ensuring decent compositional generation aligned with text prompts, particularly for the multi-object generation. In this work, we first show the fundamental reasons for such misalignment by identifying issues related to low attention activation and mask overlaps. Then we propose a compositional finetuning framework with two novel objectives, the Separate loss and the Enhance loss, that reduce object mask overlaps and maximize attention scores, respectively. Unlike conventional test-time adaptation methods, our model, once finetuned on critical parameters, is able to directly perform inference given an arbitrary multi-object prompt, which enhances the scalability and generalizability. Through comprehensive evaluations, our model demonstrates superior performance in image realism, text-image alignment, and adaptability, significantly surpassing established baselines. Furthermore, we show that training our model with a diverse range of concepts enables it to generalize effectively to novel concepts, exhibiting enhanced performance compared to models trained on individual concept pairs.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_599",
    "authors": "Vismay Modi, Nicholas Sharp, Or Perel, Shinjiro Sueda, David Levin",
    "title": "Simplicits: Mesh-free, Geometry-agnostic Elastic Simulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658184",
    "pdf_link": null,
    "abstract": "The proliferation of 3D representations, from explicit meshes to implicit neural fields and more, motivates the need for simulators agnostic to representation. We present a data-, mesh-, and grid-free solution for elastic simulation for any object in any geometric representation undergoing large, nonlinear deformations. We note that every standard geometric representation can be reduced to an occupancy function queried at any point in space, and we define a simulator atop this common interface. For each object, we fit a small implicit neural network encoding spatially varying weights that act as a reduced deformation basis. These weights are trained to learn physically significant motions in the object via random perturbations. Our loss ensures we find a weight-space basis that best minimizes deformation energy by stochastically evaluating elastic energies through Monte Carlo sampling of the deformation volume. At runtime, we simulate in the reduced basis and sample the deformations back to the original domain. Our experiments demonstrate the versatility, accuracy, and speed of this approach on data including signed distance functions, point clouds, neural primitives, tomography scans, radiance fields, Gaussian splats, surface meshes, and volume meshes, as well as showing a variety of material energies, contact models, and time integration schemes.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_896",
    "authors": "Rahul Mitra, Erick Jimenez Berumen, Megan Hofmann, Edward Chien",
    "title": "Singular Foliations for Knit Graph Design",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657487",
    "pdf_link": null,
    "abstract": "We build upon the stripes-based knit planning framework of [Mitra et al. 2023], and view the resultant stripe pattern through the lens of singular foliations. This perspective views the stripes, and thus the candidate course rows or wale columns, as integral curves of a vector field specified by the spinning form of [Knöppel et al. 2015]. We show how to tightly control the topological structure of this vector field with linear level set constraints, preventing helicing of any integral curve. Practically speaking, this obviates the stripe placement constraints of [Mitra et al. 2023] and allows for shifting and variation of the stripe frequency without introducing additional helices. En route, we make the first explicit algebraic characterization of spinning form level set structure within singular triangles, and replace the standard interpolant with an “effective” one that improves the robustness of knit graph generation. We also extend the model of [Mitra et al. 2023] to surfaces with genus, via a Morse-based cylindrical decomposition, and implement automatic singularity pairing on the resulting components.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_118",
    "authors": "Feng-Lin liu, Hongbo Fu, Yu-Kun Lai, Lin Gao",
    "title": "SketchDream: Sketch-based Text-to-3D Generation and Editing",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658120",
    "pdf_link": null,
    "abstract": "Existing text-based 3D generation methods generate attractive results but lack detailed geometry control. Sketches, known for their conciseness and expressiveness, have contributed to intuitive 3D modeling but are confined to producing texture-less mesh models within predefined categories. Integrating sketch and text simultaneously for 3D generation promises enhanced control over geometry and appearance but faces challenges from 2D-to-3D translation ambiguity and multi-modal condition integration. Moreover, further editing of 3D models in arbitrary views will give users more freedom to customize their models. However, it is difficult to achieve high generation quality, preserve unedited regions, and manage proper interactions between shape components. To solve the above issues, we propose a text-driven 3D content generation and editing method, SketchDream, which supports NeRF generation from given hand-drawn sketches and achieves free-view sketch-based local editing. To tackle the 2D-to-3D ambiguity challenge, we introduce a sketch-based multi-view image generation diffusion model, which leverages depth guidance to establish spatial correspondence. A 3D ControlNet with a 3D attention module is utilized to control multi-view images and ensure their 3D consistency. To support local editing, we further propose a coarse-to-fine editing approach: the coarse phase analyzes component interactions and provides 3D masks to label edited regions, while the fine stage generates realistic results with refined details by local enhancement. Extensive experiments validate that our method generates higher-quality results compared with a combination of 2D ControlNet and image-to-3D generation techniques and achieves detailed control compared with existing diffusion-based 3D editing approaches.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_837",
    "authors": "Shibo Liu, Yang Ji, Jia-Peng Guo, Ligang Liu, Xiao-Ming Fu",
    "title": "Smooth Bijective Projection in a High-order Shell",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658207",
    "pdf_link": null,
    "abstract": "We propose a new structure called a higher-order shell, which is composed of a set of triangular prisms. Each triangular prism is enveloped by three Bézier triangles (top, middle, and bottom) and three side surfaces, each of which is trimmed from a bilinear surface. Moreover, we define a continuous vector field to smoothly and bijectively transfer attributes between two surfaces inside the shell. Since the higher-order shell has several hard construction constraints, we apply an interior-point strategy to robustly and automatically construct a high-order shell for an input mesh. Specifically, the strategy starts from a valid linear shell with a small thickness. Then, the shell is optimized until the specified thickness is reached, where explicit checks ensure that the constraints are always satisfied. We extensively test our method on more than 8300 models, demonstrating its robustness and performance. Compared to state-of-the-art methods, our bijective projection is smoother, and the space between the shell and input mesh is more uniform.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_745",
    "authors": "Arvi Gjoka, Espen Knoop, Moritz Bächer, Denis Zorin, Daniele Panozzo",
    "title": "Soft Pneumatic Actuator Design Using Differentiable Simulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657467",
    "pdf_link": null,
    "abstract": "We propose a computational design pipeline for pneumatically-actuated soft robots interacting with their environment through contact. We optimize the shape of the robot with a shape optimization approach, using a physically-accurate high-order finite element model for the forward simulation. Our approach enables fine-grained control over both deformation and contact forces by optimizing the shape of internal cavities, which we exploit to design pneumatically-actuated robots that can assume user-prescribed poses, or apply user-controlled forces. We demonstrate the efficacy of our method on two artistic and two functional examples.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_135",
    "authors": "Yuichi Hirose, Mark Gillespie, Angelica M. Bonilla Fominaya, James McCann",
    "title": "Solid Knitting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3665662.3673257",
    "pdf_link": null,
    "abstract": "We introduce solid knitting, a new fabrication technique that combines the layer-by-layer volumetric approach of 3D printing with the topologically-entwined stitch structure of knitting to …",
    "scholar_publication": "Adjunct Proceedings of …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_411",
    "authors": "Zeyu Huang, Honghao Xu, Haibin Huang, Chongyang Ma, Hui Huang, Ruizhen Hu",
    "title": "Spatial and Surface Correspondence Field for Interaction Transfer",
    "paper_url": "https://arxiv.org/abs/2405.03221",
    "pdf_link": null,
    "abstract": "In this paper, we introduce a new method for the task of interaction transfer. Given an example interaction between a source object and an agent, our method can automatically infer both surface and spatial relationships for the agent and target objects within the same category, yielding more accurate and valid transfers. Specifically, our method characterizes the example interaction using a combined spatial and surface representation. We correspond the agent points and object points related to the representation to the target object space using a learned spatial and surface correspondence field, which represents objects as deformed and rotated signed distance fields. With the corresponded points, an optimization is performed under the constraints of our spatial and surface interaction representation and additional regularization. Experiments conducted on human-chair and hand-mug interaction transfer tasks show that our approach can handle larger geometry and topology variations between source and target shapes, significantly outperforming state-of-the-art methods.",
    "scholar_publication": "arXiv preprint arXiv …, 2024 - arxiv.org"
  },
  {
    "paper_id": "papers_181",
    "authors": "Zhimin Fan, Jie Guo, Yiming Wang, Tianyu Xiao, Hao Zhang, Chenxi Zhou, Zhenyu Chen, Pengpei Hong, Yanwen Guo, Ling-Qi Yan",
    "title": "Specular Polynomials",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658132",
    "pdf_link": null,
    "abstract": "… reformulation of specular constraints into polynomial systems… of the determinant of univariate matrix polynomials. This can be … the superiority of specular polynomial-based solutions …",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_719",
    "authors": "Etai Sella, Gal Fiebelman, Noam Atia, Hadar Averbuch-Elor",
    "title": "Spice-E: Structural Priors in 3D Diffusion Using Cross-Entity Attention",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657461",
    "pdf_link": null,
    "abstract": "We are witnessing rapid progress in automatically generating and manipulating 3D assets due to the availability of pretrained text-to-image diffusion models. However, time-consuming optimization procedures are required for synthesizing each sample, hindering their potential for democratizing 3D content creation. Conversely, 3D diffusion models now train on million-scale 3D datasets, yielding high-quality text-conditional 3D samples within seconds. In this work, we present Spice · E – a neural network that adds structural guidance to 3D diffusion models, extending their usage beyond text-conditional generation. At its core, our framework introduces a cross-entity attention mechanism that allows for multiple entities—in particular, paired input and guidance 3D shapes—to interact via their internal representations within the denoising network. We utilize this mechanism for learning task-specific structural priors in 3D diffusion models from auxiliary guidance shapes. We show that our approach supports a variety of applications, including 3D stylization, semantic shape editing and text-conditional abstraction-to-3D, which transforms primitive-based abstractions into highly-expressive shapes. Extensive experiments demonstrate that Spice · E achieves SOTA performance over these tasks while often being considerably faster than alternative methods. Importantly, this is accomplished without tailoring our approach for any specific task. We will release our code and trained models.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_728",
    "authors": "Christian Hafner, Mickaël Ly, Chris Wojtan",
    "title": "Spin-It Faster: Quadrics Solve All Topology Optimization Problems That Depend Only on Mass Moments",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658194",
    "pdf_link": null,
    "abstract": "The behavior of a rigid body primarily depends on its mass moments, which consist of the mass, center of mass, and moments of inertia. It is possible to manipulate these quantities without altering the geometric appearance of an object by introducing cavities in its interior. Algorithms that find cavities of suitable shapes and sizes have enabled the computational design of spinning tops, yo-yos, wheels, buoys, and statically balanced objects. Previous work is based, for example, on topology optimization on voxel grids, which introduces a large number of optimization variables and box constraints, or offset surface computation, which cannot guarantee that solutions to a feasible problem will always be found. In this work, we provide a mathematical analysis of constrained topology optimization problems that depend only on mass moments. This class of problems covers, among others, all applications mentioned above. Our main result is to show that no matter the outer shape of the rigid body to be optimized or the optimization objective and constraints considered, the optimal solution always features a quadric-shaped interface between material and cavities. This proves that optimal interfaces are always ellipsoids, hyperboloids, paraboloids, or one of a few degenerate cases, such as planes. This insight lets us replace a difficult topology optimization problem with a provably equivalent non-linear equation system in a small number (<10) of variables, which represent the coefficients of the quadric. This system can be solved in a few seconds for most examples, provides insights into the geometric structure of many specific applications, and lets us describe their solution properties. Finally, our method integrates seamlessly into modern fabrication workflows because our solutions are analytical surfaces that are native to the CAD domain.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_213",
    "authors": "Shinyoung Yi, Donggun Kim, Jiwoong Na, Xin Tong, Min H. Kim",
    "title": "Spin-weighted Spherical Harmonics for Polarized Light Transport",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658139",
    "pdf_link": null,
    "abstract": "The objective of polarization rendering is to simulate the interaction of light with materials exhibiting polarization-dependent behavior. However, integrating polarization into rendering is challenging and increases computational costs significantly. The primary difficulty lies in efficiently modeling and computing the complex reflection phenomena associated with polarized light. Specifically, frequency-domain analysis, essential for efficient environment lighting and storage of complex light interactions, is lacking. To efficiently simulate and reproduce polarized light interactions using frequency-domain techniques, we address the challenge of maintaining continuity in polarized light transport represented by Stokes vectors within angular domains. The conventional spherical harmonics method cannot effectively handle continuity and rotation invariance for Stokes vectors. To overcome this, we develop a new method called polarized spherical harmonics (PSH) based on the spin-weighted spherical harmonics theory. Our method provides a rotation-invariant representation of Stokes vector fields. Furthermore, we introduce frequency domain formulations of polarized rendering equations and spherical convolution based on PSH. We first define spherical convolution on Stokes vector fields in the angular domain, and it also provides efficient computation of polarized light transport, nearly on an entry-wise product in the frequency domain. Our frequency domain formulation, including spherical convolution, led to the development of the first real-time polarization rendering technique under polarized environmental illumination, named precomputed polarized radiance transfer, using our polarized spherical harmonics. Results demonstrate that our method can effectively and accurately simulate and reproduce polarized light interactions in complex reflection phenomena, including polarized environmental illumination and soft shadows.",
    "scholar_publication": "ACM Transactions on Graphics …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1042",
    "authors": "Zheng Shi, Ilya Chugunov, Mario Bijelic, Geoffroi Côté, Jiwoon Yeom, Qiang Fu, Hadi Amata, Wolfgang Heidrich, Felix Heide",
    "title": "Split-Aperture 2-in-1 Computational Cameras",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658225",
    "pdf_link": null,
    "abstract": "… We investigate a 2-in-1 computational camera that relies on depth-dependent concentric rings as a target PSF, following the design from Haim et al. [2018] but is conditioned on a …",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_331",
    "authors": "Yilin Liu, Jiale Chen, Shanshan Pan, Daniel Cohen-Or, Hao Zhang, Hui Huang",
    "title": "Split-and-Fit: Learning B-Reps via Structure-aware Voronoi Partitioning",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658155",
    "pdf_link": null,
    "abstract": "We introduce a novel method for acquiring boundary representations (B-Reps) of 3D CAD models which involves a two-step process: it first applies a spatial partitioning, referred to as the \"split\", followed by a \"fit\" operation to derive a single primitive within each partition. Specifically, our partitioning aims to produce the classical Voronoi diagram of the set of ground-truth (GT) B-Rep primitives. In contrast to prior B-Rep constructions which were bottom-up, either via direct primitive fitting or point clustering, our Split-and-Fit approach is top-down and structure-aware, since a Voronoi partition explicitly reveals both the number of and the connections between the primitives. We design a neural network to predict the Voronoi diagram from an input point cloud or distance field via a binary classification. We show that our network, coined NVD-Net for neural Voronoi diagrams, can effectively learn Voronoi partitions for CAD models from training data and exhibits superior generalization capabilities. Extensive experiments and evaluation demonstrate that the resulting B-Reps, consisting of parametric surfaces, curves, and vertices, are more plausible than those obtained by existing alternatives, with significant improvements in reconstruction quality. Code will be released on https://github.com/yilinliu77/NVDNet.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_486",
    "authors": "Honglin Chen, Hsueh-Ti Derek Liu, David I.W. Levin, Changxi Zheng, Alec Jacobson",
    "title": "Stabler Neo-Hookean Simulation: Absolute Eigenvalue Filtering for Projected Newton",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657433",
    "pdf_link": null,
    "abstract": "Volume-preserving hyperelastic materials are widely used to model near-incompressible materials such as rubber and soft tissues. However, the numerical simulation of volume-preserving hyperelastic materials is notoriously challenging within this regime due to the non-convexity of the energy function. In this work, we identify the pitfalls of the popular eigenvalue clamping strategy for projecting Hessian matrices to positive semi-definiteness during Newton’s method. We introduce a novel eigenvalue filtering strategy for projected Newton’s method to stabilize the optimization of Neo-Hookean energy and other volume-preserving variants under high Poisson’s ratio (near 0.5) and large initial volume change. Our method only requires a single line of code change in the existing projected Newton framework, while achieving significant improvement in both stability and convergence speed. We demonstrate the effectiveness and efficiency of our eigenvalue projection scheme on a variety of challenging examples and over different deformations on a large dataset.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_176",
    "authors": "Fernando de Goes, Mathieu Desbrun",
    "title": "Stochastic Computation of Barycentric Coordinates",
    "paper_url": "https://hal.science/hal-04706761/",
    "pdf_link": null,
    "abstract": "This paper presents a practical and general approach for computing barycentric coordinates through stochastic sampling. Our key insight is a reformulation of the kernel integral defining barycentric coordinates into a weighted least-squares minimization that enables Monte Carlo integration without sacrificing linear precision. Our method can thus compute barycentric coordinates directly at the points of interest, both inside and outside the cage, using just proximity queries to the cage such as closest points and ray intersections. As a result, we can evaluate barycentric coordinates for a large variety of cage representations (from quadrangulated surface meshes to parametric curves) seamlessly, bypassing any volumetric discretization or custom solves. To address the archetypal noise induced by sample-based estimates, we also introduce a denoising scheme tailored to barycentric coordinates. We demonstrate the efficiency and flexibility of our formulation by implementing a stochastic generation of harmonic coordinates, mean-value coordinates, and positive mean-value coordinates.",
    "scholar_publication": "ACM Transactions on Graphics, 2024 - hal.science"
  },
  {
    "paper_id": "papers_655",
    "authors": "Lukas Radl, Michael Steiner, Mathias Parger, Alexander Weinrauch, Bernhard Kerbl, Markus Steinberger",
    "title": "StopThePop: Sorted Gaussian Splatting for View-consistent Real-time Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658187",
    "pdf_link": null,
    "abstract": "Gaussian Splatting has emerged as a prominent model for constructing 3D representations from images across diverse domains. However, the efficiency of the 3D Gaussian Splatting rendering pipeline relies on several simplifications. Notably, reducing Gaussian to 2D splats with a single viewspace depth introduces popping and blending artifacts during view rotation. Addressing this issue requires accurate per-pixel depth computation, yet a full per-pixel sort proves excessively costly compared to a global sort operation. In this paper, we present a novel hierarchical rasterization approach that systematically resorts and culls splats with minimal processing overhead. Our software rasterizer effectively eliminates popping artifacts and view inconsistencies, as demonstrated through both quantitative and qualitative measurements. Simultaneously, our method mitigates the potential for cheating view-dependent effects with popping, ensuring a more authentic representation. Despite the elimination of cheating, our approach achieves comparable quantitative results for test images, while increasing the consistency for novel view synthesis in motion. Due to its design, our hierarchical approach is only 4% slower on average than the original Gaussian Splatting. Notably, enforcing consistency enables a reduction in the number of Gaussians by approximately half with nearly identical quality and view-consistency. Consequently, rendering performance is nearly doubled, making our approach 1.6x faster than the original Gaussian Splatting, with a 50% reduction in memory requirements. Our renderer is publicly available at https://github.com/r4dl/StopThePop.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_558",
    "authors": "Jiashun Wang, Jungdam Won, Jessica Hodgins",
    "title": "Strategy and Skill Learning for Physics-based Table Tennis Animation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657437",
    "pdf_link": null,
    "abstract": "Recent advancements in physics-based character animation leverage deep learning to generate agile and natural motion, enabling characters to execute movements such as backflips, boxing, and tennis. However, reproducing the selection and use of diverse motor skills in dynamic environments to solve complex tasks, as humans do, still remains a challenge. We present a strategy and skill learning approach for physics-based table tennis animation. Our method addresses the issue of mode collapse, where the characters do not fully utilize the motor skills they need to perform to execute complex tasks. More specifically, we demonstrate a hierarchical control system for diversified skill learning and a strategy learning framework for effective decision-making. We showcase the efficacy of our method through comparative analysis with state-of-the-art methods, demonstrating its capabilities in executing various skills for table tennis. Our strategy learning framework is validated through both agent-agent interaction and human-agent interaction in Virtual Reality, handling both competitive and cooperative tasks.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1105",
    "authors": "Boyang Deng, Richard Tucker, Zhengqi Li, Leonidas Guibas, Noah Snavely, Gordon Wetzstein",
    "title": "Streetscapes: Large-scale Consistent Street View Generation Using Autoregressive Video Diffusion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657513",
    "pdf_link": null,
    "abstract": "We present a method for generating Streetscapes—long sequences of views through an on-the-fly synthesized city-scale scene. Our generation is conditioned by language input (e.g., city name, weather), as well as an underlying map/layout hosting the desired trajectory. Compared to recent models for video generation or 3D view synthesis, our method can scale to much longer-range camera trajectories, spanning several city blocks, while maintaining visual quality and consistency. To achieve this goal, we build on recent work on video diffusion, used within an autoregressive framework that can easily scale to long sequences. In particular, we introduce a new temporal imputation method that prevents our autoregressive approach from drifting from the distribution of realistic city imagery. We train our Streetscapes system on a compelling source of data—posed imagery from Google Street View, along with contextual map data—which allows users to generate city views conditioned on any desired city layout, with controllable camera pose.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_372",
    "authors": "Rex West, Sayan Mukherjee",
    "title": "Stylized Rendering as a Function of Expectation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658161",
    "pdf_link": null,
    "abstract": "We propose a generalization of the rendering equation that captures both the realistic light transport of physically-based rendering (PBR) and a subset of non-photorealistic rendering (NPR) stylizations in a principled manner. The proposed formulation is based on the key observation that both classical transport and certain NPR stylizations can be modeled as a function of expectation. Given this observation, we generalize the recursive integrals of the rendering equation to recursive functions of expectation. As estimating functions of expectation can be challenging, especially recursive ones, we provide a toolkit for unbiased and biased estimation comprising prior work, general strategies, and a novel build-your-own strategy for constructing more complex unbiased estimators from simpler unbiased estimators. We then use this toolkit to construct a complete estimator for the proposed recursive formulation, and implement a sampling algorithm that is both conceptually simple and leverages many of the components of an ordinary path tracer. To demonstrate the practicality of the proposed method we showcase how it captures several existing stylizations like color mapping, cel shading, and cross-hatching, fuses NPR and PBR visuals, and allows us to explore visuals that were previously challenging under existing formulations.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_758",
    "authors": "Jian Ma, Junhao Liang, Chen Chen, Haonan Lu",
    "title": "Subject-Diffusion: Open Domain Personalized Text-to-image Generation Without Test-time Fine-tuning",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657469",
    "pdf_link": null,
    "abstract": "Recent progress in personalized image generation using diffusion models has been significant. However, development in the area of open-domain and test-time fine-tuning-free personalized image generation is proceeding rather slowly. In this paper, we propose Subject-Diffusion, a novel open-domain personalized image generation model that, in addition to not requiring test-time fine-tuning, also only requires a single reference image to support personalized generation of single- or two-subjects in any domain. Firstly, we construct an automatic data labeling tool and use the LAION-Aesthetics dataset to construct a large-scale dataset consisting of 76M images and their corresponding subject detection bounding boxes, segmentation masks, and text descriptions. Secondly, we design a new unified framework that combines text and image semantics by incorporating coarse location and fine-grained reference image control to maximize subject fidelity and generalization. Furthermore, we also adopt an attention control mechanism to support two-subject generation. Extensive qualitative and quantitative results demonstrate that our method have certain advantages over other frameworks in single, multiple, and human-customized image generation.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_234",
    "authors": "Jiawang Yu, Zhendong Wang",
    "title": "Super-resolution Cloth Animation With Spatial and Temporal Coherence",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658143",
    "pdf_link": null,
    "abstract": "Creating super-resolution cloth animations, which refine coarse cloth meshes with fine wrinkle details, faces challenges in preserving spatial consistency and temporal coherence across frames. In this paper, we introduce a general framework to address these issues, leveraging two core modules. The first module interleaves a simulator and a corrector. The simulator handles cloth dynamics, while the corrector rectifies differences in low-frequency features across various resolutions. This interleaving ensures prompt correction of spatial errors from the coarse simulation, effectively preventing their temporal propagation. The second module performs mesh-based super-resolution for detailed wrinkle enhancements. We decompose garment meshes into overlapping patches for adaptability to various styles and geometric continuity. Our method achieves an 8× improvement in resolution for cloth animations. We showcase the effectiveness of our method through diverse animation examples, including simple cloth pieces and intricate garments.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_918",
    "authors": "Jordan B. Juravsky, Yunrong Guo, Sanja Fidler, Xue Bin Peng",
    "title": "SuperPADL: Scaling Language-directed Physics-based Control With Progressive Supervised Distillation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657492",
    "pdf_link": null,
    "abstract": "Physically-simulated models for human motion can generate high-quality responsive character animations, often in real-time. Natural language serves as a flexible interface for controlling these models, allowing expert and non-expert users to quickly create and edit their animations. Many recent physics-based animation methods, including those that use text interfaces, train control policies using reinforcement learning (RL). However, scaling these methods beyond several hundred motions has remained challenging. Meanwhile, kinematic animation models are able to successfully learn from thousands of diverse motions by leveraging supervised learning methods. Inspired by these successes, in this work we introduce SuperPADL, a scalable framework for physics-based text-to-motion that leverages both RL and supervised learning to train controllers on thousands of diverse motion clips. SuperPADL is trained in stages using progressive distillation, starting with a large number of specialized experts using RL. These experts are then iteratively distilled into larger, more robust policies using a combination of reinforcement learning and supervised learning. Our final SuperPADL controller is trained on a dataset containing over 5000 skills and runs in real time on a consumer GPU. Moreover, our policy can naturally transition between skills, allowing for users to interactively craft multi-stage animations. We experimentally demonstrate that SuperPADL significantly outperforms RL-based baselines at this large data scale.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_350",
    "authors": "Yuta Noma, Silvia Sellán, Nicholas Sharp, Karan Singh, Alec Jacobson",
    "title": "Surface-filling Curve Flows via Implicit Medial Axes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658158",
    "pdf_link": null,
    "abstract": "We introduce a fast, robust, and user-controllable algorithm to generate surface-filling curves. We compute these curves through the gradient flow of a simple sparse energy, making our method several orders of magnitude faster than previous works. Our algorithm makes minimal assumptions on the topology and resolution of the input surface, achieving improved robustness. Our framework provides tuneable parameters that guide the shape of the output curve, making it ideal for interactive design applications.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1107",
    "authors": "Zihan Zhang, Richard Liu, Rana Hanocka, Kfir Aberman",
    "title": "TEDi: Temporally-Entangled Diffusion for Long-term Motion Synthesis",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657515",
    "pdf_link": null,
    "abstract": "The gradual nature of a diffusion process that synthesizes samples in small increments constitutes a key ingredient of Denoising Diffusion Probabilistic Models (DDPM), which have presented unprecedented quality in image synthesis and been recently explored in the motion domain. In this work, we propose to adapt the gradual diffusion concept (operating along a diffusion time-axis) into the temporal-axis of the motion sequence. Our key idea is to extend the DDPM framework to support temporally varying denoising, thereby entangling the two axes. Using our special formulation, we iteratively denoise a motion buffer that contains a set of increasingly-noised poses, which auto-regressively produces an arbitrarily long stream of frames. With a stationary diffusion time-axis, in each diffusion step we increment only the temporal-axis of the motion such that the framework produces a new, clean frame which is removed from the beginning of the buffer, followed by a newly drawn noise vector that is appended to it. This new mechanism paves the way towards a new framework for long-term motion synthesis with applications to character animation and other domains.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_820",
    "authors": "Jingyu Zhuang, Di Kang, Yan-Pei Cao, Guanbin Li, Liang Lin, Ying Shan",
    "title": "TIP-Editor: An Accurate 3D Editor Following Both Text-prompts and Image-prompts",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658205",
    "pdf_link": null,
    "abstract": "Text-driven 3D scene editing has gained significant attention owing to its convenience and user-friendliness. However, existing methods still lack accurate control of the specified appearance and location of the editing result due to the inherent limitations of the text description. To this end, we propose a 3D scene editing framework, TIP-Editor, that accepts both text and image prompts and a 3D bounding box to specify the editing region. With the image prompt, users can conveniently specify the detailed appearance/style of the target content in complement to the text description, enabling accurate control of the appearance. Specifically, TIP-Editor employs a stepwise 2D personalization strategy to better learn the representation of the existing scene and the reference image, in which a localization loss is proposed to encourage correct object placement as specified by the bounding box. Additionally, TIP-Editor utilizes explicit and flexible 3D Gaussian splatting (GS) as the 3D representation to facilitate local editing while keeping the background unchanged. Extensive experiments have demonstrated that TIP-Editor conducts accurate editing following the text and image prompts in the specified bounding box region, consistently outperforming the baselines in editing quality, and the alignment to the prompts, qualitatively and quantitatively.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_588",
    "authors": "Rui Chen, Mingyi Shi, Shaoli Huang, Ping Tan, Taku Komura, Xuelin Chen",
    "title": "Taming Diffusion Probabilistic Models for Character Control",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657440",
    "pdf_link": null,
    "abstract": "We present a novel character control framework that effectively utilizes motion diffusion probabilistic models to generate high-quality and diverse character animations, responding in real-time to a variety of dynamic user-supplied control signals. At the heart of our method lies a transformer-based Conditional Autoregressive Motion Diffusion Model (CAMDM), which takes as input the character’s historical motion and can generate a range of diverse potential future motions conditioned on high-level, coarse user control. To meet the demands for diversity, controllability, and computational efficiency required by a real-time controller, we incorporate several key algorithmic designs. These include separate condition tokenization, classifier-free guidance on past motion, and heuristic future trajectory extension, all designed to address the challenges associated with taming motion diffusion probabilistic models for character control. As a result, our work represents the first model that enables real-time generation of high-quality, diverse character animations based on user interactive control, supporting animating the character in multiple styles with a single unified model. We evaluate our method on a diverse set of locomotion skills, demonstrating the merits of our method over existing character controllers. The code and model are available at https://aiganimation.github.io/CAMDM/.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_576",
    "authors": "Jeongmin Gu, Jonghee Back, Sung-Eui Yoon, Bochang Moon",
    "title": "Target-aware Image Denoising for Inverse Monte Carlo Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658182",
    "pdf_link": null,
    "abstract": "Physically based differentiable rendering allows an accurate light transport simulation to be differentiated with respect to the rendering input, i.e., scene parameters, and it enables inferring scene parameters from target images, e.g., photos or synthetic images, via an iterative optimization. However, this inverse Monte Carlo rendering inherits the fundamental problem of the Monte Carlo integration, i.e., noise, resulting in a slow optimization convergence. An appealing approach to addressing such noise is exploiting an image denoiser to improve optimization convergence. Unfortunately, the direct adoption of existing image denoisers designed for ordinary rendering scenarios can drive the optimization into undesirable local minima due to denoising bias. It motivates us to reformulate a new image denoiser specialized for inverse rendering. Unlike existing image denoisers, we conduct our denoising by considering the target images, i.e., specific information in inverse rendering. For our target-aware denoising, we determine our denoising weights via a linear regression technique using the target. We demonstrate that our denoiser enables inverse rendering optimization to infer scene parameters robustly through a diverse set of tests.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_908",
    "authors": "Hanzhang Tu, Ruizhi Shao, Xue Dong, Shunyuan Zheng, Hao Zhang, Lili Chen, Meili Wang, Wenyu Li, Siyan Ma, Shengping Zhang, Boyao Zhou, Yebin Liu",
    "title": "Tele-Aloha: A Telepresence System With Low-budget and High-authenticity Using Sparse RGB Cameras",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657491",
    "pdf_link": null,
    "abstract": "In this paper, we present a low-budget and high-authenticity bidirectional telepresence system, Tele-Aloha, targeting peer-to-peer communication scenarios. Compared to previous systems, Tele-Aloha utilizes only four sparse RGB cameras, one consumer-grade GPU, and one autostereoscopic screen to achieve high-resolution (2048x2048), real-time (30 fps), low-latency (less than 150ms) and robust distant communication. As the core of Tele-Aloha, we propose an efficient novel view synthesis algorithm for upper-body. Firstly, we design a cascaded disparity estimator for obtaining a robust geometry cue. Additionally a neural rasterizer via Gaussian Splatting is introduced to project latent features onto target view and to decode them into a reduced resolution. Further, given the high-quality captured data, we leverage weighted blending mechanism to refine the decoded image into the final resolution of 2K. Exploiting world-leading autostereoscopic display and low-latency iris tracking, users are able to experience a strong three-dimensional sense even without any wearable head-mounted display device. Altogether, our telepresence system demonstrates the sense of co-presence in real-life experiments, inspiring the next generation of communication.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_601",
    "authors": "Giorgos Christopoulos, Lei Gao, Diego Martinez Plasencia, Marta Betcke, Ryuji Hirayama, Sriram Subramanian",
    "title": "Temporal Acoustic Point Holography",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657443",
    "pdf_link": null,
    "abstract": "… quality acoustic traps as a gradient maximizer of focal amplitudes with hologram energy (… , we also introduce saddle point optimization in computer generated holography (CGH) to …",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_920",
    "authors": "Chuhao Chen, Yuze He, Tzu-mao Li",
    "title": "Temporally Stable Metropolis Light Transport Denoising Using Recurrent Transformer Blocks",
    "paper_url": "https://link.springer.com/chapter/10.1007/978-3-031-19800-7_28",
    "pdf_link": null,
    "abstract": "… a sliding block strategy with recurrent architecture, and use a … temporal blocks, we fine-tune our network to the recurrent architecture, and propose a new loss term to strengthen stability …",
    "scholar_publication": "European conference on computer vision, 2022 - Springer"
  },
  {
    "paper_id": "papers_858",
    "authors": "Jia Li, Beibei Wang, Lu Wang, Lei Zhang",
    "title": "TensoSDF: Roughness-aware Tensorial Representation for Robust Geometry and Material Reconstruction",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658211",
    "pdf_link": null,
    "abstract": "Reconstructing objects with realistic materials from multi-view images is problematic, since it is highly ill-posed. Although the neural reconstruction approaches have exhibited impressive reconstruction ability, they are designed for objects with specific materials (e.g., diffuse or specular materials). To this end, we propose a novel framework for robust geometry and material reconstruction, where the geometry is expressed with the implicit signed distance field (SDF) encoded by a tensorial representation, namely TensoSDF. At the core of our method is the roughness-aware incorporation of the radiance and reflectance fields, which enables a robust reconstruction of objects with arbitrary reflective materials. Furthermore, the tensorial representation enhances geometry details in the reconstructed surface and reduces the training time. Finally, we estimate the materials using an explicit mesh for efficient intersection computation and an implicit SDF for accurate representation. Consequently, our method can achieve more robust geometry reconstruction, outperform the previous works in terms of relighting quality, and reduce 50% training times and 70% inference time. Codes and datasets are available at https://github.com/Riga2/TensoSDF.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_778",
    "authors": "Hugo Schott, Eric Galin, Eric Guérin, Axel Paris, Adrien Peytavie",
    "title": "Terrain Amplification Using Multi Scale Erosion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658200",
    "pdf_link": null,
    "abstract": "Modeling high-resolution terrains is a perennial challenge in the creation of virtual worlds. In this paper, we focus on the amplification of a low-resolution input terrain into a high-resolution, hydrologically consistent terrain featuring complex patterns by a multi-scale approach. Our framework combines the best of both worlds, relying on physics-inspired erosion models producing consistent erosion landmarks and introducing control at different scales, thus bridging the gap between physics-based erosion simulations and multi-scale procedural modeling. The method uses a fast and accurate approximation of different simulations, including thermal, stream power erosion and deposition performed at different scales to obtain a range of effects. Our approach provides landscape designers with tools for amplifying mountain ranges and valleys with consistent details.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_928",
    "authors": "Hongkun Zhang, Zherong Pan, Congyi Zhang, Lifeng Zhu, Xifeng Gao",
    "title": "TexPainter: Generative Mesh Texturing With Multi-view Consistency",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657494",
    "pdf_link": null,
    "abstract": "The recent success of pre-trained diffusion models unlocks the possibility of the automatic generation of textures for arbitrary 3D meshes in the wild. However, these models are trained in the screen space, while converting them to a multi-view consistent texture image poses a major obstacle to the output quality. In this paper, we propose a novel method to enforce multi-view consistency. Our method is based on the observation that latent space in a pre-trained diffusion model is noised separately for each camera view, making it difficult to achieve multi-view consistency by directly manipulating the latent codes. Based on the celebrated Denoising Diffusion Implicit Models (DDIM) scheme, we propose to use an optimization-based color-fusion to enforce consistency and indirectly modify the latent codes by gradient back-propagation. Our method further relaxes the sequential dependency assumption among the camera views. By evaluating on a series of general 3D models, we find our simple approach improves consistency and overall quality of the generated textures as compared to competing state-of-the-arts. Our implementation is available at: https://github.com/Quantuman134/TexPainter",
    "scholar_publication": "Acm siggraph 2024 conference …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_603",
    "authors": "Julia Guerrero-Viu, Milos Hasan, Arthur Roullier, Midhun Harikumar, Yiwei Hu, Paul Guerrero, Diego Gutiérrez, Belen Masia, Valentin Deschaintre",
    "title": "TexSliders: Diffusion-based Texture Editing in CLIP Space",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657444",
    "pdf_link": null,
    "abstract": "Generative models have enabled intuitive image creation and manipulation using natural language. In particular, diffusion models have recently shown remarkable results for natural image editing. In this work, we propose to apply diffusion techniques to edit textures, a specific class of images that are an essential part of 3D content creation pipelines. We analyze existing editing methods and show that they are not directly applicable to textures, since their common underlying approach, manipulating attention maps, is unsuitable for the texture domain. To address this, we propose a novel approach that instead manipulates CLIP image embeddings to condition the diffusion generation. We define editing directions using simple text prompts (e.g., “aged wood” to “new wood”) and map these to CLIP image embedding space using a texture prior, with a sampling-based approach that gives us identity-preserving directions in CLIP space. To further improve identity preservation, we project these directions to a CLIP subspace that minimizes identity variations resulting from entangled texture attributes. Our editing pipeline facilitates the creation of arbitrary sliders using natural language prompts only, with no ground-truth annotated data necessary.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1111",
    "authors": "Xuebo Ji, Zherong Pan, Xifeng Gao, Jia Pan",
    "title": "Text-guided Synthesis of Crowd Animation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657516",
    "pdf_link": null,
    "abstract": "Creating vivid crowd animations is core to immersive virtual environments in digital games. This work focuses on tackling the challenges of the crowd behavior generation problem. Existing approaches are labor-intensive, relying on practitioners to manually craft the complex behavior systems. We propose a machine learning approach to synthesize diversified dynamic crowd animation scenarios for a given environment based on a text description input. We first train two conditional diffusion models that generate text-guided agent distribution fields and velocity fields. Assisted by local navigation algorithms, the fields are then used to control multiple groups of agents. We further employ Large-Language Model (LLM) to canonicalize the general script into a structured sentence for more stable training and better scalability. To train our diffusion models, we devise a constructive method to generate random environments and crowd animations. We show that our trained diffusion models can generate crowd animations for both unseen environments and novel scenario descriptions. Our method paves the way towards automatic generating of crowd behaviors for virtual environments. Code and data for this paper are available at: https://github.com/MLZG/Text-Crowd.git.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_813",
    "authors": "Peiying Zhang, Nanxuan Zhao, Jing Liao",
    "title": "Text-to-vector Generation With Neural Path Representation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658204",
    "pdf_link": null,
    "abstract": "Vector graphics are widely used in digital art and highly favored by designers due to their scalability and layer-wise properties. However, the process of creating and editing vector graphics requires creativity and design expertise, making it a time-consuming task. Recent advancements in text-to-vector (T2V) generation have aimed to make this process more accessible. However, existing T2V methods directly optimize control points of vector graphics paths, often resulting in intersecting or jagged paths due to the lack of geometry constraints. To overcome these limitations, we propose a novel neural path representation by designing a dual-branch Variational Autoencoder (VAE) that learns the path latent space from both sequence and image modalities. By optimizing the combination of neural paths, we can incorporate geometric constraints while preserving expressivity in generated SVGs. Furthermore, we introduce a two-stage path optimization method to improve the visual and topological quality of generated SVGs. In the first stage, a pre-trained text-to-image diffusion model guides the initial generation of complex vector graphics through the Variational Score Distillation (VSD) process. In the second stage, we refine the graphics using a layer-wise image vectorization strategy to achieve clearer elements and structure. We demonstrate the effectiveness of our method through extensive experiments and showcase various applications. The project page is https://intchous.github.io/T2V-NPR.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_468",
    "authors": "Omri Avrahami, Amir Hertz, Yael Vinker, Moab Arar, Shlomi Fruchter, Ohad Fried, Daniel Cohen-Or, Dani Lischinski",
    "title": "The Chosen One: Consistent Characters in Text-to-image Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657430",
    "pdf_link": null,
    "abstract": "Recent advances in text-to-image generation models have unlocked vast potential for visual creativity. However, the users that use these models struggle with the generation of consistent characters, a crucial aspect for numerous real-world applications such as story visualization, game development, asset design, advertising, and more. Current methods typically rely on multiple pre-existing images of the target character or involve labor-intensive manual processes. In this work, we propose a fully automated solution for …",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_113",
    "authors": "Guillaume Coiffier, Etienne Corman, Guillaume Coiffier",
    "title": "The Method of Moving Frames for Surface Global Parametrization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3604282",
    "pdf_link": null,
    "abstract": "This article introduces a new representation of surface global parametrization based on Cartan’s method of moving frames. We show that a system of structure equations, characterizing the local coordinates changes with respect to a local frame system, completely characterizes the set of possible cone parametrizations. The discretization of this system provably provides necessary and sufficient conditions for the existence of a valid mapping. We are able to derive a versatile algorithm for surface parametrization, allowing feature constraints and singularities. As the first structure equation is independent of the global coordinate system, we do not require prior knowledge of cuts or cone positions. So, a single non-linear least-square problem is enough to place quantized cones while minimizing a given distortion energy. We are therefore able to take full advantage of the link between the parametrization geometry and the topology of its cone metric to solve challenging constrained parametrization problems.",
    "scholar_publication": "ACM Transactions on Graphics, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_776",
    "authors": "Zhenwei Wang, Tengfei Wang, Gerhard Hancke, Ziwei Liu, Rynson W.H. Lau",
    "title": "ThemeStation: Generating Theme-aware 3D Assets From Few Exemplars",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657471",
    "pdf_link": null,
    "abstract": "Real-world applications often require a large gallery of 3D assets that share a consistent theme. While remarkable advances have been made in general 3D content creation from text or image, synthesizing customized 3D assets following the shared theme of input 3D exemplars remains an open and challenging problem. In this work, we present ThemeStation, a novel approach for theme-aware 3D-to-3D generation. ThemeStation synthesizes customized 3D assets based on given few exemplars with two goals: 1) unity for generating 3D assets that thematically align with the given exemplars and 2) diversity for generating 3D assets with a high degree of variations. To this end, we design a two-stage framework that draws a concept image first, followed by a reference-informed 3D modeling stage. We propose a novel dual score distillation (DSD) loss to jointly leverage priors from both the input exemplars and the synthesized concept image. Extensive experiments and a user study confirm that ThemeStation surpasses prior works in producing diverse theme-aware 3D models with impressive quality. ThemeStation also enables various applications such as controllable 3D-to-3D generation.",
    "scholar_publication": "Acm siggraph 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1147",
    "authors": "Jessica Lee, Nicholas Jennings, Varun Srivastava, Ren Ng",
    "title": "Theory of Human Tetrachromatic Color Experience and Printing",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658232",
    "pdf_link": null,
    "abstract": "Genetic studies indicate that more than 50% of women are genetically tetrachromatic, expressing four distinct types of color photoreceptors (cone cells) in the retina. At least one functional tetrachromat has been identified in laboratory tests. We hypothesize that there is a large latent group in the population capable of fundamentally richer color experience, but we are not yet aware of this group because of a lack of tetrachromatic colors in the visual environment. This paper develops theory and engineering practice for fabricating tetrachromatic colors and potentially identifying tetrachromatic color vision in the wild. First, we apply general d-dimensional color theory to derive and compute all the key color structures of human tetrachromacy for the first time, including its 4D space of possible object colors, 3D space of chromaticities, and yielding a predicted 2D sphere of tetrachromatic hues. We compare this predicted hue sphere to the familiar hue circle of trichromatic color, extending the theory to predict how the higher dimensional topology produces an expanded color experience for tetrachromats. Second, we derive the four reflectance functions for the ideal tetrachromatic inkset, analogous to the well-known CMY printing basis for trichromacy. Third, we develop a method for prototyping tetrachromatic printers using a library of fountain pen inks and a multi-pass inkjet printing platform. Fourth, we generalize existing color tests - sensitive hue ordering tests and rapid isochromatic plate screening tests - to higher-dimensional vision, and prototype variants of these tests for identifying and characterizing tetrachromacy in the wild.",
    "scholar_publication": "ACM Transactions on Graphics …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_870",
    "authors": "Wonjong Jang, Yucheol Jung, Hyomin Kim, Gwangjin Ju, Chaewon Son, Jooeun Son, Seungyong Lee",
    "title": "Toonify3D: StyleGAN-based 3D Stylized Face Generator",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657480",
    "pdf_link": null,
    "abstract": "Recent advances in generative models enable high-quality facial image stylization. Toonify is a popular StyleGAN-based framework that has been widely used for facial image stylization. Our goal is to create expressive 3D faces by turning Toonify into a 3D stylized face generator. Toonify is fine-tuned with a few gradient descent steps from StyleGAN trained for standard faces, and its features would carry semantic and visual information aligned with the features of the original StyleGAN model. Based on this observation, we design a versatile 3D-lifting method for StyleGAN, StyleNormal, that regresses a surface normal map of a StyleGAN-generated face using StyleGAN features. Due to the feature alignment between Toonify and StyleGAN, although StyleNormal is trained for regular faces, it can be applied for various stylized faces without additional fine-tuning. To learn local geometry of faces under various illuminations, we introduce a novel regularization term, the normal consistency loss, based on lighting manipulation in the GAN latent space. Finally, we present Toonify3D, a fully automated framework based on StyleNormal, that can generate full-head 3D stylized avatars and support GAN-based 3D facial expression editing.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_219",
    "authors": "Taimoor Tariq, Piotr Didyk",
    "title": "Towards Motion Metamers for Foveated Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658141",
    "pdf_link": null,
    "abstract": "Foveated rendering takes advantage of the reduced spatial sensitivity in peripheral vision to greatly reduce rendering cost without noticeable spatial quality degradation. Due to its benefits, it has emerged as a key enabler for real-time high-quality virtual and augmented realities. Interestingly though, a large body of work advocates that a key role of peripheral vision may be motion detection, yet foveated rendering lowers the image quality in these regions, which may impact our ability to detect and quantify motion. The problem is critical for immersive simulations where the ability to detect and quantify movement drives actions and decisions. In this work, we diverge from the contemporary approach towards the goal of foveated graphics, and demonstrate that a loss of high-frequency spatial details in the periphery inhibits motion perception, leading to underestimating motion cues such as velocity. Furthermore, inspired by an interesting visual illusion, we design a perceptually motivated real-time technique that synthesizes controlled spatio-temporal motion energy to offset the loss in motion perception. Finally, we perform user experiments demonstrating our method's effectiveness in recovering motion cues without introducing objectionable quality degradation.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1253",
    "authors": "Nicholas Milef, John Keyser, Shu Kong",
    "title": "Towards Unstructured Unlabeled Optical Mocap: A Video Helps!",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657522",
    "pdf_link": null,
    "abstract": "Optical motion capture (mocap) requires accurately reconstructing the human body from retroreflective markers, including pose and shape. In a typical mocap setting, marker labeling is an important but tedious and error-prone step. Previous work has shown that marker labeling can be automated by using a structured template defining specific marker placements, but this places additional recording constraints. We propose to relax these constraints and solve for Unstructured Unlabeled Optical (UUO) mocap. Compared to the typical mocap setting that either labels markers or places them w.r.t a structured layout, markers in UUO mocap can be placed anywhere on the body and even on one specific limb (e.g., right leg for biomechanics research), hence it is of more practical significance. It is also more challenging. To solve UUO mocap, we exploit a monocular video captured by a single RGB camera, which does not require camera calibration. On this video, we run an off-the-shelf method to reconstruct and track a human individual, giving strong visual priors of human body pose and shape. With both the video and UUO markers, we propose an optimization pipeline towards marker identification, marker labeling, human pose estimation, and human body reconstruction. Our technical novelties include multiple hypothesis testing to optimize global orientation, and marker localization and marker-part matching to better optimize for body surface. We conduct extensive experiments to quantitatively compare our method against state-of-the-art approaches, including marker-only mocap and video-only human body/shape reconstruction. Experiments demonstrate that our method resoundingly outperforms existing methods on three established benchmark datasets for both full-body and partial-body reconstruction.",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference Papers, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_347",
    "authors": "Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, Yuval Atzmon",
    "title": "Training-free Consistent Text-to-image Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658157",
    "pdf_link": null,
    "abstract": "Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy persubject optimization or large-scale pre-training. Moreover, they struggle to align generated images …",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_279",
    "authors": "Lvmin Zhang, Maneesh Agrawala",
    "title": "Transparent Image Layer Diffusion Using Latent Transparency",
    "paper_url": "https://arxiv.org/abs/2402.17113",
    "pdf_link": null,
    "abstract": "We present LayerDiffuse, an approach enabling large-scale pretrained latent diffusion models to generate transparent images. The method allows generation of single transparent images or of multiple transparent layers. The method learns a \"latent transparency\" that encodes alpha channel transparency into the latent manifold of a pretrained latent diffusion model. It preserves the production-ready quality of the large diffusion model by regulating the added transparency as a latent offset with minimal changes to the original latent distribution of the pretrained model. In this way, any latent diffusion model can be converted into a transparent image generator by finetuning it with the adjusted latent space. We train the model with 1M transparent image layer pairs collected using a human-in-the-loop collection scheme. We show that latent transparency can be applied to different open source image generators, or be adapted to various conditional control systems to achieve applications like foreground/background-conditioned layer generation, joint layer generation, structural control of layer contents, etc. A user study finds that in most cases (97%) users prefer our natively generated transparent content over previous ad-hoc solutions such as generating and then matting. Users also report the quality of our generated transparent images is comparable to real commercial transparent assets like Adobe Stock.",
    "scholar_publication": "arXiv preprint arXiv:2402.17113, 2024 - arxiv.org"
  },
  {
    "paper_id": "papers_735",
    "authors": "Rayan Armani, Changlin Qian, Jiaxi Jiang, Christian Holz",
    "title": "Ultra Inertial Poser: Scalable Motion Capture and Tracking From Sparse Inertial Sensors and Ultra-Wideband Ranging",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657465",
    "pdf_link": null,
    "abstract": "While camera-based capture systems remain the gold standard for recording human motion, learning-based tracking systems based on sparse wearable sensors are gaining popularity. Most commonly, they use inertial sensors, whose propensity for drift and jitter have so far limited tracking accuracy. In this paper, we propose Ultra Inertial Poser, a novel 3D full body pose estimation method that constrains drift and jitter in inertial tracking via inter-sensor distances. We estimate these distances across sparse sensor setups using a lightweight embedded tracker that augments inexpensive off-the-shelf 6D inertial measurement units with ultra-wideband radio-based ranging—dynamically and without the need for stationary reference anchors. Our method then fuses these inter-sensor distances with the 3D states estimated from each sensor. Our graph-based machine learning model processes the 3D states and distances to estimate a person’s 3D full body pose and translation. To train our model, we synthesize inertial measurements and distance estimates from the motion capture database AMASS. For evaluation, we contribute a novel motion dataset of 10 participants who performed 25 motion types, captured by 6 wearable IMU+UWB trackers and an optical motion capture system, totaling 200 minutes of synchronized sensor data (UIP-DB). Our extensive experiments show state-of-the-art performance for our method over PIP and TIP, reducing position error from 13.62 to 10.65 cm (22% better) and lowering jitter from 1.56 to 0.055 km/s3 (a reduction of 97%). UIP code, UIP-DB dataset, and hardware design: https://github.com/eth-siplab/UltraInertialPoser",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1228",
    "authors": "Shaojie Bai, Te-Li Wang, Chenghui Li, Akshay Venkatesh, Tomas Simon, Chen Cao, Gabriel Schwartz, Jason Saragih, Yaser Sheikh, Shih-En Wei",
    "title": "Universal Facial Encoding of Codec Avatars From VR Headsets",
    "paper_url": "https://arxiv.org/abs/2407.13038",
    "pdf_link": null,
    "abstract": "Faithful real-time facial animation is essential for avatar-mediated telepresence in Virtual Reality (VR). To emulate authentic communication, avatar animation needs to be efficient and accurate: able to capture both extreme and subtle expressions within a few milliseconds to sustain the rhythm of natural conversations. The oblique and incomplete views of the face, variability in the donning of headsets, and illumination variation due to the environment are some of the unique challenges in generalization to unseen faces. In this paper, we present a method that can animate a photorealistic avatar in realtime from head-mounted cameras (HMCs) on a consumer VR headset. We present a self-supervised learning approach, based on a cross-view reconstruction objective, that enables generalization to unseen users. We present a lightweight expression calibration mechanism that increases accuracy with minimal additional cost to run-time efficiency. We present an improved parameterization for precise ground-truth generation that provides robustness to environmental variation. The resulting system produces accurate facial animation for unseen users wearing VR headsets in realtime. We compare our approach to prior face-encoding methods demonstrating significant improvements in both quantitative metrics and qualitative results.",
    "scholar_publication": "arXiv preprint arXiv …, 2024 - arxiv.org"
  },
  {
    "paper_id": "papers_608",
    "authors": "Ying Jiang, Chang Yu, Tianyi Xie, Xuan Li, Yutao Feng, Huamin Wang, Minchen Li, Henry Lau, Feng Gao, Yin Yang, Chenfanfu Jiang",
    "title": "VR-GS: A Physical Dynamics-aware Interactive Gaussian Splatting System in Virtual Reality",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657448",
    "pdf_link": null,
    "abstract": "As 3D content becomes increasingly prevalent, there’s a growing focus on the development of engagements with 3D virtual content. Unfortunately, traditional techniques for creating, editing, and interacting with this content are fraught with difficulties. They tend to be not only engineering-intensive but also require extensive expertise, which adds to the frustration and inefficiency in virtual object manipulation. Our proposed VR-GS system represents a leap forward in human-centered 3D content interaction, offering a seamless and intuitive user experience. By developing a physical dynamics-aware interactive Gaussian Splatting (GS) in a Virtual Reality (VR) setting, and constructing a highly efficient two-level embedding strategy alongside deformable body simulations, VR-GS ensures real-time execution with highly realistic dynamic responses. The components of our system are designed for high efficiency and effectiveness, starting from detailed scene reconstruction and object segmentation, advancing through multi-view image in-painting, and extending to interactive physics-based editing. The system also incorporates real-time deformation embedding and dynamic shadow casting, ensuring a comprehensive and engaging virtual experience.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_247",
    "authors": "Haotian Yang, Mingwu Zheng, Chongyang Ma, Yu-Kun Lai, Pengfei Wan, Haibin Huang",
    "title": "VRMM: A Volumetric Relightable Morphable Head Model",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657406",
    "pdf_link": null,
    "abstract": "In this paper, we introduce the Volumetric Relightable Morphable Model (VRMM), a novel volumetric and parametric facial prior for 3D face modeling. While recent volumetric prior models offer improvements over traditional methods like 3D Morphable Models (3DMMs), they face challenges in model learning and personalized reconstructions. Our VRMM overcomes these by employing a novel training framework that efficiently disentangles and encodes latent spaces of identity, expression, and lighting into low-dimensional …",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_926",
    "authors": "Nico Daßler, Tobias Günther",
    "title": "Variational Feature Extraction in Scientific Visualization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658219",
    "pdf_link": null,
    "abstract": "Across many scientific disciplines, the pursuit of even higher grid resolutions leads to a severe scalability problem in scientific computing. Feature extraction is a commonly chosen approach to reduce the amount of information from dense fields down to geometric primitives that further enable a quantitative analysis. Examples of common features are isolines, extremal lines, or vortex corelines. Due to the rising complexity of the observed phenomena, or in the event of discretization issues with the data, a straightforward application of textbook feature definitions is unfortunately insufficient. Thus, feature extraction from spatial data often requires substantial pre- or post-processing to either clean up the results or to include additional domain knowledge about the feature in question. Such a separate pre- or post-processing of features not only leads to suboptimal and incomparable solutions, it also results in many specialized feature extraction algorithms arising in the different application domains. In this paper, we establish a mathematical language that not only encompasses commonly used feature definitions, it also provides a set of regularizers that can be applied across the bounds of individual application domains. By using the language of variational calculus, we treat features as variational minimizers, which can be combined and regularized as needed. Our formulation not only encompasses existing feature definitions as special case, it also opens the path to novel feature definitions. This work lays the foundations for many new research directions regarding formal definitions, data representations, and numerical extraction algorithms.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_241",
    "authors": "Ryusuke Sugimoto, Christopher Batty, Toshiya Hachisuka",
    "title": "Velocity-based Monte Carlo Fluids",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657405",
    "pdf_link": null,
    "abstract": "… results when the fluid domain is … velocity-based methods can capture these physics without any change. We therefore introduce a Monte Carlo fluid solver that relies on a velocity-based …",
    "scholar_publication": "ACM SIGGRAPH 2024 Conference …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1047",
    "authors": "Vukasin Bozic, Abdelaziz Djelouah, Yang Zhang, Radu Timofte, Markus Gross, Christopher Schroers",
    "title": "Versatile Vision Foundation Model for Image and Video Colorization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657509",
    "pdf_link": null,
    "abstract": "Image and video colorization are among the most common problems in image restoration. This is an ill-posed problem and a wide variety of methods have been proposed, ranging from more traditional computer vision strategies to most recent development with transformer-based or generative neural network models. In this work we show how a latent diffusion model, pre-trained on text-to-image synthesis, can be finetuned for image colorization and provide a flexible solution for a wide variety of scenarios: high quality direct colorization with diverse results, user guided colorization through colors hints, text prompts or reference image and finally video colorization. Some works already investigated using diffusion models for colorization, however the proposed solutions are often more complex and require training a side model guiding the denoising process (à la ControlNet). Not only is this approach increasing the number of parameters and compute time, it also results in sub optimal colorization as we show. Our evaluation demonstrates that our model is the only approach that offers a wide flexibility while either matching or outperforming existing methods specialized in each sub-task, by proposing a group of universal, architecture-agnostic mechanisms which could be applied to any pre-trained diffusion model.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_564",
    "authors": "Anka H. Chen, Ziheng Liu, Yin Yang, Cem Yuksel",
    "title": "Vertex Block Descent",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658179",
    "pdf_link": null,
    "abstract": "… based on block coordinate descent that performs vertex-based … the mesh vertices, adjusting the position of a single vertex at … This offers maximized parallelism when coupled with vertex-…",
    "scholar_publication": "ACM Transactions on Graphics …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_126",
    "authors": "Lukas Lipp, David Hahn, Pierre Ecormier-Nocca, Florian Rist, Michael Wimmer, Lukas Lipp",
    "title": "View-Independent Adjoint Light Tracing for Lighting Design Optimization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3662180",
    "pdf_link": null,
    "abstract": "Differentiable rendering methods promise the ability to optimize various parameters of three-dimensional (3D) scenes to achieve a desired result. However, lighting design has so far received little attention in this field. In this article, we introduce a method that enables continuous optimization of the arrangement of luminaires in a 3D scene via differentiable light tracing. Our experiments show two major issues when attempting to apply existing methods from differentiable path tracing to this problem: First, many rendering methods produce images, which restricts the ability of a designer to define lighting objectives to image space. Second, most previous methods are designed for scene geometry or material optimization and have not been extensively tested for the case of optimizing light sources. Currently available differentiable ray-tracing methods do not provide satisfactory performance, even on fairly basic test cases in our experience. In this article, we propose, to the best of our knowledge, a novel adjoint light tracing method that overcomes these challenges and enables gradient-based lighting design optimization in a view-independent (camera-free) way. Thus, we allow the user to paint illumination targets directly onto the 3D scene or use existing baked illumination data (e.g., light maps). Using modern ray-tracing hardware, we achieve interactive performance. We find light tracing advantageous over path tracing in this setting, as it naturally handles irregular geometry, resulting in less noise and improved optimization convergence. We compare our adjoint gradients to state-of-the-art image-based differentiable rendering methods. We also demonstrate that our gradient data works with various common optimization algorithms, providing good convergence behaviour. Qualitative comparisons with real-world scenes underline the practical applicability of our method.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1039",
    "authors": "Peizhuo Li, Sebastian Starke, Yuting Ye, Olga Sorkine-Hornung",
    "title": "WalkTheDog: Cross-morphology Motion Alignment via Phase Manifolds",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657508",
    "pdf_link": null,
    "abstract": "We present a new approach for understanding the periodicity structure and semantics of motion datasets, independently of the morphology and skeletal structure of characters. Unlike existing methods using an overly sparse high-dimensional latent, we propose a phase manifold consisting of multiple closed curves, each corresponding to a latent amplitude. With our proposed vector quantized periodic autoencoder, we learn a shared phase manifold for multiple characters, such as a human and a dog, without any supervision. This is achieved by exploiting the discrete structure and a shallow network as bottlenecks, such that semantically similar motions are clustered into the same curve of the manifold, and the motions within the same component are aligned temporally by the phase variable. In combination with an improved motion matching framework, we demonstrate the manifold’s capability of timing and semantics alignment in several applications, including motion retrieval, transfer and stylization. Code and pre-trained models for this paper are available at peizhuoli.github.io/walkthedog.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_320",
    "authors": "Bailey Miller, Rohan Sawhney, Keenan Crane, Ioannis Gkioulekas",
    "title": "Walkin’ Robin: Walk on Stars With Robin Boundary Conditions",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658153",
    "pdf_link": null,
    "abstract": "Numerous scientific and engineering applications require solutions to boundary value problems (BVPs) involving elliptic partial differential equations, such as the Laplace or Poisson equations, on geometrically intricate domains. We develop a Monte Carlo method for solving such BVPs with arbitrary first-order linear boundary conditions---Dirichlet, Neumann, and Robin. Our method directly generalizes the walk on stars (WoSt) algorithm, which previously tackled only the first two types of boundary conditions, with a few simple …",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_278",
    "authors": "Yingjie Tang, Zixuan Li, Milos Hasan, Jian Yang, Beibei Wang",
    "title": "Woven Fabric Capture With a Reflection-transmission Photo Pair",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657410",
    "pdf_link": null,
    "abstract": "Digitizing woven fabrics would be valuable for many applications, from digital humans to interior design. Previous work introduces a lightweight woven fabric acquisition approach by capturing a single reflection image and estimating the fabric parameters with a differentiable geometric and shading model. The renderings of the estimated fabric parameters can closely match the photo; however, the captured reflection image is insufficient to fully characterize the fabric sample reflectance. For instance, fabrics with different thicknesses might have similar reflection images but lead to significantly different transmission. We propose to recover the woven fabric parameters from two captured images: reflection and transmission. At the core of our method is a differentiable bidirectional scattering distribution function (BSDF) model, handling reflection and transmission, including single and multiple scattering. We propose a two-layer model, where the single scattering uses an SGGX phase function as in previous work, and multiple scattering uses a new azimuthally-invariant microflake definition, which we term ASGGX. This new fabric BSDF model closely matches real woven fabrics in both reflection and transmission. We use a simple setup for capturing reflection and transmission photos with a cell phone camera and two point lights, and estimate the fabric parameters via a lightweight network, together with a differentiable optimization. We also model the out-of-focus effects explicitly with a simple solution to match the thin-lens camera better. As a result, the renderings of the estimated parameters can agree with the input images on both reflection and transmission for the first time. The code for this paper is at https://github.com/lxtyin/FabricBTDF-Recovery.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_680",
    "authors": "You Xie, Hongyi Xu, Guoxian Song, Chao Wang, Yichun Shi, Linjie Luo",
    "title": "X-Portrait: Expressive Portrait Animation With Hierarchical Motion Attention",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3641519.3657459",
    "pdf_link": null,
    "abstract": "We propose X-Portrait, an innovative conditional diffusion model tailored for generating expressive and temporally coherent portrait animation. Specifically, given a single portrait as appearance reference, we aim to animate it with motion derived from a driving video, capturing both highly dynamic and subtle facial expressions along with wide-range head movements. As its core, we leverage the generative prior of a pre-trained diffusion model as the rendering backbone, while achieve fine-grained head pose and expression control with novel controlling signals within the framework of ControlNet. In contrast to conventional coarse explicit controls such as facial landmarks, our motion control module is learned to interpret the dynamics directly from the original driving RGB inputs. The motion accuracy is further enhanced with a patch-based local control module that effectively enhance the motion attention to small-scale nuances like eyeball positions. Notably, to mitigate the identity leakage from the driving signals, we train our motion control modules with scaling-augmented cross-identity images, ensuring maximized disentanglement from the appearance reference modules. Experimental results demonstrate the universal effectiveness of X-Portrait across a diverse range of facial portraits and expressive driving sequences, and showcase its proficiency in generating captivating portrait animations with consistently maintained identity characteristics.",
    "scholar_publication": "ACM SIGGRAPH 2024 …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1221",
    "authors": "Zhexi Peng, Yin Yang, Tianjia Shao, Chenfanfu Jiang, Kun Zhou",
    "title": "X-SLAM: Scalable Dense SLAM for Task-aware Optimization Using CSFD",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658233",
    "pdf_link": null,
    "abstract": "We present X-SLAM, a real-time dense differentiable SLAM system that leverages the complex-step finite difference (CSFD) method for efficient calculation of numerical derivatives, bypassing the need for a large-scale computational graph. The key to our approach is treating the SLAM process as a differentiable function, enabling the calculation of the derivatives of important SLAM parameters through Taylor series expansion within the complex domain. Our system allows for the real-time calculation of not just the gradient, but also higher-order differentiation. This facilitates the use of high-order optimizers to achieve better accuracy and faster convergence. Building on X-SLAM, we implemented end-to-end optimization frameworks for two important tasks: camera relocalization in wide outdoor scenes and active robotic scanning in complex indoor environments. Comprehensive evaluations on public benchmarks and intricate real scenes underscore the improvements in the accuracy of camera relocalization and the efficiency of robotic navigation achieved through our task-aware optimization. The code and data are available at https://gapszju.github.io/X-SLAM.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_499",
    "authors": "Michael Fischer, Tobias Ritschel",
    "title": "ZeroGrads: Learning Local Surrogates for Non-differentiable Graphics",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658173",
    "pdf_link": null,
    "abstract": "Gradient-based optimization is now ubiquitous across graphics, but unfortunately can not be applied to problems with undefined or zero gradients. To circumvent this issue, the loss function can be manually replaced by a \"surrogate\" that has similar minima but is differentiable. Our proposed framework, ZeroGrads, automates this process by learning a neural approximation of the objective function, which in turn can be used to differentiate through arbitrary black-box graphics pipelines. We train the surrogate on an actively smoothed version of the objective and encourage locality, focusing the surrogate's capacity on what matters at the current training episode. The fitting is performed online, alongside the parameter optimization, and self-supervised, without pre-computed data or pre-trained models. As sampling the objective is expensive (it requires a full rendering or simulator run), we devise an efficient sampling scheme that allows for tractable run-times and competitive performance at little overhead. We demonstrate optimizing diverse non-convex, non-differentiable black-box problems in graphics, such as visibility in rendering, discrete parameter spaces in procedural modelling or optimal control in physics-driven animation. In contrast to other derivative-free algorithms, our approach scales well to higher dimensions, which we demonstrate on problems with up to 35k interlinked variables.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2024 - dl.acm.org"
  },
  {
    "paper_id": "papers_1072",
    "authors": "Francis Williams, Jiahui Huang, Jonathan Swartz, Gergely Klar, Vijay Thakkar, Matthew Cong, Xuanchi Ren, Ruilong Li, Clement Fuji Tsang, Sanja Fidler, Eftychios Sifakis, Ken Museth",
    "title": "fVDB : A Deep-learning Framework for Sparse, Large Scale, and High Performance Spatial Intelligence",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3658226",
    "pdf_link": null,
    "abstract": "We present fVDB, a novel GPU-optimized framework for deep learning on large-scale 3D data. fVDB provides a complete set of differentiable primitives to build deep learning architectures for common tasks in 3D learning such as convolution, pooling, attention, ray-tracing, meshing, etc. fVDB simultaneously provides a much larger feature set (primitives and operators) than established frameworks with no loss in efficiency: our operators match or exceed the performance of other frameworks with narrower scope. Furthermore, fVDB can process datasets with much larger footprint and spatial resolution than prior works, while providing a competitive memory footprint on small inputs. To achieve this combination of versatility and performance, fVDB relies on a single novel VDB index grid acceleration structure paired with several key innovations including GPU accelerated sparse grid construction, convolution using tensorcores, fast ray tracing kernels using a Hierarchical Digital Differential Analyzer algorithm (HDDA), and jagged tensors. Our framework is fully integrated with PyTorch enabling interoperability with existing pipelines, and we demonstrate its effectiveness on a number of representative tasks such as large-scale point-cloud segmentation, high resolution 3D generative modeling, unbounded scale Neural Radiance Fields, and large-scale point cloud reconstruction.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  }
]