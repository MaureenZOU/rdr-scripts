[
  {
    "paper_id": "506e62fe82",
    "authors": "Ipek Oztas, Duygu Ceylan, Aysegul Dundar",
    "title": "3D Stylization via Large Reconstruction Model",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730636",
    "pdf_link": null,
    "abstract": "With the growing success of text or image guided 3D generators, users demand more control over the generation process, appearance stylization being one of them. Given a reference image, this requires adapting the appearance of a generated 3D asset to reflect the visual style of the reference while maintaining visual consistency from multiple viewpoints. To tackle this problem, we draw inspiration from the success of 2D stylization methods that leverage the attention mechanisms in large image generation models to capture and transfer visual style. In particular, we probe if large reconstruction models, commonly used in the context of 3D generation, has a similar capability. We discover that the certain attention blocks in these models capture the appearance specific features. By injecting features from a visual style image to such blocks, we develop a simple yet effective 3D appearance stylization method. Our method does not require training or test time optimization. Through both quantitative and qualitative evaluations, we demonstrate that our approach achieves superior results in terms of 3D appearance stylization, significantly improving efficiency while maintaining high-quality visual outcomes. Code and models are available via our project website: https://github.com/ipekoztas/3D-Stylization-LRM.",
    "scholar_publication": "Proceedings of the Special Interest Group …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "c769be31e6",
    "authors": "Yen-Chi Cheng, Krishna Kumar Singh, Jae Shin Yoon, Alexander Schwing, Liang-Yan Gui, Matheus Gadelha, Paul Guerrero, Nanxuan Zhao",
    "title": "3D-Fixup: Advancing Photo Editing with 3D Priors",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730695",
    "pdf_link": null,
    "abstract": "Despite significant advances in modeling image priors via diffusion models, 3D-aware image editing remains challenging, in part because the object is only specified via a single image. To tackle this challenge, we propose 3D-Fixup, a new framework for editing 2D images guided by learned 3D priors. The framework supports difficult editing situations such as object translation and 3D rotation. To achieve this, we leverage a training-based approach that harnesses the generative power of diffusion models. As video data naturally encodes real-world physical dynamics, we turn to video data for generating training data pairs, i.e., a source and a target frame. Rather than relying solely on a single trained model to infer transformations between source and target frames, we incorporate 3D guidance from an Image-to-3D model, which bridges this challenging task by explicitly projecting 2D information into 3D space. We design a data generation pipeline to ensure high-quality 3D guidance throughout training. Results show that by integrating these 3D priors, 3D-Fixup effectively supports complex, identity coherent 3D-aware edits, achieving high-quality results and advancing the application of diffusion models in realistic image manipulation. The code is provided at https://3dfixup.github.io/.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "f507c1075e",
    "authors": "Chengan He, Junxuan Li, Tobias Kirschstein, Artem Sevastopolskiy, Shunsuke Saito, Qingyang Tan, Javier Romero, Chen Cao, Holly Rushmeier, Giljoo Nam",
    "title": "3DGH: 3D Head Generation with Composable Hair and Face",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731211",
    "pdf_link": null,
    "abstract": "We present 3DGH, an unconditional generative model for 3D human heads with composable hair and face components. Unlike previous work that entangles the modeling of hair and face, we propose to separate them using a novel data representation with template-based 3D Gaussian Splatting, in which deformable hair geometry is introduced to capture the geometric variations across different hairstyles. Based on this data representation, we design a 3D GAN-based architecture with dual generators and employ a cross-attention mechanism to model the inherent correlation between hair and face. The model is trained on synthetic renderings using carefully designed objectives to stabilize training and facilitate hair-face separation. We conduct extensive experiments to validate the design choice of 3DGH, and evaluate it both qualitatively and quantitatively by comparing with several state-of-the-art 3D GAN methods, demonstrating its effectiveness in unconditional full-head image synthesis and composable 3D hairstyle editing. More details will be available on our project page: https://c-he.github.io/projects/3dgh/.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "563d3252fc",
    "authors": "Lei Lan, Tianjia Shao, Zixuan Lu, Yu Zhang, Chenfanfu Jiang, Yin Yang",
    "title": "3DGS2: Near Second-order Converging 3D Gaussian Splatting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730687",
    "pdf_link": null,
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a mainstream solution for novel view synthesis and 3D reconstruction. By explicitly encoding a 3D scene using a collection of Gaussian kernels, 3DGS achieves high-quality rendering with superior efficiency. As a learning-based approach, 3DGS training has been dealt with the standard stochastic gradient descent (SGD) method, which offers at most linear convergence. Consequently, training often requires tens of minutes, even with GPU acceleration. This paper introduces a (near) second-order convergent training algorithm for 3DGS, leveraging its unique properties. Our approach is inspired by two key observations. First, the attributes of a Gaussian kernel contribute independently to the image-space loss, which endorses isolated and local optimization algorithms. We exploit this by splitting the optimization at the level of individual kernel attributes, analytically constructing small-size Newton systems for each parameter group, and efficiently solving these systems on GPU threads. This achieves Newton-like convergence per training image without relying on the global Hessian. Second, kernels exhibit sparse and structured coupling across input images. This property allows us to effectively utilize spatial information to mitigate overshoot during stochastic training. Our method converges an order faster than standard GPU-based 3DGS training, requiring over 10 × fewer iterations while maintaining or surpassing the quality of the compared with the SGD-based 3DGS reconstructions.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "95591b47c2",
    "authors": "Pinxuan Dai, Peiquan Zhang, Zheng Dong, Ke Xu, Yifan Peng, Dandan Ding, Yujun Shen, Yin Yang, Xinguo Liu, Rynson W.H. Lau, Weiwei Xu",
    "title": "4D Gaussian Videos with Motion Layering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731189",
    "pdf_link": null,
    "abstract": "Online free-view navigation in volumetric videos requires high-quality rendering and real-time streaming in order to provide immersive user experiences. However, existing methods (e.g., dynamic NeRF and 3DGS) may not handle dynamic scenes with complex motions, and their models may not be streamable due to storage and bandwidth constraints. In this paper, we propose a novel 4D Gaussian Video (4DGV) approach that enables the creation and streaming of photorealistic, volumetric videos for dynamic scenes over the Internet. The core of our 4DGV is a novel streamable group of Gaussians (GOG) representation based on motion layering. Each GOG consists of static and dynamic points obtained via lifting 2D segmentation into 3D in motion layering, where the deformation of each dynamic point is represented as the temporal offset of its attributes. We also adaptively convert static points back to dynamic points to handle the appearance change, (e.g., moving shadows and reflections), of static objects through optimization. To support real-time streaming of 4DGVs, we show that by applying quantization on Gaussian attributes and H.265 encoding on deformation offsets, our GOG representation can be significantly compressed (to around 6% of the original model size) without sacrificing the accuracy (PSNR loss less than 0.01dB). Extensive experiments on standard benchmarks demonstrate that our method outperforms state-of-the-art volumetric video approaches, with superior rendering quality and minimum storage overheads.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "549a18ffaa",
    "authors": "Kuo-Cheng Hsu, Li-Min Huang",
    "title": "60,000nits Full-color Native RGB Single Junction 3,386PPI Micro-OLED",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742970",
    "pdf_link": null,
    "abstract": "In this study, we independently developed a knowledge system named uNEEDXR™, successfully realizing a full-color micro-OLED device with a brightness of 60,000 nits on a silicon-based backplane. This knowledge system was developed over nine years and innovatively know-how and integrates expertise across design, processes, manufacturing, equipment, and material, overcoming the performance limitations of traditional micro-OLED architectures. The system enables high brightness, high pixel density, low power consumption, high contrast ratio, high color saturation, and tunable energy distribution (including viewing angle, wavelength, and bandwidth). Additionally, it meets customer requirements for reliability and lifespan. This technology provides a scalable production solution for near-eye display applications in augmented reality.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ccbc1dc167",
    "authors": "Nathan King, Haozhe Su, Mridul Aanjaneya, Steven Ruuth, Christopher Batty, Nathan King",
    "title": "A Closest Point Method for PDEs on Manifolds with Interior Boundary Conditions for Geometry Processing",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3673652",
    "pdf_link": null,
    "abstract": "Many geometry processing techniques require the solution of partial differential equations (PDEs) on manifolds embedded in ℝ2 or ℝ3, such as curves or surfaces. Such manifold PDEs often involve boundary conditions (e.g., Dirichlet or Neumann) prescribed at points or curves on the manifold’s interior or along the geometric (exterior) boundary of an open manifold. However, input manifolds can take many forms (e.g., triangle meshes, parametrizations, point clouds, implicit functions, etc.). Typically, one must generate a mesh to apply finite element-type techniques or derive specialized discretization procedures for each distinct manifold representation. We propose instead to address such problems in a unified manner through a novel extension of the closest point method (CPM) to handle interior boundary conditions. CPM solves the manifold PDE by solving a volumetric PDE defined over the Cartesian embedding space containing the manifold and requires only a closest point representation of the manifold. Hence, CPM supports objects that are open or closed, orientable or not, and of any codimension. To enable support for interior boundary conditions, we derive a method that implicitly partitions the embedding space across interior boundaries. CPM’s finite difference and interpolation stencils are adapted to respect this partition while preserving second-order accuracy. Additionally, we develop an efficient sparse-grid implementation and numerical solver that can scale to tens of millions of degrees of freedom, allowing PDEs to be solved on more complex manifolds. We demonstrate our method’s convergence behavior on selected model PDEs and explore several geometry processing problems: diffusion curves on surfaces, geodesic distance, tangent vector field design, harmonic map construction, and reaction-diffusion textures. Our proposed approach thus offers a powerful and flexible new tool for a range of geometry processing tasks on general manifold representations.",
    "scholar_publication": "ACM Transactions on …, 2024 - dl.acm.org"
  },
  {
    "paper_id": "35a767fa6a",
    "authors": "Seonghyeon Kim, Chang Wook Seo, Kwanggyoon Seo, Seung Han Song, Junyong Noh",
    "title": "A Deep Learning-based Virtual Oculoplastic Surgery Simulator",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731426",
    "pdf_link": null,
    "abstract": "Oculoplastic surgery is a critical treatment for various eye conditions, such as ptosis, which can cause both aesthetic and functional issues. Due to the anxiety about the outcome, patients are often hesitant to undergo the necessary procedures required for the surgery. Virtual oculoplastic surgery simulation technology offers a solution to alleviate these concerns by providing realistic previews of post-surgical results. In this paper, we present a novel deep learning-based virtual oculoplastic surgery simulation system that addresses the limitations of existing methods. The proposed system aims to improve the accuracy of simulations by considering the anatomical structure and characteristics of the eye. Our method utilizes a deformable parametric mesh to enhance the controllability of the image transformation process. Furthermore, the combination of a style-based generator and a neural texture has been implemented to generate high-quality results. The proposed system is expected to facilitate better communication between doctors and patients by providing anatomically inspired high-quality simulation results. The development of this advanced virtual simulation system has the potential to enhance patient experiences and improve satisfaction with outcomes in the field of oculoplastic surgery.",
    "scholar_publication": "ACM Transactions on Graphics …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "c61bd07305",
    "authors": "Zhuodong Li, Fei Hou, Wencheng Wang, Xuquan Lu, Ying He",
    "title": "A Divide-and-Conquer Approach for Global Orientation of Non-Watertight Scene-Level Point Clouds with 0-1 Integer Optimization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730923",
    "pdf_link": null,
    "abstract": "Orienting point clouds is a fundamental problem in computer graphics and 3D vision, with applications in reconstruction, segmentation, and analysis. While significant progress has been made, existing approaches mainly focus on watertight, object-level 3D models. The orientation of large-scale, non-watertight 3D scenes remains an underexplored challenge. To address this gap, we propose DACPO (Divide-And-Conquer Point Orientation), a novel framework that leverages a divide-and-conquer strategy for scalable and robust point cloud orientation. Rather than attempting to orient an unbounded scene at once, DACPO segments the input point cloud into smaller, manageable blocks, processes each block independently, and integrates the results through a global optimization stage. For each block, we introduce a two-step process: estimating initial normal orientations by a randomized greedy method and refining them by an adapted iterative Poisson surface reconstruction. To achieve consistency across blocks, we model inter-block relationships using an an undirected graph, where nodes represent blocks and edges connect spatially adjacent blocks. To reliably evaluate orientation consistency between adjacent blocks, we introduce the concept of the visible connected region, which defines the region over which visibility-based assessments are performed. The global integration is then formulated as a 0-1 integer-constrained optimization problem, with block flip states as binary variables. Despite the combinatorial nature of the problem, DACPO remains scalable by limiting the number of blocks (typically a few hundred for 3D scenes) involved in the optimization. Experiments on benchmark datasets demonstrate DACPO's strong performance, particularly in challenging large-scale, non-watertight scenarios where existing methods often fail. The source code is available at https://github.com/zd-lee/DACPO.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "ab2728d55c",
    "authors": "Louis Sugy",
    "title": "A Fast Parallel Median Filtering Algorithm Using Hierarchical Tiling",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730709",
    "pdf_link": null,
    "abstract": "Median filtering is a non-linear smoothing technique widely used in digital image processing to remove noise while retaining sharp edges. It is particularly well suited to removing outliers (impulse noise) or granular artifacts (speckle noise). However, the high computational cost of median filtering can be prohibitive. Sorting-based algorithms excel with small kernels but scale poorly with increasing kernel diameter, in contrast to constant-time methods characterized by higher constant factors but better scalability, such as histogram-based approaches or the 2D wavelet matrix.This paper introduces a novel algorithm, leveraging the separability of the sorting problem through hierarchical tiling to minimize redundant computations. We propose two variants: a data-oblivious selection network that can operate entirely within registers, and a data-aware version utilizing random-access memory. These achieve per-pixel complexities of O(klog (k)) and O(k), respectively, for a k × k kernel — unprecedented for sorting-based methods. Our CUDA implementation is up to 5 times faster than the current state of the art on a modern GPU and is the fastest median filter in most cases for 8-, 16-, and 32-bit data types and kernels from 3 × 3 to 75 × 75.",
    "scholar_publication": "Proceedings of the Special Interest Group on Computer …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "a57f006fa7",
    "authors": "Laurent Belcour, Alban Fichet, Pascal Barla",
    "title": "A Fluorescent Material Model for Non-Spectral Editing & Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730721",
    "pdf_link": null,
    "abstract": "Fluorescent materials are characterized by a spectral reradiation toward longer wavelengths. Recent work [Fichet et al. ] has shown that the rendering of fluorescence in a non-spectral engine is possible through the use of appropriate reduced reradiation matrices. But the approach has limited expressivity, as it requires the storage of one reduced matrix per fluorescent material, and only works with measured fluorescent assets.In this work, we introduce an analytical approach to the editing and rendering of fluorescence in a non-spectral engine. It is based on a decomposition of the reduced reradiation matrix, and an analytically-integrable Gaussian-based model of the fluorescent component. The model reproduces the appearance of fluorescent materials accurately, especially with the addition of a UV basis. Most importantly, it grants variations of fluorescent material parameters in real-time, either for the editing of fluorescent materials, or for the dynamic spatial variation of fluorescence properties across object surfaces. A simplified one-Gaussian fluorescence model even allows for the artist-friendly creation of plausible fluorescent materials from scratch, requiring only a few reflectance colors as input.",
    "scholar_publication": "Proceedings of the Special Interest Group …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "34cb8714e1",
    "authors": "Zhengze Liu, Yuchi Huo, Yifan Peng, Rui Wang",
    "title": "A Fully-statistical Wave Scattering Model for Heterogeneous Surfaces",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730828",
    "pdf_link": null,
    "abstract": "Heterogeneous surfaces exhibit spatially varying geometry and material, and therefore admit diverse appearances. Existing computer graphics works can only model heterogeneity using explicit structures or statistical parameters that describe a coarser level of detail. We extend the boundary by introducing a new model that describes the heterogeneous surfaces fully statistically at the microscopic level, with rich geometry and material details that are comparable to the wavelengths of light. We treat the heterogeneous surfaces as a mixture of stochastic vector processes. We adapt the well-known generalized Harvey-Shack theory to quantify the mean scattered intensity, i.e., the BRDF of these surfaces. We further explore the covariance statistic of the scattered field and derive its rank-1 decomposition. This leads to a practical algorithm that samples the speckles (fluctuating intensities) from the statistics, enriching the appearance without explicit definition of heterogeneous surfaces. The formulations are analytic, and we validate the quantities by comprehensive numerical simulations. Our heterogeneous surface model demonstrates various applications including corrosion (natural), particle deposition (man-made), and height-correlated mixture (artistic). Code for this paper is available at https://github.com/Rendering-at-ZJU/HeteroSurface.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "b20aa9dd62",
    "authors": "Mengyun Liu, Kai Bai, Xiaopei Liu",
    "title": "A Hybrid Near-wall Model for Kinetic Simulation of Turbulent Boundary Layer Flows",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730829",
    "pdf_link": null,
    "abstract": "Turbulent boundary layer represents one of the most complex but interesting phenomena in fluid flows. While the generation and alteration of sheared vortices in various interacting scales near the boundary are visually appealing, it is difficult to correctly replicate such phenomena by simulation, especially at high Reynolds numbers. Practical methodologies typically incorporate empirical wall modeling to substantially curtail the computational expenses while retaining physical consistency. Nevertheless, these are predominantly applicable to steady-state flow solvers. While complex scenarios involving dynamic fluid-solid interaction and its application to create time-dependent flow phenomena invariably necessitate unsteady flow solvers, the underlying wall modeling techniques are imprecise, leading to a different formation of near-wall vortices, especially for the highly efficient lattice Boltzmann solver operating on Cartesian grids. In this paper, we propose a novel hybrid near-wall model for the lattice Boltzmann solver, which can handle turbulent boundary layer flows in a simple and efficient manner, inspired by measuring the degree of boundary layer separation. Our model comprises both macroscopic and mesoscopic algebraic models, which collaborate to let the low dissipation lattice Boltzmann solver naturally form the turbulent boundary layer appropriately. By leveraging the multi-resolution technique, accurate simulation outcomes can be obtained. Our model is parameterized to approximate different physical attributes of the solid surface that can potentially influence the boundary layer distribution, and comparable boundary layer flow behaviors can be simulated at various grid resolutions. Rigorous benchmark tests are carried out to validate our model at different grid resolutions by comparing with experimental data and visualizations. We showcase the applications of our new model in both facilitating computational design and generating visual animations, accompanied by specific examples and comparisons with actual experimental setups and photographic images. All demonstrations affirm the physical consistency of our solver even when simulated with a relatively coarse grid resolution.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "c3b3795f43",
    "authors": "Ryota Koiso, Suyeon Choi, Manu Gopakumar, Brian Chao, Jacqueline Yang, Gordon Wetzstein",
    "title": "A Large-Étendue Direct-View Holographic Display System",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742975",
    "pdf_link": null,
    "abstract": "Direct-view 3D displays enable immersive experiences but often cause visual discomfort from relying on binocular disparity alone. Holographic displays offer an ultimate solution by reconstructing the full light wavefront, but face scalability and viewing freedom limitations due to the spatial-bandwidth product and high cost of fine-pitch phase-only SLMs. We present a system combining an amplitude-only display with ultra-high pixel count and dynamic optical steering for a fully 3D eye box. By axially translating a lens, we expand the eye box in depth beyond the capabilities of conventional pupil-steering methods. We further extend SGD-based hologram optimization to support dual light sources and an amplitude-only SLM, enabling stereoscopic delivery with suppressed crosstalk. Our prototype shows accurate depth cues, paving the way for scalable, high-quality holographic displays with expanded viewing freedom.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "8745f16f0b",
    "authors": "Juhyeon Kim, Craig Benko, Magnus Wrenninge, Ryusuke Villemin, Zeb Barber, Wojciech Jarosz, Adithya Pediredla",
    "title": "A Monte Carlo Rendering Framework for Simulating Optical Heterodyne Detection",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731150",
    "pdf_link": null,
    "abstract": "Optical heterodyne detection (OHD) employs coherent light and optical interference techniques (Fig. 1-(A)) to extract physical parameters, such as velocity or distance, which are encoded in the frequency modulation of the light. With its superior signal-to-noise ratio compared to incoherent detection methods, such as time-of-flight lidar, OHD has become integral to applications requiring high sensitivity, including autonomous navigation, atmospheric sensing, and biomedical velocimetry. However, current simulation tools for OHD focus narrowly on specific applications, relying on domain-specific settings like restricted reflection functions, scene configurations, or single-bounce assumptions, which limit their applicability. In this work, we introduce a flexible and general framework for spectral-domain simulation of OHD. We demonstrate that classical radiometry-based path integral formulation can be adapted and extended to simulate the OHD measurements in the spectral domain. This enables us to leverage the rich modeling and sampling capabilities of existing Monte Carlo path tracing techniques. Our formulation shares structural similarities with transient rendering but operates in the spectral domain and accounts for the Doppler effect (Fig. 1-(B)). While simulators for the Doppler effect in incoherent (intensity) detection methods exist, they are largely not suitable to simulate OHD. We use a microsurface interpretation to show that these two Doppler imaging techniques capture different physical quantities and thus need different simulation frameworks. We validate the correctness and predictive power of our simulation framework by qualitatively comparing the simulations with real-world captured data for three different OHD applications—FMCW lidar, blood flow velocimetry, and wind Doppler lidar (Fig. 1-(C)).",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "e99c2902f3",
    "authors": "Duowen Chen, Junwei Zhou, Bo Zhu, Duowen Chen",
    "title": "A Neural Particle Level Set Method for Dynamic Interface Tracking",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730399",
    "pdf_link": null,
    "abstract": "We propose a neural particle level set (Neural PLS) method to accommodate tracking and evolving dynamic neural representations. At the heart of our approach is a set of oriented particles serving dual roles of interface trackers and sampling seeders. These dynamic particles are used to evolve the interface and construct neural representations on a multi-resolution grid-hash structure to hybridize coarse sparse distance fields and multi-scale feature encoding. Based on these parallel implementations and neural-network-friendly architectures, our neural particle level set method combines the computational merits on both ends of the traditional particle level sets and the modern implicit neural representations, in terms of feature representation and dynamic tracking. We demonstrate the efficacy of our approach by showcasing its performance surpassing traditional level-set methods in both benchmark tests and physical simulations.",
    "scholar_publication": "ACM Transactions on Graphics, 2025 - dl.acm.org"
  },
  {
    "paper_id": "4fa056a2f9",
    "authors": "Shuri Futamura, Ryuichi Inui, Tomoki Matsumoto, Yasuhisa Nakano, Tatsuji Tokiwa",
    "title": "A Novel Maxwellian Optics Combining Spherical Multi Pinholes and TMD for Enhanced Field of View",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743022",
    "pdf_link": null,
    "abstract": "In this study, we propose a novel Maxwellian optics that achieves a wide field of view and evaluate its effectiveness through 2D and 3D simulations. The fundamental principle of the proposed system is based on Maxwellian view, a form of retinal projection that can mitigate the vergence–accommodation conflict. Conventional Maxwellian optics typically employ a pinhole to focus light on a single point within the pupil, enabling the projection of sharp images onto the retina with a wide field of view, independent of the eye's accommodation. However, a major limitation of such systems is the disappearance of the image when the eye rotates and the convergence point shifts outside the pupil. To address this issue, we propose a novel Maxwellian optical system that combines a spherical multi-pinhole (SMP) with a transmissive mirror device (TMD).",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "1b2526e161",
    "authors": "Rafael Wampfler, Chen Yang, Dillon Elste, Nikola Kovacevic, Philine Witzig, Markus Gross",
    "title": "A Platform for Interactive AI Character Experiences",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730762",
    "pdf_link": null,
    "abstract": "From movie characters to modern science fiction — bringing characters into interactive, story-driven conversations has captured imaginations across generations. Achieving this vision is highly challenging and requires much more than just language modeling. It involves numerous complex AI challenges, such as conversational AI, maintaining character integrity, managing personality and emotions, handling knowledge and memory, synthesizing voice, generating animations, enabling real-world interactions, and integration with physical environments. Recent advancements in the development of foundation models, prompt engineering, and fine-tuning for downstream tasks have enabled researchers to address these individual challenges. However, combining these technologies for interactive characters remains an open problem. We present a system and platform for conveniently designing believable digital characters, enabling a conversational and story-driven experience while providing solutions to all of the technical challenges. As a proof-of-concept, we introduce Digital Einstein, which allows users to engage in conversations with a digital representation of Albert Einstein about his life, research, and persona. While Digital Einstein exemplifies our methods for a specific character, our system is flexible and generalizes to any story-driven or conversational character. By unifying these diverse AI components into a single, easy-to-adapt platform, our work paves the way for immersive character experiences, turning the dream of lifelike, story-based interactions into a reality.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "c0bcb54226",
    "authors": "Max Kohlbrenner, Marc Alexa",
    "title": "A Polyhedral Construction of Empty Spheres in Discrete Distance Fields",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730748",
    "pdf_link": null,
    "abstract": "Lie sphere geometry provides a unified representation of points, oriented spheres and hyperplanes in Euclidean d-space as the subset of lines in that are contained in a certain quadric. The natural scalar product in this construction is zero if two elements are in oriented contact. We show how the sign of this product can be used to decide if spheres are disjoint. This allows us to model the space of spheres that are not intersecting a given union of spheres as the intersection of half-spaces (and the quadric). The maximal spheres are on the boundary of this set and can be computed by first constructing the intersection of half-spaces, which is a convex hull problem, and then intersecting edges of the hull against the quadric, which are the roots of a univariate quadratic. We demonstrate the method at the example of contouring a discrete signed distance field: every sample of the signed distance field represents an empty spheres and the zero-level contour has to be disjoint from the union of these spheres. Maximal spheres outside the empty spheres provide samples on the zero-level contour. The quality of this sample set is comparable to existing methods relying on optimization, while being deterministic and faster in practice.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "5f211c2c14",
    "authors": "Guirec Maloisel, Ruben Grandia, Christian Schumacher, Espen Knoop, Moritz Bächer",
    "title": "A Versatile Quaternion-Based Constrained Rigid Body Dynamics",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730872",
    "pdf_link": null,
    "abstract": "We present a constrained Rigid Body Dynamics (RBD) that guarantees satisfaction of kinematic constraints, enabling direct simulation of complex mechanical systems with arbitrary kinematic structures. To ensure constraint satisfaction, we use an implicit integration scheme. For this purpose, we derive compatible dynamic equations expressed through the quaternion time derivative, adopting an additive approach to quaternion updates instead of a multiplicative one, while enforcing quaternion unit-length as a constraint. We support all joints between rigid bodies that restrict subsets of the three translational or three rotational degrees of freedom, including position- and force-based actuation. Their constraints are formulated such that Lagrange multipliers are interpretable as joint forces and torques. We discuss a unified solution strategy for systems with redundant constraints, overactuation, and passive degrees of freedom, by eliminating redundant constraints and navigating the subspaces spanned by multipliers. As our method uses a standard additive update, we can interface with unconditionally-stable implicit integrators. Moreover, the simulation can readily be made differentiable as we show with examples.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "dd1215118f",
    "authors": "Lucas N. Alegre, Agon Serifi, Ruben Grandia, David Müller, Espen Knoop, Moritz Bächer",
    "title": "AMOR: Adaptive Character Control through Multi-Objective Reinforcement Learning",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730656",
    "pdf_link": null,
    "abstract": "Reinforcement learning (RL) has significantly advanced the control of physics-based and robotic characters that track kinematic reference motion. However, methods typically rely on a weighted sum of conflicting reward functions, requiring extensive tuning to achieve a desired behavior. Due to the computational cost of RL, this iterative process is a tedious, time-intensive task. Furthermore, for robotics applications, the weights need to be chosen such that the policy performs well in the real world, despite inevitable sim-to-real gaps. To address these challenges, we propose a multi-objective reinforcement learning framework that trains a single policy conditioned on a set of weights, spanning the Pareto front of reward trade-offs. Within this framework, weights can be selected and tuned after training, significantly speeding up iteration time. We demonstrate how this improved workflow can be used to perform highly dynamic motions with a robot character. Moreover, we explore how weight-conditioned policies can be leveraged in hierarchical settings, using a high-level policy to dynamically select weights according to the current task. We show that the multi-objective policy encodes a diverse spectrum of behaviors, facilitating efficient adaptation to novel tasks.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "5c119ba8e1",
    "authors": "Huanyu Chen, Jiahao Wen, Jernej Barbič",
    "title": "ANIME-Rod: Adjustable Nonlinear Isotropic Materials for Elastic Rods",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731208",
    "pdf_link": null,
    "abstract": "We give a method to simulate large deformations of 3D elastic rods under arbitrary nonlinear isotropic 3D solid materials. Rod elastic energies in existing graphics literature are derived from volumetric models under the small-strain linearization assumptions. While the resulting equations can and are commonly applied to large deformations, the material modeling has been limited to a single material, namely linear Hooke law. Starting from any 3D solid nonlinear isotropic elastic energy density function ψ, we derive our rod elastic energy by subjecting the 3D solid volumetric material to the limit process whereby rod thickness is decreased to zero. This enables us to explain rod stretching, bending and twisting in a unified model. Care must be taken to adequately model cross-sectional in-plane and out-of-plane deformations. Our key insight is to compute the three cross-sectional deformation modes corresponding to bending (in the two directions) and twisting, using linear theory. Then, given any ψ, we use these modes to derive an analytical formula for a 5D \"macroscopic\" large-deformation rod elastic energy function of the local longitudinal stretch, radial scaling, the two bending curvatures and torsion. Our model matches linear theory for small deformations, including cross-sectional shrinkage due to Poisson's effect, and produces correct bending and torsional constants. Our experiments demonstrate that our energy closely matches volumetric FEM even under large stretches and curvatures, whereas commonly used methods in graphics deviate from it. We also compare to closely related work from mechanics literature; we give an explicit expansion of all energy terms in terms of the rod cross-section diameter, allowing independent adjustment of stretching, bending and twisting. Finally, we observe an inherent limitation in the ability of rod models to control nonlinear bendability and twistability. We propose to \"relax\" rod physics to more easily control nonlinear bending and twisting in computer graphics applications.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "045521b897",
    "authors": "Navid Ansari, Hans-Peter Seidel, Vahid Babaei",
    "title": "Accelerated Gamut Discovery via Massive Parallelization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730712",
    "pdf_link": null,
    "abstract": "This paper presents a scalable framework for efficiently discovering the performance gamut of different processes. Gamut boundaries comprise the set of highest-performing solutions within a design space. While sampling methods are often inefficient or prone to premature convergence, Bayesian optimization struggles with taking advantage of existing large-scale parallel computation or experimentation. To address these challenges, we utilize Bayesian neural networks as scalable surrogates for performance prediction and uncertainty estimation. We further introduce a novel acquisition function that combines the diversity-driven exploration of stochastic optimization with the information-efficient exploitation of Bayesian optimization. This enables generating large, high-quality batches of samples. Our approach leverages large batch sizes to reduce the number of iterations needed for optimization. We demonstrate its effectiveness on real-world engineering and robotic problems, achieving faster and more extensive discovery of the performance gamut. Code and data are available at https://gitlab.mpi-klsb.mpg.de/nansari/lbn_mobo.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "626c422ebb",
    "authors": "Behrooz Zarebavani, Danny M. Kaufman, David I. W. Levin, Maryam Mehri Dehnavi",
    "title": "Adaptive Algebraic Reuse of Reordering in Cholesky Factorizations with Dynamic Sparsity Patterns",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731179",
    "pdf_link": null,
    "abstract": "We introduce Parth, a fill-reducing ordering method for sparse Cholesky solvers with dynamic sparsity patterns (e.g., in physics simulations with contact or geometry processing with local remeshing). Parth facilitates the selective reuse of fill-reducing orderings when sparsity patterns exhibit temporal coherence, avoiding full symbolic analysis by localizing the effect of dynamic sparsity changes on the ordering vector. We evaluated Parth on over 175,000 linear systems collected from both physics simulations and geometry processing applications, and show that for some of the most challenging physics simulations, it achieves up to 14x reordering runtime speedup, resulting in a 2x speedup in Cholesky solve time—even on top of well-optimized solvers such as Apple Accelerate and Intel MKL.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "1000a5f819",
    "authors": "Bernhard Braun, Jan Bender, Nils Thuerey",
    "title": "Adaptive Phase-Field-FLIP for Very Large Scale Two-Phase Fluid Simulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730854",
    "pdf_link": null,
    "abstract": "Capturing the visually compelling features of large-scale water phenomena, such as the spray clouds of crashing waves, stormy seas, or waterfalls, involves simulating not only the water but also the motion of the air interacting with it. However, current solutions in the visual effects industry still largely rely on single-phase solvers and non-physical \"white-water\" heuristics. To address these limitations, we present Phase-Field-FLIP (PF-FLIP), a hybrid Eulerian/Lagrangian method for the fully physics-based simulation of very large-scale, highly turbulent multiphase flows at high Reynolds numbers and high fluid density contrasts. PF-FLIP transports mass and momentum in a consistent, non-dissipative manner and, unlike most existing multiphase approaches, does not require a surface reconstruction step. Furthermore, we employ spatial adaptivity across all critical components of the simulation algorithm, including the pressure Poisson solver. We augment PF-FLIP with a dual multiresolution scheme that couples an efficient treeless adaptive grid with adaptive particles, along with a fast adaptive Poisson solver tailored for high-density-contrast multiphase flows. Our method enables the simulation of two-phase flow scenarios with a level of physical realism and detail previously unattainable in graphics, supporting billions of particles and adaptive 3D resolutions with thousands of grid cells per dimension on a single workstation.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "86dbe32747",
    "authors": "Jiaqi Wu, Kun Xu",
    "title": "Adding Regional Control for Continuous Remeshing via Attention Flows",
    "paper_url": "https://dl.acm.org/doi/full/10.1145/3721250.3743017",
    "pdf_link": null,
    "abstract": "2 MethodFigure 1 illustrates the systematic workflow of our approach. The pre-building phase employs a continuous remeshing framework [Palfinger 2022] to generate an initial coarse mesh topology through multi-view image integration. Subsequently, we integrate attention mechanisms into the mesh structure by embedding attention values at each vertex, represented by vertex color (dark regions indicate enhanced focus, shown in Figure 2 (a)), which guide subsequent refinement. During attention-based remeshing process, attention cues directly modulate edge splitting operations through adaptive length thresholds, while our proposed attention flow mechanism propagates saliency information across neighboring vertices.",
    "scholar_publication": "Proceedings of the Special Interest Group on Computer …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "00323ffca6",
    "authors": "Kemeng Huang, Xinyu Lu, Huancheng Lin, Taku Komura, Minchen Li, Kemeng Huang",
    "title": "Advancing GPU IPC for Stiff Affine-Deformable Simulation",
    "paper_url": "https://arxiv.org/abs/2411.06224",
    "pdf_link": null,
    "abstract": "Incremental Potential Contact (IPC) is a widely used, robust, and accurate method for simulating complex frictional contact behaviors. However, achieving high efficiency remains a major challenge, particularly as material stiffness increases, which leads to slower Preconditioned Conjugate Gradient (PCG) convergence, even with the state-of-the-art preconditioners. In this paper, we propose a fully GPU-optimized IPC simulation framework capable of handling materials across a wide range of stiffnesses, delivering consistent high …",
    "scholar_publication": "arXiv preprint arXiv:2411.06224, 2024 - arxiv.org"
  },
  {
    "paper_id": "05147db5fe",
    "authors": "Haruki Kato, NAOKI HASHIMOTO",
    "title": "Aerial 3D Display With Ultra-Wide Viewing Zone Using Polarization Characteristics of LCDs",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742990",
    "pdf_link": null,
    "abstract": "We propose a naked-eye stereoscopic display with an ultra-wide viewing zone by applying the display principle of general LCDs. By replacing the polarizer of an LCD with a reflective polarizer and arranging them three-dimensionally, this technology refracts light rays freely and enables an expansion of the viewing zone. In this study, we created a prototype and confirmed that the viewing zone expanded.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "faaa6f2738",
    "authors": "Mingfeng Tang, Ningna Wang, Ziyuan Xie, Jianwei Hu, Ke Xie, Xiaohu Guo, Hui Huang",
    "title": "Aerial Path Online Planning for Urban Scene Updation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730639",
    "pdf_link": null,
    "abstract": "We present the first scene-update aerial path planning algorithm specifically designed for detecting and updating change areas in urban environments. While existing methods for large-scale 3D urban scene reconstruction focus on achieving high accuracy and completeness, they are inefficient for scenarios requiring periodic updates, as they often re-explore and reconstruct entire scenes, wasting significant time and resources on unchanged areas. To address this limitation, our method leverages prior reconstructions and change probability statistics to guide UAVs in detecting and focusing on areas likely to have changed. Our approach introduces a novel changeability heuristic to evaluate the likelihood of scene changes, driving the planning of two flight paths: a prior path informed by static priors and a dynamic real-time path that adapts to newly detected changes. Extensive experiments on real-world urban datasets demonstrate that our method significantly reduces flight time and computational overhead while maintaining high-quality updates comparable to full-scene re-exploration and reconstruction. These contributions pave the way for efficient, scalable, and adaptive UAV-based scene updates in complex urban environments.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "74b7deceff",
    "authors": "Yuqing Zhang, Hao Xu, Yiqian Wu, Sirui Chen, Sirui Lin, Xiang Li, Xifeng Gao, Xiaogang Jin",
    "title": "AlignTex: Pixel-Precise Texture Generation from Multi-view Artwork",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731158",
    "pdf_link": null,
    "abstract": "Current 3D asset creation pipelines typically consist of three stages: creating multi-view concept art, producing 3D meshes based on the artwork, and painting textures for the meshes—an often labor-intensive process. Automated texture generation offers significant acceleration, but prior methods, which fine-tune 2D diffusion models with multi-view input images, often fail to preserve pixel-level details. These methods primarily emphasize semantic and subject consistency, which do not meet the requirements of artwork-guided texture workflows. To address this, we present AlignTex, a novel framework for generating high-quality textures from 3D meshes and multi-view artwork, ensuring both appearance detail and geometric consistency. AlignTex operates in two stages: aligned image generation and texture refinement. The core of our approach, AlignNet, resolves complex misalignments by extracting information from both the artwork and the mesh, generating images compatible with orthographic projection while maintaining geometric and visual fidelity. After projecting aligned images into the texture space, further refinement addresses seams and self-occlusion using an inpainting model and a geometry-aware texture dilation method. Experimental results demonstrate that AlignTex outperforms baseline methods in generation quality and efficiency, offering a practical solution to enhance 3D asset creation in gaming and film production.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "dd3cd1aee4",
    "authors": "James McCann",
    "title": "An Infinity Mirror Without Apparent Mirroring",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743015",
    "pdf_link": null,
    "abstract": "An infinity mirror is an optical novelty that uses facing mirrors – at least one of which is partially transparent to allow viewing – to present the appearance of an infinite tunnel of copies of a scene. One limitation of infinity mirrors is that alternate reflections of the scene are – by necessity – reflected, which means one cannot create, e.g., speed tunnel effects where lights chase into or out of the apparent tunnel. I present a prototype infinity mirror that uses light cells to overcome this limitation. These cells have a different appearance when viewed from the front and back, apparently breaking the symmetry between the primary and reflected versions of the scene. The cells are made from a 3D-printed baffle and diffuser and lit with off-the-shelf programmable LED strips, resulting in an overall inexpensive-to-produce design. In this poster I discuss the construction of my prototype infinity mirror, demonstrate some simple speed tunnel effects, and discuss the design trade-offs in my simple light-cell design.",
    "scholar_publication": "Proceedings of the Special Interest Group on Computer …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "3791129549",
    "authors": "Yu Takano, Akinobu Maejima, Shugo Yamaguchi, Shigeo Morishima",
    "title": "Anime Colorization Using Segment Matching With Candidate Colors",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743016",
    "pdf_link": null,
    "abstract": "We propose a new automatic colorization method for anime line drawings using segment matching with a few reference images. To address the limitations of existing segment matching methods in handling large motion gaps or small regions, the authors introduce patch-based few-shot colorization and a color shuffling process to estimate candidate colors for subsequent segment matching. This addresses the nonlinear movements that is unique to anime, and optical flow estimation struggles with. The paper demonstrates that the proposed method improves accuracy compared to the state-of-the-art segment matching method.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "024d5e65da",
    "authors": "Inbar Gat, Sigal Raab, Guy Tevet, Yuval Reshef, Amit Haim Bermano, Daniel Cohen-Or",
    "title": "AnyTop: Character Animation Diffusion with Any Topology",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730621",
    "pdf_link": null,
    "abstract": "Generating motion for arbitrary skeletons is a longstanding challenge in computer graphics, remaining largely unexplored due to the scarcity of diverse datasets and the irregular nature of the data. In this work, we introduce AnyTop, a diffusion model that generates motions for diverse characters with distinct motion dynamics, using only their skeletal structure as input. Our work features a transformer-based denoising network, tailored for arbitrary skeleton learning, integrating topology information into the traditional attention mechanism. Additionally, by incorporating textual joint descriptions into the latent feature representation, AnyTop learns semantic correspondences between joints across diverse skeletons. Our evaluation demonstrates that AnyTop generalizes well, even with as few as three training examples per topology, and can produce motions for unseen skeletons as well. Furthermore, our model’s latent space is highly informative, enabling downstream tasks such as joint correspondence, temporal segmentation, and motion editing. Our webpage, https://anytop2025.github.io/Anytop-page, includes links to videos and code.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "1ee1a9d0d5",
    "authors": "Yufan Deng, Yuhao Zhang, Chen Geng, Shangzhe Wu, Jiajun Wu",
    "title": "Anymate: A Dataset and Baselines for Learning 3D Object Rigging",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730743",
    "pdf_link": null,
    "abstract": "Rigging and skinning are essential steps to create realistic 3D animations, often requiring significant expertise and manual effort. Traditional attempts at automating these processes rely heavily on geometric heuristics and often struggle with objects of complex geometry. Recent data-driven approaches show potential for better generality, but are often constrained by limited training data. We present the Anymate Dataset, a large-scale dataset of 230K 3D assets paired with expert-crafted rigging and skinning information—70 times larger than existing datasets. Using this dataset, we propose a learning-based auto-rigging framework with three sequential modules for joint, connectivity, and skinning weight prediction. We systematically design and experiment with various architectures as baselines for each module and conduct comprehensive evaluations on our dataset to compare their performance. Our models significantly outperform existing methods, providing a foundation for comparing future methods in automated rigging and skinning. Code and dataset can be found at https://anymate3d.github.io/.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "247a05756e",
    "authors": "Naoki Hashimoto, Yuki Inada",
    "title": "Anywhere Door Experience: Projection Mapping for Enhancing Entertainment and Immersion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742967",
    "pdf_link": null,
    "abstract": "In this study, we propose an experience inspired by the Anywhere Door concept, in which users transition between multiple life-sized projected virtual spaces by opening, closing, and passing through a physical door. We demonstrate that this approach not only enhances the entertainment value of the visual experience but also increases the sense of immersion in the destination virtual space.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "2a3a59891a",
    "authors": "Yang Zhou, Tao Huang, Ravi Ramamoorthi, Pradeep Sen, Ling-Qi Yan, Ling-Qi Yan",
    "title": "Appearance-Preserving Scene Aggregation for Level-of-Detail Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3708343",
    "pdf_link": null,
    "abstract": "Creating an appearance-preserving level-of-detail (LoD) representation for arbitrary 3D scenes is a challenging problem. The appearance of a scene is an intricate combination of both geometry and material models and is further complicated by correlation due to the spatial configuration of scene elements. We present a novel volumetric representation for the aggregated appearance of complex scenes and a pipeline for LoD generation and rendering. The core of our representation is the Aggregated Bidirectional Scattering Distribution Function (ABSDF) that summarizes the far-field appearance of all surfaces inside a voxel. We propose a closed-form factorization of the ABSDF that accounts for spatially varying and orientation-varying material parameters. We tackle the challenge of capturing the correlation existing locally within a voxel and globally across different parts of the scene. Our method faithfully reproduces appearance and achieves higher quality than existing scene filtering methods. The memory footprint and rendering cost of our representation are decoupled from the original scene complexity.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "c7325ad9a8",
    "authors": "Pengfei Zhu, Jie Guo, Yifan Liu, Qi Sun, Yanxiang Wang, Keheng Xu, Ligang Liu, Yanwen Guo",
    "title": "Appearance-aware Multi-view SVBRDF Reconstruction via Deep Reinforcement Learning",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730718",
    "pdf_link": null,
    "abstract": "Recent advancements in deep learning have revolutionized the reconstruction of spatially-varying surface reflectance of real-world objects. Many existing methods have successfully recovered high-quality reflectance maps using a remarkably limited number of images captured by a lightweight handheld camera and a flash-like light source. As the samples become sparse, the choice of the sampling set has a significant impact on the results. To determine the best sampling set for each material while ensuring minimal capture costs, we introduce an appearance-aware adaptive sampling method in this paper. We model the sampling process as a sequential decision-making problem, and employ a deep reinforcement learning (DRL) framework to solve it. At each step, an agent (NBVL Planner), after trained on a specially designed dataset, plans the next best view-lighting (NBVL) pair based on the appearance of the material recognized so far. Once stopped, the sequence of the NBVLs constitutes the best sampling set for the material. We show, through extensive experiments on both synthetic materials and real-world cases, that the best sampling set extracted by our method outperforms other sampling sets, especially for challenging materials featuring globally-varying specular reflectance.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "93be9243b0",
    "authors": "Zhanyu Yang, Aryamaan Jain, Guillaume Cordonnier, Marie-Paule Cani, Zhaopeng Wang, Bedrich Benes",
    "title": "Arenite: A Physics-based Sandstone Simulator",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731201",
    "pdf_link": null,
    "abstract": "… A large sandstone structure generated by Arenite. The user defines simple initial … ), and the physics-based simulation generates the results. We introduce Arenite, a novel physics-based …",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "4e82d959ed",
    "authors": "Zicong Peng, Yicheng Zhan, Josef Spjut, Kaan Akşit",
    "title": "Assessing Learned Models for Phase-only Hologram Compression",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742993",
    "pdf_link": null,
    "abstract": "We evaluate the performance of four common learned models utilizing INR and VAE structures for compressing phase-only holograms in holographic displays. The evaluated models include a vanilla MLP, SIREN [Sitzmann et al. ], and FilmSIREN [Chan et al. ], with TAESD [Bohan ] as the representative VAE model. Our experiments reveal that a pretrained image VAE, TAESD, with 2.2M parameters struggles with phase-only hologram compression, revealing the need for task-specific adaptations. Among the INR s, SIREN with 4.9k parameters achieves compression with high quality in the reconstructed 3D images (PSNR = 34.54 dB). These results emphasize the effectiveness of INR s and identify the limitations of pretrained image compression VAE s for hologram compression task.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "217c89f1f6",
    "authors": "Lanjiong Li, Guanhua Zhao, Lingting Zhu, Zeyu Cai, Lequan Yu, Jian Zhang, Zeyu Wang",
    "title": "AssetDropper: Asset Extraction via Diffusion Models with Reward-Driven Optimization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730731",
    "pdf_link": null,
    "abstract": "Recent research on generative models has primarily focused on creating product-ready visual outputs; however, designers often favor access to standardized asset libraries, a domain that has yet to be significantly enhanced by generative capabilities. Although open-world scenes provide ample raw materials for designers, efficiently extracting high-quality, standardized assets remains a challenge. To address this, we introduce AssetDropper, the first generative framework designed to extract any asset from reference images, providing artists with an open-world asset palette. Our model adeptly extracts a front view of selected subjects from input images, effectively handling complex scenarios such as perspective distortion and subject occlusion. We establish a synthetic dataset of more than 200,000 image-subject pairs and a real-world benchmark with thousands more for evaluation, facilitating the exploration of future research in downstream tasks. Furthermore, to ensure precise asset extraction that aligns well with the image prompts, we employ a pre-trained reward model to achieve a closed loop with feedback. We design the reward model to perform an inverse task that pastes the extracted assets back into the reference sources, which assists training with additional consistency and mitigates hallucination. Extensive experiments show that, with the aid of reward-driven optimization, AssetDropper achieves the state-of-the-art results in asset extraction. Our code and dataset are available at https://github.com/Lanjiong-Li/AssetDropper.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "0c411767c0",
    "authors": "Di Zhang, Ligang Liu",
    "title": "Asymptotic analysis and design of linear elastic shell lattice metamaterials",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730888",
    "pdf_link": null,
    "abstract": "We present an asymptotic analysis of shell lattice metamaterials based on Ciarlet's shell theory, introducing a new metric—asymptotic directional stiffness (ADS)—to quantify how the geometry of the middle surface governs the effective stiffness. We prove a convergence theorem that rigorously characterizes ADS and establishes its upper bound, along with necessary and sufficient condition for achieving it. As a key result, our theory provides the first rigorous explanation for the high bulk modulus observed in Triply Periodic Minimal Surfaces (TPMS)-based shell lattices. To optimize ADS on general periodic surfaces, we propose a triangular-mesh-based discretization and shape optimization framework. Numerical experiments validate the theoretical findings and demonstrate the effectiveness of the optimization under various design objectives. Our implementation is available at https://github.com/lavenklau/minisurf.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "5eda7167a1",
    "authors": "Chris Giles, Elie Diaz, Cem Yuksel",
    "title": "Augmented Vertex Block Descent",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731195",
    "pdf_link": null,
    "abstract": "… blocks smashed by a sphere using 4 iterations, taking 3.5 ms (9.8 ms including collision detection) per frame on an NVIDIA RTX 4090 GPU. Vertex Block Descent … it using an augmented …",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "ba9c429335",
    "authors": "Bowen Zheng, Ke Chen, Yuxin Yao, Zijiao Zeng, Xinwei Jiang, He Wang, Joan Lasenby, Xiaogang Jin",
    "title": "AutoKeyframe: Autoregressive Keyframe Generation for Human Motion Synthesis and Editing",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730664",
    "pdf_link": null,
    "abstract": "Keyframing has long been the cornerstone of standard character animation pipelines, offering precise control over detailed postures and dynamics. However, this approach is labor-intensive, necessitating significant manual effort. Automating this process while balancing the trade-off between minimizing manual input and maintaining full motion control has therefore been a central research challenge. In this work, we introduce AutoKeyframe, a novel framework that simultaneously accepts dense and sparse control signals for motion generation by generating keyframes directly. Dense signals govern the overall motion trajectory, while sparse signals define critical key postures at specific timings. This approach substantially reduces manual input requirements while preserving precise control over motion. The generated keyframes can be easily edited to serve as detailed control signals. AutoKeyframe operates by automatically generating keyframes from dense root positions, which can be determined through arc-length parameterization of the trajectory curve. This process is powered by an autoregressive diffusion model, which facilitates keyframe generation and incorporates a skeleton-based gradient guidance technique for sparse spatial constraints and frame editing. Extensive experiments demonstrate the efficacy of AutoKeyframe, achieving high-quality motion synthesis with precise and intuitive control.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "8384705371",
    "authors": "Chengzhu He, Zhendong Wang, Zhaorui Meng, Junfeng Yao, Shihui Guo, Huamin Wang",
    "title": "Automated Task Scheduling for Cloth and Deformable Body Simulations in Heterogeneous Computing Environments",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730625",
    "pdf_link": null,
    "abstract": "The concept of the Internet of Things (IoT) has driven the development of system-on-a-chip (SoC) technology for embedded and mobile systems, which may define the future of next-generation computation. In SoC devices, efficient cloth and deformable body simulations require parallelized, heterogeneous computation across multiple processing units. The key challenge in heterogeneous computation lies in task distribution, which must account for varying inter-task dependencies and communication costs. This paper proposes a novel framework for automated task scheduling to optimize simulation performance by minimizing communication overhead and aligning tasks with the specific strengths of each device. To achieve this, we introduce an efficient scheduling method based on the Heterogeneous Earliest Finish Time (HEFT) algorithm, adapted for hybrid systems. We model simulation tasks—such as those in iterative methods like Jacobi and Gauss-Seidel—as a Directed Acyclic Graph (DAG). To maximize the parallelism of nonlinear Gauss-Seidel simulation tasks, we present an innovative asynchronous Gauss-Seidel method with specialized data synchronization across units. Additionally, we employ task merging and tailored task-sorting strategies for Gauss-Seidel tasks to achieve an optimal balance between convergence and efficiency. We validate the effectiveness of our framework across various simulations, including XPBD, vertex block descent, and second-order stencil descent, using Apple M-series processors with both CPU and GPU cores. By maximizing computational efficiency and reducing processing times, our method achieves superior simulation frame rates compared to approaches that rely on individual devices in isolation. The source code with hybrid Metal/C++ implementation is available at https://github.com/ChengzhuUwU/libAtsSim.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "2ed9e09383",
    "authors": "Maksim Golyadkin, Innokentiy Humonen, Yanis Plevokas, Ekaterina Bureeva, Ekaterina Alexandrova, Ilya Makarov",
    "title": "Automatic Interpretation of Ancient Egyptian Texts for Education and Research",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743025",
    "pdf_link": null,
    "abstract": "We introduce a pipeline for interpreting Ancient Egyptian hieroglyphic texts combining OCR, transliteration, and translation. Designed for the low-resource data, our system improves accessibility for learners and efficiency for researchers. We evaluate its performance on a new diverse dataset reflective of real-world conditions.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "5987dee71c",
    "authors": "Longwen Zhang, Qixuan Zhang, Haoran Jiang, Yinuo Bai, Wei Yang, Lan Xu, Jingyi Yu",
    "title": "BANG: Dividing 3D Assets via Generative Exploded Dynamics",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730840",
    "pdf_link": null,
    "abstract": "3D creation has always been a unique human strength, driven by our ability to deconstruct and reassemble objects using our eyes, mind and hand. However, current 3D design tools struggle to replicate this natural process, requiring considerable artistic expertise and manual labor. This paper introduces BANG, a novel generative approach that bridges 3D generation and reasoning, allowing for intuitive and flexible part-level decomposition of 3D objects. At the heart of BANG is \"Generative Exploded Dynamics\", which creates a smooth sequence of exploded states for an input geometry, progressively separating parts while preserving their geometric and semantic coherence. BANG utilizes a pre-trained large-scale latent diffusion model, fine-tuned for exploded dynamics with a lightweight exploded view adapter, allowing precise control over the decomposition process. It also incorporates a temporal attention module to ensure smooth transitions and consistency across time. BANG enhances control with spatial prompts, such as bounding boxes and surface regions, enabling users to specify which parts to decompose and how. This interaction can be extended with multimodal models like GPT-4, enabling 2D-to-3D manipulations for more intuitive and creative workflows. The capabilities of BANG extend to generating detailed part-level geometry, associating parts with functional descriptions, and facilitating component-aware 3D creation and manufacturing workflows. Additionally, BANG offers applications in 3D printing, where separable parts are generated for easy printing and reassembly. In essence, BANG enables seamless transformation from imaginative concepts to detailed 3D assets, offering a new perspective on creation that resonates with human intuition.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "5b97499b7e",
    "authors": "Omer Dahary, Yehonathan Cohen, Or Patashnik, Kfir Aberman, Daniel Cohen-Or",
    "title": "Be Decisive: Noise-Induced Layouts for Multi-Subject Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730631",
    "pdf_link": null,
    "abstract": "Generating multiple distinct subjects remains a challenge for existing text-to-image diffusion models. Complex prompts often lead to subject leakage, causing inaccuracies in quantities, attributes, and visual features. Preventing leakage among subjects necessitates knowledge of each subject’s spatial location. Recent methods provide these spatial locations via an external layout control. However, enforcing such a prescribed layout often conflicts with the innate layout dictated by the sampled initial noise, leading to misalignment with the model’s prior. In this work, we introduce a new approach that predicts a spatial layout aligned with the prompt, derived from the initial noise, and refines it throughout the denoising process. By relying on this noise-induced layout, we avoid conflicts with externally imposed layouts and better preserve the model’s prior. Our method employs a small neural network to predict and refine the evolving noise-induced layout at each denoising step, ensuring clear boundaries between subjects while maintaining consistency. Experimental results show that this noise-aligned strategy achieves improved text-image alignment and more stable multi-subject generation compared to existing layout-guided techniques, while preserving the rich diversity of the model’s original distribution.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "498983dc4f",
    "authors": "Zhimin Fan, Chen Wang, Yiming Wang, Boxuan Li, Yuxuan Guo, Ling-Qi Yan, Yanwen Guo, Jie Guo",
    "title": "Bernstein Bounds for Caustics",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731145",
    "pdf_link": null,
    "abstract": "… A significant advantage is that the Bernstein coefficients of a polynomial offer valuable … key property of the Bernstein coefficients, which provide conservative bounds for rational functions…",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "18f9ab51c6",
    "authors": "Cody Tucker",
    "title": "Beyond Automation: Fostering Agency Between Humans, Algorithms, and Machines in Computational Design for Digital Fabrication",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742992",
    "pdf_link": null,
    "abstract": "This project explores the role of the designer in digital fabrication workflows as digitization leads to higher levels of design automation. As digital technologies are adopted to streamline design to manufacturing workflows, elements of the creative process can become standardized to improve production efficiency at the cost of designer autonomy and product customization. In order to ensure designers’ agency and increase product variation, the Carrara project presents a collaborative tool utilizing agent-based modeling (ABM) to represent designers, fabrication machines, and algorithms as active co-participants in the design process. This co-participatory workflow enables a generative, scalable product line that takes advantage of digital efficiencies while providing the designer with autonomy and control in the creative process.",
    "scholar_publication": "Proceedings of the Special Interest Group on Computer …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "969a65f9fc",
    "authors": "Yingyu Yang, Xiaohong Jia, Bolun Wang, Jieyin Yang, Shiqing Xin, Dong-Ming Yan",
    "title": "Boolean Operation for CAD Models Using a Hybrid Representation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730908",
    "pdf_link": null,
    "abstract": "Boolean operations for Boundary Representation (B-Rep) models are among the most commonly used functions in Computer Aided Design (CAD) systems. They are also one of the most delicate soft modules, with challenges arising from complex algorithmic flows and efficiency and accuracy issues, especially in extreme cases. Common issues encountered in processing complex models include low efficiency, missing results, and non-watertightness. In this paper, we propose a novel algorithm for efficient and accurate Boolean operations on B-Rep models. This is achieved by establishing a bijective mapping between B-Rep models and the corresponding triangle meshes with controllable approximation error, thus mapping B-Rep Boolean operations to mesh Boolean operations. By using conservative intersection detection on the mesh to locate all surface intersection curves and carefully handling degeneration and topology errors, we ensure that the results are consistently watertight and correct. We demonstrate the superior efficiency of the proposed method using the open-source geometry engine OCCT, the commercial engine ACIS, and the commercial software Rhino as benchmarks.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "cbeb75426d",
    "authors": "Mingi Lee, Dongsu Zhang, Clément Jambon, Young Min Kim",
    "title": "BrepDiff: Single-Stage B-rep Diffusion Model",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730698",
    "pdf_link": null,
    "abstract": "The Boundary Representation (B-rep) is a widely used 3D model representation of most consumer products designed with CAD software. However, its highly irregular and sparse set of relationships poses significant challenges for designing a generative model tailored to B-reps. Existing approaches use multi-stage approaches to satisfy the complex constraints sequentially. As a result, the final geometry cannot incorporate user edits due to the non-deterministic dependencies between cascaded stages. In contrast, we propose BrepDiff, a single-stage diffusion model for B-rep generation. We present a masked UV grid representation consisting of structured point samples from faces, serving as input for a diffusion transformer. By introducing an asynchronous and shifted noise schedule, we improve the training signal, enabling the diffusion model to better capture the distribution of UV grids. The explicitness of our masked UV grid representation enables users to intuitively understand and freely design surface geometry without being constrained by topological validity. The interconnectivity can be derived from the face layout, which is later processed into a valid solid volume during post-processing. Our approach achieves performance on par with state-of-the-art cascaded models while offering complex and diverse manipulations of geometry and topology, such as shape completion, merging, and interpolation.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ac3fc6b707",
    "authors": "Junming Huang, Chi Wang, Letian Li, Changxin Huang, Qiang Dai, Weiwei Xu",
    "title": "BuildingBlock: A Hybrid Approach for Structured Building Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730705",
    "pdf_link": null,
    "abstract": "Three-dimensional building generation is vital for applications in gaming, virtual reality, and digital twins, yet current methods face challenges in producing diverse, structured, and hierarchically coherent buildings. We propose BuildingBlock, a hybrid approach that integrates generative models, procedural content generation (PCG), and large language models (LLMs) to address these limitations. Specifically, our method introduces a two-phase pipeline: the Layout Generation Phase (LGP) and the Building Construction Phase (BCP). LGP reframes box-based layout generation as a point-cloud generation task, utilizing a newly constructed architectural dataset and a Transformer-based diffusion model to create globally consistent layouts. With LLMs, these layouts are extended into rule-based hierarchical designs, seamlessly incorporating component styles and spatial structures. The BCP leverages these layouts to guide PCG, enabling local-customizable, high-quality structured building generation. Experimental results demonstrate BuildingBlock ’s effectiveness in generating diverse and hierarchically structured buildings, achieving state-of-the-art results on multiple benchmarks, and paving the way for scalable and intuitive architectural workflows.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "fa7d53eb1c",
    "authors": "Michele Vidulis, Klara Mundilova, Quentin Becker, Florin Isvoranu, Mark Pauly",
    "title": "C-Tubes: Design and Optimization of Tubular Structures Composed of Developable Strips",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730933",
    "pdf_link": null,
    "abstract": "We introduce C-tubes, 3D tubular structures composed of developable surface strips. C-tubes can be understood as a generalization of Monge surfaces—a special class of sweep surfaces—towards the recently introduced conenets. This observation allows formulating a constructive algorithm to create tubular structures that ensures developability of the constituent surfaces, while significantly broadening the design space. Our novel form-finding tool enables design exploration by solving for the input variables of the constructive algorithm so that the C-tube best conforms to user-specified objectives. We discuss several case studies that illustrate the versatility of our approach for the design and fabrication of complex structures, with applications in architecture, furniture, and lighting design.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "3fa122caf3",
    "authors": "Xiaodi Yuan, Fanbo Xiang, Yin Yang, Hao Su",
    "title": "C5D: Sequential Continuous Convex Collision Detection Using Cone Casting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731151",
    "pdf_link": null,
    "abstract": "In physics-based simulation of rigid or nearly rigid objects, collisions often become the primary performance bottleneck, particularly when enforcing intersection-free constraints. Previous simulation frameworks rely on primitive-level CCD algorithms. Due to the large number of colliding surface primitives to process, those methods are computationally intensive and heavily dependent on advanced parallel computing resources such as GPUs, which are often inaccessible due to competing tasks or capped threading capacity in applications like policy training for robotics. To address these limitations, we propose a sequential CCD algorithm for convex shapes undergoing constant affine motion. This approach uses the conservative advancement method to iteratively refine a lower-bound estimate of the TOI, exploiting the linearity of affine motion and the efficiency of convex shape distance computation. Our CCD algorithm integrates seamlessly into the ABD framework, achieving a 10-fold speed-up over primitive-level CCD. Its high single-threaded efficiency further enables significant throughput improvements via scene-level parallelism, making it well-suited for resource-constrained environments.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "627e66b801",
    "authors": "Kaixin Yao, Longwen Zhang, Xinhao Yan, Yan Zeng, Qixuan Zhang, Jiayuan Gu, Wei Yang, Lan Xu, Jingyi Yu",
    "title": "CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730841",
    "pdf_link": null,
    "abstract": "Recovering high-quality 3D scenes from a single RGB image is a challenging task in computer graphics. Current methods often struggle with domain-specific limitations or low-quality object generation. To address these, we propose CAST (Component-Aligned 3D Scene Reconstruction from a Single RGB Image), a novel method for 3D scene reconstruction. CAST starts by extracting object-level 2D segmentation and relative depth information from the input image, followed by using a GPT-based model to analyze inter …",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "13cc2f85c0",
    "authors": "Michael Liu, Xinlei Wang, Minchen Li",
    "title": "CK-MPM: A Compact-Kernel Material Point Method",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731155",
    "pdf_link": null,
    "abstract": "The Material Point Method (MPM) has become a cornerstone of physics-based simulation, widely used in geomechanics and computer graphics for modeling phenomena such as granular flows, viscoelasticity, fracture mechanics, etc. Despite its versatility, the original MPM suffers from cell-crossing instabilities caused by discontinuities in particle-grid transfer kernels. Existing solutions mostly mitigate these issues by adopting smoother shape functions, but at the cost of increased numerical diffusion and computational overhead due to larger kernel support. In this paper, we propose a novel C2-continuous compact kernel for MPM that achieves a unique balance in terms of stability, accuracy, and computational efficiency. Our method integrates seamlessly with Affine Particle-In-Cell (APIC) and Moving Least Squares (MLS) MPM, while only doubling the number of grid nodes associated with each particle compared to linear kernels. At its core is an innovative dual-grid framework, which associates particles with grid nodes exclusively within the cells they occupy on two staggered grids, ensuring consistent and stable force computations. We demonstrate that our method can be conveniently implemented using a domain-specific language, Taichi, or based on open-source GPU MPM frameworks, achieving faster runtime and less numerical diffusion compared to quadratic B-spline MPM. Comprehensive validation through unit tests, comparative studies, and stress tests demonstrates the efficacy of our approach in conserving both linear and angular momentum, handling stiff materials, and scaling efficiently for large-scale simulations. Our results highlight the transformative potential of compact, high-order kernels in advancing MPM's capabilities for stable, accurate, and high-performance simulations.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "84e6c9b1f7",
    "authors": "Xueqi Ma, Yilin Liu, Tianlong Gao, Qirui Huang, Hui Huang",
    "title": "CLR-Wire: Towards Continuous Latent Representations for 3D Curve Wireframe Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730638",
    "pdf_link": null,
    "abstract": "We introduce CLR-Wire, a novel framework for 3D curve-based wireframe generation that integrates geometry and topology into a unified Continuous Latent Representation. Unlike conventional methods that decouple vertices, edges, and faces, CLR-Wire encodes curves as Neural Parametric Curves along with their topological connectivity into a continuous and fixed-length latent space using an attention-driven variational autoencoder (VAE). This unified approach facilitates joint learning and generation of both geometry and topology. To generate wireframes, we employ a flow matching model to progressively map Gaussian noise to these latents, which are subsequently decoded into complete 3D wireframes. Our method provides fine-grained modeling of complex shapes and irregular topologies, and supports both unconditional generation and generation conditioned on point cloud or image inputs. Experimental results demonstrate that, compared with state-of-the-art generative approaches, our method achieves substantial improvements in accuracy, novelty, and diversity, offering an efficient and comprehensive solution for CAD design, geometric reconstruction, and 3D content creation.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "e590ea8338",
    "authors": "Peng Li, Suizhi Ma, Jialiang Chen, Yuan Liu, Congyi Zhang, Wei Xue, Wenhan Luo, Alla Sheffer, Wenping Wang, Yike Guo",
    "title": "CMD: Controllable Multiview Diffusion for 3D Editing and Progressive Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730722",
    "pdf_link": null,
    "abstract": "Recently, 3D generation methods have shown their powerful ability to automate 3D model creation. However, most 3D generation methods only rely on an input image or a text prompt to generate a 3D model, which lacks the control of each component of the generated 3D model. Any modifications of the input image lead to an entire regeneration of the 3D models. In this paper, we introduce a new method called CMD that generates a 3D model from an input image while enabling flexible local editing of each component of the 3D model. In CMD, we formulate the 3D generation as a conditional multiview diffusion model, which takes the existing or known parts as conditions and generates the edited or added components. This conditional multiview diffusion model not only allows the generation of 3D models part by part but also enables local editing of 3D models according to the local revision of the input image without changing other 3D parts. Extensive experiments are conducted to demonstrate that CMD decomposes a complex 3D generation task into multiple components, improving the generation quality. Meanwhile, CMD enables efficient and flexible local editing of a 3D model by just editing one rendered image.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "08ae3caa31",
    "authors": "Michal Edelstein, Hsueh-Ti Derek Liu, Mirela Ben-Chen",
    "title": "CageNet: A Meta-Framework for Learning on Wild Meshes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730654",
    "pdf_link": null,
    "abstract": "Learning on triangle meshes has recently proven to be instrumental to a myriad of tasks, from shape classification, to segmentation, to deformation and animation, to mention just a few. While some of these applications are tackled through neural network architectures which are tailored to the application at hand, many others use generic frameworks for triangle meshes where the only customization required is the modification of the input features and the loss function. Our goal in this paper is to broaden the applicability of these generic frameworks to “wild” meshes, i.e. meshes in-the-wild which often have multiple components, non-manifold elements, disrupted connectivity, or a combination of these. We propose a configurable meta-framework based on the concept of caged geometry: Given a mesh, a cage is a single component manifold triangle mesh that envelopes it closely. Generalized barycentric coordinates map between functions on the cage, and functions on the mesh, allowing us to learn and test on a variety of data, in different applications. We demonstrate this concept by learning segmentation and skinning weights on difficult data, achieving better performance to state of the art techniques on wild meshes.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "aba1f047ba",
    "authors": "Zhouheng Du, Nima Davari, Li Li, Nodir Kodirov",
    "title": "Capsule: Efficient Player Isolation for Datacenters",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742988",
    "pdf_link": null,
    "abstract": "Cloud gaming is increasingly popular. A challenge for cloud provider is to efficiently operate their datacenters, i.e., keep datacenter utilization high: a non-trivial task due to application variety. Cloud datacenter resources are also diverse, e.g., CPUs, GPUs, NPUs. We propose player-level isolation to address this challenge. We implemented such an isolation mechanism in Open 3D Engine (O3DE) with Capsule. Capsule allows multiple players to efficiently share one GPU. It is efficient because computation can be reused across players. Our evaluations show that Capsule can increase datacenter resource utilization by accommodating up to 2.25 × more players, without degrading player gaming experience. Capsule is also application agnostic. We ran four applications on Capsule-based O3DE with no application changes. Our experiences show that Capsule design can be adopted by other game engines to increase datacenter utilization across cloud providers.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "b89426150b",
    "authors": "Qinghe Wang, Yawen Luo, Xiaoyu Shi, Xu Jia, Huchuan Lu, Tianfan Xue, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai",
    "title": "CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730755",
    "pdf_link": null,
    "abstract": "In this work, we present CineMaster, a novel framework for 3D-aware and controllable text-to-video generation. Our goal is to empower users with comparable controllability as professional film directors: precise placement of objects within the scene, flexible manipulation of both objects and camera in 3D space, and intuitive layout control over the rendered frames. To achieve this, CineMaster operates in two stages. In the first stage, we design an interactive workflow that allows users to intuitively construct 3D-aware control signals by positioning object bounding boxes and defining camera movements within the 3D space. In the second stage, these control signals—comprising rendered depth maps, camera trajectories, and object class labels—serve as the guidance for a text-to-video diffusion model, ensuring to generate the user-intended video content. Furthermore, to overcome the scarcity of in-the-wild datasets with 3D object motion and camera pose annotations, we carefully establish an automated data annotation pipeline that extracts 3D bounding boxes and camera trajectories from large-scale video data. Extensive qualitative and quantitative experiments demonstrate that CineMaster significantly outperforms existing methods and implements prominent 3D-aware text-to-video generation.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ecb94a4ace",
    "authors": "Mengdi Wang, Fan Feng, Junlin Li, Bo Zhu",
    "title": "Cirrus: Adaptive Hybrid Particle-Grid Flow Maps on GPU",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731190",
    "pdf_link": null,
    "abstract": "We propose the adaptive hybrid particle-grid flow map method, a novel flow-map approach that leverages Lagrangian particles to simultaneously transport impulse and guide grid adaptation, introducing a fully adaptive flow map-based fluid simulation framework. The core idea of our method is to maintain flow-map trajectories separately on grid nodes and particles: the grid-based representation tracks long-range flow maps at a coarse spatial resolution, while the particle-based representation tracks both long and short-range flow maps, enhanced by their gradients, at a fine resolution. This hybrid Eulerian-Lagrangian flow-map representation naturally enables adaptivity for both advection and projection steps. We implement this method in Cirrus, a GPU-based fluid simulation framework designed for octree-like adaptive grids enhanced with particle trackers. The efficacy of our system is demonstrated through numerical tests and various simulation examples, achieving up to 512 × 512 × 2048 effective resolution on an RTX 4090 GPU. We achieve a 1.5 to 2× speedup with our GPU optimization over the Particle Flow Map method on the same hardware, while the adaptive grid implementation offers efficiency gains of one to two orders of magnitude by reducing computational resource requirements. The source code has been made publicly available at: https://wang-mengdi.github.io/proj/25-cirrus/.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "70723ebe3e",
    "authors": "Marco Maida, Alberto Crescini, Marco Perronet, Elena Camuffo",
    "title": "Claycode: Stylable and Deformable 2D Scannable Codes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730853",
    "pdf_link": null,
    "abstract": "This paper introduces Claycode, a novel 2D scannable code designed for extensive stylization and deformation. Unlike traditional matrix-based codes (e.g., QR codes), Claycodes encode their message in a tree structure. During the encoding process, bits are mapped into a topology tree, which is then depicted as a nesting of color regions drawn within the boundaries of a target polygon shape. When decoding, Claycodes are extracted and interpreted in real-time from a camera stream. We detail the end-to-end pipeline and show that Claycodes allow for extensive stylization without compromising their functionality. We then empirically demonstrate Claycode's high tolerance to heavy deformations, outperforming traditional 2D scannable codes in scenarios where they typically fail.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "565f159112",
    "authors": "Zhiqi Li, Candong Lin, Duowen Chen, Xinyi Zhou, Shiying Xiong, Bo Zhu",
    "title": "Clebsch Gauge Fluid on Particle Flow Maps",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731194",
    "pdf_link": null,
    "abstract": "We propose a novel gauge fluid solver that evolves Clebsch wave functions on particle flow maps (PFMs). The key insight underlying our work is that particle flow maps exhibit superior performance in transporting point elements—such as Clebsch components—compared to line and surface elements, which were the focus of previous methods relying on impulse and vortex gauge variables for flow maps. Our Clebsch PFM method incorporates three main contributions: a novel gauge transformation enabling accurate transport of wave functions on particle flow maps, an enhanced velocity reconstruction method for coarse grids, and a PFM-based simulation framework designed to better preserve fine-scale flow structures. We validate the Clebsch PFM method through a wide range of benchmark tests and simulation examples, ranging from leapfrogging vortex rings and vortex reconnections to Kelvin-Helmholtz instabilities, demonstrating that our method outperforms its impulse- or vortex-based counterparts on particle flow maps, particularly in preserving and evolving small-scale features.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "e5a2e0b866",
    "authors": "Shibo Liu, Ligang Liu, Xiao-Ming Fu",
    "title": "Closed-form Generalized Winding Numbers of Rational Parametric Curves for Robust Containment Queries",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730886",
    "pdf_link": null,
    "abstract": "We derive closed-form expressions for generalized winding numbers of rational parametric curves for robust containment queries. Given an oriented rational parametric curve and a query point, the generalized winding number can be reformulated to an integral of a rational polynomial. The key to computing the integral lies in using the residue theorem. Then, add up the contributions of each curve to obtain the generalized winding numbers of a set of rational parametric curves. Furthermore, the derivatives of generalized winding numbers are easily derived. Consequently, the expressions for generalized winding numbers are concise and computationally efficient, becoming faster than state-of-the-art methods. Moreover, the computational costs for various query points are almost the same.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "40e75c7cb1",
    "authors": "Taisei Omine, Naoyuki Kawabata, Fuminori Homma",
    "title": "Co-Speech Gesture and Facial Expression Generation for Non-Photorealistic 3D Characters",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742976",
    "pdf_link": null,
    "abstract": "With the advancement of conversational AI, research on bodily expressions, including gestures and facial expressions, has also progressed. However, many existing studies focus on photorealistic avatars, making them unsuitable for non-photorealistic characters, such as those found in anime. This study proposes methods for expressing emotions, including exaggerated expressions unique to non-photorealistic characters, by utilizing expression data extracted from comics and dialogue-specific semantic gestures. A user study demonstrated significant improvements across multiple aspects when compared to existing research.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "0f41fa7df2",
    "authors": "Junhao Zhuang, Lingen Li, Xuan Ju, Zhaoyang Zhang, Chun Yuan, Ying Shan",
    "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730660",
    "pdf_link": null,
    "abstract": "The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control.A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: https://zhuang2002.github.io/Cobra/.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "111b544915",
    "authors": "Jipeng Sun, Kaixuan Wei, Thomas Eboli, Congli Wang, Cheng Zheng, Zhihao Zhou, Arka Majumdar, Wolfgang Heidrich, Felix Heide",
    "title": "Collaborative On-Sensor Array Cameras",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731200",
    "pdf_link": null,
    "abstract": "Modern nanofabrication techniques have enabled us to manipulate the wave-front of light with sub-wavelength-scale structures, offering the potential to replace bulky refractive surfaces in conventional optics with ultrathin metasurfaces. In theory, arrays of nanoposts provide unprecedented control over manipulating the wavefront in terms of phase, polarization, and amplitude at the nanometer resolution. A line of recent work successfully investigates flat computational cameras that replace compound lenses with a single metalens or an array of metasurfaces a few millimeters from the sensor. However, due to the inherent wavelength dependence of metalenses, in practice, these cameras do not match their refractive counterparts in image quality for broadband imaging, and may even suffer from hallucinations when relying on generative reconstruction methods. In this work, we investigate a collaborative array of metasurface elements that are jointly learned to perform broadband imaging. To this end, we learn a nanophotonics array with 100-million nanoposts that is end-to-end jointly optimized over the full visible spectrum—a design task that existing inverse design methods or learning approaches cannot support due to memory and compute limitations. We introduce a distributed meta-optics learning method to tackle this challenge. This allows us to optimize a large parameter array along with a learned metaatom proxy and a non-generative reconstruction method that is parallax-aware and noise-aware. The proposed camera performs favorably in simulation and in all experimental tests irrespective of the scene illumination spectrum.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "2f079c0a6d",
    "authors": "András Simon, Danwu Chen, Philipp Urban, Vincent Duveiller, Henning Lübbe",
    "title": "Color Matching and Biomimicry for Multi-material Dental 3D Printing",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730708",
    "pdf_link": null,
    "abstract": "The growing global demand for removable partial and full dentures, driven by an aging population and the high prevalence of edentulism, emphasizes the importance of advancing manufacturing solutions. Multi-material jetting, with newly regulatory-approved dental resins, facilitates the production of monolithic, full-color dentures, reducing manual labor and enabling advanced aesthetic customization.We propose a practical method for dental layer biomimicry and multi-spot shade matching using multi-material 3D printing. It integrates seamlessly into workflows combining dental CAD tools, which compute the outer shape of dentures, and industrial multi-material slicers.We introduce a morphable star-shape descriptor to embed enamel, dentin, and root layers with controlled thicknesses into outer tooth geometries. To compute per-layer material ratios, we first present a forward model to predict the color of printed layered slabs mimicking inner tooth structures with variable translucencies. The slab design enables reasonable color predictions on layered tooth shapes. Based on the forward model, we propose a backward model to compute per-layer material ratios for given layer-translucencies to achieve color matches at multiple points on the tooth surface.The method’s effectiveness is demonstrated by fabricating various dentures with custom layers that accurately replicate VITA classical shades, showcasing its practical and versatile application in denture manufacturing.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "57f45bdd90",
    "authors": "Hongbo Zhao, Jiaxing Li, Peiyi Zhang, Peng Xiao, Jianxin Lin, Yijun Wang",
    "title": "ColorSurge: Bringing Vibrancy and Efficiency to Automatic Video Colorization via Dual-Branch Fusion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730736",
    "pdf_link": null,
    "abstract": "Automatic video colorization poses challenges, requiring efficient generation of results that ensure frame and color consistency. Previous video colorization works often suffer from issues such as color flickering, bleeding, artifacts, and low color richness due to the inherent ambiguity and limitations of the models. While diffusion-based video-to-video approaches can produce customized colorization models through fine-tuning, their high inference costs limit their suitability for real-time scenarios. To address these challenges, we propose ColorSurge, a lightweight network for efficient end-to-end video colorization. ColorSurge employs a dual-branch structure, consisting of a grayscale branch and a color branch. In the grayscale branch, we extract the semantic content of grayscale videos and reconstruct and output features at different spatial scales. In the color branch, we introduce learnable color tokens and fuse these multi-scale semantic features through stacked Color Alchemy Blocks (CABs). Within each CAB, we incorporate Color Spatial Transformer Blocks (CSTB) and Color Temporal Transformer Blocks (CTTB) to constrain the spatial harmony and temporal consistency of colors. Finally, we use a Color Mapper to unify the grayscale and color features, mapping them to obtain the final colorized video result. Extensive experiments demonstrate that our method significantly outperforms previous state-of-the-art models in both qualitative and quantitative evaluations. Our code and model are available at https://github.com/ABTols/ColorSurge.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "6027ba8fba",
    "authors": "Youngsik Yun, Jeongmin Bae, Hyunseung Son, Seoha Kim, Hahyun Lee, Gun Bang, Youngjung Uh",
    "title": "Compensating Spatiotemporally Inconsistent Observations for Online Dynamic 3D Gaussian Splatting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730678",
    "pdf_link": null,
    "abstract": "Online reconstruction of dynamic scenes is significant as it enables learning scenes from live-streaming video inputs, while existing offline dynamic reconstruction methods rely on recorded video inputs. However, previous online reconstruction approaches have primarily focused on efficiency and rendering quality, overlooking the temporal consistency of their results, which often contain noticeable artifacts in static regions. This paper identifies that errors such as noise in real-world recordings affect temporal inconsistency in online reconstruction. We propose a method that enhances temporal consistency in online reconstruction from observations with temporal inconsistency which is inevitable in cameras. We show that our method restores the ideal observation by subtracting the learned error. We demonstrate that applying our method to various baselines significantly enhances both temporal consistency and rendering quality across datasets. Code, video results, and checkpoints are available at https://bbangsik13.github.io/OR2.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "fa0ae82628",
    "authors": "Aviv Segall, Jing Ren, Martin Schwarz, Olga Sorkine-Hornung",
    "title": "Computational Modeling of Gothic Microarchitecture",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730649",
    "pdf_link": null,
    "abstract": "Gothic microarchitecture is a design phenomenon widely observed in late medieval European art, comprising sculptural works that emulate the forms and structural composition of monumental Gothic architecture. Despite its prevalence in preserved artifacts, the design and construction methods of Gothic microarchitecture used by artisans remain a mystery, as these processes were orally transmitted and rarely documented. The Basel goldsmith drawings (“Basler Goldschmiedrisse”), a collection of over 200 late 15th-century design drawings from the Upper Rhine region, provide a rare glimpse into the workshop practices of Gothic artisans. This collection consists of unpaired 2D drawings, including top-view and side-view projections of Gothic microarchitecture, featuring nested curve networks without annotations or explicitly articulated design principles. Understanding these 2D drawings and reconstructing the 3D objects they represent has long posed a significant challenge due to the lack of documentation and the complexity of the designs. In this work, we propose a framework of simple yet expressive geometric principles to model Gothic microarchitecture as 3D curve networks, using limited input such as historical 2D drawings. Our approach formalizes a historically informed design space, constrained to tools traditionally available to artisans–namely compass and straightedge–and enables faithful reproduction of Gothic microarchitecture that conforms to physical artifacts. Our framework is intuitive and efficient, allowing users to interactively create 3D Gothic microarchitecture with minimal effort. It bridges the gap between historical artistry and modern computational design, while also shedding light on a lost chapter of Gothic craftsmanship.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "55cf9ee60d",
    "authors": "Yuexiang Ji, Akinobu Maejima, Yotam Sechayk, Yuki Koyama, Takeo Igarashi",
    "title": "Confidence Estimation of Few-Shot Patch-Based Learning for Anime-Style Colorization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742964",
    "pdf_link": null,
    "abstract": "In hand-drawn anime production, automatic colorization is used to boost productivity, where line drawings are automatically colored based on reference frames. However, the results sometimes include wrong color estimations, requiring artists to carefully inspect each region and correct colors—a time-consuming and labor-intensive task. To support this process, we propose a confidence estimation method that indicates the confidence level of colorization for each region of the image. Our method compares local patches in the colorized result and the reference frame.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "40e73e9266",
    "authors": "Paul Himmler, Tobias Günther",
    "title": "Conformal First Passage for Epsilon-free Walk-on-Spheres",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730942",
    "pdf_link": null,
    "abstract": "In recent years, grid-free Monte Carlo methods have gained increasing popularity for solving fundamental partial differential equations. For a given point in the domain, the Walk-on-Spheres method solves a boundary integral equation by integrating recursively over the largest possible sphere. When the walks approach boundaries with Dirichlet conditions, the number of path vertices increases considerably, since the step size becomes smaller with decreasing distance to the boundary. In practice, the walks are terminated once they reach an epsilon-shell around the boundary. This, however, introduces bias, leading to a trade-off between accuracy and performance. Instead of using spheres, we propose to utilize geometric primitives that share more than one point with the boundary to increase the likelihood of immediately terminating. Along the boundary of those new geometric primitives a sampling probability is needed, which corresponds to the exit probability of a Brownian motion. This is known as a first passage problem. Utilizing that Laplace equations are invariant under conformal maps, we transform exit points from unit circles to the exit points of our geometric primitives, for which we describe a suitable placement strategy. With this, we obtain a novel approach to solve the Laplace equation in two dimensions, which does not require an epsilon-shell, significantly reduces the number of path vertices, and reduces inaccuracies near Dirichlet boundaries.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "bb5578d50f",
    "authors": "Yijie Liu, Taiyuan Zhang, Xiaoxiao Yan, Nuoming Liu, Bo Ren",
    "title": "Controllable Complex Freezing Dynamics Simulation on Thin Films",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731170",
    "pdf_link": null,
    "abstract": "The freezing of thin films is a mesmerizing natural phenomenon, inspiring photographers to capture its beauty through their lenses and digital artists to recreate its allure using effects tools. In this paper, we present a novel method for physically simulating the intricate freezing dynamics on thin films. By accounting for the influence of phase and temperature changes on surface tension, our method reproduces Marangoni freezing and the \"Snow-Globe Effect\", characterized by swirling ice dendrites on the film. We introduce a novel Phase Map method on top of the state-of-the-art Moving Eulerian-Lagrangian Particles (MELP) meshless framework, enabling dendritic crystal simulation on mobile particles and offering precise control over freezing patterns. We demonstrate that our method is able to capture a wide range of dynamic freezing processes of soap bubbles and is stable for complex boundaries in our experiments.",
    "scholar_publication": "ACM Transactions on Graphics …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "a894607141",
    "authors": "Karlis Martins Briedis, Abdelaziz Djelouah, Raphaël Ortiz, Markus Gross, Christopher Schroers",
    "title": "Controllable Tracking-Based Video Frame Interpolation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730598",
    "pdf_link": null,
    "abstract": "Temporal video frame interpolation has been an active area of research in recent years, with a primary focus on motion estimation, compensation, and synthesis of the final frame. While recent methods have shown good quality results in many cases, they can still fail in challenging scenarios. Moreover, they typically produce fixed outputs with no means of control, further limiting their application in film production pipelines. In this work, we address the less explored problem of user-assisted frame interpolation to improve quality and enable control over the appearance and motion of interpolated frames. To this end, we introduce a tracking-based video frame interpolation method that utilizes sparse point tracks, first estimated and interpolated with existing point tracking methods and then optionally refined by the user. Additionally, we propose a mechanism for controlling the levels of hallucination in interpolated frames through inference-time model weight adaptation, allowing a continuous trade-off between hallucination and blurriness.Even without any user input, our model achieves state-of-the-art results in challenging test cases. By using points tracked over the whole sequence, we can use better motion trajectory interpolation methods, such as cubic splines, to more accurately represent the true motion and achieve significant improvements in results. Our experiments demonstrate that refining tracks and their trajectories through user interactions significantly improves the quality of interpolated frames.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "8ee6ff245c",
    "authors": "Amirhossein Alimohammadi, Aryan Mikaeili, Sauradip Nag, Negar Hassanpour, Andrea Tagliasacchi, Ali Mahdavi-Amiri",
    "title": "Cora: Correspondence-aware image editing using few step diffusion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730650",
    "pdf_link": null,
    "abstract": "Image editing is an important task in computer graphics, vision, and VFX, with recent diffusion-based methods achieving fast and high-quality results. However, edits requiring significant structural changes, such as non-rigid deformations, object modifications, or content generation, remain challenging. Existing few step editing approaches produce artifacts such as irrelevant texture or struggle to preserve key attributes of the source image (e.g., pose). We introduce Cora , a novel editing framework that addresses these limitations by introducing correspondence-aware noise correction and interpolated attention maps. Our method aligns textures and structures between the source and target images through semantic correspondence, enabling accurate texture transfer while generating new content when necessary. Cora offers control over the balance between content generation and preservation. Extensive experiments demonstrate that, quantitatively and qualitatively, Cora excels in maintaining structure, textures, and identity across diverse edits, including pose changes, object addition, and texture refinements. User studies confirm that Cora delivers superior results, outperforming alternatives.Project Page & source code: cora-edit.github.io",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "e618b5f348",
    "authors": "Qingqin Hua, Pascal Grittmann, Philipp Slusallek",
    "title": "Correct your balance heuristic: Optimizing balance-style multiple importance sampling weights",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730819",
    "pdf_link": null,
    "abstract": "Multiple importance sampling (MIS) is a vital component of most rendering algorithms. MIS computes a weighted sum of samples from many different techniques to achieve generalization, that is, to handle a wide range of scene types and lighting effects. A key factor to the performance of MIS is the choice of weighting function. The go-to default - the balance heuristic - performs well in many cases, but prior work has shown that it can yield unsatisfactory results. A number of challenges cause this suboptimal performance, including low-variance techniques, sample correlation, and unknown sampling densities. Prior work has suggested improvements for some of these problems, but a general optimal solution has yet to be found. We propose a general and practical weight correction scheme: We optimize, on-the-fly, a set of correction factors that are multiplied into any baseline MIS heuristic (e.g., balance or power). We demonstrate that this approach yields consistently better equal-time performance on two rendering applications: bidirectional algorithms and resampled importance sampling for direct illumination.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "14ce595b84",
    "authors": "Wenbin Song, Heng Zhang, Yang Wang, Xiaopei Liu",
    "title": "Creating Fluid-Interactive Virtual Agents by an Efficient Simulator with Local-domain Control",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730848",
    "pdf_link": null,
    "abstract": "In the realm of digital twin systems, establishing simulation environments for creating and testing virtual agents has garnered substantial attention across various applications. The obtained control policies endow virtual agents with more realistic behaviors and interactive capabilities, finding applications in both computer animation and robotic control. While rigid-body simulators are widely used for virtual agents, achieving similar feats in fluid environments presents formidable challenges due to high complexity and exorbitant costs. One major reason is that most fluid simulators feature a fixed domain, which struggles to enable agents to freely navigate in an unbounded, obstacle-filled space, especially when computational resources are limited, thus restricting their wide utility for creating virtual agents. In this paper, we introduce a novel fluid-solid interaction simulator grounded in an efficient lattice Boltzmann solver. A key feature of this simulator is a dynamically moving local domain that encircles the agent, offering greater flexibility for obtaining control policy while maintaining efficiency in simulation. Previous methods, which anchored a square moving local domain along with the agent, suffered from severe spurious flows when the agent underwent rapid acceleration especially when the domain had to rotate, such as during a U-turn. This led to inaccurate results and instability. Conversely, we propose a novel domain-tracking method that harnesses optimal control techniques to address this issue. Our approach not only bolsters local-domain simulation stability, but also improves efficiency by employing a slender domain, which broadens the application scope of direct fluid-solid interactions for virtual agents. We validate our method by comparing simulations to physical phenomena and obtaining control policies for various virtual agents to accomplish challenging tasks. This effort culminates in a series of animations that vividly demonstrate the efficacy of the entire framework potentially used in both computer animation and robotics.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "6a3fed5cec",
    "authors": "Sean Memery, Kevin Denamganaï, Jiaxin Zhang, Zehai Tu, Yiwen Guo, Kartic Subr",
    "title": "CueTip: An Interactive and Explainable Physics-aware Pool Assistant",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730742",
    "pdf_link": null,
    "abstract": "We present an interactive and explainable automated coaching assistant called CueTip for a variant of pool/billiards. CueTip’s novelty lies in its combination of three features: a natural-language interface, an ability to perform contextual, physics-aware reasoning, and that its explanations are rooted in a set of predetermined guidelines developed by domain experts. We instrument a physics simulator so that it generates event traces in natural language alongside traditional state traces. Event traces lend themselves to interpretation by language models, which serve as the interface to our assistant. We design and train a neural adaptor that decouples tactical choices made by CueTip from its interactivity and explainability allowing it to be reconfigured to mimic any pool playing agent. Our experiments show that CueTip enables contextual query-based assistance and explanations while maintaining the strength of the agent in terms of win rate (improving it in some situations). The explanations generated by CueTip are physically-aware and grounded in the expert rules and are therefore more reliable.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "f12c82a27f",
    "authors": "Rahul Mitra, Mattéo Couplet, Tongtong Wang, Megan Hoffman, Kui Wu, Edward Chien",
    "title": "Curl Quantization for Automatic Placement of Knit Singularities",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730715",
    "pdf_link": null,
    "abstract": "We develop a method for automatic placement of knit singularities based on curl quantization, extending the knit-planning frameworks of Mitra et al. [; ]. Stripe patterns are generated that closely follow the isolines of an underlying knitting time function, and has course and wale singularities in regions of high curl for the normalized time function gradient and its 90° rotated field, respectively. Singularities are placed in an iterative fashion, and we show that this strategy allows us to easily maintain the structural constraints necessary for machine-knitting, e.g., the helix-free constraint, and to satisfy user constraints such as stripe alignment and singularity placement. Our more performant approach obviates the need for a mixed-integer solve [Mitra et al. ], manual fixing of singularity positions, or the running of a singularity matching procedure in post-processing [Mitra et al. ]. Our global optimization also produces smooth knit graphs that provide quick simulation-free previews of rendered knits without the surface artifacts of competing methods. Furthermore, we extend our method to the popular cut-and-sew garment design paradigm. We validate our method by machine-knitting and rendering yarn-based visualizations of prototypical models in the 3D and cut-and-sew settings.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "d166a78d59",
    "authors": "Zhe Kong, Le Li, Yong Zhang, Feng Gao, Shaoshu Yang, Tao Wang, Kaihao Zhang, Zhuoliang Kang, Xiaoming Wei, Guanying Chen, Wenhan Luo",
    "title": "DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730699",
    "pdf_link": null,
    "abstract": "Real-world video super-resolution (VSR) presents significant challenges due to complex and unpredictable degradations. Although some recent methods utilize image diffusion models for VSR and have shown improved detail generation capabilities, they still struggle to produce temporally consistent frames. We attempt to use Stable Video Diffusion (SVD) combined with ControlNet to address this issue. However, due to the intrinsic image-animation characteristics of SVD, it is challenging to generate fine details using only low-quality videos. To tackle this problem, we propose DAM-VSR, an appearance and motion disentanglement framework for VSR. This framework disentangles VSR into appearance enhancement and motion control problems. Specifically, appearance enhancement is achieved through reference image super-resolution, while motion control is achieved through video ControlNet. This disentanglement fully leverages the generative prior of video diffusion models and the detail generation capabilities of image super-resolution models. Furthermore, equipped with the proposed motion-aligned bidirectional sampling strategy, DAM-VSR can conduct VSR on longer input videos. DAM-VSR achieves state-of-the-art performance on real-world data and AIGC data, demonstrating its powerful detail generation capabilities.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "932b658aab",
    "authors": "KyeongMin Kim, SeungWon Seo, DongHeun Han, HyeongYeop Kang, KyeongMin Kim",
    "title": "DAMO: A Deep Solver for Arbitrary Marker Configuration in Optical Motion Capture",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3695865",
    "pdf_link": null,
    "abstract": "Marker-based optical motion capture (mocap) systems are increasingly utilized for acquiring 3D human motion, offering advantages in capturing the subtle nuances of human movement, style consistency, and ease of obtaining desired motion. Motion data acquisition via mocap typically requires laborious marker labeling and motion reconstruction, recent deep-learning solutions have aimed to automate the process. However, such solutions generally presuppose a fixed marker configuration to reduce learning complexity, thereby limiting flexibility. To overcome the limitation, we introduce DAMO, an end-to-end deep solver, proficiently inferring arbitrary marker configurations and optimizing pose reconstruction. DAMO outperforms state-of-the-art like SOMA and MoCap-Solver in scenarios with significant noise and unknown marker configurations. We expect that DAMO will meet various practical demands such as facilitating dynamic marker configuration adjustments during capture sessions, processing marker clouds irrespective of whether they employ mixed or entirely unknown marker configurations, and allowing custom marker configurations to suit distinct capture scenarios. DAMO code and pretrained models are available at https://github.com/CritBear/damo.",
    "scholar_publication": "ACM Transactions on Graphics, 2024 - dl.acm.org"
  },
  {
    "paper_id": "2939b068af",
    "authors": "Janghyeok Han, Gyujin Sim, Geonung Kim, Hyun-Seung Lee, Kyuha Choi, Youngseok Han, Sunghyun Cho",
    "title": "DC-VSR: Spatially and Temporally Consistent Video Super-Resolution with Video Diffusion Prior",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730719",
    "pdf_link": null,
    "abstract": "Video super-resolution (VSR) aims to reconstruct a high-resolution (HR) video from a low-resolution (LR) counterpart. Achieving successful VSR requires producing realistic HR details and ensuring both spatial and temporal consistency. To restore realistic details, diffusion-based VSR approaches have recently been proposed. However, the inherent randomness of diffusion, combined with their tile-based approach, often leads to spatio-temporal inconsistencies. In this paper, we propose DC-VSR, a novel VSR approach to produce spatially and temporally consistent VSR results with realistic textures. To achieve spatial and temporal consistency, DC-VSR adopts a novel Spatial Attention Propagation (SAP) scheme and a Temporal Attention Propagation (TAP) scheme that propagate information across spatio-temporal tiles based on the self-attention mechanism. To enhance high-frequency details, we also introduce Detail-Suppression Self-Attention Guidance (DSSAG), a novel diffusion guidance scheme. Comprehensive experiments demonstrate that DC-VSR achieves spatially and temporally consistent, high-quality VSR results, outperforming previous approaches.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "f98e60ff30",
    "authors": "Maxine Perroni-Scharf, Zachary Ferguson, Thomas Butruille, Carlos Portela, Mina Konaković Luković",
    "title": "Data-Efficient Discovery of Hyperelastic TPMS Metamaterials with Extreme Energy Dissipation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730759",
    "pdf_link": null,
    "abstract": "Triply periodic minimal surfaces (TPMS) are a class of metamaterials with a variety of applications and well-known primitive morphologies. We present a new method for discovering novel microscale TPMS structures with exceptional energy-dissipation capabilities, achieving double the energy absorption of the best existing TPMS primitive structure. Our approach employs a parametric representation, allowing seamless interpolation between structures and representing a rich TPMS design space. As simulations are intractable for efficiently optimizing microscale hyperelastic structures, we propose a sample-efficient computational strategy for rapid discovery with limited empirical data from 3D-printed and tested samples that ensures high-fidelity results. We achieve this by leveraging a predictive uncertainty-aware Deep Ensembles model to identify which structures to fabricate and test next. We iteratively refine our model through batch Bayesian optimization, selecting structures for fabrication that maximize exploration of the performance space and exploitation of our energy-dissipation objective. Using our method, we produce the first open-source dataset of hyperelastic microscale TPMS structures, including a set of novel structures that demonstrate extreme energy dissipation capabilities, and show several potential applications of these structures.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "144a3754d6",
    "authors": "Jing-En Jiang, Hanxiao Wang, Mingyang Zhao, Dong-Ming Yan, Shuangmin Chen, Shiqing Xin, Changhe Tu, Wenping Wang",
    "title": "DeFillet: Detection and Removal of Fillet Regions in Polygonal CAD Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731166",
    "pdf_link": null,
    "abstract": "Filleting is a fundamental operation in CAD systems, akin to a ball rolling between two adjacent surface patches, resulting in a seamless connection. The reverse process, which we refer to as DeFillet in this paper, is crucial for CAE analysis and secondary design phases. However, it presents significant challenges, particularly when the input data originates from surface reconstruction or discretization processes. Our DeFillet algorithm is inspired by the observation that the rolling-ball center defines an osculating sphere, while the Voronoi diagram of surface samples provides sufficiently many rolling-ball center candidates. By leveraging this insight, we compute a transformation between the Voronoi vertices and the surface samples, enabling the efficient identification of fillet regions. Subsequently, we formulate the reconstruction of sharp features as a quadratic optimization problem. Our method's effectiveness has been validated through extensive testing using self-constructed models and 100 filleted models selected from the Fusion 360 Gallery dataset. The code for this paper is publicly available at https://github.com/xiaowuga/DeFillet.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "8ab9fdf00e",
    "authors": "Fanchao Zhong, Yang Wang, Peng-Shuai Wang, Lin Lu, Haisen Zhao",
    "title": "DeepMill: Neural Accessibility Learning for Subtractive Manufacturing",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730657",
    "pdf_link": null,
    "abstract": "Manufacturability is vital for product design and production, with accessibility being a key element, especially in subtractive manufacturing. Traditional methods for geometric accessibility analysis are time-consuming and struggle with scalability, while existing deep learning approaches in manufacturability analysis often neglect geometric challenges in accessibility and are limited to specific model types. In this paper, we introduce DeepMill, the first neural framework designed to accurately and efficiently predict inaccessible and occlusion regions under varying machining tool parameters, applicable to both CAD and freeform models. To address the challenges posed by cutter collisions and the lack of extensive training datasets, we construct a cutter-aware dual-head octree-based convolutional neural network (O-CNN) and generate an inaccessible and occlusion regions analysis dataset with a variety of cutter sizes for network training. Experiments demonstrate that DeepMill achieves 94.7% accuracy in predicting inaccessible regions and 88.7% accuracy in identifying occlusion regions, with an average processing time of 0.04 seconds for finely-tessellated geometries. Based on the outcomes, DeepMill implicitly captures both local and global geometric features, as well as the complex interactions between cutters and intricate 3D models. Code is publicly available at https://github.com/fanchao98/DeepMill.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "676a6a4678",
    "authors": "Rong Liu, Dylan Sun, Meida Chen, Yue Wang, Andrew Feng",
    "title": "Deformable Beta Splatting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730716",
    "pdf_link": null,
    "abstract": "… To overcome this limitation, we first derive a deformable Beta Kernel function. We then illustrate how this flexible kernel can be applied to both geometric splatting and color encoding, …",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "d0b9037db1",
    "authors": "Weitao You, Yinyu Lu, Zirui Ma, Nan Li, Mingxu Zhou, Xue Zhao, Pei Chen, Lingyun Sun",
    "title": "DesignManager: An Agent-Powered Copilot for Designers to Integrate AI Design Tools into Creative Workflows",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730919",
    "pdf_link": null,
    "abstract": "Creative design is an inherently complex and iterative process characterized by continuous exploration, evaluation, and refinement. While recent advances in generative AI have demonstrated remarkable potential in supporting specific design tasks, there remains a critical gap in understanding how these technologies can enhance the holistic design process rather than just isolated stages. This paper introduces DesignManager, a novel AI-powered design support system that aims to transform how designers collaborate with AI throughout their creative workflow. Through a formative study examining designers' current practices with generative AI, we identified key challenges and opportunities in integrating AI into the creative design process. Based on these insights, we developed DesignManager as an interactive copilot system that provides node-based visualization of design evolution, enabling designers to track, modify, and branch their design processes while maintaining meaningful dialogue-based collaboration. The system offers two collaboration modes: DesignManager-guiding and Designer-guiding. Designers can engage in conversational interactions with the DesignManager to obtain design inspiration and tool recommendations, and proactively advance the design progress. The system employs an agent framework to manage decoupled contextual information emerged during the design process, facilitating deep understanding of designers' needs and providing context-aware assistance. Our technical evaluation validated the effectiveness of context decoupling and the use of agent framework, while the open-ended user study with experts demonstrated that DesignManager successfully supports intuitive intention expression, flexible process control, and deeper creative articulation. This work contributes to the understanding of how AI can evolve from task-specific tools to collaborative partners in creative design processes.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "96c98bbf11",
    "authors": "Haikuan Zhu, Hongbo Li, Hsueh-Ti Derek Liu, Wenping Wang, Jing Hua, Zichun Zhong",
    "title": "Designing 3D Anisotropic Frame Fields with Odeco Tensors",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731181",
    "pdf_link": null,
    "abstract": "This paper introduces a method to synthesize a 3D tensor field within a constrained geometric domain represented as a tetrahedral mesh. Whereas previous techniques optimize for isotropic fields, we focus on anisotropic tensor fields that are smooth and aligned with the domain boundary or user guidance. The key ingredient of our method is a novel computational design framework, built on top of the symmetric orthogonally decomposable (odeco) tensor representation, to optimize the stretching ratios and orientations for each tensor in the domain. In contrast to past techniques designed only for isotropic tensors, we demonstrate the efficacy of our approach in generating smooth volumetric tensor fields with high anisotropy and shape conformity, especially for the domain with complex shapes. We apply these anisotropic tensor fields to various applications, such as anisotropic meshing, structural mechanics, and fabrication.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "0133d875dc",
    "authors": "Shunsuke Hirata, Yuta Noma, Koya Narumi, Yoshihiro Kawahara",
    "title": "Designing Balancing Toys Through Mass and Shape Optimization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743002",
    "pdf_link": null,
    "abstract": "We propose an algorithm to interactively design freeform balancing toys that stably balance on a single point of contact. We achieve this by positioning the center of mass outside the model’s surface while deforming the external surface. Our approach relies on a simple energy function that is fast to evaluate and optimize, allowing an interactive design process. The results confirm the feasibility of creating stable balancing toys via standard 3D printing, expanding the possibilities for mechanical design.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "959b1e9057",
    "authors": "Hewen Xiao, Xiuping Liu, Hang Zhao, Jian Liu, Kai Xu",
    "title": "Designing Pin-pression Gripper and Learning its Dexterous Grasping with Online In-hand Adjustment",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730880",
    "pdf_link": null,
    "abstract": "We introduce a novel design of parallel-jaw grippers drawing inspiration from pin-pression toys. The proposed pin-pression gripper features a distinctive mechanism in which each finger integrates a 2D array of pins capable of independent extension and retraction. This unique design allows the gripper to instantaneously customize its finger's shape to conform to the object being grasped by dynamically adjusting the extension/retraction of the pins. In addition, the gripper excels in in-hand re-orientation of objects for enhanced grasping stability again via dynamically adjusting the pins. To learn the dynamic grasping skills of pin-pression grippers, we devise a dedicated reinforcement learning algorithm with careful designs of state representation and reward shaping. To achieve a more efficient grasp-while-lift grasping mode, we propose a curriculum learning scheme. Extensive evaluations demonstrate that our design, together with the learned skills, leads to highly flexible and robust grasping with much stronger generality to unseen objects than alternatives. We also highlight encouraging physical results of sim-to-real transfer on a physically manufactured pin-pression gripper, demonstrating the practical significance of our novel gripper design and grasping skill. Demonstration videos for this paper are available at https://github.com/siggraph-pin-pression-gripper/pin-pression-gripper-video.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "570740cc0d",
    "authors": "Ugo Finnendahl, Markus Worchel, Tobias Jüterbock, Daniel Wujecki, Fabian Brinkmann, Stefan Weinzierl, Marc Alexa",
    "title": "Differentiable Geometric Acoustic Path Tracing using Time-Resolved Path Replay Backpropagation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730900",
    "pdf_link": null,
    "abstract": "Differentiable rendering has become a key ingredient in solving challenging inverse problems in computer graphics and vision. Existing systems can simulate and differentiate the spatial propagation of light. We exploit the duality of light transport simulations and geometric acoustics to apply differential rendering techniques to established acoustic simulation methods. The resulting system is capable of simulating sound according to the geometrical acoustics model and computing derivatives of the output energy spectrograms with respect to arbitrary parameters of the scene, including materials, emitters, microphones, and scene geometry. Contrary to current differentiable transient rendering, we can handle arbitrary simulation depths and achieve constant memory and linear execution times by presenting a temporal extension of Path Replay Backpropagation [Vicini et al. 2021]. We verify our model against established simulation software, and demonstrate the capabilities of optimization with gradients at examples of inverse acoustics and optimizing room parameters. This opens up a new field of research for acoustic optimization that could be as impactful for the acoustic community as differentiable rendering was for the graphics community.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "db22d5b4c7",
    "authors": "Xiaoyu Huang, Takara Truong, Yunbo Zhang, Fangzhou Yu, Jean Pierre Sleiman, Jessica Hodgins, Koushil Sreenath, Farbod Farshidian",
    "title": "Diffuse-CLoC: Guided Diffusion for Physics-based Character Look-ahead Control",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731206",
    "pdf_link": null,
    "abstract": "We present Diffuse-CLoC, a guided diffusion framework for physics-based look-ahead control that enables intuitive, steerable, and physically realistic motion generation. While existing kinematics motion generation with diffusion models offer intuitive steering capabilities with inference-time conditioning, they often fail to produce physically viable motions. In contrast, recent diffusion-based control policies have shown promise in generating physically realizable motion sequences, but the lack of kinematics prediction limits their steerability. Diffuse-CLoC addresses these challenges through a key insight: modeling the joint distribution of states and actions within a single diffusion model makes action generation steerable by conditioning it on the predicted states. This approach allows us to leverage established conditioning techniques from kinematic motion generation while producing physically realistic motions. As a result, we achieve planning capabilities without the need for a high-level planner. Our method handles a diverse set of unseen long-horizon downstream tasks through a single pre-trained model, including static and dynamic obstacle avoidance, motion in-betweening, and task-space control. Experimental results show that our method significantly outperforms the traditional hierarchical framework of high-level motion diffusion and low-level tracking.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "d38e823df9",
    "authors": "Weizhou Liu, Jiaze Li, Xuhui Chen, Fei Hou, Shiqing Xin, Xingce Wang, Zhongke Wu, Chen Qian, Ying He, Ying He",
    "title": "Diffusing Winding Gradients (DWG): A Parallel and Scalable Method for 3D Reconstruction from Unoriented Point Clouds",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3727873",
    "pdf_link": null,
    "abstract": "This article presents Diffusing Winding Gradients (DWG) for reconstructing watertight surfaces from unoriented point clouds. Our method exploits the alignment between the gradients of the screened generalized winding number (GWN) field—a robust variant of the standard GWN field—and globally consistent normals to orient points. Starting with an unoriented point cloud, DWG initially assigns a random normal to each point. It computes the corresponding screened GWN field and extracts a level set whose iso-value is the average of GWN values across all input points. The gradients of this level set are then utilized to update the point normals. This cycle of recomputing the screened GWN field and updating point normals is repeated until the screened GWN level sets stabilize and their gradients cease to change. Unlike conventional methods, DWG does not rely on solving linear systems or optimizing objective functions, which simplifies its implementation and enhances its suitability for efficient parallel execution. Experimental results demonstrate that DWG significantly outperforms existing methods in terms of runtime performance. For large-scale models with 10 to 20 million points, our CUDA implementation on an NVIDIA GTX 4090 GPU achieves speeds 30 to 120 times faster than iPSR, the leading sequential method, tested on a high-end PC with an Intel i9 CPU. Furthermore, by employing a screened variant of GWN, DWG demonstrates enhanced robustness against noise and outliers and proves effective for models with thin structures and real-world inputs with overlapping and misaligned scans. For source code and additional results, visit our project webpage: https://dwgtech.github.io/.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "fcbcac2d59",
    "authors": "Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, Wenping Wang, Yuan Liu",
    "title": "Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730607",
    "pdf_link": null,
    "abstract": "Diffusion models have demonstrated impressive performance in generating high-quality videos from text prompts or images. However, precise control over the video generation process—such as camera manipulation or content editing—remains a significant challenge. Existing methods for controlled video generation are typically limited to a single control type, lacking the flexibility to handle diverse control demands. In this paper, we introduce Diffusion as Shader (DaS), a novel approach that supports multiple video control tasks within a unified architecture. Our key insight is that achieving versatile video control necessitates leveraging 3D control signals, as videos are fundamentally 2D renderings of dynamic 3D content. Unlike prior methods limited to 2D control signals, DaS leverages 3D tracking videos as control inputs, making the video diffusion process inherently 3D-aware. This innovation allows DaS to achieve a wide range of video controls by simply manipulating the 3D tracking videos. A further advantage of using 3D tracking videos is their ability to effectively link frames, significantly enhancing the temporal consistency of the generated videos. With just 3 days of fine-tuning on 8 H800 GPUs using less than 10k videos, DaS demonstrates strong control capabilities across diverse tasks, including mesh-to-video generation, camera control, motion transfer, and object manipulation. More results are available in the supplementary materials.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "bb17bfecb9",
    "authors": "Filipe Nascimento, Fabricio S. Sousa, Afonso Paiva",
    "title": "Digital Animation of Powder-Snow Avalanches",
    "paper_url": "https://www.teses.usp.br/teses/disponiveis/55/55134/tde-25062024-163222/en.php",
    "pdf_link": null,
    "abstract": "Physically based animation of fluids such as smoke, water, and fire provides some of the most stunning computer graphics in the entertainment industry. However, several phenomena still need to be fully understood, and their formulations are still the focus of intense research in other fields, such as Physics and Civil Engineering. That is the case of snow avalanches, whose numerical modeling is challenging due to their complex dynamics. The manipulation of such phenomena is new to computer graphics, and few works exist. This project aims to bring such formulations to the field of computer graphics regarding the digital animation of powder-snow avalanches.",
    "scholar_publication": "2024 - teses.usp.br"
  },
  {
    "paper_id": "25d51a405a",
    "authors": "Kuo Zhang, Zhiqi Gao, Shuai Zhang, Mingrui He, Shuochen Zhao, Mengyao Guo",
    "title": "Digitizing Devotion: Virtual Religious Spaces for Cultural Preservation and Transmission",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742966",
    "pdf_link": null,
    "abstract": "\"Digitizing Devotion\" utilizes advanced oblique photography and AI to create immersive virtual reconstructions of sacred spaces, preserving traditional worship practices for the global diaspora while ensuring cultural continuity across generations and geographical boundaries.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "50b40675a7",
    "authors": "Theo Braune, Mark Gillespie, Yiying Tong, Mathieu Desbrun",
    "title": "Discrete Torsion of Connection Forms on Simplicial Meshes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731197",
    "pdf_link": null,
    "abstract": "While discrete (metric) connections have become a staple of n-vector field design and analysis on simplicial meshes, the notion of torsion of a discrete connection has remained unstudied. This is all the more surprising as torsion is a crucial component in the fundamental theorem of Riemannian geometry, which introduces the existence and uniqueness of the Levi-Civita connection induced by the metric. In this paper, we extend the existing geometry processing toolbox by providing torsion control over discrete connections. Our approach consists in first introducing a new discrete Levi-Civita connection for a metric with locally-constant curvature to replace the hinge connection of a triangle mesh whose curvature is concentrated at singularities; from this reference connection, we define the discrete torsion of a connection to be the discrete dual 1-form by which a connection deviates from our discrete Levi-Civita connection. We discuss how the curvature and torsion of a discrete connection can then be controlled and assigned in a manner consistent with the continuous case. We also illustrate our approach through theoretical analysis and practical examples arising in vector and frame design.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "97d34f1d30",
    "authors": "Danzel Serrano, Przemyslaw Musialski",
    "title": "Disentangled Phoneme-Prosody Mapping for Controllable 3D Facial Animation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743011",
    "pdf_link": null,
    "abstract": "We introduce a two-stage pipeline that gives artists fine-grained, input-level control of audio-driven 3D facial animation. Stage 1 learns a latent relative-motion prior from neutral/offset position maps, confining deformations to realistic shapes. Stage 2 projects an explainable phoneme–prosody vector into this space, so visemes and expressions are editable in feature space. Early experiments show preserved lip-sync and natural motion, narrowing the gap between fidelity and control.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "a69aa2ab54",
    "authors": "Chun-Cheng Hsu, Wei-Chen Yen, Jen-Kai Liu, Ping-Hsuan Han",
    "title": "DiversePuppetry: An Immersive Multi-User Puppetry System Based on Asymmetric Interaction",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742973",
    "pdf_link": null,
    "abstract": "This work presents DiversePuppetry, an immersive and asymmetric puppetry interaction system that integrates a virtual reality head-mounted display (VR-HMD), a mixed reality head-mounted display (MR-HMD), and a CAVE Automatic Virtual Environment (VR-CAVE). In this project, traditional Taiwanese Budaixi puppets were digitized and incorporated into diverse forms of immersive experiences. This study explores an interactive and immersive platform for puppetry through multiple modes of control. The findings highlight the potential of asymmetric immersive interaction, offering puppetry culture a novel way of creating a complete and immersive digital experience.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "183bd503b3",
    "authors": "Yuan-Yuan Cheng, Qing Fang, Ligang Liu, Xiao-Ming Fu",
    "title": "Divide-and-Conquer Embedding",
    "paper_url": "http://openaccess.thecvf.com/content_CVPR_2019/html/Sanakoyeu_Divide_and_Conquer_the_Embedding_Space_for_Metric_Learning_CVPR_2019_paper.html",
    "pdf_link": null,
    "abstract": "… We propose an effective and easily adaptive divide and conquer algorithm for deep metric learning. We divide the data into multiple groups (sub-problems) to reduce the complexity and …",
    "scholar_publication": "Proceedings of the …, 2019 - openaccess.thecvf.com"
  },
  {
    "paper_id": "16a4567539",
    "authors": "Jorge Condor, Sébastien Speierer, Lukas Bode, Božič Aljaž, Simon Green, Piotr Didyk, Adrián Jarabo, Jorge Condor",
    "title": "Don’t Splat your Gaussians: Volumetric Ray-Traced Primitives for Modeling and Rendering Scattering and Emissive Media",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3711853",
    "pdf_link": null,
    "abstract": "Efficient scene representations are essential for many computer graphics applications. A general unified representation that can handle both surfaces and volumes simultaneously remains a research challenge. In this work we propose a compact and efficient alternative to existing volumetric representations for rendering such as voxel grids. Inspired by recent methods for scene reconstruction that leverage mixtures of three-dimensional Gaussians to model radiance fields, we formalize and generalize the modeling of scattering and emissive media using mixtures of simple kernel-based volumetric primitives. We introduce closed-form solutions for transmittance and free-flight distance sampling for different kernels and propose several optimizations to use our method efficiently within any off-the-shelf volumetric path tracer. We demonstrate our method in both forward and inverse rendering of complex scattering media. Furthermore, we adapt and showcase our method in radiance field optimization and rendering, providing additional flexibility compared to current state of the art given its ray-tracing formulation. We also introduce the Epanechnikov kernel and demonstrate its potential as an efficient alternative to the traditionally used Gaussian kernel in scene reconstruction tasks. The versatility and physically based nature of our approach allows us to go beyond radiance fields and bring to kernel-based modeling and rendering any path-tracing enabled functionality such as scattering, relighting, and complex camera models.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "4e8016a4ad",
    "authors": "Yansong Qu, Dian Chen, Xinyang Li, Xiaofan Li, Shengchuan Zhang, Liujuan Cao, Rongrong Ji",
    "title": "Drag Your Gaussian: Effective Drag-Based Editing  with Score Distillation for 3D Gaussian Splatting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730600",
    "pdf_link": null,
    "abstract": "Recent advancements in generative models have significantly propelled 3D scene editing. While existing methods excel at text-guided texture modifications for 3D representations like 3D Gaussian Splatting (3DGS), they struggle with geometric transformations (e.g., rotating a character’s head) and lack precise spatial control over edits due to the inherent ambiguity of language-driven guidance. To address these limitations, we introduce DYG, a 3D drag-based editing framework for 3DGS. Users intuitively define editing regions using 3D masks and specify desired transformations through pairs of control points. DYG integrates the implicit triplane representation to establish the geometric scaffold of editing results, effectively overcoming suboptimal editing outcomes caused by the sparsity of 3DGS in the desired editing regions. Additionally, we incorporate a drag-based Latent Diffusion Model through the proposed Drag-SDS loss, enabling flexible, multi-view consistent, and fine-grained editing. Extensive experiments demonstrate that DYG enables effective drag-based editing, outperforming other baselines in terms of editing effect and quality. Additional results are available on our project page: https://quyans.github.io/Drag-Your-Gaussian/.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ac0d66ca04",
    "authors": "Cheng-Chih Tsai, Tse-Yu Pan",
    "title": "DreamCraft: Interactive 3D Scene Creation From Editable Panorama in Virtual Reality",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743026",
    "pdf_link": null,
    "abstract": "Creating interactive 3D scenes often requires technical expertise and significant time, limiting accessibility for non-experts. To address this, we present DreamCraft, a VR system enabling users to intuitively generate and edit interactive 3D environments from panoramas without professional skills. DreamCraft supports panorama generation, interactive object selection, panorama editing, and 3D reconstruction. By combining techniques like 3D Gaussian Splatting (3DGS), object segmentation, and 2D-to-3D conversion, it streamlines immersive scene creation. A user study confirmed its usability, ease of learning, and creative potential, positioning DreamCraft as a step toward accessible 3D content creation.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "da8e7346fa",
    "authors": "Yuanpeng Tu, Xi Chen, Ser-Nam Lim, Hengshuang Zhao",
    "title": "DreamMask: Boosting Open-vocabulary Panoptic Segmentation with Synthetic Data",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730684",
    "pdf_link": null,
    "abstract": "Open-vocabulary panoptic segmentation has received significant attention due to its applicability in the real world.Despite claims of robust generalization, we find that the advancements of previous works are attributed mainly to trained categories, exposing a lack of generalization to novel classes.In this paper, we explore boosting existing models from a data-centric perspective.We propose DreamMask, which systematically explores how to generate training data in the open-vocabulary setting, and how to train the model with both real and synthetic data.For the first part, we propose an automatic data generation pipeline with off-the-shelf models. We propose crucial designs for vocabulary expansion, layout arrangement, data filtering, etc.Equipped with these techniques, our generated data could significantly outperform the manually collected web data.To train the model with generated data, a synthetic-real alignment loss is designed to bridge the representation gap, bringing noticeable improvements across multiple benchmarks.In general, DreamMask significantly simplifies the collection of large-scale training data, serving as a plug-and-play enhancement for existing methods. For instance, when trained on COCO and tested on ADE20K, the model equipped with DreamMask outperforms the previous state-of-the-art by a substantial margin of 2.1% mIoU.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "83a11ab8c0",
    "authors": "Xuan Li, Chang Yu, Wenxin Du, Ying Jiang, Tianyi Xie, Yunuo Chen, Yin Yang, Chenfanfu Jiang",
    "title": "Dress-1-to-3: Single Image to Simulation-Ready 3D Outfit with Diffusion Prior and Differentiable Physics",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731177",
    "pdf_link": null,
    "abstract": "Recent advances in large models have significantly advanced image-to-3D reconstruction. However, the generated models are often fused into a single piece, limiting their applicability in downstream tasks. This paper focuses on 3D garment generation, a key area for applications like virtual try-on with dynamic garment animations, which require garments to be separable and simulation-ready. We introduce Dress-1-to-3, a novel pipeline that reconstructs physics-plausible, simulation-ready separated garments with sewing patterns and humans from an in-the-wild image. Starting with the image, our approach combines a pre-trained image-to-sewing pattern generation model for creating coarse sewing patterns with a pre-trained multi-view diffusion model to produce multi-view images. The sewing pattern is further refined using a differentiable garment simulator based on the generated multi-view images. Versatile experiments demonstrate that our optimization approach substantially enhances the geometric alignment of the reconstructed 3D garments and humans with the input image. Furthermore, by integrating a texture generation module and a human motion generation module, we produce customized physics-plausible and realistic dynamic garment demonstrations. Our project page is https://dress-1-to-3.github.io/.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "686e0cdad3",
    "authors": "Shaohua Mo, Chuankun Zheng, Zihao Lin, Dianbing Xi, Qi Ye, Rui Wang, Hujun Bao, Yuchi Huo",
    "title": "Dual-Band Feature Fusion for Neural Global Illumination with Multi-Frequency Reflections",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730733",
    "pdf_link": null,
    "abstract": "In this paper, we present a novel neural global illumination approach that enables multi-frequency reflections in dynamic scenes. Our method utilizes object-centric, spatial feature grids as the core framework to model rendering effects implicitly. A lightweight scene query, based on single-bounce ray tracing, is then performed on these feature grids to extract principal and secondary features separately. The principal features can capture a wide range of relatively low-frequency global illumination effects, such as diffuse indirect lighting and reflections on rough surfaces. In contrast, the secondary features can provide sparse scene-specific reflection details, typically with much higher frequencies than the final observed radiance. Inspired by the physical processes of light propagation, we introduce a novel dual-band feature fusion module that seamlessly blends these two types of features, generating fused features capable of modeling multi-frequency rendering effects. Additionally, we propose a two-stage training strategy tailored to accommodate the distinct characteristics of each feature type, significantly enhancing the overall quality and reducing artifacts in the rendered results. Experimental results demonstrate that our method delivers high-quality, multi-frequency dynamic reflections, outperforming state-of-the-art baselines, including path tracing with screen-space neural denoising and other neural global illumination methods.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "475ee69ff5",
    "authors": "Weizheng Zhang, Hao Pan, Lin Lu, Xiaowei Duan, Xin Yan, Ruonan Wang, Qiang Du",
    "title": "DualMS: Implicit Dual-Channel Minimal Surface Optimization for Heat Exchanger Design",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730700",
    "pdf_link": null,
    "abstract": "Heat exchangers are critical components in a wide range of engineering applications, from energy systems to chemical processing, where efficient thermal management is essential. The design objectives for heat exchangers include maximizing the heat exchange rate while minimizing the pressure drop, requiring both a large interface area and a smooth internal structure. State-of-the-art designs, such as triply periodic minimal surfaces (TPMS), have proven effective in optimizing heat exchange efficiency. However, TPMS designs are constrained by predefined mathematical equations, limiting their adaptability to freeform boundary shapes. Additionally, TPMS structures do not inherently control flow directions, which can lead to flow stagnation and undesirable pressure drops.This paper presents DualMS, a novel computational framework for optimizing dual-channel minimal surfaces specifically for heat exchanger designs in freeform shapes. To the best of our knowledge, this is the first attempt to directly optimize minimal surfaces for two-fluid heat exchangers, rather than relying on TPMS. Our approach formulates the heat exchange maximization problem as a constrained connected maximum cut problem on a graph, with flow constraints guiding the optimization process. To address undesirable pressure drops, we model the minimal surface as a classification boundary separating the two fluids, incorporating an additional regularization term for area minimization. We employ a neural network that maps spatial points to binary flow types, enabling it to classify flow skeletons and automatically determine the surface boundary. DualMS demonstrates greater flexibility in surface topology compared to TPMS and achieves superior thermal performance, with lower pressure drops while maintaining a similar heat exchange rate under the same material cost. The project is open-sourced at https://github.com/weizheng-zhang/DualMS.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "345836c868",
    "authors": "Anindita Ghosh, Bing Zhou, Rishabh Dabral, Jian Wang, Vladislav Golyanik, Christian Theobalt, Philipp Slusallek, Chuan Guo",
    "title": "DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730741",
    "pdf_link": null,
    "abstract": "We present DuetGen, a novel framework for generating interactive two-person dances from music. The key challenge of this task lies in the inherent complexities of two-person dance interactions, where the partners need to synchronize both with each other and with the music. Inspired by the recent advances in motion synthesis, we propose a two-stage solution: encoding two-person motions into discrete tokens and then generating these tokens from music. To effectively capture intricate interactions, we represent both dancers’ motions as a unified whole to learn the necessary motion tokens, and adopt a coarse-to-fine learning strategy in both the stages. Our first stage utilizes a VQ-VAE that hierarchically separates high-level semantic features at a coarse temporal resolution from low-level details at a finer resolution, producing two discrete token sequences at different abstraction levels. Subsequently, in the second stage, two generative masked transformers learn to map music signals to these dance tokens: the first producing high-level semantic tokens, and the second, conditioned on music and these semantic tokens, producing the low-level tokens. We train both transformers to learn to predict randomly masked tokens within the sequence, enabling them to iteratively generate motion tokens by filling an empty token sequence during inference. Through the hierarchical masked modeling and dedicated interaction representation, DuetGen achieves the generation of synchronized and interactive two-person dances across various genres. Extensive experiments and user studies on a benchmark duet dance dataset demonstrate state-of-the-art performance of DuetGen in motion realism, music-dance alignment, and partner coordination. Code and model weights are available at https://github.com/anindita127/DuetGen.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "d6538d5d49",
    "authors": "Zhonghe Ruan, Junwei Liu, Min Fan, Haiyan Li",
    "title": "Dust in Time: Exploring Embodied Experience of Time via an Interactive Installation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743021",
    "pdf_link": null,
    "abstract": "This paper introduces Dust in Time, a tangible and embodied art installation that allows the audience to interact with physical hourglasses and virtual particles through embodied gestures and motions. We describe the design concept and technical details of this installation. Through this conceptual tangible interactive installation, we aim to explore how tangible and embodied interaction can be used to represent the concepts of time and its relationship with human beings and promote both explicit and implicit interaction.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "0965aeed1f",
    "authors": "Rameen Abdal, Or Patashnik, Ivan Skorokhodov, Willi Menapace, Aliaksandr Siarohin, Sergey Tulyakov, Daniel Cohen-Or, Kfir Aberman",
    "title": "Dynamic Concepts Personalization from Single Videos",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730644",
    "pdf_link": null,
    "abstract": "Personalizing generative text-to-image models has seen remarkable progress, but extending this personalization to text-to-video models presents unique challenges. Unlike static concepts, personalizing text-to-video models has the potential to capture dynamic concepts – entities defined not only by their appearance but also by their motion.In this paper, we introduce Set-and-Sequence, a novel framework for personalizing Diffusion Transformers (DiTs)–based generative video models with dynamic concepts. Our approach imposes a spatio-temporal weight space within an architecture that does not explicitly separate spatial and temporal features. This is achieved in two key stages. First, we fine-tune Low-Rank Adaptation (LoRA) layers using an unordered set of frames from the video to learn an appearance LoRA basis that represents the appearance, free from temporal interference. In the second stage, with the appearance LoRAs frozen, we augment their coefficients with Motion Residuals and fine-tune them on the full video sequence, capturing motion dynamics. Our Set-and-Sequence framework resulting in a spatio-temporal weight space effectively embeds dynamic concepts into the video model’s output domain, enabling unprecedented editability and compositionality, and setting a new benchmark for personalizing dynamic concepts.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "34b231c3d8",
    "authors": "Ahmed H. Mahmoud, Serban D. Porumbescu, John D. Owens",
    "title": "Dynamic Mesh Processing on the GPU",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731162",
    "pdf_link": null,
    "abstract": "… We present a system for dynamic triangle mesh processing entirely on the GPU. … to mesh connectivity and attributes. By partitioning the mesh into small patches, we process all dynamic …",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ffeae117dd",
    "authors": "Karim Salem, Damien Rohmer, Niranjan Kalyanasundaram, Victor Zordan",
    "title": "Dynamic Skinning: Kinematics-Driven Cartoon Effects for Articulated Characters",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743004",
    "pdf_link": null,
    "abstract": "We propose Dynamic Skinning (DS), an extension of rig skinning which exhibits the appearance of a physical phenomena without the need for simulation. Our approach applies offsets from traditional skinning to produce these effects based on time-delayed, filtered joint motion. We showcase a number of effects including 1) time-varying oscillation and 2) time delay across skeletal bones to produce what we call delayed linear blend skinning (dLBS) directly on skinning computation. Our approach is easy to control by artists with simple input parameters and the method is compatible with standard rigged characters.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "960f9207c5",
    "authors": "Zhiqi Li, Ruicheng Wang, Junlin Li, Duowen Chen, Sinan Wang, Bo Zhu",
    "title": "EDGE: Epsilon-Difference Gradient Evolution for Buffer-Free Flow Maps",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731193",
    "pdf_link": null,
    "abstract": "We propose the Epsilon Difference Gradient Evolution (EDGE) method for accurate flow-map calculation on grids via Hermite interpolation without using velocity buffers. Our key idea is to integrate Gradient Evolution for accurate first-order derivatives and a tetrahedron-based Epsilon Difference scheme to compute higher-order derivatives with reduced memory consumption. EDGE achieves O (1) memory usage, independent of flow map length, while maintaining vorticity preservation comparable to buffer-based methods. We validate our methods across diverse vortical flow scenarios, demonstrating up to 90% backward map memory reduction and significant computational efficiency, broadening the applicability of flow-map methods to large-scale and complex fluid simulations.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "d5bbd39657",
    "authors": "Zhiping Qiu, Yitong Jin, Yuan Wang, Yi Shi, Chao Tan, Chongwu Wang, Xiaobing Li, Feng Yu, Tao Yu, Qionghai Dai",
    "title": "ELGAR: Expressive Cello Performance Motion Generation for Audio Rendition",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730756",
    "pdf_link": null,
    "abstract": "The art of instrument performance stands as a vivid manifestation of human creativity and emotion. Nonetheless, generating instrument performance motions is a highly challenging task, as it requires not only capturing intricate movements but also reconstructing the complex dynamics of the performer-instrument interaction. While existing works primarily focus on modeling partial body motions, we propose Expressive ceLlo performance motion Generation for Audio Rendition (ELGAR), a state-of-the-art diffusion-based framework for whole-body fine-grained instrument performance motion generation solely from audio. To emphasize the interactive nature of the instrument performance, we introduce Hand Interactive Contact Loss (HICL) and Bow Interactive Contact Loss (BICL), which effectively guarantee the authenticity of the interplay. Moreover, to better evaluate whether the generated motions align with the semantic context of the music audio, we design novel metrics specifically for string instrument performance motion generation, including finger-contact distance, bow-string distance, and bowing score. Extensive evaluations and ablation studies are conducted to validate the efficacy of the proposed methods. In addition, we put forward a motion generation dataset SPD-GEN, collated and normalized from the MoCap dataset SPD. As demonstrated, ELGAR has shown great potential in generating instrument performance motions with complicated and fast interactions, which will promote further development in areas such as animation, music education, interactive art creation, etc. Our code and SPD-GEN dataset are available at https://github.com/Qzping/ELGAR.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "3fc546f0ce",
    "authors": "Hendrik Junkawitsch, Guoxing Sun, Heming Zhu, Christian Theobalt, Marc Habermann",
    "title": "EVA: Expressive Virtual Avatars from Multi-view Videos",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730677",
    "pdf_link": null,
    "abstract": "With recent advancements in neural rendering and motion capture algorithms, remarkable progress has been made in photorealistic human avatar modeling, unlocking immense potential for applications in virtual reality, augmented reality, remote communication, and industries such as gaming, film, and medicine. However, existing methods fail to provide complete, faithful, and expressive control over human avatars due to their entangled representation of facial expressions and body movements. In this work, we introduce Expressive Virtual Avatars (EVA), an actor-specific, fully controllable, and expressive human avatar framework that achieves high-fidelity, lifelike renderings in real time while enabling independent control of facial expressions, body movements, and hand gestures. Specifically, our approach designs the human avatar as a two-layer model: an expressive template geometry layer and a 3D Gaussian appearance layer. First, we present an expressive template tracking algorithm that leverages coarse-to-fine optimization to accurately recover body motions, facial expressions, and non-rigid deformation parameters from multi-view videos. Next, we propose a novel decoupled 3D Gaussian appearance model designed to effectively disentangle body and facial appearance. Unlike unified Gaussian estimation approaches, our method employs two specialized and independent modules to model the body and face separately. Experimental results demonstrate that EVA surpasses state-of-the-art methods in terms of rendering quality and expressiveness, validating its effectiveness in creating full-body avatars. This work represents a significant advancement towards fully drivable digital human models, enabling the creation of lifelike digital avatars that faithfully replicate human geometry and appearance.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "1571ee8edf",
    "authors": "Junkai Huang, Saswat Subhajyoti Mallick, Alejandro Amat, Marc Ruiz Olle, Albert Mosella-Montoro, Bernhard Kerbl, Francisco Vicente Carrasco, Fernando De la Torre",
    "title": "Echoes of the Coliseum: Towards 3D Live streaming of Sports Events",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731214",
    "pdf_link": null,
    "abstract": "Human-centered live events have always played a pivotal role in shaping culture and fostering social connections. Traditional 2D live transmissions fail to replicate the immersive quality of physical attendance. Addressing this gap, this paper proposes LiveSplats, a framework towards real-time, photo-realistic 3D reconstructions of live events using high-performance 3D Gaussian Splatting. Our solution capitalizes on strong geometric priors to optimize through distributed processing and load balancing, enabling interactive, freely explorable 3D experiences. By dividing scene reconstruction into actor-centric and environment-specific tasks, we employ hierarchical coarse-to-fine optimization to rapidly and accurately reconstruct human actors based on pose data, refining their geometry and appearance with photometric loss. For static environments, we focus on view-dependent appearance changes, streamlining rendering efficiency and maximizing GPU performance. To facilitate evaluation, we introduce (and distribute) a synthetic benchmark dataset of basketball games, offering high visual fidelity as ground truth. In both our synthetic benchmark and publicly available benchmarks, LiveSplats consistently outperforms existing approaches. The dataset is available at https://humansensinglab.github.io/basket-multiview.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "c5a37b0f48",
    "authors": "Marcelo Sandoval-Castañeda, Bryan Russell, Josef Sivic, Gregory Shakhnarovich, Fabian David Caba Heilbron",
    "title": "EditDuet: A Multi-Agent System for Video Non-Linear Editing",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730761",
    "pdf_link": null,
    "abstract": "Automated tools for video editing and assembly have applications ranging from filmmaking and advertisement to content creation for social media. Previous video editing work has mainly focused on either retrieval or user interfaces, leaving actual editing to the user. In contrast, we propose to automate the core task of video editing, formulating it as sequential decision making process. Ours is a multi-agent approach. We design an Editor agent and a Critic agent. The Editor takes as input a collection of video clips together with natural language instructions and uses tools commonly found in video editing software to produce an edited sequence. On the other hand, the Critic gives natural language feedback to the editor based on the produced sequence or renders it if it is satisfactory. We introduce a learning-based approach for enabling effective communication across specialized agents to address the language-driven video editing task. Finally, we explore an LLM-as-a-judge metric for evaluating the quality of video editing system and compare it with general human preference. We evaluate our system’s output video sequences qualitatively and quantitatively through a user study and find that our system vastly outperforms existing approaches in terms of coverage, time constraint satisfaction, and human preference. Please see our companion supplemental video for qualitative results.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "e951ffe1fb",
    "authors": "Shiva Sinaei, Chuanjun Zheng, Kaan Akşit, Daisuke Iwai",
    "title": "Efficient Proxy Raytracer for Optical Systems Using Implicit Neural Representations",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742994",
    "pdf_link": null,
    "abstract": "Ray tracing is a widely used technique for modeling optical systems, involving sequential surface-by-surface computations which can be computationally intensive. We propose Ray2Ray, a novel method that leverages implicit neural representations to model optical systems with greater efficiency, eliminating the need for surface-by-surface computations in a single pass end-to-end model. Ray2Ray learns the mapping between rays emitted from a given source and their corresponding rays after passing through a given optical system in a physically accurate manner. We train Ray2Ray on nine off-the-shelf optical systems, acheiving positional errors on the order of 1 μm and angular deviations on the order 0.01 degrees in the estimated output rays. Our work highlights the potentials of neural representations as a proxy optical raytracer.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "dcb6c75a03",
    "authors": "Siyuan Shen, Tianjia Shao, Kun Zhou, Chenfanfu Jiang, Sheldon Andrews, Victor Zordan, Yin Yang",
    "title": "Elastic Locomotion with Mixed Second-order Differentiation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730685",
    "pdf_link": null,
    "abstract": "We present a framework of elastic locomotion, which allows users to enliven an elastic body to produce interesting locomotion by prescribing its high-level kinematics. We formulate this problem as an inverse simulation problem and seek the optimal muscle activations to drive the body to complete the desired actions. We employ the interior-point method to model wide-area contacts between the body and the environment with logarithmic barrier penalties. The core of our framework is a mixed second-order differentiation algorithm. By combining both analytic differentiation and numerical differentiation modalities, a general-purpose second-order differentiation scheme is made possible. Specifically, we augment complex-step finite difference (CSFD) with reverse automatic differentiation (AD). We treat AD as a generic function, mapping a computing procedure to its derivative w.r.t. output loss, and promote CSFD along the AD computation. To this end, we carefully implement all the arithmetics used in elastic locomotion, from elementary functions to linear algebra and matrix operation for CSFD promotion. With this novel differentiation tool, elastic locomotion can directly exploit Newton’s method and use its strong second-order convergence to find the needed activations at muscle fibers. This is not possible with existing first-order inverse or differentiable simulation techniques. We showcase a wide range of interesting locomotions of soft bodies and creatures to validate our method.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "1ecd091f5b",
    "authors": "Nuri Ryu, Jiyun Won, Jooeun Son, Minsu Gong, Joo-Haeng Lee, Sunghyun Cho",
    "title": "Elevating 3D Models: High-Quality Texture and Geometry Refinement from a Low-Quality Model",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730701",
    "pdf_link": null,
    "abstract": "High-quality 3D assets are essential for various applications in computer graphics and 3D vision but remain scarce due to significant acquisition costs. To address this shortage, we introduce Elevate3D, a novel framework that transforms readily accessible low-quality 3D assets into higher quality. At the core of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that significantly improves texture quality while preserving the appearance and geometry while fixing its degradations. Furthermore, Elevate3D operates in a view-by-view manner, alternating between texture and geometry refinement. Unlike previous methods that have largely overlooked geometry refinement, our framework leverages geometric cues from images refined with HFS-SDEdit by employing state-of-the-art monocular geometry predictors. This approach ensures detailed and accurate geometry that aligns seamlessly with the enhanced texture. Elevate3D outperforms recent competitors by achieving state-of-the-art quality in 3D model refinement, effectively addressing the scarcity of high-quality open-source 3D assets.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "24602b21a3",
    "authors": "Hyun Jo Jang, Hakki Karaimer, Michael Brown",
    "title": "Emulating Emulsion: A Compact Physically-Based Model for Film Colour",
    "paper_url": "https://dl.acm.org/doi/full/10.1145/3721250.3743014",
    "pdf_link": null,
    "abstract": "2 MethodOur proposed model maps a digital RAW image R digital to an emulated RAW image of the film scan R scan, as a three-stage analytic pipeline (Figure 1):",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "9d1d65e3ec",
    "authors": "Hao Wang, Taogang Hou, Tianhui Liu, Jiaxin Li, Tianmiao Wang, Hao Wang, Taogang Hou",
    "title": "Encoded Marker Clusters for Auto-Labeling in Optical Motion Capture",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3716847",
    "pdf_link": null,
    "abstract": "Marker-based optical motion capture (MoCap) is a vital tool in applications such as virtual production, and movement sciences. However, reconstructing scattered MoCap data into real motion sequences is challenging, and data processing is time-consuming and labor-intensive. Here we propose a novel framework for MoCap auto-labeling and matching. In this framework, we designed novel clusters of reflective markers called auto-labeling encoded marker clusters (AEMCs), including clusters with an explicit header (AEMCs-E) and an implicit header (AEMCs-I). Combining cluster design and coding theory gives each cluster a unique codeword for MoCap auto-labeling and matching. Moreover, we provide a method of mapping and decoding for cluster labeling. The labeling results are only determined by the intrinsic characteristics of the clusters instead of the skeleton structure or posture of the subjects. Compared with commercial software and data-driven methods, our method has better labeling accuracy in heterogeneous targets and unknown marker layouts, which demonstrates the promising application of motion capture in humans, rigid or flexible robots.",
    "scholar_publication": "ACM Transactions on Graphics, 2025 - dl.acm.org"
  },
  {
    "paper_id": "edec5711c6",
    "authors": "Yuou Sun, Bailin Deng, Juyong Zhang, Yuou Sun",
    "title": "End-to-end Surface Optimization for Light Control",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3732284",
    "pdf_link": null,
    "abstract": "Designing a freeform surface to reflect or refract light to achieve a target distribution is a challenging inverse problem. In this article, we propose an end-to-end optimization strategy for an optical surface mesh. Our formulation leverages a novel differentiable rendering model, and is directly driven by the difference between the resulting light distribution and the target distribution. We also enforce geometric constraints related to fabrication requirements, to facilitate CNC milling and polishing of the designed surface. To address the issue of local minima, we formulate a face-based optimal transport problem between the current mesh and the target distribution, which makes effective large changes to the surface shape. The combination of our optimal transport update and rendering-guided optimization produces an optical surface design with a resulting image closely resembling the target, while the geometric constraints in our optimization help to ensure consistency between the rendering model and the final physical results. The effectiveness of our algorithm is demonstrated on a variety of target images using both simulated rendering and physical prototypes.",
    "scholar_publication": "ACM Transactions on Graphics, 2025 - dl.acm.org"
  },
  {
    "paper_id": "b337570fc5",
    "authors": "Crane He Chen, Vladimir Kim",
    "title": "Escher Tile Deformation via Closed-Form Solution",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730681",
    "pdf_link": null,
    "abstract": "We present a real-time deformation method for Escher tiles—interlocking organic forms that seamlessly tessellate the plane following symmetry rules. We formulate the problem as determining a periodic displacement field. The goal is to deform Escher tiles without introducing gaps or overlaps. The resulting displacement field is obtained in closed form by an analytical solution. Our method processes tiles of 17 wallpaper groups across various representations such as images and meshes. Rather than treating tiles as mere boundaries, we consider them as textured shapes, ensuring that both the boundary and interior deform simultaneously. To enable fine-grained artistic input, our interactive tool features a user-controllable adaptive fall-off parameter, allowing precise adjustment of locality and supporting deformations with meaningful semantic control. We demonstrate the effectiveness of our method through various examples, including photo editing and shape sculpting, showing its use in applications such as fabrication and animation.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "46c1ae7ab0",
    "authors": "Erick Menezes, Helena Leal, João Vítor Moura, Victor Araujo, Soraia Raupp Musse",
    "title": "Evaluating Skin Tone Biases in Virtual Human Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743031",
    "pdf_link": null,
    "abstract": "We evaluate skin tone bias in a real-time rendering engine using 80 MetaHumans covering all 10 levels of the Monk Skin Tone (MST) scale. Two color pipelines are compared: MST-RS, which uses standard RGB reference swatches, and MST-CS, based on cheek-sampled RGB values from real photographs. We apply patch-based metrics, median RGB intensity. MST-RS exhibits a smooth, monotonic RGB decline from MST 1 to 10, while MST-CS reveals geometry-sensitive, non-linear variations and gamut compression in darker tones. These differences highlight potential rendering biases and support the need for tone-aware shader validation.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "756767782f",
    "authors": "Sinnie Choi, Delsther James Edralin, Eric Tang, Mark Harmon, Julien Roy",
    "title": "Evaluating the Effectiveness of Configurable Virtual Reality System for Multi-sensory Spatial Audio Training",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743041",
    "pdf_link": null,
    "abstract": "Enhanced Auditory Reality Simulation for Improved Mapping (EARSIM) is a stand-alone, virtual reality (VR) application that procedurally configures a multi-sensory cue system to deliver adaptive auditory localization tasks. In a pilot study, twenty-one participants completed three 40-second sessions with progressively increasing sensory cues. Median localization accuracy decreased monotonically as the number of cues increased, suggesting that the dynamic cue system was effective in modulating task difficulty. These results validate EARSIM as a configurable platform and its potential for future clinical applications in auditory rehabilitation.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "0d9f8f526c",
    "authors": "Tanja Nuijten, Hannes Sturm, Vincent Maurer, Avina Graefe",
    "title": "Exploring AI Frame Interpolation Techniques for Watercolour Animation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742971",
    "pdf_link": null,
    "abstract": "The animated short film Sensual explores a novel workflow for hand-painted watercolor animation, blending traditional artistic methods with AI-based frame interpolation techniques. By combining compositing with the Real-Time Intermediate Flow Estimation (RIFE) image interpolation network, we significantly reduced production time while maintaining the unique hand-painted aesthetic.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "2e6ec318ab",
    "authors": "Yen-Hua Lai, Chieh-Hsin Liu, Yu-Hsiang Weng, Ping-Hsuan Han, Chien-Hsing Chou, Wen-Hsin Chiu",
    "title": "Exploring Distance Management in Immersive Combat Sports Training With Encountered-Type Haptic Feedback",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743005",
    "pdf_link": null,
    "abstract": "Distance management is a crucial component of immersive combat sports training. However, limited research has explored the distance management in virtual reality (VR) combat training. Our preliminary study invited professional boxers to engage with a VR combat training system, aiming to evaluate the effects of encountered-type haptic feedback via tracking system. The results indicate that haptic feedback led to shorter punch distances and a lower movement ratio. However, no significant differences were observed in step count or the average distance to the opponent. These findings suggest that haptic feedback supports more efficient distance management, allowing users to move less while maintaining effective positioning.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "d5929497ff",
    "authors": "Li-En Lai, Chi-Yu Lin, Tse-Yu Pan, Ping-Hsuan Han",
    "title": "Exploring Real-Time Water Surface Simulation for Immersive Virtual Reality Using Marker-Based Tracking",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742995",
    "pdf_link": null,
    "abstract": "Although real-time fluid simulation in virtual environments has been widely explored, existing systems often rely on virtual models and predefined parameters, limiting their ability to capture the complexity of physical water flow. To address this, we propose a marker-based VR system that simulates water surface dynamics by tracking real-world water flow using ArUco markers. The system analyzes floating marker trajectories to generate a FlowMap, which is applied to a virtual water surface in Unity for real-time flow simulation. A controllable circular pool with water-jet units was used to create varying flow conditions, and computer vision techniques converted the data into directional vector fields. The FlowMap is continuously updated and interpolated to reduce visual lag. We implemented the prototype in a VR environment and verified the accuracy of the generated flow patterns. Results demonstrate the potential of this sensor-driven approach for realistic water simulations in immersive VR.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "c36c0d90f8",
    "authors": "Yunji Seo, Young Sun Choi, HyunSeung Son, Youngjung Uh",
    "title": "FLoD: Integrating Flexible Level of Detail into 3D Gaussian Splatting for Customizable Rendering",
    "paper_url": "https://arxiv.org/abs/2408.12894",
    "pdf_link": null,
    "abstract": "3D Gaussian Splatting (3DGS) achieves fast and high-quality renderings by using numerous small Gaussians, which leads to significant memory consumption. This reliance on a large number of Gaussians restricts the application of 3DGS-based models on low-cost devices due to memory limitations. However, simply reducing the number of Gaussians to accommodate devices with less memory capacity leads to inferior quality compared to the quality that can be achieved on high-end hardware. To address this lack of scalability, we propose integrating a Flexible Level of Detail (FLoD) to 3DGS, to allow a scene to be rendered at varying levels of detail according to hardware capabilities. While existing 3DGSs with LoD focus on detailed reconstruction, our method provides reconstructions using a small number of Gaussians for reduced memory requirements, and a larger number of Gaussians for greater detail. Experiments demonstrate our various rendering options with tradeoffs between rendering quality and memory usage, thereby allowing real-time rendering across different memory constraints. Furthermore, we show that our method generalizes to different 3DGS frameworks, indicating its potential for integration into future state-of-the-art developments. Project page: https://3dgs-flod.github.io/flod.github.io/",
    "scholar_publication": "arXiv preprint arXiv:2408.12894, 2024 - arxiv.org"
  },
  {
    "paper_id": "315a5ceba1",
    "authors": "Avinab Saha, Yu-Chih Chen, Jean-Charles Bazin, Christian Häne, Ioannis Katsavounidis, Alexandre Chapiro, Alan Bovik",
    "title": "FaceExpressions-70k: A Dataset of Perceived Expression Differences",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730653",
    "pdf_link": null,
    "abstract": "Facial expressions are key to human communication, conveying emotions and intentions. Given the rising popularity of digital humans and avatars, the ability to accurately represent facial expressions in real time has become an important topic. However, quantifying perceived differences between pairs of expressions is difficult, and no comprehensive subjective datasets are available for testing. This work introduces a new dataset targeting this problem: FaceExpressions-70k. Obtained via crowdsourcing, our dataset contains 70,500 subjective expression comparisons rated by over 1,000 study participants We demonstrate the applicability of the dataset for training perceptual expression difference models and guiding decisions on acceptable latency and sampling rates for facial expressions when driving a face avatar.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "1f938eb50f",
    "authors": "Yuxuan Han, Junfeng Lyu, Kuan Sheng, Minghao Que, Qixuan Zhang, Lan Xu, Feng Xu",
    "title": "Facial Appearance Capture at Home with Patch-Level Reflectance Prior",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730825",
    "pdf_link": null,
    "abstract": "Existing facial appearance capture methods can reconstruct plausible facial reflectance from smartphone-recorded videos. However, the reconstruction quality is still far behind the ones based on studio recordings. This paper fills the gap by developing a novel daily-used solution with a co-located smartphone and flashlight video capture setting in a dim room. To enhance the quality, our key observation is to solve facial reflectance maps within the data distribution of studio-scanned ones. Specifically, we first learn a diffusion prior over the Light Stage scans and then steer it to produce the reflectance map that best matches the captured images. We propose to train the diffusion prior at the patch level to improve generalization ability and training stability, as current Light Stage datasets are in ultra-high resolution but limited in data size. Tailored to this prior, we propose a patch-level posterior sampling technique to sample seamless full-resolution reflectance maps from this patch-level diffusion model. Experiments demonstrate our method closes the quality gap between low-cost and studio recordings by a large margin, opening the door for everyday users to clone themselves to the digital world.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "6d80330137",
    "authors": "Youyang Du, Lu Wang, Beibei Wang",
    "title": "Facial Microscopic Structures Synthesis from a Single Unconstrained Image",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730760",
    "pdf_link": null,
    "abstract": "Obtaining 3D faces with microscopic structures from a single unconstrained image is challenging. The complexities of wrinkles and pores at a microscopic level, coupled with the blurriness of the input image, raise the difficulty. However, the distribution of wrinkles and pores tends to follow a specialized pattern, which can provide a strong prior for synthesizing them. Therefore, a key to microstructure synthesis is a parametric wrinkles and pore model with controllable semantic parameters. Additionally, ensuring differentiability is essential for enabling optimization through gradient descent methods. To this end, we propose a novel framework designed to reconstruct facial micro-wrinkles and pores from naturally captured images efficiently. At the core of our framework is a differentiable representation of wrinkles and pores via a graph neural network (GNN), which can simulate the complex interactions between adjacent wrinkles by multiple graph convolutions. Furthermore, to overcome the problem of inconsistency between the blurry input and clear wrinkles during optimization, we proposed a Direction Distribution Similarity that ensures that the wrinkle-directional features remain consistent. Consequently, our framework can synthesize facial micro-structures from a blurry skin image patch, which is cropped from a natural-captured facial image, in around an average of 2 seconds. Our framework can seamlessly integrate with existing macroscopic facial detail reconstruction methods to enhance their detailed appearance. We showcase this capability on several works, including DECA, HRN, and FaceScape.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "b311442984",
    "authors": "Daniel Scrivener, Daniel Cui, Ellis Coldren, Mazdak Abulnaga, Mikhail Bessmeltsev, Edward Chien",
    "title": "Faraday Cage Estimation of Normals for Point Clouds and Ribbon Sketches",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731212",
    "pdf_link": null,
    "abstract": "We propose a novel method (FaCE) for normal estimation of unoriented point clouds and VR ribbon sketches that leverages a modeling of the Faraday cage effect. Input points, or a sampling of the ribbons, form a conductive cage and shield the interior from external fields. The gradient of the maximum field strength over external field scenarios is used to estimate a normal at each input point or ribbon. The electrostatic effect is modeled with a simple Poisson system, accommodating intuitive user-driven sculpting via the specification of point charges and Faraday cage points. On inputs sampled from clean, watertight meshes, our method achieves comparable normal quality to existing methods tailored for this scenario. On inputs containing interior structures and artifacts, our method produces superior surfacing output when combined with Poisson Surface Reconstruction. In the case of ribbon sketches, our method accommodates sparser ribbon input while maintaining an accurate geometry, allowing for greater flexibility in the artistic process. We demonstrate superior performance to an existing approach for surfacing ribbon sketches in this sparse setting.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "b0ed18cf9f",
    "authors": "Sihui Ji, Yiyang Wang, Xi Chen, Xiaogang Xu, Hao Luo, Hengshuang Zhao",
    "title": "FashionComposer: Compositional Fashion Image Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730663",
    "pdf_link": null,
    "abstract": "We present FashionComposer for compositional fashion image generation. Unlike previous methods, FashionComposer is highly flexible. It takes multi-modal input (i.e., text prompt, parametric human model, garment image, and face image) and supports personalizing the appearance, pose, and figure of the human and assigning multiple garments in one pass. To achieve this, we first develop a universal framework capable of handling diverse input modalities. We construct scaled training data to enhance the model’s robust compositional capabilities. To accommodate multiple reference images (garments and faces) seamlessly, we organize these references in a single image as an “asset library” and employ a reference UNet [Hu et al. ] to extract appearance features. To inject the appearance features into the correct pixels in the generated result, we propose subject-binding attention. It binds the appearance features from different “assets” with the corresponding text features. In this way, the model could understand each asset according to their semantics, supporting arbitrary numbers and types of reference images. As a comprehensive solution, FashionComposer also supports many other applications like human album generation, diverse virtual try-on tasks, etc.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "6e1219b660",
    "authors": "Ziqiu Zeng, Siyuan Luo, Fan Shi, Zhongkai Zhang",
    "title": "Fast But Accurate: A Real-Time Hyperelastic Simulator with Robust Frictional Contact",
    "paper_url": "https://arxiv.org/abs/2503.15078",
    "pdf_link": null,
    "abstract": "We present a GPU-friendly framework for real-time implicit simulation of elastic material in the presence of frictional contacts. The integration of hyperelasticity, non-interpenetration contact, and friction in real-time simulations presents formidable nonlinear and non-smooth problems, which are highly challenging to solve. By incorporating nonlinear complementarity conditions within the local-global framework, we achieve rapid convergence in addressing these challenges. While the structure of local-global methods is …",
    "scholar_publication": "arXiv preprint arXiv:2503.15078, 2025 - arxiv.org"
  },
  {
    "paper_id": "cd3724168d",
    "authors": "Kai Li, Xiaohong Jia, Falai Chen, Kai Li",
    "title": "Fast Determination and Computation of Self-intersections for NURBS Surfaces",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3727620",
    "pdf_link": null,
    "abstract": "Self-intersections of NURBS surfaces are unavoidable during the CAD modeling process, especially in operations such as offset or sweeping. The existence of self-intersections might cause problems in the subsequent simulation and manufacturing process. Therefore, fast detection of self-intersections of NURBS is highly demanded in industrial applications. Self-intersections are essentially singular points on the surface. Although there is a long history of exploring singular points in mathematics community, the fast and robust determination and computation of self-intersections have been a challenging problem in practice. In this article, we construct an algebraic signature whose non-negativity is proven to be sufficient for excluding the existence of self-intersections from a global perspective. An efficient algorithm for determining the existence of self-intersections is provided by recursively using this signature. Once the self-intersection is detected, if necessary, the self-intersection locus can also be computed via a further recursively cross-use of this signature and the surface-surface intersection function. Various experiments and comparisons with existing methods, as well as geometry kernels, including OCCT and ACIS, validate the robustness and efficiency of our algorithm. We also adapt our algorithm to self-intersection elimination, self-intersection trimming, and applications in mesh generation, Boolean operation, and shelling.",
    "scholar_publication": "ACM Transactions on Graphics, 2025 - dl.acm.org"
  },
  {
    "paper_id": "78b0484870",
    "authors": "Ben Weiss",
    "title": "Fast Isotropic Median Filtering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730763",
    "pdf_link": null,
    "abstract": "… median filter is dramatically faster and higher quality than the state of the art. Median filtering is … We have introduced a novel fast algorithm for median and general percentile filtering with …",
    "scholar_publication": "Proceedings of the Special Interest Group on Computer …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "e6371a341b",
    "authors": "Dewen Guo, Zhendong Wang, Zegao Liu, Sheng Li, Guoping Wang, Yin Yang, Huamin Wang",
    "title": "Fast Physics-Based Modeling of Knots and Ties using Templates",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730622",
    "pdf_link": null,
    "abstract": "Knots and ties are captivating elements of digital garments and accessories, but they have been notoriously challenging and computationally expensive to model manually. In this paper, we propose a physics-based modeling system for knots and ties using templates. The primary challenge lies in transforming cloth pieces into desired knot and tie configurations in a controllable, penetration-free manner, particularly when interacting with surrounding meshes. To address this, we introduce a pipe-like parametric knot template representation, defined by a Bézier curve as its medial axis and an adaptively adjustable radius for enhanced flexibility and variation. This representation enables customizable knot sizes, shapes, and styles while ensuring intersection-free results through robust collision detection techniques. Using the defined knot template, we present a mapping and penetration-free initialization method to transform selected cloth regions from UV space into the initial 3D knot shape. We further enable quasistatic simulation of knots and their surrounding meshes through a fast and reliable collision handling and simulation scheme. Our experiments demonstrate the system’s effectiveness and efficiency in modeling a wide range of digital knots and ties with diverse styles and shapes, including configurations that were previously impractical to create manually.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "8ee6b3de81",
    "authors": "Siyuan Chen, Yixin Chen, Jonathan Panuelos, Otman Benchekroun, Yue Chang, Eitan Grinspun, Zhecheng Wang",
    "title": "Fast Subspace Fluid Simulation with a Temporally-Aware Basis",
    "paper_url": "https://arxiv.org/abs/2502.05339",
    "pdf_link": null,
    "abstract": "… a novel reduced-order fluid simulation technique leveraging Dynamic Mode Decomposition (DMD) to achieve fast, memory-efficient, and user-controllable subspace simulation. We …",
    "scholar_publication": "arXiv preprint arXiv …, 2025 - arxiv.org"
  },
  {
    "paper_id": "11409b1ca7",
    "authors": "Ryan Capouellez, Rodrigo Singh, Martin Heistermann, David Bommes, Denis Zorin",
    "title": "Feature-Aligned Parametrization in Penner Coordinates",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731216",
    "pdf_link": null,
    "abstract": "Parametrization is a key element of many geometric modeling tasks. Seamless parametrization, in particular, is needed as a starting point for many algorithms for quadrangulation and conversion to high-order patches, as well as for the construction of seamless texture maps and displacement maps. Seamless parametrizations are difficult to compute robustly, in part because, in general, it is not known if one exists for a given mesh connectivity or for a particular configuration of singularities. Recently, Penner-coordinate-based methods that allow for connectivity changes have been shown to achieve a perfect success rate on a widely used dataset (Thingi10k). However, previously proposed Penner coordinate methods do not support sharp feature alignment or soft alignment with preferred directions on the surface, both of which are important for practical applications, especially those involving models with sharp features. In this paper, we extend Penner coordinates to surfaces with sharp features to which the parametrization needs to be aligned. Our algorithm extends the holonomy signature description of seamless parametrizations to surfaces with marked feature curves. We describe sufficient conditions for obtaining feasible solutions and describe a two-phase method to efficiently enforce feature constraints or minimize residual errors when solutions are unattainable. We demonstrate that the resulting algorithm works robustly on the Thingi10k dataset with automatic feature labeling, and the resulting seamless parametrizations can be optimized, quantized, and quadrangulated, completing the quad mesh generation pipeline.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "6cbfe67149",
    "authors": "Huibiao Wen, Guilong He, Rui Xu, Shuangmin Chen, Shiqing Xin, Zhenyu Shu, Taku Komura, Jieqing Feng, Wenping Wang, Changhe Tu",
    "title": "Feature-Preserving Mesh Repair via Restricted Power Diagram",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730671",
    "pdf_link": null,
    "abstract": "Mesh repair is a critical process in 3D geometry processing aimed at correcting errors and imperfections in polygonal meshes to produce watertight, manifold, and feature-preserving meshes suitable for downstream tasks. While errors such as degeneracies, duplication, holes, and overlaps can be addressed through standard repair processes, cracks along trimmed curves require special attention and should ideally be repaired to align with sharp feature lines.In this paper, we present a unified framework for repairing diverse mesh imperfections by leveraging a manifold wrap surface as a mediating agent. The primary role of the wrap surface is to define spatial connections between points on the original surface, thereby decoupling the challenges of edge connectivity and point relocation during repair. Throughout the process, our algorithm operates on the dual objects: the original defective mesh and the manifold wrap surface. The implementation begins by extracting a set of samples from the wrap surface and projecting them onto the original surface. These projected samples are optimized by minimizing the quadratic error relative to the tangent planes of neighboring points on the original surface. Notably, samples far from feature lines remain unchanged, while samples near feature lines converge to those lines even when the input surface lacks correct mesh topology. We then assign an adaptive weight to each sample based on the squared moving distance. By introducing this weight setting, we observe that the restricted power diagram prioritizes connectivity along feature lines, thereby effectively preserving sharp features. Through extensive experiments, we demonstrate the superiority of our proposed algorithm over existing methodologies in terms of manifoldness, watertightness, topological correctness, triangle quality, and feature preservation.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "cf9fca912d",
    "authors": "Rachel McDonnell, Bharat Vyas, Uros Sikimic, Pisut Wisessing",
    "title": "Feeling Blue or Seeing Red? Investigating the effect of light color, shadow and realism on the perception of emotion of real and virtual humans",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730728",
    "pdf_link": null,
    "abstract": "Cinematic lighting is a powerful tool used in film-making to create a mood or atmosphere and to influence the audience’s perception and emotional response to a scene. For example, red can be used to increase feelings of anxiety or excitement, while blue might have a more calming effect. These responses can be harnessed to enhance the storytelling. Previous studies in Psychology have shown that light color has a direct impact on the perception of emotions and feelings. However, there is a lack of controlled empirical studies for understanding if lighting alone can alter the interpretation of emotion. Realistic virtual humans are an underused tool to study these effects in a controlled manner as they retain the same emotional expression across lighting conditions, and can display the same emotion across different genders and races. In this paper, we focus on studying the effect of light temperature, color, and shadow on the interpretation of emotions of realistic virtual humans, and compare to a human photo baseline. We are particularly interested in recognition of emotion, emotion intensity, and genuineness of the emotion. Our findings can be used by developers to increase the emotional intensity and genuineness of their virtual humans.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "422506485a",
    "authors": "Zhongxuan Liang, Wei Du, Xiao-Ming Fu",
    "title": "Field Smoothness-Controlled Partition for Quadrangulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730889",
    "pdf_link": null,
    "abstract": "We propose a novel partition method for reliable feature-aligned quadrangulation. The core insight of the partition is that smooth streamlines distant from singularities are more suitable as patch boundaries. This allows singularities to be enclosed within patches, resulting in straighter patch boundaries and reducing the distorting influence of singularities. Accordingly, we introduce a new patch quality control mechanism that keeps the patch boundaries inside regions with high field smoothness. Combined with other common metrics (e.g., aligning boundaries with field and feature lines), we develop a practical partition algorithm that first iteratively traces paths in field smoothness-controlled regions to form patches and then removes redundant paths to simplify the patch layout. We demonstrate the effectiveness and practicability of our partitions by using them to generate quality quad meshes on a massive test data set. Compared with state-of-the-art methods, our approach produces quad meshes with significantly enhanced quality while maintaining similar reliability, validating the core insight. Code and data for this paper are at https://github.com/AnderLiang/Field-Smoothness-Controlled-Quadrangulation.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "297cb7591e",
    "authors": "Shiyi Zhang, Junhao Zhuang, Zhaoyang Zhang, Ying Shan, Yansong Tang",
    "title": "FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730683",
    "pdf_link": null,
    "abstract": "Action customization involves generating videos where the subject performs actions dictated by input control signals. Current methods use pose-guided or global motion customization but are limited by strict constraints on spatial structure such as layout, skeleton, and viewpoint consistency, reducing adaptability across diverse subjects and scenarios. To overcome these limitations, we propose FlexiAct, which transfers actions from a reference video to an arbitrary target image. Unlike existing methods, FlexiAct allows for variations in layout, viewpoint, and skeletal structure between the subject of the reference video and the target image, while maintaining identity consistency. Achieving this requires precise action control, spatial structure adaptation, and consistency preservation. To this end, we introduce RefAdapter, a lightweight image-conditioned adapter that excels in spatial adaptation and consistency preservation, surpassing existing methods in balancing appearance consistency and structural flexibility. Additionally, based on our observations, the denoising process exhibits varying levels of attention to motion (low frequency) and appearance details (high frequency) at different timesteps. So we propose FAE (Frequency-aware Action Extraction), which, unlike existing methods that rely on separate spatial-temporal architectures, directly achieves action extraction during the denoising process. Experiments demonstrate that our method effectively transfers actions to subjects with diverse layouts, skeletons, and viewpoints. We release our code and model weights to support further research at FlexiAct.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "13569053c6",
    "authors": "Dong Xiao, Renjie Chen",
    "title": "Flexible 3D Cage-based Deformation via Green Coordinates on Bézier Patches",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730630",
    "pdf_link": null,
    "abstract": "Cage-based deformation is a fundamental problem in geometry processing, where a cage, a user-specified boundary of a region, is used to deform the ambient space of a given mesh. Traditional 3D cages are typically composed of triangles and quads. While quads can represent non-planar regions when their four corners are not coplanar, they form ruled surfaces with straight isoparametric curves, which limits their ability to handle curved and high-curvature deformations. In this work, we extend the cage for curved boundaries using Bézier patches, enabling flexible and high-curvature deformations with only a few control points. The higher-order structure of the Bézier patch also allows for the creation of a more compact and precise curved cage for the input model. Based on Green’s third identity, we derive the Green coordinates for the Bézier cage, achieving shape-preserving deformation with smooth surface boundaries. These coordinates are defined based on the vertex positions and normals of the Bézier control net. Given that the coordinates are approximately calculated through the Riemann summation, we propose a global projection technique to ensure that the coordinates accurately conform to the linear reproduction property. Experimental results show that our method achieves high performance in handling curved and high-curvature deformations.",
    "scholar_publication": "Proceedings of the Special Interest Group on Computer …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "8eb4bf87e0",
    "authors": "Duowen Chen, Zhiqi Li, Taiyuan Zhang, Jinjin He, Junwei Zhou, Bart G. van Bloemen Waanders, Bo Zhu",
    "title": "Fluid Simulation on Compressible Flow Maps",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731192",
    "pdf_link": null,
    "abstract": "This paper presents a unified compressible flow map framework designed to accommodate diverse compressible flow systems, including high-Mach-number flows (e.g., shock waves and supersonic aircraft), weakly compressible systems (e.g., smoke plumes and ink diffusion), and incompressible systems evolving through compressible acoustic quantities (e.g., free-surface shallow water). At the core of our approach is a theoretical foundation for compressible flow maps based on Lagrangian path integrals, a novel advection scheme for the conservative transport of density and energy, and a unified numerical framework for solving compressible flows with varying pressure treatments. We validate our method across three representative compressible flow systems, characterized by varying fluid morphologies, governing equations, and compressibility levels, demonstrating its ability to preserve and evolve spatiotemporal features such as vortical structures and wave interactions governed by different flow physics. Our results highlight a wide range of novel phenomena, from ink torus breakup to delta wing tail vortices and vortex shedding on free surfaces, significantly expanding the range of fluid systems that flow-map methods can handle.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "a4fa18f8ff",
    "authors": "Sinan Wang, Junwei Zhou, Fan Feng, Zhiqi Li, Yuchen Sun, Duowen Chen, Greg Turk, Bo Zhu",
    "title": "Fluid Simulation on Vortex Particle Flow Maps",
    "paper_url": "https://arxiv.org/abs/2505.21946",
    "pdf_link": null,
    "abstract": "We propose the Vortex Particle Flow Map (VPFM) method to simulate incompressible flow with complex vortical evolution in the presence of dynamic solid boundaries. The core insight of our approach is that vorticity is an ideal quantity for evolution on particle flow maps, enabling significantly longer flow map distances compared to other fluid quantities like velocity or impulse. To achieve this goal, we developed a hybrid Eulerian-Lagrangian representation that evolves vorticity and flow map quantities on vortex particles, while reconstructing velocity on a background grid. The method integrates three key components: (1) a vorticity-based particle flow map framework, (2) an accurate Hessian evolution scheme on particles, and (3) a solid boundary treatment for no-through and no-slip conditions in VPFM. These components collectively allow a substantially longer flow map length (3-12 times longer) than the state-of-the-art, enhancing vorticity preservation over extended spatiotemporal domains. We validated the performance of VPFM through diverse simulations, demonstrating its effectiveness in capturing complex vortex dynamics and turbulence phenomena.",
    "scholar_publication": "arXiv preprint arXiv …, 2025 - arxiv.org"
  },
  {
    "paper_id": "beccda18f9",
    "authors": "Grace Todd, Mike Bailey",
    "title": "Foliager: Procedural Forest Generation From Natural Language Using Scientific Data and AI",
    "paper_url": "https://dl.acm.org/doi/full/10.1145/3721250.3743024",
    "pdf_link": null,
    "abstract": "Figure 1: A 100-year-old simulated forest generated by Foliager using the prompt\" Bend, Oregon,\" shown across four seasons. Tree species, growth parameters, and seasonal changes in foliage color and leaf presence were procedurally generated by the system. Background environment was composited post-generation for visual context.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "d5f6042a11",
    "authors": "DongHeun Han, Byungmin Kim, RoUn Lee, KyeongMin Kim, Hyoseok Hwang, HyeongYeop Kang",
    "title": "ForceGrip: Reference-Free Curriculum Learning for Realistic Grip Force Control in VR Hand Manipulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730738",
    "pdf_link": null,
    "abstract": "Realistic Hand manipulation is a key component of immersive virtual reality (VR), yet existing methods often rely on a kinematic approach or motion-capture datasets that omit crucial physical attributes such as contact forces and finger torques. Consequently, these approaches prioritize tight, one-size-fits-all grips rather than reflecting users’ intended force levels. We present ForceGrip, a deep learning agent that synthesizes realistic hand manipulation motions, faithfully reflecting the user’s grip force intention. Instead of mimicking predefined motion datasets, ForceGrip uses generated training scenarios—randomizing object shapes, wrist movements, and trigger input flows—to challenge the agent with a broad spectrum of physical interactions. To effectively learn from these complex tasks, we employ a three-phase curriculum learning framework comprising Finger Positioning, Intention Adaptation, and Dynamic Stabilization. This progressive strategy ensures stable hand-object contact, adaptive force control based on user inputs, and robust handling under dynamic conditions. Additionally, a proximity reward function enhances natural finger motions and accelerates training convergence. Quantitative and qualitative evaluations reveal ForceGrip’s superior force controllability and plausibility compared to state-of-the-art methods. Demo videos are available as supplementary material and the code is provided at https://han-dongheun.github.io/ForceGrip.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "500d901b48",
    "authors": "Zhiyu Zhang, Rui Wang",
    "title": "From Style to Identity: AI Pipelines for Visual and Character Coherence in Film",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742968",
    "pdf_link": null,
    "abstract": "This study contrasts two generative AI (GenAI) workflows (Figure ) addressing visual and character consistency and introduces a filmmaker-oriented framework for AI-assisted production, grounded in two practice-based short films.",
    "scholar_publication": "Proceedings of the Special Interest Group on Computer …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "617da6d173",
    "authors": "Kihong Choi, Daeyoul Park, Keehoon Hong",
    "title": "Full-Color Natural Light Holographic Video Camera",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742987",
    "pdf_link": null,
    "abstract": "This paper presents a compact handheld type holographic video camera system capable of capturing real-time, full-color complex hologram videos under natural lighting conditions. By integrating a geometric phase lens with a polarization image sensor, our system captures interference patterns without requiring specialized lighting or bulky equipment. We successfully apply conventional 2D video super-resolution techniques to the complex holograms, significantly enhancing both resolution and visibility while preserving digital refocusing capabilities. Our experimental results demonstrate that this approach satisfies three critical requirements for practical modern cameras: operation under incoherent lighting, robustness to mobile shooting condition, and compact design. This work represents an advancement toward practical holographic media applications, particularly for broadcast content production in extended reality and mixed reality environments.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "0c687b30da",
    "authors": "Pedro Pena, Karthik Mohan Kumar, Damian Andrysiak, Kunal Tyagi, Rama Harihara",
    "title": "G-FED: G-Buffer Guided Frame Extrapolation in Video Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742972",
    "pdf_link": null,
    "abstract": "Rendering near-final quality previews requires a great number of samples per pixel. Recently, diffusion models have shown superior denoising capabilities, but suffer from large variance. This is further amplified by the spatial and temporal inconsistencies introduced by diffusion models. In our pipeline, we propose the use of multiple control features and forward projections to denoise 1 sample per pixel frames & extrapolate a high-quality frame to generate a consistent and controllable sequence of high-quality frames.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "cc0095a9c4",
    "authors": "Zhengming Yu, Tianye Li, Jingxiang Sun, Omer Shapira, Seonwook Park, Michael Stengel, Matthew Chan, Xin Li, Wenping Wang, Koki Nagano, Shalini De Mello",
    "title": "GAIA: Generative Animatable Interactive Avatars with Expression-conditioned Gaussians",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730737",
    "pdf_link": null,
    "abstract": "3D generative models of faces trained on in-the-wild image collections have improved greatly in recent times, offering better visual fidelity and view consistency. Making such generative models animatable is a hard yet rewarding task, with applications in virtual AI agents, character animation, and telepresence. However, it is not trivial to learn a well-behaved animation model with the generative setting, as the learned latent space aims to best capture the data distribution, often omitting details such as dynamic appearance and entangling animation with other factors that affect controllability. We present GAIA: Generative Animatable Interactive Avatars, which is able to generate high-fidelity 3D head avatars for both realistic animation and rendering. To achieve consistency during animation, we learn to generate Gaussians embedded in an underlying morphable model for human heads via a shared UV parameterization. For modeling realistic animation, we further design the generator to learn expression-conditioned details for both geometric deformation and dynamic appearance. Finally, facing an inevitable entanglement problem between facial identity and expression, we propose a novel two-branch architecture that encourages the generator to disentangle identity and expression. On existing benchmarks, GAIA achieves state-of-the-art performance in visual quality as well as realistic animation. The generated Gaussian-based avatar supports highly efficient animation and rendering, making it readily available for interactive animation and appearance editing.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "d046166d4f",
    "authors": "Stephen Pasch, Joel Salzman, Changxi Zheng",
    "title": "GBake: Baking 3D Gaussian Splats Into Reflection Probes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742978",
    "pdf_link": null,
    "abstract": "The growing popularity of 3D Gaussian Splatting has created the need to integrate traditional computer graphics techniques and assets in splatted environments. Since 3D Gaussian primitives encode lighting and geometry jointly as appearance, meshes are relit improperly when inserted directly in a mixture of 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a specialized tool for baking reflection probes from Gaussian-splatted scenes that enables realistic reflection mapping of traditional 3D meshes in the Unity game engine.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "9bd98c9ff3",
    "authors": "Henglei Lv, Bailin Deng, Jianzhu Guo, Xiaoqiang Liu, Pengfei Wan, Di Zhang, Lin Gao",
    "title": "GSHeadRelight: Fast Relightability for 3D Gaussian Head Synthesis",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730614",
    "pdf_link": null,
    "abstract": "Relighting and novel view synthesis of human portraits are essential in applications such as portrait photography, virtual reality (VR), and augmented reality (AR). Despite recent progress, 3D-aware portrait relighting remains challenging due to the demands for photorealistic rendering, real-time performance, and generalization to unseen subjects. Existing works either rely on supervision from limited and expensive light stage captured data or produce suboptimal results. Moreover, many works are based on generative NeRFs, which suffer from poor 3D consistency and low real-time performance. We resort to recent progress on generative 3D Gaussians and design a lighting model based on a unified neural radiance transfer representation, which responds linearly to incident light. Using only in-the-wild images, our method achieves state-of-the-art relighting results and a significantly faster rendering speed (× 12) compared to previous 3D-aware portrait relighting research.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "c747f75364",
    "authors": "Zinuo You, Stamatios Georgoulis, Anpei Chen, Siyu Tang, Dengxin Dai",
    "title": "GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730757",
    "pdf_link": null,
    "abstract": "Video stabilization is pivotal for video processing, as it removes unwanted shakiness while preserving the original user motion intent. Existing approaches, depending on the domain they operate, suffer from several issues (e.g. geometric distortions, excessive cropping, poor generalization) that degrade the user experience. To address these issues, we introduce GaVS, a novel 3D-grounded approach that reformulates video stabilization as a temporally-consistent ‘local reconstruction and rendering’ paradigm. Given 3D camera poses, we augment a reconstruction model to predict Gaussian Splatting primitives, and finetune it at test-time, with multi-view dynamics-aware photometric supervision and cross-frame regularization, to produce temporally-consistent local reconstructions. The model are then used to render each stabilized frame. We utilize a scene extrapolation module to avoid frame cropping. Our method is evaluated on a repurposed dataset, instilled with 3D-grounded information, covering samples with diverse camera motions and scene dynamics. Quantitatively, our method is competitive with or superior to state-of-the-art 2D and 2.5D approaches in terms of conventional task metrics and new geometry consistency. Qualitatively, our method produces noticeably better results compared to alternatives, validated by the user study. Project Page: sinoyou.github.io/gavs.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "f6fcec154a",
    "authors": "Yuki Tatsukawa, Anran Qi, I-Chao Shen, Takeo Igarashi",
    "title": "GarmentImage: Raster Encoding of Garment Sewing Patterns with Diverse Topologies",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730632",
    "pdf_link": null,
    "abstract": "Garment sewing patterns are the design language behind clothing, yet their current vector-based digital representations weren’t built with machine learning in mind. Vector-based representation encodes a sewing pattern as a discrete set of panels, each defined as a sequence of lines and curves, stitching information between panels and the placement of each panel around a body. However, this representation causes two major challenges for neural networks: discontinuity in latent space between patterns with different topologies and limited generalization to garments with unseen topologies in the training data. In this work, we introduce GarmentImage, a unified raster-based sewing pattern representation. GarmentImage encodes a garment sewing pattern’s geometry, topology and placement into multi-channel regular grids. Machine learning models trained on GarmentImage achieve seamless transitions between patterns with different topologies and show better generalization capabilities compared to models trained on vector-based representation. We demonstrate the effectiveness of GarmentImage across three applications: pattern exploration in latent space, text-based pattern editing, and image-to-pattern prediction. The results show that GarmentImage achieves superior performance on these applications using only simple convolutional networks.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "f7a0f56dd8",
    "authors": "Zhi Zhou, Chao Li, Zhenyuan Zhang, Mingcong Tang, Zibin Li, Shuhang Luan, Zhangjin Huang",
    "title": "Gaussian Compression for Precomputed Indirect Illumination",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730758",
    "pdf_link": null,
    "abstract": "Precomputed global illumination (GI) techniques, such as light probes, particularly focus on capturing indirect illumination and have gained widespread adoption. However, as the scale of the scenes continues to expand, the demand for storage space and runtime memory for light probes also increases substantially. To address this issue, we propose a novel Gaussian fitting compression technique specifically designed for light field probes, which enables the use of denser samples to describe illumination in complex scenes. The core idea of our method is utilizing low-bit adaptive Gaussian functions to store the latent representation of light probes, enabling parallel and high-speed decompression on the GPU. Additionally, we implement a custom gradient propagation process to replace conventional inference frameworks, like PyTorch, ensuring an exceptional compression speed.At the same time, by constructing a cascaded light field texture in real-time, we avoid the need for baking and storing a large number of redundant light field probes arranged in the form of 3D textures. This approach allows us to achieve further compression of the memory while maintaining high visual quality and rendering speed. Compared to traditional methods based on Principal Component Analysis (PCA), our approach consistently yields superb results across various test scenarios, achieving compression ratios of up to 1:50.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "18afefbcbd",
    "authors": "Jingrui Xing, Bin Wang, Mengyu Chu, Baoquan Chen",
    "title": "Gaussian Fluids: A Grid-Free Fluid Solver based on Gaussian Spatial Representation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730620",
    "pdf_link": null,
    "abstract": "We present a grid-free fluid solver featuring a novel Gaussian representation. Drawing inspiration from the expressive capabilities of 3D Gaussian Splatting in multi-view image reconstruction, we model the continuous flow velocity as a weighted sum of multiple Gaussian functions. This representation is continuously differentiable, which enables us to derive spatial differentials directly and solve the time-dependent PDE via a custom first‑order optimization tailored to fluid dynamics. Compared to traditional discretizations, which …",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "6ce3703213",
    "authors": "Suyeon Choi, Brian Chao, Jacqueline Yang, Manu Gopakumar, Gordon Wetzstein",
    "title": "Gaussian Wave Splatting for Computer-Generated Holography",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731163",
    "pdf_link": null,
    "abstract": "State-of-the-art neural rendering methods optimize Gaussian scene representations from a few photographs for novel-view synthesis. Building on these representations, we develop an efficient algorithm, dubbed Gaussian Wave Splatting, to turn these Gaussians into holograms. Unlike existing computergenerated holography (CGH) algorithms, Gaussian Wave Splatting supports accurate occlusions and view-dependent effects for photorealistic scenes by leveraging recent advances in neural rendering. Specifically, we derive a closed-form solution for a 2D Gaussian-to-hologram transform that supports occlusions and alpha blending. Inspired by classic computer graphics techniques, we also derive an efficient approximation of the aforementioned process in the Fourier domain that is easily parallelizable and implement it using custom CUDA kernels. By integrating emerging neural rendering pipelines with holographic display technology, our Gaussian-based CGH framework paves the way for next-generation holographic displays.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "e11d3aeb4e",
    "authors": "MI CHANG, EUN HYE JANG, WOOJIN KIM, DAESUB YOON",
    "title": "Gaze Entropy and Driver Safety: Understanding Cognitive Failure and Situational Response Before Take-over",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742974",
    "pdf_link": null,
    "abstract": "Understanding the driver's cognitive state is critical in conditionally autonomous driving, particularly when responding to Take-Over Requests (TOR). However, existing approaches rely primarily on visual attention and are limited in capturing fundamental cognitive failures. This study proposes a quantifiable framework that identifies such failures through gaze entropy analysis and links the driver's gaze behavior to accident risk.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "8f16b29796",
    "authors": "Yuezhi Yang, Haitao Yang, George Kiyohiro Nakayama, Xiangru Huang, Leonidas Guibas, Qixing Huang",
    "title": "GenAnalysis: Joint Shape Analysis by Learning Man-Made Shape Generators with Deformation Regularizations",
    "paper_url": "https://yyuezhi.github.io/assets/pdf/GenAnalysis_small.pdf",
    "pdf_link": null,
    "abstract": "Shape analysis, is a fundamental research area in geometry processing that enjoys many applications [Mitra et al. 2014; Xu et al. 2017].",
    "scholar_publication": "arXiv preprint arXiv …, 2025 - yyuezhi.github.io"
  },
  {
    "paper_id": "d3d83d7f04",
    "authors": "Lvmin Zhang, Chuan Yan, Yuwei Guo, Jinbo Xing, Maneesh Agrawala",
    "title": "Generating Past and Future in Digital Painting Processes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731160",
    "pdf_link": null,
    "abstract": "We present a framework to generate past and future processes for drawing process videos. Given a canvas image uploaded by a user, the framework can generate both preceding and succeeding states of the drawing process, and the generated states can be reused as inputs for further state generation. We observe that the user queries typically have one-to-one or many-to-many states, and in many cases, involve non-contiguous states. This necessitates a backend that solves a set-to-set problem with arbitrary combinations of past or future states. To this end, we repurpose video diffusion models to learn the set-to-set mapping with pretrained video priors. We implement the system with strong diffusion transformer backbones (e.g., CogVideoX and LTXVideo) and high-quality data processing (e.g., sampling short shots from long videos of real drawing records). Experiments show that the generated states are diverse in drawing contexts and resemble human drawing processes. This capability may aid artists in visualizing potential outcomes, generating creative inspirations, or refining existing workflows.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "d0ecb652a3",
    "authors": "Nithin Raghavan, Krishna Mullia, Alexander Trevithick, Fujun Luan, Miloš Hašan, Ravi Ramamoorthi",
    "title": "Generative Neural Materials",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730746",
    "pdf_link": null,
    "abstract": "… for neural material generation. Left: An example scene with a number of neural materials in … In summary, we demonstrated a first generative diffusion model sampling neural materials …",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "5f3f91c4c7",
    "authors": "Yongtao Ge, Kangyang Xie, Guangkai Xu, Mingyu Liu, Li Ke, Longtao Huang, Hui Xue, Hao Chen, Chunhua Shen",
    "title": "Generative Video Matting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730642",
    "pdf_link": null,
    "abstract": "… Our goal is to adapt pre-trained video generative models for video matting with minimal … the traditional video matting regression problem can be reformulated as a conditional generative …",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "0eb33e6639",
    "authors": "Saeed Hadadan, Benedikt Bitterli, Tizian Zeltner, Jan Novák, Fabrice Rousselle, Jacob Munkberg, Jon Hasselgren, Bartlomiej Wronski, Matthias Zwicker",
    "title": "Generative detail enhancement for physically based materials",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730751",
    "pdf_link": null,
    "abstract": "We present a tool for enhancing the detail of physically based materials using an off-the-shelf diffusion model and inverse rendering. Our goal is to increase the visual fidelity of existing materials by adding, for instance, signs of wear, aging, and weathering that are tedious to author. To obtain realistic appearance with minimal user effort, we leverage a generative image model trained on a large dataset of natural images. Given the geometry, UV mapping, and basic appearance of an object, we proceed as follows: We render multiple views of the object and use them, together with an appearance-defining text prompt, to condition a diffusion model. The generated details are then backpropagated from the enhanced images to the material parameters via inverse rendering. For inverse rendering to be successful, the generated appearance has to be consistent across all the images. We propose two priors to address the multi-view consistency of the diffusion model. First, we ensure that the noise that seeds the diffusion process is itself consistent across views by integrating it from a view-independent UV space. Second, we enforce spatial consistency by biasing the attention mechanism via a projective constraint so that pixels attend strongly to their corresponding pixel locations in other views. Our approach does not require any training or finetuning of the diffusion model, is agnostic to the used material model, and the enhanced material properties, i.e., 2D PBR textures, can be further edited by artists. We demonstrate prompt-based material edits exhibiting high levels of realism and detail. This project is available at https://generative-detail.github.io.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "9d019a8a9e",
    "authors": "Zizhou Huang, Maxwell Paik, Zachary Ferguson, Daniele Panozzo, Denis Zorin",
    "title": "Geometric Contact Potential",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731142",
    "pdf_link": null,
    "abstract": "… We present a systematic derivation of a continuum potential … set of natural requirements for contact potentials, including the … of our potential that is a drop-in replacement for the potential …",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "43ce7f1350",
    "authors": "Jiawei Huang, Shaokun Zheng, Kun Xu, Yoshifumi Kitamura, Jiaping Wang",
    "title": "Guided Lens Sampling for Efficient Monte Carlo Circle-of-Confusion Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730608",
    "pdf_link": null,
    "abstract": "We introduce a guided lens sampling method for efficient rendering of circles of confusion (CoCs). While traditional Monte Carlo techniques simulate depth-of-field (DoF) effects by perturbing camera rays at the lens, uniform lens sampling often results in significant noise by failing to prioritize rays toward highlight regions in the scene. Although path guiding has proven effective for global illumination by learning importance distributions for incoming radiance, no comparable guiding technique for CoCs exists, primarily due to the strong parallax between adjacent pixels. We model highlight spots in world space using a globally shared radiance field, which is then transformed into lens space through a bipolar-cone projection to guide camera ray generation. We implement this theory using 3D Gaussians, achieving fast, robust guiding with minimal computational and storage overhead, making it suitable for production rendering. We also propose two extensions to further enhance local adaptation. Our experiments show that this approach significantly improves the sampling efficiency for CoC rendering.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "7df5fce64f",
    "authors": "Tianyu Huang, Jingwang Ling, Shuang Zhao, Feng Xu",
    "title": "Guiding-Based Importance Sampling for Walk on Stars",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730593",
    "pdf_link": null,
    "abstract": "Walk on stars (WoSt) has shown its power in being applied to Monte Carlo methods for solving partial differential equations, but the sampling techniques in WoSt are not satisfactory, leading to high variance. We propose a guiding-based importance sampling method to reduce the variance of WoSt. Drawing inspiration from path guiding in rendering, we approximate the directional distribution of the recursive term of WoSt using online-learned parametric mixture distributions, decoded by a lightweight neural field. This adaptive approach enables importance sampling the recursive term, which lacks shape information before computation. We introduce a reflection technique to represent guiding distributions at Neumann boundaries and incorporate multiple importance sampling with learnable selection probabilities to further reduce variance. We also present a practical GPU implementation of our method. Experiments show that our method effectively reduces variance compared to the original WoSt, given the same time or the same sample budget. Code and data for this paper are at https://github.com/tyanyuy3125/elaina.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "e6f62273f9",
    "authors": "Zhiming Hu, Daniel Haeufle, Syn Schmitt, Andreas Bulling",
    "title": "HOIGaze: Gaze Estimation During Hand-Object Interactions in Extended Reality Exploiting Eye-Hand-Head Coordination",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730692",
    "pdf_link": null,
    "abstract": "We present HOIGaze – a novel learning-based approach for gaze estimation during hand-object interactions (HOI) in extended reality (XR). HOIGaze addresses the challenging HOI setting by building on one key insight: Eye, hand, and head movements are closely coordinated during HOIs and this coordination can be exploited to identify samples that are most useful for gaze estimator training – as such, effectively denoising the training data. This denoising approach is in stark contrast to previous gaze estimation methods that treated all training samples as equal. Specifically, we propose: 1) a novel hierarchical framework that first recognises the hand currently visually attended to and then estimates gaze direction based on the attended hand; 2) a new gaze estimator that uses cross-modal Transformers to fuse head and hand-object features extracted using a convolutional neural network and a spatio-temporal graph convolutional network; and 3) a novel eye-head coordination loss that upgrades training samples belonging to the coordinated eye-head movements. We evaluate HOIGaze on the HOT3D and Aria digital twin (ADT) datasets and show that it significantly outperforms state-of-the-art methods, achieving an average improvement of 15.6% on HOT3D and 6.0% on ADT in mean angular error. To demonstrate the potential of our method, we further report significant performance improvements for the sample downstream task of eye-based activity recognition on ADT. Taken together, our results underline the significant information content available in eye-hand-head coordination and, as such, open up an exciting new direction for learning-based gaze estimation.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "d445dd7e49",
    "authors": "Hibiki Kirihata, Tomokazu Ishikawa",
    "title": "Hand Gesture-Driven Vertical Teleportation: Navigating Complex Height Differences in VR",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743020",
    "pdf_link": null,
    "abstract": "This paper presents two novel teleportation methods for VR environments that address limitations of conventional parabola-based approaches when navigating varying heights. The SphereBackcast and Penetration methods utilize straight-line specification for intuitive movement to elevated locations. Experiments with 22 participants showed our methods significantly outperformed parabola-based teleportation for height differences above 2m, while maintaining comparable performance on flat terrain. NASA-TLX and SUS evaluations confirmed improved usability and reduced cognitive load, indicating these methods can be readily integrated into existing VR applications.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "fac9e3af77",
    "authors": "Hao Xu, Yinqiao Wang, Niloy Mitra, Shuaicheng Liu, Pheng Ann Heng, Chi-Wing Fu",
    "title": "Hand-Shadow Poser",
    "paper_url": "https://arxiv.org/abs/2505.07012",
    "pdf_link": null,
    "abstract": "… A gallery of hand shadow arts created by our Hand-Shadow Poser for shapes of diverse everyday objects (C3) from [Sikora 2001] and the Internet. For each case, the top left shows the …",
    "scholar_publication": "arXiv preprint arXiv …, 2025 - arxiv.org"
  },
  {
    "paper_id": "3418c2d04e",
    "authors": "Adam Hsieh",
    "title": "Haunting Horizons: Human–AI Auto-Cartography of Tasmanian Island Experience",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743010",
    "pdf_link": null,
    "abstract": "Lutruwita/Tasmania's island conditions are often misperceived as isolated and unchanging. Building on Giada Peterle's concept of auto-cartography, this paper explores Tasmania's dynamic island identity through human-AI mapping. This is achieved through the creation of haunting horizons, an interactive installation powered by a customised generative AI model. By translating my island experience into a training dataset, this work positions human-AI auto-cartography as an embodied, affective process, enabling artists and participants to engage with maps and reflect on their relations to place in new ways.",
    "scholar_publication": "Proceedings of the Special Interest Group on Computer …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "578de7e0aa",
    "authors": "Tobias Kohler, Martin Heistermann, David Bommes",
    "title": "HexHex: Highspeed Extraction of Hexahedral Meshes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730940",
    "pdf_link": null,
    "abstract": "Modern hexahedral mesh generation relies on integer-grid maps (IGM), which map the Cartesian grid of integer iso-surfaces to a structure-aligned and conforming hexahedral cell complex discretizing the target shape. The hexahedral mesh is formed by iso-surfaces of the map such that an extraction algorithm is needed to convert the implicit map representation into an explicit mesh. State-of-the-art algorithms have been designed with two goals in mind, i.e., (i) unconditional robustness and (ii) tolerance to map defects in the form of inverted or degenerate tetrahedra. Because of significant advancements in the generation of locally injective maps, the tolerance to map defects has become irrelevant. At the same time, there is a growing demand for efficiently handling significantly larger mesh complexities, unfortunately not well served by the state-of-the-art since the tolerance to map defects induces a high runtime cost. Consequently, we present HexHex, a novel (unconditionally robust) hexahedral mesh extraction algorithm for locally injective integer-grid maps designed for maximal performance and scalability. Key contributions include a novel and highly compact mesh data structure based on so-called propellers and a conservative rasterization technique, significantly reducing the number of required exact predicate tests. HexHex not only offers lower asymptotic runtime complexities from a theoretical perspective but also lower constants, enabling in practice a 30x speedup for medium-sized examples and a larger speedup for more complex inputs, specifically when the hex-to-tet ratio is large. We provide a C++ reference implementation, supporting multi-core parallelization and the extraction of curved (piecewise-linear) hexahedral mesh edges and faces, e.g., valuable for subsequent higher-order mesh generation.",
    "scholar_publication": "ACM Transactions on Graphics …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "afadaa533b",
    "authors": "Kaoru Sasaki, Kazuhito Sato, Shugo Yamaguchi, Keitaro Tanaka, Shigeo Morishima",
    "title": "Hide A Bit: A Training-Free and High-Fidelity Steganography Method for 3D Gaussian Splatting Based on Bit Manipulation and RSA Encryption",
    "paper_url": "https://dl.acm.org/doi/full/10.1145/3721250.3743009",
    "pdf_link": null,
    "abstract": "2 methodWe propose a training-free method for hiding a 3DGS model (the message) within an existing pre-trained 3DGS model (the scene). The method embeds the encrypted upper bits of the message parameters into the lower bits of the scene parameters. It comprises three components, each described in the following subsections.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "bd35b879cd",
    "authors": "Xiang Zhang, Yang Zhang, Lukas Mehl, Markus Gross, Christopher Schroers",
    "title": "High-Fidelity Novel View Synthesis via Splatting-Guided Diffusion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730669",
    "pdf_link": null,
    "abstract": "Despite recent advances in Novel View Synthesis (NVS), generating high-fidelity views from single or sparse observations remains challenging. Existing splatting-based approaches often produce distorted geometry due to splatting errors. While diffusion-based methods leverage rich 3D priors to achieve improved geometry, they often suffer from texture hallucination. In this paper, we introduce SplatDiff, a pixel-splatting-guided video diffusion model designed to synthesize high-fidelity novel views from a single image. Specifically, we propose an aligned synthesis strategy for precise control of target viewpoints and geometry-consistent view synthesis. To mitigate texture hallucination, we design a texture bridge module that enables high-fidelity texture generation through adaptive feature fusion. In this manner, SplatDiff leverages the strengths of splatting and diffusion for geometrically consistent, high-fidelity view synthesis. Extensive experiments verify the state-of-the-art performance of SplatDiff in single-view NVS. Additionally, without extra training, SplatDiff shows remarkable zero-shot performance across diverse tasks, including sparse-view NVS and stereo video conversion.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "7a4fdea5e5",
    "authors": "Zixuan Lu, Ziheng Liu, Lei Lan, Huamin Wang, Yuko Ishiwaka, Chenfanfu Jiang, Kui Wu, Yin Yang",
    "title": "High-performance CPU Cloth Simulation Using Domain-decomposed Projective Dynamics",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731182",
    "pdf_link": null,
    "abstract": "Whenever the concept of high-performance cloth simulation is brought up, GPU acceleration is almost always the first that comes to mind. Leveraging immense parallelization, GPU algorithms have demonstrated significant success recently, whereas CPU methods are somewhat overlooked. Indeed, the need for an efficient CPU simulator is evident and pressing. In many scenarios, high-end GPUs may be unavailable or are already allocated to other tasks, such as rendering and shading. A high-performance CPU alternative can greatly boost the overall system capability and user experience. Inspired by this demand, this paper proposes a CPU algorithm for high-resolution cloth simulation. By partitioning the garment model into multiple (but not massive) sub-meshes or domains, we assign per-domain computations to individual CPU processors. Borrowing the idea of projective dynamics that breaks the computation into global and local steps, our key contribution is a new parallelization paradigm at domains for both global and local steps so that domain-level calculations are sequential and lightweight. The CPU has much fewer processing units than a GPU. Our algorithm mitigates this disadvantage by wisely balancing the scale of the parallelization and convergence. We validate our method in a wide range of simulation problems involving high-resolution garment models. Performance-wise, our method is at least one order faster than existing CPU methods, and it delivers a similar performance compared with the state-of-the-art GPU algorithms in many examples, but without using a GPU.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "e44083139d",
    "authors": "Corentin Salaun, Martin Balint, Laurent Belcour, Eric Heitz, Gurprit Singh, Karol Myszkowski",
    "title": "Histogram Stratification for Spatio-Temporal Reservoir Sampling",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730723",
    "pdf_link": null,
    "abstract": "Monte Carlo (MC) rendering is a widely used approach for photorealistic image synthesis, yet real-time applications often limit sampling to one path per pixel, resulting in high noise levels. To mitigate this, resampled importance sampling (RIS) has shown promise by approximating ideal sample distributions through a discrete set of candidates, avoiding the complexity of neural models or data-intensive structures. However, current RIS techniques often rely on random sampling, which fails to maximize the potential of the candidate pool. We propose a two step approach that first organizes samples candidates into local histograms and then sample the histogram using Quasi Monte Carlo and antithetic patterns. This can be done with minimal overhead and allows to reduce error in rendering to increase visual quality. Additionally, we show how it can be combined with blue noise error distribution to perceptually reduce noise artifacts. Our approach yields a higher-quality resampling estimator with enhanced noise reduction, demonstrating significant improvements in real-time rendering tasks.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "5337d679a4",
    "authors": "Yilin Liu, Duoteng Xu, Xingyao Yu, Xiang Xu, Daniel Cohen-Or, Hao Zhang, Hui Huang",
    "title": "HoLa: B-Rep Generation using a Holistic Latent Representation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730842",
    "pdf_link": null,
    "abstract": "We introduce a novel representation for learning and generating Computer-Aided Design (CAD) models in the form of boundary representations (B-Reps). Our representation unifies the continuous geometric properties of B-Rep primitives in different orders (e.g., surfaces and curves) and their discrete topological relations in a holistic latent (HoLa) space. This is based on the simple observation that the topological connection between two surfaces is intrinsically tied to the geometry of their intersecting curve. Such a prior allows us to reformulate topology learning in B-Reps as a geometric reconstruction problem in Euclidean space. Specifically, we eliminate the presence of curves, vertices, and all the topological connections in the latent space by learning to distinguish and derive curve geometries from a pair of surface primitives via a neural intersection network. To this end, our holistic latent space is only defined on surfaces but encodes a full B-Rep model, including the geometry of surfaces, curves, vertices, and their topological relations. Our compact and holistic latent space facilitates the design of a first diffusion-based generator to take on a large variety of inputs including point clouds, single/multi-view images, 2D sketches, and text prompts. Our method significantly reduces ambiguities, redundancies, and incoherences among the generated B-Rep primitives, as well as training complexities inherent in prior multi-step B-Rep learning pipelines, while achieving greatly improved validity rate over current state of the art: 82% vs. ≈50%.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "793d0a1dba",
    "authors": "Florian Schiffers, Grace Kuo, Nathan Matsuda, Douglas Lanman, Oliver Cossairt, Florian Schiffers, Oliver Cossairt",
    "title": "HoloChrome: Polychromatic Illumination for Speckle Reduction in Holographic Near-Eye Displays",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3732935",
    "pdf_link": null,
    "abstract": "Holographic displays hold the promise of providing authentic depth cues, resulting in enhanced immersive visual experiences for near-eye applications. However, current holographic displays are hindered by speckle noise, which limits accurate reproduction of color and texture in displayed images. We present HoloChrome, a polychromatic holographic display framework designed to mitigate these limitations. HoloChrome utilizes an ultrafast, wavelength-adjustable laser and a dual-Spatial Light Modulator (SLM) architecture, enabling the multiplexing of a large set of discrete wavelengths across the visible spectrum. By leveraging spatial separation in our dual-SLM setup, we independently manipulate speckle patterns across multiple wavelengths. This novel approach effectively reduces speckle noise through incoherent averaging achieved by wavelength multiplexing, specifically by using a single SLM pattern to modulate multiple wavelengths simultaneously on one or more SLM devices. Our method is complementary to existing speckle reduction techniques, offering a new pathway to address this challenge. Furthermore, the use of polychromatic illumination broadens the achievable color gamut compared to traditional three-color primary holographic displays.Our simulations and tabletop experiments validate that HoloChrome significantly reduces speckle noise and expands the color gamut. These advancements enhance the performance of holographic near-eye displays, moving us closer to practical, immersive next-generation visual experiences.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "b75379815d",
    "authors": "Zhiyuan Yu, Zhe Li, Hujun Bao, Can Yang, Xiaowei Zhou",
    "title": "HumanRAM: Feed-forward Human Reconstruction and Animation Model using Transformers",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730605",
    "pdf_link": null,
    "abstract": "3D human reconstruction and animation are long-standing topics in computer graphics and vision. However, existing methods typically rely on sophisticated dense-view capture and/or time-consuming per-subject optimization procedures. To address these limitations, we propose HumanRAM, a novel feed-forward approach for generalizable human reconstruction and animation from monocular or sparse human images. Our approach integrates human reconstruction and animation into a unified framework by introducing explicit pose conditions, parameterized by a shared SMPL-X neural texture, into transformer-based large reconstruction models (LRM). Given monocular or sparse input images with associated camera parameters and SMPL-X poses, our model employs scalable transformers and a DPT-based decoder to synthesize realistic human renderings under novel viewpoints and novel poses. By leveraging the explicit pose conditions, our model simultaneously enables high-quality human reconstruction and high-fidelity pose-controlled animation. Experiments show that HumanRAM significantly surpasses previous methods in terms of reconstruction accuracy, animation fidelity, and generalization performance on real-world datasets.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "a21551c006",
    "authors": "Xinrui Liu, Longxiulin Deng, Abe Davis",
    "title": "Hybrid Tours: A Clip-based System for",
    "paper_url": "https://www.cs.cornell.edu/abe/projects/hybridtours/Hybrid_Tours_Compressed.pdf",
    "pdf_link": null,
    "abstract": "Long-take touring (LTT) videos have become an increasingly popular way to showcase large physical spaces. These videos are characterized by long, smooth camera trajectories that seamlessly connect different parts of a complex scene in one uninterrupted shot. However, capturing LTT video is very difficult; typically, the camera trajectory must be choreographed ahead of time and captured in one take by a skilled drone or steadicam operator. This traditional workflow, which we call the all-real workflow, often requires several attempts to successfully capture the desired camera path in a single shot, which can quickly become expensive and, in some environments, dangerous.Image-based rendering (IBR) techniques like NeRF [Mildenhall et al. 2020] and 3D Gaussian splatting (3DGS)[Kerbl et al. 2023] have made alternative all-virtual workflows increasingly popular. In such workflows, the user first scans and reconstructs an environment, then plans and renders a camera trajectory using their",
    "scholar_publication": "ACM Transactions on Graphics, 2025 - cs.cornell.edu"
  },
  {
    "paper_id": "f4e2e5e96c",
    "authors": "Alvin Shi, Haomiao Wu, Theodore Kim",
    "title": "Hyper-Dimensional Deformation Simulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730730",
    "pdf_link": null,
    "abstract": "… et al. 2022] and their rigid body simulation [Bosch 2020]. We present the first hyper-dimensional simulation of 4D deformation. This requires generalizing many pieces of the simulation …",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "300d5db6e4",
    "authors": "Abraham Beauferris, Wei Sen Loi",
    "title": "HyperParamBRDF: Fast Parametric Reflectance via Hypernetworks and Physics-Based Simulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743039",
    "pdf_link": null,
    "abstract": "Simulating and fabricating plasmonic nanostructures for specific colors is slow and costly. HyperParamBRDF exploits a hypernetwork to learn a parametric reflectance model from physical parameters. Trained on sparse FDTD data, it infers BRDFs in milliseconds, achieving > 107 × speedup with high fidelity, enabling real-time appearance exploration for complex simulated materials.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "f3a2ad135f",
    "authors": "Kaizhi Yang, Liu Dai, Isabella Liu, Xiaoshuai Zhang, Xiaoyan Sun, Xuejin Chen, Zexiang Xu, Hao Su",
    "title": "IMLS-Splatting: Efficient Mesh Reconstruction from Multi-view Images via Point Representation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731210",
    "pdf_link": null,
    "abstract": "Multi-view mesh reconstruction has long been a challenging problem in graphics and computer vision. In contrast to recent volumetric rendering methods that generate meshes through post-processing, we propose an end-to-end mesh optimization approach called IMLS-Splatting. Our method leverages the sparsity and flexibility of point clouds to efficiently represent the underlying surface. To achieve this, we introduce a splatting-based differentiable Implicit Moving-Least Squares (IMLS) algorithm that enables the fast conversion of point clouds into SDFs and texture fields, optimizing both mesh reconstruction and rasterization. Additionally, the IMLS representation ensures that the reconstructed SDF and mesh maintain continuity and smoothness without the need for extra regularization. With this efficient pipeline, our method enables the reconstruction of highly detailed meshes in approximately 11 minutes, supporting high-quality rendering and achieving state-of-the-art reconstruction performance. Our code is available at https://github.com/SilenKZYoung/IMLS-Splatting.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "618d983c38",
    "authors": "Sara Dorfman, Dana Cohen-Bar, Rinon Gal, Daniel Cohen-Or",
    "title": "IP-Composer: Semantic Composition of Visual Concepts",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730624",
    "pdf_link": null,
    "abstract": "Content creators often draw inspiration from multiple visual sources, combining distinct elements to craft new compositions. Modern computational approaches now aim to emulate this fundamental creative process. Although recent diffusion models excel at text-guided compositional synthesis, text as a medium often lacks precise control over visual details. Image-based composition approaches can capture more nuanced features, but existing methods are typically limited in the range of concepts they can capture, and require expensive training procedures or specialized data. We present IP-Composer, a novel training-free approach for compositional image generation that leverages multiple image references simultaneously, while using natural language to describe the concept to be extracted from each image. Our method builds on IP-Adapter, which synthesizes novel images conditioned on an input image’s CLIP embedding. We extend this approach to multiple visual inputs by crafting composite embeddings, stitched from the projections of multiple input images onto concept-specific CLIP-subspaces identified through text. Through comprehensive evaluation, we show that our approach enables more precise control over a larger range of visual concept compositions.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ea589f7f8e",
    "authors": "Yuxin Zhang, Minyan Luo, Weiming Dong, Xiao Yang, Haibin Huang, Chongyang Ma, Oliver Deussen, Tong-Yee Lee, Changsheng Xu",
    "title": "IP-Prompter: Training-Free Theme-Specific Image Generation via Dynamic Visual Prompting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730670",
    "pdf_link": null,
    "abstract": "The stories and characters that captivate us as we grow up shape unique fantasy worlds, with images serving as the primary medium for visually experiencing these realms. Personalizing generative models through fine-tuning with theme-specific data has become a prevalent approach in text-to-image generation. However, unlike object customization, which focuses on learning specific objects, theme-specific generation encompasses diverse elements such as characters, scenes, and objects. Such diversity also introduces a key challenge: how to adaptively generate multi-character, multi-concept, and continuous theme-specific images (TSI). Moreover, fine-tuning approaches often come with significant computational overhead, time costs, and risks of overfitting. This paper explores a fundamental question: Can image generation models directly leverage images as contextual input, similarly to how large language models use text as context? To address this, we present IP-Prompter, a novel training-free TSI generation method. IP-Prompter introduces visual prompting, a mechanism that integrates reference images into generative models, allowing users to seamlessly specify the target theme without requiring additional training. To further enhance this process, we propose a Dynamic Visual Prompting (DVP) mechanism, which iteratively optimizes visual prompts to improve the accuracy and quality of generated images. Our approach enables diverse applications, including consistent story generation, character design, realistic character generation, and style-guided image generation. Comparative evaluations against state-of-the-art personalization methods demonstrate that IP-Prompter achieves significantly better results and excels in maintaining character identity preserving, style consistency and text alignment, offering a robust and flexible solution for theme-specific image generation. Our project page: https://ip-prompter.github.io/.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "232928af7a",
    "authors": "Yunxiang Zhang, Bingxuan Li, Alexandr Kuznetsov, Akshay Jindal, Stavros Diolatzis, Kenneth Chen, Anton Sochenov, Anton Kaplanyan, Qi Sun",
    "title": "Image-GS: Content-Adaptive Image Representation via 2D Gaussians",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730596",
    "pdf_link": null,
    "abstract": "Neural image representations have emerged as a promising approach for encoding and rendering visual data. Combined with learning-based workflows, they demonstrate impressive trade-offs between visual fidelity and memory footprint. Existing methods in this domain, however, often rely on fixed data structures that suboptimally allocate memory or compute-intensive implicit models, hindering their practicality for real-time graphics applications.Inspired by recent advancements in radiance field rendering, we introduce Image-GS, a content-adaptive image representation based on 2D Gaussians. Leveraging a custom differentiable renderer, Image-GS reconstructs images by adaptively allocating and progressively optimizing a group of anisotropic, colored 2D Gaussians. It achieves a favorable balance between visual fidelity and memory efficiency across a variety of stylized images frequently seen in graphics workflows, especially for those showing non-uniformly distributed features and in low-bitrate regimes. Moreover, it supports hardware-friendly rapid random access for real-time usage, requiring only 0.3K MACs to decode a pixel. Through error-guided progressive optimization, Image-GS naturally constructs a smooth level-of-detail hierarchy. We demonstrate its versatility with several applications, including texture compression, semantics-aware compression, and joint image compression and restoration.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "0e71a502ef",
    "authors": "Zhenyu Wang, Min Lu",
    "title": "Image-Space Collage and Packing with Differentiable Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730690",
    "pdf_link": null,
    "abstract": "Collage and packing techniques are widely used to organize geometric shapes into cohesive visual representations, facilitating the representation of visual features holistically, as seen in image collages and word clouds. Traditional methods often rely on object-space optimization, requiring intricate geometric descriptors and energy functions to handle complex shapes. In this paper, we introduce a versatile image-space collage technique. Leveraging a differentiable renderer, our method effectively optimizes the object layout with image-space losses, bringing the benefit of fixed complexity and easy accommodation of various shapes. Applying a hierarchical resolution strategy in image space, our method efficiently optimizes the collage with fast convergence, large coarse steps first and then small precise steps. The diverse visual expressiveness of our approach is demonstrated through various examples. Experimental results show that our method achieves an order of magnitude speedup performance compared to state-of-the-art techniques.",
    "scholar_publication": "Proceedings of the Special Interest Group on Computer …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "a9b0bca595",
    "authors": "Kai Yan, Cheng Zhang, Sébastien Speierer, Guangyan Cai, Yufeng Zhu, Zhao Dong, Shuang Zhao",
    "title": "Image-space Adaptive Sampling for Fast Inverse Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730627",
    "pdf_link": null,
    "abstract": "Inverse rendering is crucial for many scientific and engineering disciplines. Recent progress in differentiable rendering has led to efficient differentiation of the full image formation process with respect to scene parameters, enabling gradient-based optimization.However, computational demands pose a significant challenge for differentiable rendering, particularly when rendering all pixels during inverse rendering from high-resolution/multi-view images. This computational cost leads to slow performance in each iteration of inverse rendering. Meanwhile, naively reducing the sampling budget by uniformly sampling pixels to render in each iteration can result in high gradient variance during inverse rendering, ultimately degrading overall performance.Our goal is to accelerate inverse rendering by reducing the sampling budget without sacrificing overall performance. In this paper, we introduce a novel image-space adaptive sampling framework to accelerate inverse rendering by dynamically adjusting pixel sampling probabilities based on gradient variance and contribution to the loss function. Our approach efficiently handles high-resolution images and complex scenes, with faster convergence and improved performance compared to uniform sampling, making it a robust solution for efficient inverse rendering.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "0be8c34b84",
    "authors": "Xinyu Yi, Shaohua Pan, Feng Xu",
    "title": "Improving Global Motion Estimation in Sparse IMU-based Motion Capture with Physics",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730822",
    "pdf_link": null,
    "abstract": "By learning human motion priors, motion capture can be achieved by 6 inertial measurement units (IMUs) in recent years with the development of deep learning techniques, even though the sensor inputs are sparse and noisy. However, human global motions are still challenging to be reconstructed by IMUs. This paper aims to solve this problem by involving physics. It proposes a physical optimization scheme based on multiple contacts to enable physically plausible translation estimation in the full 3D space where the z-directional motion is usually challenging for previous works. It also considers gravity in local pose estimation which well constrains human global orientations and refines local pose estimation in a joint estimation manner. Experiments demonstrate that our method achieves more accurate motion capture for both local poses and global motions. Furthermore, by deeply integrating physics, we can also estimate 3D contact, contact forces, joint torques, and interacting proxy surfaces. Code is available at https://xinyu-yi.github.io/GlobalPose/.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "3c581cd581",
    "authors": "Cyprien Plateau--Holleville, Benjamin Stamm, Vincent Nivoliers, Maxime Maria, Stéphane Mérillou",
    "title": "In Search of Empty Spheres: 3D Apollonius Diagrams on GPU",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730868",
    "pdf_link": null,
    "abstract": "We present a novel comprehensive construction algorithm of Apollonius diagrams designed for GPUs. Efficient and robust algorithms have been proposed for the computation of Voronoi diagrams or Power diagrams. In contrast, Apollonius cells are neither convex nor bounded by straight boundaries, making their computation complex, especially in more than two dimensions. Their parallel computation also represents a challenge because of the sequential nature of state-of-the-art algorithms. In this article, we tackle the computation of these diagrams from the geometry of their cells. Our strategy is based on a core cell topology update allowing the iterative insertion of new sites found through nearest neighbor queries. To benefit from the highly parallel environment of modern GPUs and fit their memory restriction, we define a lightweight data structure allowing the representation of the complex topology of Apollonius cells. Additionally, we provide several space exploration procedures for their efficient construction under both homogeneous and heterogeneous spatial distributions. Our method outperforms the fastest state-of-the-art CPU implementation while computing the complete geometry. As a possible use case, we show an application for molecular illustration.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "30c1ea53b2",
    "authors": "Mia Tang, Yael Vinker, Chuan Yan, Lvmin Zhang, Maneesh Agrawala",
    "title": "Instance Segmentation of Scene Sketches Using Natural Image Priors",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730606",
    "pdf_link": null,
    "abstract": "Sketch segmentation involves grouping pixels within a sketch that belong to the same object or instance. It serves as a valuable tool for sketch editing tasks, such as moving, scaling, or removing specific components. While image segmentation models have demonstrated remarkable capabilities in recent years, sketches present unique challenges for these models due to their sparse nature and wide variation in styles. We introduce InkLayer, a method for instance segmentation of raster scene sketches. Our approach adapts state-of-the-art image segmentation and object detection models to the sketch domain by employing class-agnostic fine-tuning and refining segmentation masks using depth cues. Furthermore, our method organizes sketches into sorted layers, where occluded instances are inpainted, enabling advanced sketch editing applications. As existing datasets in this domain lack variation in sketch styles, we construct a synthetic scene sketch segmentation dataset, InkScenes, featuring sketches with diverse brush strokes and varying levels of detail. We use this dataset to demonstrate the robustness of our approach. Code and data for this paper are released at project page: https://inklayer.github.io.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "1a129b86ae",
    "authors": "Etai Sella, Yanir Kleiman, Hadar Averbuch-Elor",
    "title": "InstanceGen: Image Generation with Instance-level Instructions",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730613",
    "pdf_link": null,
    "abstract": "Despite rapid advancements in the capabilities of generative models, pretrained text-to-image models still struggle in capturing the semantics conveyed by complex prompts that compound multiple objects and instance-level attributes. Consequently, we are witnessing growing interests in integrating additional structural constraints, typically in the form of coarse bounding boxes, to better guide the generation process in such challenging cases. In this work, we take the idea of structural guidance a step further by making the observation that contemporary image generation models can directly provide a plausible fine-grained structural initialization. We propose a technique that couples this image-based structural guidance with LLM-based instance-level instructions, yielding output images that adhere to all parts of the text prompt, including object counts, instance-level attributes, and spatial relations between instances. Additionally, we contribute CompoundPrompts, a benchmark composed of complex prompts with three difficulty levels in which object instances are progressively compounded with attribute descriptions and spatial relations. Extensive experiments demonstrate that our method significantly surpasses the performance of prior models, particularly over complex multi-object and multi-attribute use cases.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "a756e86501",
    "authors": "Wonjong Jang, Yucheol Jung, Gyeongmin Lee, Seungyong Lee",
    "title": "Instant Self-Intersection Repair for 3D Meshes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731427",
    "pdf_link": null,
    "abstract": "Self-intersection repair in static 3D surface meshes presents unique challenges due to the absence of temporal motion and penetration depth information—two critical elements typically leveraged in physics-based approaches. We introduce a novel framework that transforms local contact handling into a global repair strategy through a combination of local signed tangent-point energies and their gradient diffusion. At the heart of our method is a key insight: rather than computing expensive global repulsive potentials, we can effectively approximate long-range interactions by diffusing energy gradients from local contacts throughout the mesh surface. In turn, resolving complex self-intersections reduces to simply propagating local repulsive energies through standard diffusion mechanics and iteratively solving tractable local optimizations. We further accelerate convergence through our momentum-based optimizer, which adaptively regulates momentum based on gradient statistics to prevent overshooting while maintaining rapid intersection repair. The resulting algorithm handles a variety of challenging scenarios, from shallow contacts to deep penetrations, while providing computational efficiency suitable for interactive applications.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "33857b08eb",
    "authors": "Howard Zhang, Yuval Alaluf, Sizhuo Ma, Achuta Kadambi, Jian Wang, Kfir Aberman",
    "title": "InstantRestore: Single-Step Personalized Face Restoration with Shared-Image Attention",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730628",
    "pdf_link": null,
    "abstract": "Face image restoration aims to enhance degraded facial images while addressing challenges such as diverse degradation types, real-time processing demands, and, most crucially, the preservation of identity-specific features. Existing methods often struggle with slow processing times and suboptimal restoration, especially under severe degradation, failing to accurately reconstruct finer-level identity details. To address these issues, we introduce InstantRestore, a novel framework that leverages a single-step image diffusion model and an attention-sharing mechanism for fast and personalized face restoration. Additionally, InstantRestore incorporates a novel landmark attention loss, aligning key facial landmarks to refine the attention maps, enhancing identity preservation. At inference time, given a degraded input and a small (∼ 4) set of reference images, InstantRestore performs a single forward pass through the network to achieve near real-time performance. Unlike prior approaches that rely on full diffusion processes or per-identity model tuning, InstantRestore offers a scalable solution suitable for large-scale applications. Extensive experiments demonstrate that InstantRestore outperforms existing methods in quality and speed, making it an appealing choice for identity-preserving face restoration.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "9bbec78434",
    "authors": "Yu Suzuki, Naoya Iwamoto, Shigeo Morishima",
    "title": "Interactive Camerawork Authoring System for Free-Viewpoint Dance Contents",
    "paper_url": "https://dl.acm.org/doi/full/10.1145/3721250.3743030",
    "pdf_link": null,
    "abstract": "Recent advances in free-viewpoint visual technologies, such as 4D Gaussian Splatting, offer the potential for deeper immersion and more diverse viewing experiences. However, these contents often require viewers to actively select arbitrary viewpoints, which can increase cognitive burden. To provide an intuitive and high-quality free-viewpoint content experience, we argue for the necessity of a camerawork authoring system that interactively suggests suitable camerawork based on the content and user queries. Especially in dance camerawork, there should be strong correlations with multiple modalities. Existing approaches using Diffusion Transformers for generating camerawork [Wang et al. 2024a, b] face several challenges. These include jittering artifacts in the camerawork that degrade quality, as well as limitations in interactivity when accommodating user preferences.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "be9cbe65b5",
    "authors": "Weikun Peng, Sota Taira, Chris Careaga, Yağız Aksoy",
    "title": "Interactive Object Insertion With Differentiable Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743019",
    "pdf_link": null,
    "abstract": "2 MethodOur pipeline consists of multiple steps that can be carried out in an iterative fashion via user interaction. Our interface represents a modern take on prior automatic object insertion methods from the literature [Karsch et al. 2011]. Our scene reconstruction process follows closely the methodology of [Careaga and Aksoy 2025]. In this section, we detail each step of our pipeline. The entire process is summarized in Figure 2.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "731c408e9a",
    "authors": "Davide Sforza, Marzia Riso, Filippo Muzzini, Nicola Capodieci, Fabio Pellacini",
    "title": "Interactive Optimization of Scaffolded Procedural Patterns",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730667",
    "pdf_link": null,
    "abstract": "A procedural program is the representation of a family of assets that share the same structural or semantic properties, whose final appearance is determined by different parameter assignments. Identifying the parameter values that define a desired asset is usually a time-consuming operation, since it requires manually tuning parameters separately and in a non-intuitive manner. In the domain of procedural patterns, recent works focused on estimating parameter values to match a target render or sketch, using parameter optimization or inference via neural networks. However, these approaches are neither fast enough for interactive design nor precise enough to give direct control. In this work, we propose an interactive method for procedural parameter estimation based on the idea of scaffolded procedural patterns. A scaffolded procedural pattern is a sequence of procedural programs that model a pattern in a coarse-to-fine manner, in which the desired pattern appearance is reached step-by-step by inheriting previously optimized parameters. Through scaffolding, patterns are more straightforward to sketch for users and easier to optimize for most algorithms. In our implementation, patterns are represented as procedural signed distance functions whose parameters are estimated with a gradient-free optimization method that runs in real-time on the GPU. We show that scaffolded patterns can be created with a node-based interface familiar to artists. We validate our approach by creating and interactively editing several scaffolded patterns. We show the effectiveness of scaffolding through a user study, where scaffolding enhances both the output quality and the editing experience with respect to approaches that optimize the procedural parameters all at once. We also perform a comparison with previous strategies and provide several recordings of real-time editing sessions in the accompanying materials.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "b5e3c616f2",
    "authors": "Boyoung Lim, Jusub Kim",
    "title": "Interactive Trailers and Posters That Enhance Viewing Intentions Through ‘Breaking the Fourth Wall’",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743001",
    "pdf_link": null,
    "abstract": "The rise of video streaming has shifted video consumption from traditional venues like theaters to mobile and social media platforms. However, promotional strategies have not kept pace—posters and trailers are still used on mobile devices without leveraging their unique capabilities. This paper presents a new approach to boosting viewing intent through interactive engagement. It introduces interactive posters and trailers that break the \"Fourth Wall,\" allowing characters to communicate directly with users. A prototype enabling dialog-based interaction was tested with 33 participants in their 20s and 30s. Results showed that these interactive experiences significantly increased anticipation and intent to watch the film.",
    "scholar_publication": "Proceedings of the Special Interest Group on Computer …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "0c2bfda0a9",
    "authors": "Zizhou Huang, Chrystiano Araújo, Andrew Kunz, Denis Zorin, Daniele Panozzo, Victor Zordan",
    "title": "Intersection-Free Garment Retargeting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730590",
    "pdf_link": null,
    "abstract": "… In this paper, the goal is to retarget artist-designed garments on a … garments, we propose a training-free method that performs optimizations on the mesh representation of the garments, …",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "f0ac74bdc8",
    "authors": "Ruizhi Shao, Yinghao Xu, Yujun Shen, Ceyuan Yang, Yang Zheng, Changan Chen, Yebin Liu, Gordon Wetzstein",
    "title": "Interspatial Attention for Efficient 4D Human Video Generation",
    "paper_url": "https://arxiv.org/abs/2505.15800",
    "pdf_link": null,
    "abstract": "Generating photorealistic videos of digital humans in a controllable manner is crucial for a plethora of applications. Existing approaches either build on methods that employ template-based 3D representations or emerging video generation models but suffer from poor quality or limited consistency and identity preservation when generating individual or multiple digital humans. In this paper, we introduce a new interspatial attention (ISA) mechanism as a scalable building block for modern diffusion transformer (DiT)--based video generation models. ISA is a new type of cross attention that uses relative positional encodings tailored for the generation of human videos. Leveraging a custom-developed video variation autoencoder, we train a latent ISA-based diffusion model on a large corpus of video data. Our model achieves state-of-the-art performance for 4D human video synthesis, demonstrating remarkable motion consistency and identity preservation while providing precise control of the camera and body poses. Our code and model are publicly released at https://dsaurus.github.io/isa4d/.",
    "scholar_publication": "arXiv preprint arXiv …, 2025 - arxiv.org"
  },
  {
    "paper_id": "4b8798e902",
    "authors": "Linjie Lyu, Valentin Deschaintre, Yannick Hold-Geoffroy, Milos Hasan, Jae Shin Yoon, Thomas Leimkuehler, Christian Theobalt, Iliyan Georgiev",
    "title": "IntrinsicEdit: Precise generative image manipulation in intrinsic space",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731173",
    "pdf_link": null,
    "abstract": "Generative diffusion models have advanced image editing by delivering high-quality results through intuitive interfaces such as prompts, scribbles, and semantic drawing. However, these interfaces lack precise control, and associated editing methods often specialize in a single task. We introduce a versatile workflow for a range of editing tasks which operates in an intrinsic-image latent space, enabling semantic, local manipulation with pixel precision while automatically handling effects like reflections and shadows. We build on the RGB↔X diffusion framework and address its key deficiencies: the lack of identity preservation and the need to update multiple channels to achieve plausible results. We propose an edit-friendly diffusion inversion and prompt-embedding optimization to enable precise and efficient editing of only the relevant channels. Our method achieves identity preservation and resolves global illumination, without requiring task-specific model fine-tuning. We demonstrate state-of-the-art performance across a variety of tasks on complex images, including material adjustments, object insertion and removal, global relighting, and their combinations.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "0d46fead2e",
    "authors": "Pengbin Tang, Bernhard Thomaszewski, Stelian Coros, Bernd Bickel",
    "title": "Inverse Design of Discrete Interlocking Materials with Desired Mechanical Behavior",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730675",
    "pdf_link": null,
    "abstract": "We present a computational approach for designing Discrete Interlocking Materials (DIMs) with desired mechanical properties. Unlike conventional elastic materials, DIMs are kinematic materials governed by internal contacts among elements. These contacts induce anisotropic deformation limits that depend on the shape and topology of the elements. To enable gradient-based design optimization of DIMs with desired deformation limits, we introduce an implicit representation of interlocking elements based on unions of tori. Using this low-dimensional representation, we simulate DIMs with smoothly evolving contacts, allowing us to predict changes in deformation limits as a function of shape parameters. With this toolset in hand, we optimize for element shape parameters to design heterogeneous DIMs that best approximate prescribed limits. We demonstrate the effectiveness of our method by designing discrete interlocking materials with diverse limit profiles for in- and out-of-plane deformation and validate our method on fabricated physical prototypes.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "b8928333e4",
    "authors": "Quentin Becker, Oliver Gross, Mark Pauly",
    "title": "Inverse Geometric Locomotion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731187",
    "pdf_link": null,
    "abstract": "… animation involve solving combinations of inverse kinematics and motion planning … based on a geometric formulation of locomotion—we address the inverse problem of optimizing …",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "a35fcef419",
    "authors": "Lei Lan, Zixuan Lu, Chun Yuan, Weiwei Xu, Hao Su, Huamin Wang, Chenfanfu Jiang, Yin Yang",
    "title": "JGS2: Near Second-order Converging Jacobi/Gauss-Seidel for GPU Elastodynamics",
    "paper_url": "https://arxiv.org/abs/2506.06494",
    "pdf_link": null,
    "abstract": "In parallel simulation, convergence and parallelism are often seen as inherently conflicting objectives. Improved parallelism typically entails lighter local computation and weaker coupling, which unavoidably slow the global convergence. This paper presents a novel GPU algorithm that achieves convergence rates comparable to fullspace Newton's method while maintaining good parallelizability just like the Jacobi method. Our approach is built on a key insight into the phenomenon of overshoot. Overshoot occurs when a local solver aggressively minimizes its local energy without accounting for the global context, resulting in a local update that undermines global convergence. To address this, we derive a theoretically second-order optimal solution to mitigate overshoot. Furthermore, we adapt this solution into a pre-computable form. Leveraging Cubature sampling, our runtime cost is only marginally higher than the Jacobi method, yet our algorithm converges nearly quadratically as Newton's method. We also introduce a novel full-coordinate formulation for more efficient pre-computation. Our method integrates seamlessly with the incremental potential contact method and achieves second-order convergence for both stiff and soft materials. Experimental results demonstrate that our approach delivers high-quality simulations and outperforms state-of-the-art GPU methods with 50 to 100 times better convergence.",
    "scholar_publication": "arXiv preprint arXiv …, 2025 - arxiv.org"
  },
  {
    "paper_id": "e20441b09a",
    "authors": "Jeongmin Gu, Bochang Moon",
    "title": "James-Stein Gradient Combiner for Inverse Monte Carlo Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730714",
    "pdf_link": null,
    "abstract": "Inferring scene parameters such as BSDFs and volume densities from user-provided target images has been achieved using a gradient-based optimization framework, which iteratively updates the parameters using the gradient of a loss function defined by the differences between rendered and target images. The gradient can be unbiasedly estimated via a physics-based rendering, ie, differentiable Monte Carlo rendering. However, the estimated gradient can become noisy unless a large number of samples are used for gradient …",
    "scholar_publication": "Proceedings of the Special Interest Group on Computer …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "0548c41b7d",
    "authors": "Xuejun Hu, Jinfan Lu, Kun Xu",
    "title": "Kernel Predicting Neural Shadow Maps",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730645",
    "pdf_link": null,
    "abstract": "Existing neural shadow mapping methods [Datta et al. ] have shown to be promising in generating high quality soft shadows. However, it demonstrates limited generalizability to new scenes. In this paper, we present a novel neural method, named kernel predicting neural shadow mapping to address this issue. Specifically, we explicitly model soft shadow values as pixelwise local filtering from nearby base shadow values (i.e., the classic hard shadow values) in the screen space, where the local filter weights are predicted through a trained neural network. We use dilated filters as the representation of our local filters to maintain a balance between computational efficiency and receptive field of a local filter. We further enhance shadow quality by replacing the classic shadow map algorithm [Williams ] with moment shadow maps [Peters and Klein ] to generate the base shadows values. With carefully designed filters, input features, and loss functions with temporal regularization, our method runs in real-time framerates (i.e., >100 fps for 2048 × 1024 resolution), produces temporally-stable soft shadows with good generalizability, and consistently beats state-of-the-art methods in both visual qualities and numeric measures. Code and model weights are available at https://github.com/Hoosus/KPNSM.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "088941a9a6",
    "authors": "Arjun Lakshmipathy, Jessica Hodgins, Nancy Pollard, Arjun Lakshmipathy",
    "title": "Kinematic Motion Retargeting for Contact-Rich Anthropomorphic Manipulations",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3723872",
    "pdf_link": null,
    "abstract": "Hand motion capture data are now relatively easy to obtain, even for complicated grasps; however, these data are of limited use without the ability to retarget it onto the hands of a specific character or robot. The target hand may differ dramatically in geometry, number of degree of freedom (DOF), or number of fingers. We present a simple but effective framework capable of kinematically retargeting human hand-object manipulations from a publicly available dataset to diverse target hands through the exploitation of contact areas. We do so by formulating the retargeting operation as a nonisometric shape matching problem and use a combination of both surface contact and marker data to progressively estimate, refine, and fit the final target hand trajectory using inverse kinematics. Foundational to our framework is the introduction of a novel shape matching process, which we show enables predictable and robust transfer of contact data over full manipulations (pregrasp, pickup, in-hand re-orientation, and release) while providing an intuitive means for artists to specify correspondences with relatively few inputs. We validate our framework through demonstrations across five different hands and six motions of different objects. We additionally demonstrate a bimanual task, perform stress tests, and compare our method against existing hand retargeting approaches. Finally, we demonstrate our method enabling novel capabilities such as object substitution and the ability to visualize the impact of hand design choices over full trajectories.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "a745b00b80",
    "authors": "Yisheng He, Xiaodong Gu, Xiaodan Ye, Chao Xu, Zhengyi Zhao, Yuan Dong, Weihao Yuan, Zilong Dong, Liefeng Bo",
    "title": "LAM: Large Avatar Model for One-shot Animatable Gaussian Head",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730706",
    "pdf_link": null,
    "abstract": "We present LAM, an innovative Large Avatar Model for animatable Gaussian head reconstruction from a single image. Unlike previous methods that require extensive training on captured video sequences or rely on auxiliary neural networks for animation and rendering during inference, our approach generates Gaussian heads that are immediately animatable and renderable. Specifically, LAM creates an animatable Gaussian head in a single forward pass in seconds, enabling reenactment and rendering without additional networks or post-processing steps. This capability allows for seamless integration into existing rendering pipelines, ensuring real-time animation and rendering across a wide range of platforms, including mobile phones. The centerpiece of our framework is the canonical Gaussian attributes generator, which utilizes FLAME canonical points as queries. These points interact with multi-scale image features through a Transformer to accurately predict Gaussian attributes in the canonical space. The reconstructed canonical Gaussian avatar can then be animated utilizing standard linear blend skinning (LBS) with corrective blendshapes as the FLAME model did and rendered in real-time on various platforms. The experiments demonstrate that LAM outperforms state-of-the-art methods on existing benchmarks.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "9d25c09732",
    "authors": "Ziyi Chang, He Wang, George Koulieris, Hubert Shum",
    "title": "Large-Scale Multi-Character Interaction Synthesis",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730750",
    "pdf_link": null,
    "abstract": "Generating large-scale multi-character interactions is a challenging and important task in character animation. Multi-character interactions involve not only natural interactive motions but also characters coordinated with each other for transition. For example, a dance scenario involves characters dancing with partners and also characters coordinated to new partners based on spatial and temporal observations. We term such transitions as coordinated interactions and decompose them into interaction synthesis and transition …",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ec7a00a489",
    "authors": "Sihui Ji, Hao Luo, Xi Chen, Yuanpeng Tu, Yiyang Wang, Hengshuang Zhao",
    "title": "LayerFlow: A Unified Model for Layer-aware Video Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730662",
    "pdf_link": null,
    "abstract": "We present LayerFlow, a unified solution for layer-aware video generation. Given per-layer prompts, LayerFlow generates videos for the transparent foreground, clean background, and blended scene. It also supports versatile variants like decomposing a blended video or generating the background for the given foreground and vice versa. Starting from a text-to-video diffusion transformer, we organize the videos of different layers as sub-clips, and leverage layer embeddings to distinguish each clip and the corresponding layer-wise prompts. In this way, we seamlessly support the aforementioned variants in one unified framework. For the lack of high-quality layer-wise training videos, we design a multi-stage training strategy to accommodate static images with high-quality layer annotations. Specifically, we first train the model with low-quality video data. Then, we tune a motion LoRA to make the model compatible with static frames. Afterward, we train the content LoRA on the mixture of image data with high-quality layered images along with copy-pasted video data. During inference, we remove the motion LoRA thus generating smooth videos with desired layers.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "9bbcbb67fe",
    "authors": "Shuai Yang, Jing Tan, Mengchen Zhang, Tong Wu, Gordon Wetzstein, Ziwei Liu, Dahua Lin",
    "title": "LayerPano3D: Layered 3D Panorama for Hyper-Immersive Scene Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730643",
    "pdf_link": null,
    "abstract": "3D immersive scene generation is a challenging yet critical task in computer vision and graphics. A desired virtual 3D scene should 1) exhibit omnidirectional view consistency, and 2) allow for large-range exploration in complex scene hierarchies. Existing methods either rely on successive scene expansion via inpainting or employ panorama representation to represent large FOV scene environments. However, the generated scene suffers from semantic drift during expansion and is unable to handle occlusion among scene hierarchies. To tackle these challenges, we introduce LayerPano3D, a novel framework for full-view, explorable panoramic 3D scene generation from a single text prompt. Our key insight is to decompose a reference 2D panorama into multiple layers at different depth levels, where each layer reveals the unseen space from the reference views via diffusion prior. LayerPano3D comprises multiple dedicated designs: 1) We introduce a new panorama dataset Upright360 , comprising 9k high-quality and upright panorama images, and finetune the advanced Flux model on Upright360 for high-quality, upright and consistent panorama generation related tasks. 2) We pioneer the Layered 3D Panorama as underlying representation to manage complex scene hierarchies and lift it into 3D Gaussians to splat detailed 360-degree omnidirectional scenes with unconstrained viewing paths. Extensive experiments demonstrate that our framework generates state-of-the-art 3D panoramic scene in both full view consistency and immersive exploratory experience. We believe that LayerPano3D holds promise for advancing 3D panoramic scene creation with numerous applications. More examples please visit our webpage: ys-imtech.github.io/projects/LayerPano3D/",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "3c48c7849a",
    "authors": "Yuchen Sun, Junlin Li, Ruicheng Wang, Sinan Wang, Zhiqi Li, Bart G. van Bloemen Waanders, Bo Zhu",
    "title": "Leapfrog Flow Maps for Real-Time Fluid Simulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731180",
    "pdf_link": null,
    "abstract": "We propose Leapfrog Flow Maps (LFM) to simulate incompressible fluids with rich vortical flows in real time. Our key idea is to use a hybrid velocityimpulse scheme enhanced with leapfrog method to reduce the computational workload of impulse-based flow map methods, while possessing strong ability to preserve vortical structures and fluid details. In order to accelerate the impulse-to-velocity projection, we develop a fast matrix-free Algebraic Multigrid Preconditioned Conjugate Gradient (AMGPCG) solver with customized GPU optimization, which makes projection comparable with impulse evolution in terms of time cost. We demonstrate the performance of our method and its efficacy in a wide range of examples and experiments, such as real-time simulated burning fire ball and delta wingtip vortices.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "8ebf784ce6",
    "authors": "Ziqi Wang, Wenjun Liu, Jingwen Wang, Gabriel Vallat, Fan Shi, Stefana Parascho, Maryam Kamgarpour",
    "title": "Learning to Assemble with Alternative Plans",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730824",
    "pdf_link": null,
    "abstract": "We present a reinforcement learning framework for constructing assemblies composed of rigid parts, which are commonly seen in many historical masonry buildings and bridges. Traditional construction methods for such structures often depend on dense scaffolding to stabilize their intermediate assembly steps, making the process both labor-intensive and time-consuming. This work utilizes multiple robots to collaboratively assemble structures, offering temporary support by holding parts in place without additional scaffolding. Precomputing the robotic assembly process to ensure structural stability involves a time-consuming offline process due to the combinatorial nature of its search space. However, the precomputed assembly plans may get disrupted during real-world execution due to unforeseen changes, such as setup modifications or delays in part delivery. Recomputing these plans using traditional offline methods results in significant project delays. Therefore, we propose a reinforcement learning-based approach in which a neural network is trained to efficiently generate alternative assembly plans for a given structure online, enabling adaptation to external changes. To enable effective and efficient training, we introduce three key innovations: a GPU-based stability simulator for parallelizing simulations, a novel curriculum-based training scheme to address sparse rewards during training, and a new graph neural network architecture for efficiently encoding assembly geometry. We validate our approach by training reinforcement learning agents on various assemblies and evaluating their performance on unseen assembly tasks. Furthermore, we demonstrate the effectiveness of our framework in planning multi-robot assembly processes, effectively handling disruptions in both simulation and physical environments.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "17296e1d53",
    "authors": "Fengqi LIU, Longji Huang, Zhengyu Huang, Zeyu Wang",
    "title": "Learning to Draw Is Learning to See: Analyzing Eye Tracking Patterns for Assisted Observational Drawing",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730734",
    "pdf_link": null,
    "abstract": "Drawing is an artistic process involving extensive observation. Understanding how professional artists observe as they draw has significant value because it offers insight into their perception patterns and acquired skills. While previous studies used eye tracking to analyze the drawing process, they fell short in aligning gaze data with drawing actions due to the spatial and temporal gaps between observation and drawing in a model-to-paper setup. This paper presents a study in an image-to-image setup, in which artists observe a reference image and draw on a blank canvas on a tablet, capturing a clearer mapping between eye movements and drawn strokes. Our analysis demonstrates a strong spatial correlation between observed regions and corresponding strokes. We further find that artists initially follow a more structured region-by-region approach and then switch to a less constrained sequence for details. Based on these findings, we develop an assistive interface that integrates real-time visual guidance from professional artists’ eye tracking data, enabling novices to emulate their observation and drawing strategies. A user study shows that novices can draw significantly more accurate shapes using our assistive interface, highlighting the importance of modeling observation and the potential of leveraging eye tracking data in future educational and creativity support tools. Our datasets, analysis code, and assistive interface are available at https://github.com/CISLab-HKUST/Learning-to-Draw-Is-Learning-to-See.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "5f970751e1",
    "authors": "Yue Chang, Mengfei Liu, Zhecheng Wang, Peter Yichen Chen, Eitan Grinspun",
    "title": "Lifting the Winding Number: Precise Discontinuities in Neural Fields for Physics Simulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730597",
    "pdf_link": null,
    "abstract": "Cutting thin-walled deformable structures is common in daily life, but poses significant challenges for simulation due to the introduced spatial discontinuities. Traditional methods rely on mesh-based domain representations, which require frequent remeshing and refinement to accurately capture evolving discontinuities. These challenges are further compounded in reduced-space simulations, where the basis functions are inherently geometry- and mesh-dependent, making it difficult or even impossible for the basis to represent the diverse family of discontinuities introduced by cuts.Recent advances in representing basis functions with neural fields offer a promising alternative, leveraging their discretization-agnostic nature to represent deformations across varying geometries. However, the inherent continuity of neural fields is an obstruction to generalization, particularly if discontinuities are encoded in neural network weights.We present Wind Lifter, a novel neural representation designed to accurately model complex cuts in thin-walled deformable structures. Our approach constructs neural fields that reproduce discontinuities precisely at specified locations, without “baking in” the position of the cut line. To achieve this, we augment the input coordinates of the neural field with the generalized winding number of any given cut line, effectively lifting the input from two to three dimensions. Lifting allows the network to focus on the easier problem of learning a 3D everywhere-continuous volumetric field, while a corresponding restriction operator enables the final output field to precisely resolve strict discontinuities. Crucially, our approach does not embed the discontinuity in the neural network’s weights, opening avenues to generalization of cut placement.Our method achieves real-time simulation speeds and supports dynamic updates to cut line geometry during the simulation. Moreover, the explicit representation of discontinuities makes our neural field intuitive to control and edit, offering a significant advantage over traditional neural fields, where discontinuities are embedded within the network’s weights, and enabling new applications that rely on general cut placement.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "520320208a",
    "authors": "Minseok Chae, Chun Chen, Seung-Woo Nam, Yoonchan Jeong",
    "title": "Light Pipe Holographic Display: Bandwidth-preserved Kaleidoscopic Guiding for AR Glasses",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731429",
    "pdf_link": null,
    "abstract": "In this paper, we present a holographic display using a light pipe for augmented reality, and the hologram rendering method via bandwidth-preserved kaleidoscopic guiding method. Conventional augmented reality displays typically share optical architectures where the light engine and image combiner are adjacent. Minimizing the size of both components is highly challenging, and most commercial and research prototypes of augmented reality displays are bulky, front-heavy and sight-obstructing. Here, we propose the use of light pipe to decouple and spatially reposition the light engine from the image combiner, enabling a pragmatic glasses-type design. Through total internal reflection, light pipes have an advantage in guiding the full angular bandwidth regardless of its length. By modeling such kaleidoscopic guiding of the wavefront inside the light pipe and applying it to holographic image generation, we successfully separate the light engine from the image combiner, making the front of the device clear and lightweight. We experimentally validate that the proposed light pipe system delivers virtual images with high-quality and 3D depth cues. We further present a method to simulate and compensate for light pipe misalignment, enhancing the robustness and practicality of the proposed system.",
    "scholar_publication": "ACM Transactions on Graphics …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "d0105123af",
    "authors": "Nadav Magar, Amir Hertz, Eric Tabellion, Yael Pritch, Alex Rav-Acha, Ariel Shamir, Yedid Hoshen",
    "title": "LightLab: Controlling Light Sources in Images with Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730696",
    "pdf_link": null,
    "abstract": "We present a simple, yet effective diffusion-based method for fine-grained, parametric control over light sources in an image. Existing relighting methods either rely on multiple input views to perform inverse rendering at inference time, or fail to provide explicit control over light changes. Our method fine-tunes a diffusion model on a small set of real raw photograph pairs, supplemented by synthetically rendered images at scale, to elicit its photorealistic prior for the relighting task. We leverage the linearity of light to synthesize image pairs depicting controlled light changes of either a target light source or ambient illumination. Using this data and an appropriate fine-tuning scheme, we train a model for precise illumination changes with explicit control over light intensity and color. Lastly, we show how our method can achieve compelling light editing results, and outperforms existing methods based on user preference.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "f22ff2ae40",
    "authors": "Jiong Chen, Florian Schäfer, Mathieu DESBRUN",
    "title": "Lightning-fast Boundary Element Method",
    "paper_url": "https://inria.hal.science/hal-05063377/",
    "pdf_link": null,
    "abstract": "Boundary element methods (BEM) for solving linear elliptic partial differential equations have gained traction in a wide range of graphics applications: they eliminate the need for volumetric meshing by solving for variables exclusively on the domain boundary through a linear boundary integral equation (BIE). However, BEM often generate dense and ill-conditioned linear systems that lead to poor computational scalability and substantial memory demands for large-scale problems, limiting their applicability and efficiency in practice. In this paper, we address these limitations by generalizing the Kaporin-based approach to asymmetric preconditioning: we construct a sparse approximation of the inverse-LU factorization of arbitrary BIE matrices in a massively parallel manner. Our sparse inverse-LU factorization, when employed as a preconditioner for the generalized minimal residual (GMRES) method, significantly enhances the efficiency of BIE solves, often yielding orders-of-magnitude speedups in solving times.",
    "scholar_publication": "ACM Transactions on Graphics, 2025 - inria.hal.science"
  },
  {
    "paper_id": "5065765a97",
    "authors": "Khoa Do, David Coeurjolly, Pooran Memari, Nicolas Bonneel",
    "title": "Linear-Time Transport with Rectified Flows",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731147",
    "pdf_link": null,
    "abstract": "Matching probability distributions allows to compare or interpolate them, or model their manifold. Optimal transport is a tool that solves this matching problem. However, despite the development of numerous exact and approximate algorithms, these approaches remain too slow for large datasets due to the inherent challenge of optimizing transport plans. Taking intuitions from recent advances in rectified flows we propose an algorithm that, while not resulting in optimal transport plans, produces transport plans from uniform densities to densities stored on grids that resemble the optimal ones in practice. Our algorithm has linear-time complexity with respect to the problem size and is embarrassingly parallel. It is also trivial to implement, essentially computing three summed-area tables and advecting particles with velocities easily computed from these tables using simple arithmetic. This already allows for applications such as stippling and area-preserving mesh parameterization. Combined with linearized transport ideas, we further extend our approach to match two non-uniform distributions. This allows for wider applications such as shape interpolation or barycenters, matching the quality of more complex optimal or approximate transport solvers while resulting in orders of magnitude speedups. We illustrate our applications in 2D and 3D.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "0bbd61e36d",
    "authors": "Jungnam Park, Euikyun Jung, Jehee Lee, Jungdam Won",
    "title": "MAGNET: Muscle Activation Generation Networks for Diverse Human Movement",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730617",
    "pdf_link": null,
    "abstract": "We introduce MAGNET (Muscle Activation Generation Networks), a scalable framework for reconstructing full-body muscle activations across diverse human movements. Our approach employs musculoskeletal simulation with a novel two-level controller architecture trained using three-stage learning methods. Additionally, we develop distilled models tailored for solving downstream tasks or generating real-time muscle activations, even on edge devices. The efficacy of our framework is demonstrated through examples of daily life and challenging behaviors, as well as comprehensive evaluations.",
    "scholar_publication": "Proceedings of the Special Interest Group …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "916bd5a65e",
    "authors": "Changhao Li, Yu Xin, Xiaowei Zhou, Ariel Shamir, Hao Zhang, Ligang Liu, Ruizhen Hu",
    "title": "MASH: Masked Anchored SpHerical Distances for 3D Shape Representation and Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730610",
    "pdf_link": null,
    "abstract": "We introduce Masked Anchored SpHerical Distances (MASH), a novel multi-view and parametrized representation of 3D shapes. Inspired by multi-view geometry and motivated by the importance of perceptual shape understanding for learning 3D shapes, MASH represents a 3D shape as a collection of observable local surface patches, each defined by a spherical distance function emanating from an anchor point. We further leverage the compactness of spherical harmonics to encode the MASH functions, combined with a generalized view cone with a parameterized base that masks the spatial extent of the spherical function to attain locality. We develop a differentiable optimization algorithm capable of converting any point cloud into a MASH representation accurately approximating ground-truth surfaces with arbitrary geometry and topology. Extensive experiments demonstrate that MASH is versatile for multiple applications including surface reconstruction, shape generation, completion, and blending, achieving superior performance thanks to its unique representation encompassing both implicit and explicit features. More information and resources can be found at: https://chli.top/MASH.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "4f8b6645cf",
    "authors": "Chunlei Li, Peng Yu, Tiantian Liu, Siyuan Yu, Yuting Xiao, Shuai Li, Aimin Hao, Yang Gao, Qinping Zhao",
    "title": "MGPBD: A Multigrid Accelerated Global XPBD Solver",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730720",
    "pdf_link": null,
    "abstract": "We introduce a novel Unsmoothed Aggregation (UA) Algebraic Multigrid (AMG) method combined with Preconditioned Conjugate Gradient (PCG) to overcome the limitations of Extended Position-Based Dynamics (XPBD) in high-resolution and high-stiffness simulations. While XPBD excels in simulating deformable objects due to its speed and simplicity, its nonlinear Gauss-Seidel (GS) solver often struggles with low-frequency errors, leading to instability and stalling issues, especially in high-resolution, high-stiffness simulations. Our multigrid approach addresses these issues efficiently by leveraging AMG. To reduce the computational overhead of traditional AMG, where prolongator construction can consume up to two-thirds of the runtime, we propose a lazy setup strategy that reuses prolongators across iterations based on matrix structure and physical significance. Furthermore, we introduce a simplified method for constructing near-kernel components by applying a few sweeps of iterative methods to the homogeneous equation, achieving convergence rates comparable to adaptive smoothed aggregation (adaptive-SA) at a lower computational cost. Experimental results demonstrate that our method significantly improves convergence rates and numerical stability, enabling efficient and stable high-resolution simulations of deformable objects.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "205469a90e",
    "authors": "Tianyang Xue, Longdu Liu, Lin Lu, Paul Henderson, Pengbin Tang, Haochen Li, Jikai Liu, Haisen Zhao, Hao Peng, Bernd Bickel",
    "title": "MIND: Microstructure INverse Design with Generative Hybrid Neural Representation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730682",
    "pdf_link": null,
    "abstract": "The inverse design of microstructures plays a pivotal role in optimizing metamaterials with specific, targeted physical properties. While traditional forward design methods are constrained by their inability to explore the vast combinatorial design space, inverse design offers a compelling alternative by directly generating structures that fulfill predefined performance criteria. However, achieving precise control over both geometry and material properties remains a significant challenge due to their intricate interdependence. Existing approaches, which typically rely on voxel or parametric representations, often limit design flexibility and structural diversity.In this work, we present a novel generative model that integrates latent diffusion with Holoplane, an advanced hybrid neural representation that simultaneously encodes both geometric and physical properties. This combination ensures superior alignment between geometry and properties. Our approach generalizes across multiple microstructure classes, enabling the generation of diverse, tileable microstructures with significantly improved property accuracy and enhanced control over geometric validity, surpassing the performance of existing methods. We introduce a multi-class dataset encompassing a variety of geometric morphologies, including truss, shell, tube, and plate structures, to train and validate our model. Experimental results demonstrate the model’s ability to generate microstructures that meet target properties, maintain geometric validity, and integrate seamlessly into complex assemblies. Additionally, we explore the potential of our framework through the generation of new microstructures, cross-class interpolation, and the infilling of heterogeneous microstructures. Code and data for this paper are at https://github.com/TimHsue/MIND.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "c328092a8d",
    "authors": "Mamoru Akiyoshi, Ayumu Sato, Suguru Saito",
    "title": "Manu-Grid: UI for Parameter Estimation of Tilt Angle Extended Generalized Projection Function in Illustrations",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743003",
    "pdf_link": null,
    "abstract": "Compositing a background image and a foreground image produced from a 3D object requires a projection function that ensures consistency in the scene. We modified the generalized projection of [Yoshimura and Saito ] to allow tilted-angle images and introduced a user interface, Manu-Grid, to estimate the parameters of the projection function corresponding to the drawing method used in a background image. The interface has a useful characteristic that when a user manipulates a vanishing direction, a vanishing line, and a reference point on the ground, the others are pinned.",
    "scholar_publication": "Proceedings of the Special Interest Group …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "23683431fc",
    "authors": "Michael Birsak, John Femiani, Biao Zhang, Peter Wonka",
    "title": "MatCLIP: Light- and Shape-Insensitive Assignment of PBR Material Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730740",
    "pdf_link": null,
    "abstract": "Assigning realistic materials to 3D models remains a significant challenge in computer graphics. We propose MatCLIP, a novel method that extracts shape- and lighting-insensitive descriptors of Physically Based Rendering (PBR) materials to assign plausible textures to 3D objects based on images, such as the output of Latent Diffusion Models (LDMs) or photographs. Matching PBR materials to static images is challenging because the PBR representation captures the dynamic appearance of materials under varying viewing angles, shapes, and lighting conditions. By extending an Alpha-CLIP-based model on material renderings across diverse shapes and lighting, and encoding multiple viewing conditions for PBR materials, our approach generates descriptors that bridge the domains of PBR representations with photographs or renderings, including LDM outputs. This enables consistent material assignments without requiring explicit knowledge of material relationships between different parts of an object. MatCLIP achieves a top-1 classification accuracy of 76.6%, outperforming state-of-the-art methods such as PhotoShape and MatAtlas by over 15 percentage points on publicly available datasets. Our method can be used to construct material assignments for 3D shape datasets such as ShapeNet, 3DCoMPaT++, and Objaverse. All code and data will be released at https://birsakm.github.io/matclip/.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "6b4961f01c",
    "authors": "Xiaohe Ma, Valentin Deschaintre, Milos Hasan, Fujun Luan, Kun Zhou, Hongzhi Wu, Yiwei Hu",
    "title": "MaterialPicker: Multi-Modal DiT-Based Material Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731199",
    "pdf_link": null,
    "abstract": "High-quality material generation is key for virtual environment authoring and inverse rendering. We propose MaterialPicker, a multi-modal material generator leveraging a Diffusion Transformer (DiT) architecture, improving and simplifying the creation of high-quality materials from text prompts and/or photographs. Our method can generate a material based on an image crop of a material sample, even if the captured surface is distorted, viewed at an angle or partially occluded, as is often the case in photographs of natural scenes. We further allow the user to specify a text prompt to provide additional guidance for the generation. We finetune a pre-trained DiT-based video generator into a material generator, where each material map is treated as a frame in a video sequence. We evaluate our approach both quantitatively and qualitatively and show that it enables more diverse material generation and better distortion correction than previous work.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "3f5899f94c",
    "authors": "Ana Dodik, Isabella Yu, Kartik Chandra, Jonathan Ragan-Kelley, Joshua Tenenbaum, Vincent Sitzmann, Justin Solomon",
    "title": "Meschers: Geometry Processing of Impossible Objects",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731422",
    "pdf_link": null,
    "abstract": "Impossible objects, geometric constructions that humans can perceive but that cannot exist in real life, have been a topic of intrigue in visual arts, perception, and graphics, yet no satisfying computer representation of such objects exists. Previous work embeds impossible objects in 3D, cutting them or twisting/bending them in the depth axis. Cutting an impossible object changes its local geometry at the cut, which can hamper downstream graphics applications, such as smoothing, while bending makes it difficult to relight the object. Both of these can invalidate geometry operations, such as distance computation. As an alternative, we introduce Meschers, meshes capable of representing impossible constructions akin to those found in M.C. Escher's woodcuts. Our representation has a theoretical foundation in discrete exterior calculus and supports the use-cases above, as we demonstrate in a number of example applications. Moreover, because we can do discrete geometry processing on our representation, we can inverse-render impossible objects. We also compare our representation to cut and bend representations of impossible objects.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "1bd1b2ba5a",
    "authors": "Federico Sichetti, Enrico Puppo, Zizhou Huang, Marco Attene, Denis Zorin, Daniele Panozzo",
    "title": "MiSo: A DSL for robust and efficient MINIMIZE and SOLVE problems",
    "paper_url": "https://ieeexplore.ieee.org/abstract/document/4663943/",
    "pdf_link": null,
    "abstract": "… VU CIC AND BOCHE: ROBUST QoS-CONSTRAINED OPTIMIZATION … The main problem of interest in this paper is the transceiver optimization with the goal of minimizing the total …",
    "scholar_publication": "IEEE Transactions on Signal Processing, 2008 - ieeexplore.ieee.org"
  },
  {
    "paper_id": "54852a3ad5",
    "authors": "Sean Hardesty Lewis",
    "title": "Minecraft to 3D: A Pipeline for High-Fidelity Reconstruction of Minecraft Worlds",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743044",
    "pdf_link": null,
    "abstract": "We introduce Minecraft to 3D, a novel pipeline that automatically converts any Minecraft world into a high-quality polygonal scene. A 3D convolutional network recognises Minecraft’s default objects, the block surface is resampled into a smooth height‑map, and each recognised object is substituted with a high‑quality 3D model chosen from an external library. Object locations, orientations, and tags are preserved, a separate water plane is exported for engine‑level ocean rendering, and the final scene opens natively in modern 3D engines. The pipeline processes a one‑square‑kilometre world in under three minutes on a single consumer GPU, enabling educators, indie developers, and artists to move rapidly from voxel sketches to fully lit environments.",
    "scholar_publication": "Proceedings of the Special Interest Group on Computer …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "9e12358b51",
    "authors": "Jiaju Ma, Maneesh Agrawala",
    "title": "MoVer: Motion Verification for Motion Graphics Animations",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731209",
    "pdf_link": null,
    "abstract": "While large vision-language models can generate motion graphics animations from text prompts, they regularly fail to include all spatio-temporal properties described in the prompt. We introduce MoVer, a motion verification DSL based on first-order logic that can check spatio-temporal properties of a motion graphics animation. We identify a general set of such properties that people commonly use to describe animations (e.g., the direction and timing of motions, the relative positioning of objects, etc.). We implement these properties as predicates in MoVer and provide an execution engine that can apply a MoVer program to any input SVG-based motion graphics animation. We then demonstrate how MoVer can be used in an LLM-based synthesis and verification pipeline for iteratively refining motion graphics animations. Given a text prompt, our pipeline synthesizes a motion graphics animation and a corresponding MoVer program. Executing the verification program on the animation yields a report of the predicates that failed and the report can be automatically fed back to LLM to iteratively correct the animation. To evaluate our pipeline, we build a synthetic dataset of 5600 text prompts paired with ground truth MoVer verification programs. We find that while our LLM-based pipeline is able to automatically generate a correct motion graphics animation for 58.8% of the test prompts without any iteration, this number raises to 93.6% with up to 50 correction iterations. Our code and dataset are at https://mover-dsl.github.io.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "8f19b3096a",
    "authors": "Xiuli Bi, Jianfei Yuan, Bo Liu, Yong Zhang, Xiaodong Cun, Chi-Man Pun, Bin Xiao",
    "title": "Mobius: Text to Seamless Looping Video Generation via Latent Shift",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730744",
    "pdf_link": null,
    "abstract": "We present Mobius, a novel method to generate seamlessly looping videos from text descriptions directly without any user annotations, thereby creating new visual materials for the multi-media presentation. Our method repurposes the pre-trained video latent diffusion model for generating looping videos from text prompts without any training. During inference, we first construct a latent cycle by connecting the starting and ending noise of the videos. Given that the temporal consistency can be maintained by the context of the video diffusion model, we perform multi-frame latent denoising by gradually shifting the first-frame latent to the end in each step. As a result, the denoising context varies in each step while maintaining consistency throughout the inference process. Moreover, the latent cycle in our method can be of any length. This extends our latent-shifting approach to generate seamless looping videos beyond the scope of the video diffusion model’s context. Unlike previous cinemagraphs, the proposed method does not require an image as appearance, which will restrict the motions of the generated results. Instead, our method can produce more dynamic motion and better visual quality. We conduct multiple experiments and comparisons to verify the effectiveness of the proposed method, demonstrating its efficacy in different scenarios. All the code will be made available.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "b6b41d3d0e",
    "authors": "Yifang Pan, Karan Singh, Luiz Gustavo Hafemann",
    "title": "Model See Model Do: Speech-Driven Facial Animation with Style Control",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730672",
    "pdf_link": null,
    "abstract": "Speech-driven 3D facial animation plays a key role in applications such as virtual avatars, gaming, and digital content creation. While existing methods have made significant progress in achieving accurate lip synchronization and generating basic emotional expressions, they often struggle to capture and effectively transfer nuanced performance styles. We propose a novel example-based generation framework that conditions a latent diffusion model on a reference style clip to produce highly expressive and temporally coherent facial animations. To address the challenge of accurately adhering to the style reference, we introduce a novel conditioning mechanism called style basis, which extracts key poses from the reference and additively guides the diffusion generation process to fit the style without compromising lip synchronization quality. This approach enables the model to capture subtle stylistic cues while ensuring that the generated animations align closely with the input speech. Extensive qualitative, quantitative, and perceptual evaluations demonstrate the effectiveness of our method in faithfully reproducing the desired style while achieving superior lip synchronization across various speech scenarios.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "3a965ecf10",
    "authors": "Venkataram Edavamadathil Sivaram, Ravi Ramamoorthi, Tzu-Mao Li",
    "title": "Modeling and Rendering Glow Discharge",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730674",
    "pdf_link": null,
    "abstract": "Previous research in material models for surface and volume scattering has enabled highly realistic scenes in modern rendering systems. However, there has been comparatively little study of light sources in computer graphics despite their critical importance in illuminating and bringing life into these scenes. In the real world, photons are emitted through numerous physical processes including combustion, incandescence, and fluorescence. The qualities of light produced in each of these processes are unique to their physics, making them interesting to study individually.In this work, we propose a model for glow discharge, a form of light-emitting electrostatic discharge commonly found in Neon lights and gas discharge lamps. We take inspiration from works in computational physics and develop an efficient point-wise solver for the emission due to glow discharge suitable for traditional volume rendering systems. Our model distills the complex mechanics of this process into a set of flexible and interpretable parameters. We demonstrate that our model can replicate the visual qualities of glow discharge under varying gases.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "b948b52a8e",
    "authors": "Markus Worchel, Marc Alexa",
    "title": "Moment Bounds are Differentiable: Efficiently Approximating Measures in Inverse Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730899",
    "pdf_link": null,
    "abstract": "All rendering methods aim at striking a balance between realism and efficiency. This is particularly relevant for differentiable rendering, where the additional aspect of differentiablity w.r.t. scene parameters causes increased computational complexity while, on the other hand, in the common application of inverse rendering, the diverse effects of real image formation must be faithfully reproduced. An important effect in rendering is the attenuation of light as it travels through different media (visibility, shadows, transmittance, transparency). This can be modeled as an integral over non-negative functions and has been successfully approximated in forward rendering by so-called moments. We show that moment-based approximations are differentiable in the parameters defining the moments, and that this leads to efficient and practical methods for inverse rendering. In particular, we demonstrate the method at the examples of shadow mapping and visibility in volume rendering, leading to approximations that are similar in efficiency to existing ad-hoc techniques while being significantly more accurate.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "d14173571a",
    "authors": "Niladri Shekhar Dutt, Duygu Ceylan, Niloy Mitra",
    "title": "MonetGPT: Solving Puzzles Enhances MLLMs’ Image Retouching Skills",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730926",
    "pdf_link": null,
    "abstract": "Retouching is an essential task in post-manipulation of raw photographs. Generative editing, guided by text or strokes, provides a new tool accessible to users but can easily change the identity of the original objects in unacceptable and unpredictable ways. In contrast, although traditional procedural edits, as commonly supported by photoediting tools (eg, Gimp, Lightroom), are conservative, they are still preferred by professionals. Unfortunately, professional quality retouching involves many individual procedural editing operations that …",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "f12a13dcce",
    "authors": "Songyin Wu, Zhaoyang Lv, Yufeng Zhu, Duncan Frost, Zhengqin Li, Ling-Qi Yan, Carl Ren, Richard Newcombe, Zhao Dong",
    "title": "Monocular Online Reconstruction with Enhanced Detail Preservation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730659",
    "pdf_link": null,
    "abstract": "We propose an online 3D Gaussian-based dense mapping framework for photorealistic details reconstruction from a monocular image stream. Our approach addresses two key challenges in monocular online reconstruction: distributing Gaussians without relying on depth maps and ensuring both local and global consistency in the reconstructed maps. To achieve this, we introduce two key modules: the Hierarchical Gaussian Management Module for effective Gaussian distribution and the Global Consistency Optimization Module for maintaining alignment and coherence at all scales. In addition, we present the Multi-level Occupancy Hash Voxels (MOHV), a structure that regularizes Gaussians for capturing details across multiple levels of granularity. MOHV ensures accurate reconstruction of both fine and coarse geometries and textures, preserving intricate details while maintaining overall structural integrity. Compared to state-of-the-art RGB-only and even RGB-D methods, our framework achieves superior reconstruction quality with high computational efficiency. Moreover, it integrates seamlessly with various tracking systems, ensuring generality and scalability. Project page: https://poiw.github.io/MODP/.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "41da2d899b",
    "authors": "Bailey Miller, Rohan Sawhney, Keenan Crane, Ioannis Gkioulekas",
    "title": "Monte Carlo PDE simulation in participating media",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731152",
    "pdf_link": null,
    "abstract": "… problem setting of PDE simulation in participating media. We … algorithms simulate expected light transport in participating media… in PDE simulation, by developing volumetric Monte Carlo …",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "c5ad20deac",
    "authors": "Naoki Agata, Takeo Igarashi",
    "title": "Motion Control via Metric-Aligning Motion Matching",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730665",
    "pdf_link": null,
    "abstract": "We introduce a novel method for controlling a motion sequence using an arbitrary temporal control sequence using temporal alignment. Temporal alignment of motion has gained significant attention owing to its applications in motion control and retargeting. Traditional methods rely on either learned or hand-craft cross-domain mappings between frames in the original and control domains, which often require large, paired, or annotated datasets and time-consuming training. Our approach, named Metric-Aligning Motion Matching, achieves alignment by solely considering within-domain distances. It computes distances among patches in each domain and seeks a matching that optimally aligns the two within-domain distances. This framework allows for the alignment of a motion sequence to various types of control sequences, including sketches, labels, audio, and another motion sequence, all without the need for manually defined mappings or training with annotated data. We demonstrate the effectiveness of our approach through applications in efficient motion control, showcasing its potential in practical scenarios.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "6e3de0b6c6",
    "authors": "Luozhou Wang, Ziyang Mai, Guibao Shen, Yixun Liang, Xin Tao, Pengfei Wan, Di Zhang, Yijun Li, Yingcong Chen",
    "title": "Motion Inversion for Video Customization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730735",
    "pdf_link": null,
    "abstract": "In this work, we present a novel approach for motion customization in video generation, addressing the widespread gap in the exploration of motion representation within video generative models. Recognizing the unique challenges posed by the spatiotemporal nature of video, our method introduces Motion Embeddings, a set of explicit, temporally coherent embeddings derived from a given video. These embeddings are designed to integrate seamlessly with the temporal transformer modules of video diffusion models, modulating self-attention computations across frames without compromising spatial integrity. Our approach provides a compact and efficient solution to motion representation, utilizing two types of embeddings: a Motion Query-Key Embedding to modulate the temporal attention map and a Motion Value Embedding to modulate the attention values. Additionally, we introduce an inference strategy that excludes spatial dimensions from the Motion Query-Key Embedding and applies a debias operation to the Motion Value Embedding, both designed to debias appearance and ensure the embeddings focus solely on motion. Our contributions include the introduction of a tailored motion embedding for customization tasks and a demonstration of the practical advantages and effectiveness of our method through extensive experiments. Project page: https://wileewang.github.io/MotionInversion/",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "c8932159ef",
    "authors": "Bohong Chen, Yumeng Li, Youyi Zheng, Yao-Xiang Ding, Kun Zhou",
    "title": "Motion-example-controlled Co-speech Gesture Generation Leveraging Large Language Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730611",
    "pdf_link": null,
    "abstract": "The automatic generation of controllable co-speech gestures has recently gained growing attention. While existing systems typically achieve gesture control through predefined categorical labels or implicit pseudo-labels derived from motion examples, these approaches often compromise the rich details present in the original motion examples. We present MECo, a framework for motion-example-controlled co-speech gesture generation by leveraging large language models (LLMs). Our method capitalizes on LLMs’ comprehension capabilities through fine-tuning to simultaneously interpret speech audio and motion examples, enabling the synthesis of gestures that preserve example-specific characteristics while maintaining speech congruence. Departing from conventional pseudo-labeling paradigms, we position motion examples as explicit query contexts within the prompt structure to guide gesture generation. Experimental results demonstrate state-of-the-art performance across three metrics: Fréchet Gesture Distance (FGD), motion diversity, and example-gesture similarity. Furthermore, our framework enables granular control of individual body parts and accommodates diverse input modalities including motion clips, static poses, human video sequences, and textual descriptions.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "0577d506fc",
    "authors": "Jinbo Xing, Long Mai, Cusuh Ham, Jiahui Huang, Aniruddha Mahapatra, Chi-Wing Fu, Tien-Tsin Wong, Feng Liu",
    "title": "MotionCanvas: Cinematic Shot Design with Controllable Image-to-Video Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730604",
    "pdf_link": null,
    "abstract": "This paper presents a method that allows users to design cinematic video shots in the context of image-to-video generation. Shot design, a critical aspect of filmmaking, involves meticulously planning both camera movements and object motions in a scene. However, enabling intuitive shot design in modern image-to-video generation systems presents two main challenges: first, effectively capturing user intentions on the motion design, where both camera movements and scene-space object motions must be specified jointly; and second, representing motion information that can be effectively utilized by a video diffusion model to synthesize the image animations. To address these challenges, we introduce MotionCanvas, a method that integrates user-driven controls into image-to-video (I2V) generation models, allowing users to control both object and camera motions in a scene-aware manner. By connecting insights from classical computer graphics and contemporary video generation techniques, we demonstrate the ability to achieve 3D-aware motion control in I2V synthesis without requiring costly 3D-related training data. MotionCanvas enables users to intuitively depict scene-space motion intentions, and translates them into spatiotemporal motion-conditioning signals for video diffusion models. We demonstrate the effectiveness of our method on a wide range of real-world image content and shot-design scenarios, highlighting its potential to enhance the creative workflows in digital content creation and adapt to various image and video editing applications. Code and model weights are at https://motion-canvas25.github.io",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ed40bbbb22",
    "authors": "Pascal Guehl, Rémi Allègre, Guillaume Gilet, Basile Sauvage, Marie-Paule Cani, Jean-Michel Dischler",
    "title": "Multi-Dimensional Procedural Wave Noise",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730928",
    "pdf_link": null,
    "abstract": "While precise spectral control can be achieved through sparse convolution, corresponding state of the art noise models are typically too expensive for solid noise. We introduce an alternative, wave-based procedural noise model, fast enough to be used in any dimension. We express the noise in the spectral domain and then apply an inverse Fourier transform (FT), requiring the computation of a multidimensional integral. Our contribution is a novel, efficient way to perform this computation, using a sum of precomputed complex-valued hyperplanar wave-functions, oriented in random directions. We show that using suitable wave profiles and combination operators, our model is able to extend to 3D a number of Gaussian and non-Gaussian noises, including Gabor, by-example and Phasor noises, as well as generate novel cellular noises. Our versatile and controllable solid noise model is very compact, a key feature for complex power spectrum and animated noises. We illustrate this through the design of 2D, 3D, and 3D+t materials using color, transparency and style transfer functions.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "268d8243d7",
    "authors": "Wenning Xu, Shiyu Fan, Paul Henderson, Edmond S. L. Ho",
    "title": "Multi-Person Interaction Generation from Two-Person Motion Priors",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730688",
    "pdf_link": null,
    "abstract": "Generating realistic human motion with high-level controls is a crucial task for social understanding, robotics, and animation. With high-quality MOCAP data becoming more available recently, a wide range of data-driven approaches have been presented. However, modelling multi-person interactions still remains a less explored area. In this paper, we present Graph-driven Interaction Sampling, a method that can generate realistic and diverse multi-person interactions by leveraging existing two-person motion diffusion models as motion priors. Instead of training a new model specific to multi-person interaction synthesis, our key insight is to spatially and temporally separate complex multi-person interactions into a graph structure of two-person interactions, which we name the Pairwise Interaction Graph. We thus decompose the generation task into simultaneous single-person motion generation conditioned on one other’s motion. In addition, to reduce artifacts such as interpenetrations of body parts in generated multi-person interactions, we introduce two graph-dependent guidance terms into the diffusion sampling scheme. Unlike previous work, our method can produce various high-quality multi-person interactions without having repetitive individual motions. Extensive experiments demonstrate that our approach consistently outperforms existing methods in reducing artifacts when generating a wide range of two-person and multi-person interactions.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "f122ebad74",
    "authors": "Zhimin Fan, Yiming Wang, Chenxi Zhou, Ling-Qi Yan, Yanwen Guo, Jie Guo",
    "title": "Multiple Importance Reweighting for Path Guiding",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731144",
    "pdf_link": null,
    "abstract": "Contemporary path guiding employs an iterative training scheme to fit radiance distributions. However, existing methods combine the estimates generated in each iteration merely within image space, overlooking differences in the convergence of distribution fitting over individual light paths. This paper formulates the estimation combination task as a path reweighting process. To compute spatio-directional varying combination weights, we propose multiple importance reweighting, leveraging the importance distributions from multiple guiding iterations. We demonstrate that our proposed path-level reweighting makes guiding algorithms less sensitive to noise and overfitting in distributions. This facilitates a finer subdivision of samples both spatially and temporally (i.e., over iterations), which leads to additional improvements in the accuracy of distributions and samples. Inspired by adaptive multiple importance sampling (AMIS), we introduce a simple yet effective mixture-based weighting scheme with theoretically guaranteed consistency, demonstrating good practical performance compared to alternative weighting schemes. To further foster usage with high sample rates, we introduce a hyperparameter that controls the size of sample storage. When this size limit is exceeded, low-valued samples are splatted during rendering and reweighted using a partial mixture of distributions. We found limiting the storage size reduces memory overhead and keeps variance reduction and bias comparable to the unlimited ones. Our method is largely agnostic to the underlying guiding method and compatible with conventional pixel reweighting techniques. Extensive evaluations underscore the feasibility of our approach in various scenes, achieving variance reduction with negligible bias over state-of-the-art solutions within equal sample rates and rendering time.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "177ebdf3fb",
    "authors": "Luchao Qi, Jiaye Wu, Bang Gong, Annie Wang, David Jacobs, Roni Sengupta",
    "title": "MyTimeMachine: Personalized Facial Age Transformation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731172",
    "pdf_link": null,
    "abstract": "Facial aging is a complex process, highly dependent on multiple factors like gender, ethnicity, lifestyle, etc., making it extremely challenging to learn a global aging prior to predict aging for any individual accurately. Existing techniques often produce realistic and plausible aging results, but the re-aged images often do not resemble the person's appearance at the target age and thus need personalization. In many practical applications of virtual aging, e.g. VFX in movies and TV shows, access to a personal photo collection of the user depicting aging in a small time interval (20~40 years) is often available. However, naive attempts to personalize global aging techniques on personal photo collections often fail. Thus, we propose MyTimeMachine (MyTM), a method that combines a global aging prior with a personalized photo collection (ranging from as few as 10 images, ideally 50) to learn individualized age transformations. We introduce a novel Adapter Network that combines personalized aging features with global aging features and generates a re-aged image with StyleGAN2. We also introduce three loss functions to personalize the Adapter Network with personalized aging loss, extrapolation regularization, and adaptive w-norm regularization. Our method demonstrates strong performance on fair-use imagery of widely recognizable individuals, producing photorealistic and identity-consistent age transformations that generalize well across diverse appearances. It also extends naturally to video, delivering high-quality, temporally consistent results that closely resemble actual appearances at target ages—outperforming state-of-the-art approaches.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "b6127fe2c9",
    "authors": "Giulio Viganò, Maks Ovsjanikov, Simone Melzi",
    "title": "NAM: Neural Adjoint Maps for refinement of shape correspondences",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730943",
    "pdf_link": null,
    "abstract": "… of functional maps. Central to our contribution is the concept of Neural Adjoint Maps, a novel neural … of functional maps for estimating correspondence between manifolds. Fostering our …",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "40d6d01f58",
    "authors": "Akshat Dave, Tianyi Zhang, Aaron Young, Ramesh Raskar, Wolfgang Heidrich, Ashok Veeraraghavan, Akshat Dave",
    "title": "NeST: Neural Stress Tensor Tomography by leveraging 3D Photoelasticity",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3723873",
    "pdf_link": null,
    "abstract": "Photoelasticity enables full-field stress analysis in transparent objects through stress-induced birefringence. Existing techniques are limited to two-dimensional (2D) slices and require destructively slicing the object. Recovering the internal three-dimensional (3D) stress distribution of the entire object is challenging, as it involves solving a tensor tomography problem and handling phase wrapping ambiguities. We introduce NeST, an analysis-by-synthesis approach for reconstructing 3D stress tensor fields as neural implicit representations from polarization measurements. Our key insight is to jointly handle phase unwrapping and tensor tomography using a differentiable forward model based on Jones calculus. Our non-linear model faithfully matches real captures, unlike prior linear approximations. We develop an experimental multi-axis polariscope setup to capture 3D photoelasticity and experimentally demonstrate that NeST reconstructs the internal stress distribution for objects with varying shape and force conditions. Additionally, we showcase novel applications in stress analysis, such as visualizing photoelastic fringes by virtually slicing the object and viewing photoelastic fringes from unseen viewpoints. NeST paves the way for scalable non-destructive 3D photoelastic analysis.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "150df23cf4",
    "authors": "Or Patashnik, Rinon Gal, Daniil Ostashev, Sergey Tulyakov, Kfir Aberman, Daniel Cohen-Or",
    "title": "Nested Attention: Semantic-aware Attention Values for Concept Personalization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730634",
    "pdf_link": null,
    "abstract": "Personalizing text-to-image models to generate images of specific subjects across diverse scenes and styles is a rapidly advancing field. Current approaches often face challenges in maintaining a balance between identity preservation and alignment with the input text prompt. Some methods rely on a single textual token to represent a subject, which limits expressiveness, while others employ richer representations but disrupt the model’s prior, diminishing prompt alignment. In this work, we introduce Nested Attention, a novel mechanism that injects a rich and expressive image representation into the model’s existing cross-attention layers. Our key idea is to generate query-dependent subject values, derived from nested attention layers that learn to select relevant subject features for each region in the generated image. We integrate these nested layers into an encoder-based personalization method, and show that they enable high identity preservation while adhering to input text prompts. Our approach is general and can be trained on various domains. Additionally, its prior preservation allows us to combine multiple personalized subjects from different domains in a single image.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "e173b2c0a8",
    "authors": "Qiujie Dong, Huibiao Wen, Rui Xu, Shuangmin Chen, Jiaran Zhou, Shiqing Xin, Changhe Tu, Taku Komura, Wenping Wang",
    "title": "NeurCross: A Neural Approach to Computing Cross Fields for Quad Mesh Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731159",
    "pdf_link": null,
    "abstract": "Quadrilateral mesh generation plays a crucial role in numerical simulations within Computer-Aided Design and Engineering (CAD/E). Producing high-quality quadrangulation typically requires satisfying four key criteria. First, the quadrilateral mesh should closely align with principal curvature directions. Second, singular points should be strategically placed and effectively minimized. Third, the mesh should accurately conform to sharp feature edges. Lastly, quadrangulation results should exhibit robustness against noise and minor geometric variations. Existing methods generally involve first computing a regular cross field to represent quad element orientations across the surface, followed by extracting a quadrilateral mesh aligned closely with this cross field. A primary challenge with this approach is balancing the smoothness of the cross field with its alignment to pre-computed principal curvature directions, which are sensitive to small surface perturbations and often ill-defined in spherical or planar regions. To tackle this challenge, we propose NeurCross, a novel framework that simultaneously optimizes a cross field and a neural signed distance function (SDF), whose zero-level set serves as a proxy of the input shape. Our joint optimization is guided by three factors: faithful approximation of the optimized SDF surface to the input surface, alignment between the cross field and the principal curvature field derived from the SDF surface, and smoothness of the cross field. Acting as an intermediary, the neural SDF contributes in two essential ways. First, it provides an alternative, optimizable base surface exhibiting more regular principal curvature directions for guiding the cross field. Second, we leverage the Hessian matrix of the neural SDF to implicitly enforce cross field alignment with principal curvature directions, thus eliminating the need for explicit curvature extraction. Extensive experiments demonstrate that NeurCross outperforms the state-of-the-art methods in terms of singular point placement, robustness against surface noise and surface undulations, and alignment with principal curvature directions and sharp feature curves.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "a252d072e6",
    "authors": "Liwen Wu, Sai Bi, Zexiang Xu, Hao Tan, Kai Zhang, Fujun Luan, Haolin Lu, Ravi Ramamoorthi",
    "title": "Neural BRDF Importance Sampling by Reparameterization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730679",
    "pdf_link": null,
    "abstract": "Neural bidirectional reflectance distribution functions (BRDFs) have emerged as popular material representations for enhancing realism in physically-based rendering. Yet their importance sampling remains a significant challenge. In this paper, we introduce a reparameterization-based formulation of neural BRDF importance sampling that seamlessly integrates into the standard rendering pipeline with precise generation of BRDF samples. The reparameterization-based formulation transfers the distribution learning task to a problem of identifying BRDF integral substitutions. In contrast to previous methods that rely on invertible networks and multi-step inference to reconstruct BRDF distributions, our model removes these constraints, which offers greater flexibility and efficiency. Our variance and performance analysis demonstrates that our reparameterization method achieves the best variance reduction in neural BRDF renderings while maintaining high inference speeds compared to existing baselines.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "e2c17bd01f",
    "authors": "Tao Liu, Tianyu Zhang, Yongxue Chen, Weiming Wang, Yu Jiang, Yuming Huang, Charlie C.L. Wang",
    "title": "Neural Co-Optimization of Structural Topology, Manufacturable Layers, and Path Orientations for Fiber-Reinforced Composites",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730922",
    "pdf_link": null,
    "abstract": "We propose a neural network-based computational framework for the simultaneous optimization of structural topology, curved layers, and path orientations to achieve strong anisotropic strength in fiber-reinforced thermoplastic composites while ensuring manufacturability. Our framework employs three implicit neural fields to represent geometric shape, layer sequence, and fiber orientation. This enables the direct formulation of both design and manufacturability objectives - such as anisotropic strength, structural volume, machine motion control, layer curvature, and layer thickness - into an integrated and differentiable optimization process. By incorporating these objectives as loss functions, the framework ensures that the resultant composites exhibit optimized mechanical strength while remaining its manufacturability for filament-based multi-axis 3D printing across diverse hardware platforms. Physical experiments demonstrate that the composites generated by our co-optimization method can achieve an improvement of up to 33.1% in failure loads compared to composites with sequentially optimized structures and manufacturing sequences.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "39b7cebaec",
    "authors": "Pedro Figueiredo, Qihao He, Steve Bako, Nima Khademi Kalantari",
    "title": "Neural Importance Sampling of Many Lights",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730754",
    "pdf_link": null,
    "abstract": "We propose a neural approach for estimating spatially varying light selection distributions to improve importance sampling in Monte Carlo rendering, particularly for complex scenes with many light sources. Our method uses a neural network to predict the light selection distribution at each shading point based on local information, trained by minimizing the KL-divergence between the learned and target distributions in an online manner. To efficiently manage hundreds or thousands of lights, we integrate our neural approach with light hierarchy techniques, where the network predicts cluster-level distributions and existing methods sample lights within clusters. Additionally, we introduce a residual learning strategy that leverages initial distributions from existing techniques, accelerating convergence during training. Our method achieves superior performance across diverse and challenging scenes.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "3b3ea5d9b7",
    "authors": "Gilles Daviet, Tianchang Shen, Nicholas Sharp, David Levin, Gilles Daviet",
    "title": "Neurally Integrated Finite Elements for Differentiable Elasticity on Evolving Domains",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3727874",
    "pdf_link": null,
    "abstract": "We present an elastic simulator for domains defined as evolving implicit functions, which is efficient, robust, and differentiable with respect to both shape and material. This simulator is motivated by applications in 3D reconstruction: it is increasingly effective to recover geometry from observed images as implicit functions, but physical applications require accurately simulating and optimizing-for the behavior of such shapes under deformation, which has remained challenging. Our key technical innovation is to train a small neural network to fit quadrature points for robust numerical integration on implicit grid cells. When coupled with a Mixed Finite Element formulation, this yields a smooth, fully differentiable simulation model connecting the evolution of the underlying implicit surface to its elastic response. We demonstrate the efficacy of our approach on forward simulation of implicits, direct simulation of 3D shapes during editing, and novel physics-based shape and topology optimizations in conjunction with differentiable rendering.",
    "scholar_publication": "ACM Transactions on Graphics, 2025 - dl.acm.org"
  },
  {
    "paper_id": "edb90fdc8e",
    "authors": "Peter Michael, Zekun Hao, Serge Belongie, Abe Davis, Peter Michael",
    "title": "Noise-Coded Illumination for Forensic and Photometric Video Analysis",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3742892",
    "pdf_link": null,
    "abstract": "The proliferation of advanced tools for manipulating video has led to an arms race, pitting those who wish to sow disinformation against those who want to detect and expose it. Unfortunately, time favors the ill-intentioned in this race, with fake videos growing increasingly difficult to distinguish from real ones. At the root of this trend is a fundamental advantage held by those manipulating media: equal access to a distribution of what we consider authentic (i.e., “natural”) video. In this paper, we show how coding very subtle, noise-like modulations into the illumination of a scene can help combat this advantage by creating an information asymmetry that favors verification. Our approach effectively adds a temporal watermark to any video recorded under coded illumination. However, rather than encoding a specific message, this watermark encodes an image of the unmanipulated scene as it would appear lit only by the coded illumination. We show that even when an adversary knows that our technique is being used, creating a plausible coded fake video amounts to solving a second, more difficult version of the original adversarial content creation problem at an information disadvantage. This is a promising avenue for protecting high-stakes settings like public events and interviews, where the content on display is a likely target for manipulation, and while the illumination can be controlled, the cameras capturing video cannot.",
    "scholar_publication": "ACM Transactions on Graphics, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ce6baebab0",
    "authors": "Si-Tong Wei, Rui-Huan Wang, Chuan-Zhi Zhou, Baoquan Chen, Peng-Shuai Wang",
    "title": "OctGPT: Octree-based Multiscale Autoregressive Models for 3D Shape Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730601",
    "pdf_link": null,
    "abstract": "Autoregressive models have achieved remarkable success across various domains, yet their performance in 3D shape generation lags significantly behind that of diffusion models. In this paper, we introduce OctGPT, a novel multiscale autoregressive model for 3D shape generation that dramatically improves the efficiency and performance of prior 3D autoregressive approaches, while rivaling or surpassing state-of-the-art diffusion models. Our method employs a serialized octree representation to efficiently capture the hierarchical and spatial structures of 3D shapes. Coarse geometry is encoded via octree structures, while fine-grained details are represented by binary tokens generated using a vector quantized variational autoencoder (VQVAE), transforming 3D shapes into compact multiscale binary sequences suitable for autoregressive prediction. To address the computational challenges of handling long sequences, we incorporate octree-based transformers enhanced with 3D rotary positional encodings, scale-specific embeddings, and token-parallel generation schemes. These innovations reduce training time by 13 folds and generation time by 69 folds, enabling the efficient training of high-resolution 3D shapes, e.g.,10243, on just four NVIDIA 4090 GPUs only within days. OctGPT showcases exceptional versatility across various tasks, including text-, sketch-, and image-conditioned generation, as well as scene-level synthesis involving multiple objects. Extensive experiments demonstrate that OctGPT accelerates convergence and improves generation quality over prior autoregressive methods, offering a new paradigm for high-quality, scalable 3D content creation. Our code and trained models are available at https://github.com/octree-nn/octgpt.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "da75021597",
    "authors": "Anka H. Chen, Jerry Hsu, Ziheng Liu, Miles Macklin, Yin Yang, Cem Yuksel",
    "title": "Offset Geometric Contact",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731205",
    "pdf_link": null,
    "abstract": "… We present a novel contact model, termed Offset Geometric Contact (OGC), … offsetting each face along its normal direction, ensuring orthogonal contact forces, thus allows large contact …",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "b7c4d48d52",
    "authors": "Alon Feldman, Mirela Ben-Chen",
    "title": "On Planar Shape Interpolation With Logarithmic Metric Blending",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730697",
    "pdf_link": null,
    "abstract": "We present an interpolation method for planar shapes using logarithmic metric blending. Our approach generalizes prior work on pullback metrics to a framework, allowing us to employ different techniques, such as logarithmic blending of symmetric positive definite matrices, to have precise control over both conformal and area distortions. Key contributions include generalizing the continuous blending scheme and its adaptation to discrete mesh interpolation through different conformal and isometric parameterizations. Experimental …",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "302504f663",
    "authors": "Andreas Meuleman, Ishaan Shah, Alexandre Lanvin, Bernhard Kerbl, George Drettakis",
    "title": "On-the-fly Reconstruction for Large-Scale Novel View Synthesis from Unposed Images",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730913",
    "pdf_link": null,
    "abstract": "Radiance field methods such as 3D Gaussian Splatting (3DGS) allow easy reconstruction from photos, enabling free-viewpoint navigation. Nonetheless, pose estimation using Structure from Motion and 3DGS optimization can still each take between minutes and hours of computation after capture is complete. SLAM methods combined with 3DGS are fast but struggle with wide camera baselines and large scenes. We present an on-the-fly method to produce camera poses and a trained 3DGS immediately after capture. Our method can handle dense and wide-baseline captures of ordered photo sequences and large-scale scenes. To do this, we first introduce fast initial pose estimation, exploiting learned features and a GPU-friendly mini bundle adjustment. We then introduce direct sampling of Gaussian primitive positions and shapes, incrementally spawning primitives where required, significantly accelerating training. These two efficient steps allow fast and robust joint optimization of poses and Gaussian primitives. Our incremental approach handles large-scale scenes by introducing scalable radiance field construction, progressively clustering 3DGS primitives, storing them in anchors, and offloading them from the GPU. Clustered primitives are progressively merged, keeping the required scale of 3DGS at any viewpoint. We evaluate our solution on a variety of datasets and show that it can provide on-the-fly processing of all the capture scenarios and scene sizes we target. At the same time our method remains competitive - in speed, image quality, or both - with other methods that only handle specific capture styles or scene sizes.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "c240c0f7cd",
    "authors": "Jia-Peng Zhang, Cheng-Feng Pu, Meng-Hao Guo, Yan-Pei Cao, Shi-Min Hu",
    "title": "One Model to Rig Them All: Diverse Skeleton Rigging with UniRig",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730930",
    "pdf_link": null,
    "abstract": "The rapid evolution of 3D content creation, encompassing both AI-powered methods and traditional workflows, is driving an unprecedented demand for automated rigging solutions that can keep pace with the increasing complexity and diversity of 3D models. We introduce UniRig, a novel, unified framework for automatic skeletal rigging that leverages the power of large autoregressive models and a bone-point cross-attention mechanism to generate both high-quality skeletons and skinning weights. Unlike previous methods that struggle with complex or non-standard topologies, UniRig accurately predicts topologically valid skeleton structures thanks to a new Skeleton Tree Tokenization method that efficiently encodes hierarchical relationships within the skeleton. To train and evaluate UniRig, we present Rig-XL, a new large-scale dataset of over 14,000 rigged 3D models spanning a wide range of categories. UniRig significantly outperforms state-of-the-art academic and commercial methods, achieving a 215% improvement in rigging accuracy and a 194% improvement in motion accuracy on challenging datasets. Our method works seamlessly across diverse object categories, from detailed anime characters to complex organic and inorganic structures, demonstrating its versatility and robustness. By automating the tedious and time-consuming rigging process, UniRig has the potential to speed up animation pipelines with unprecedented ease and efficiency. Project Page: https://zjp-shadow.github.io/works/UniRig/",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "b1c6ced420",
    "authors": "Jiahao Wen, Jernej Barbic, Danny Kaufman",
    "title": "Optimal r-Adaptive In-Timestep Remeshing for Elastodynamics",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731204",
    "pdf_link": null,
    "abstract": "We propose a coupled mesh-adaptation model and physical simulation algorithm to jointly generate, per timestep, optimal adaptive remeshings and implicit solutions for the simulation of frictionally contacting elastodynamics. To do so, we begin with Ferguson et al.'s [2023] recently developed in-timestep remeshing (ITR) framework, which proposes an Incremental Potential based objective for mesh refinement, and a corresponding, locally greedy remeshing algorithm to minimize it. While this initial ITR framework demonstrates significant improvements, its greedy remeshing does not generate optimal meshes, and so does not converge to improving physical solutions with increasing mesh resolution. In practice, due to lack of optimality, the original ITR framework can and will find mesh and state solutions with unnecessarily low-quality geometries and corresponding physical solution artifacts. At the same time, we also identify additional fundamental challenges to adaptive simulation in terms of both ITR's original remeshing objective and its corresponding optimization problem formulation. In this work, in order to extend the ITR framework to high-quality, optimal in-timestep remeshing, we first construct a new remeshing objective function built from simple, yet critical, updates to the Incremental Potential energy, and a corresponding constrained model problem, whose minimizers provide locally optimal remeshings for physical problems. We then propose a new in-timestep remeshing optimization that jointly solves, per-timestep, for a new locally optimal remeshing and the next physical state defined upon it. To evaluate and demonstrate our extension of the ITR framework, we apply it to the optimal r-adaptive ITR simulation of frictionally contacting elasto-dynamics and statics. To enable r-adaptivity we additionally propose a new numerical method to robustly compute derivatives of the L2-projection operator necessary for optimal mesh-to-mesh state mappings within solves, a constraint model to enable on-boundary node adaptivity, and an efficient Newton-type optimization method for practically solving each per-timestep r-adaptive ITR solution. We extensively evaluate our method on challenging large-deformation and frictionally contacting scenarios. Here we observe optimal r-adaptivity captures comparable and better accuracy than unadapted meshes orders-of-magnitude larger, with corresponding significant advantages in both computation speedup and decrease in memory usage.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "ef90ef2bef",
    "authors": "Bo Yang, Ying Cao",
    "title": "Order Matters: Learning Element Ordering for Graphic Design Generation",
    "paper_url": "https://borisyang326.github.io/project/ordermatters/pdf/ordermatters_tog.pdf",
    "pdf_link": null,
    "abstract": "Graphic designs, such as advertisements, posters and documents, are a common form of creative mediums that e ectively delivers the intended message to audiences. To create good graphic designs, one usually needs to navigate through a large solution space spanned by various fundamental dimensions (eg, color, text, image and layout), while considering some important aesthetic and functional objectives. As a result, graphic design is often an iterative process, requiring extensive professional knowledge, rich hands-on experience and some creativity. The importance and challenges of graphic design have motivated an emerging stream of research e orts in developing computational models and systems to facilitate the design process, with many encouraging results. Recent deep generative models (eg, autoregressive models and di usion models) have demonstrated revolutionary capabilities and have seen great success in image and language generation. This stimulates tremendous interest in generative modeling of some core components of graphic design (eg, layout)[Gupta et al. 2021; Inoue et al. 2023a; Jyothi et al. 2019; Li et al. 2019] and complete designs [In-oue et al. 2024; Jia et al. 2023; Yamaguchi 2021]. Unlike images that are pixels organized on 2D grids, graphic designs are essentially composed of basic multi-modal elements, such as text and images, and are characterized by complex global relationships between the elements—the choice and con guration of one elements hinge crucially on those of other elements in a design and vice versa. This makes the attention-based Transformer architecture the de facto choice of neural backbone in most of the prior generative models for graphic design, due to its inductive bias towards learning the relationships between elements. To adapt powerful classes of generative models with Transformer backbones into the graphic design domain, graphic designs are formulated as ordered sequences of tokens [Arroyo et al. 2021; Gupta et al. 2021; Inoue et al. 2023a; Jiang et al. 2023a; Zhang et al. 2023]. Unfortunately, unlike sentences in which the positions of words are inherently available, the order of graphic design elements is unknown. To mitigate this issue, existing methods sort elements per design randomly or using simple handcrafted rules (eg, raster order)[Gupta et al. 2021; Inoue et al. 2023a; Zhang et al. 2023]. Despite promising results, it remains unclear whether such simple ordering approaches hinder the performance of the trained generative models. Thus, it is natural to ask: is there any better order that can optimize the capabilities of graphic design generation models?In this paper, we aim to demystify the signi cance of element ordering in generative models for graphic design. We show that the random and raster order used by existing works is sub-optimal for the performance of the generation models and seek to nd a better order that can contribute to a noticeable improvement of generation quality. We refrain from utilizing any hand-engineered rules, and instead learn an element ordering strategy end-to-end from unlabeled data. For this end, we propose a model, dubbed as Generative Order Learner, or GOL for short, which encompasses two core components: an ordering network and a design generator. The ordering network learns to sort a set of elements in a design based on their semantic, spatial and stylistic properties, while the design generator is trained to autoregressively generate design tokens in the order predicted by the ordering network. The two components are trained simultaneously, in such a way that the ordering network is guided to predict the order that can …",
    "scholar_publication": "ACM Trans. Graph, 2025 - borisyang326.github.io"
  },
  {
    "paper_id": "239d830cab",
    "authors": "Kangeun Lee, Sungyoung Kim",
    "title": "PAAP: Performer-Aware Automatic Panning System",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742989",
    "pdf_link": null,
    "abstract": "We propose PAAP (Performer-Aware Automatic Panning System), the first system to automatically track performer(s) and generate spatial audio panning data integrated with a Digital Audio Workstation (DAW). The system pipeline consists of three main stages: (1) visual cue analysis via performer tracking and monocular depth estimation, (2) spatial information prediction using a custom algorithm that produces DAW-compatible panning parameters, and (3) integration of industry-standard DAW using embedded script processing. We tested and validated the technical feasibility and real-world applicability including Open Sound Control (OSC) based real-time processing. To our knowledge, this is the first complete study of an automatic panning with DAW and we anticipate PAAP to streamline live and studio music production.",
    "scholar_publication": "Proceedings of the Special Interest Group on Computer …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ac45385769",
    "authors": "Michael Xu, Yi Shi, KangKang Yin, Xue Bin Peng",
    "title": "PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730616",
    "pdf_link": null,
    "abstract": "Humans excel in navigating diverse, complex environments with agile motor skills, exemplified by parkour practitioners performing dynamic maneuvers, such as climbing up walls and jumping across gaps. Reproducing these agile movements with simulated characters remains challenging, in part due to the scarcity of motion capture data for agile terrain traversal behaviors and the high cost of acquiring such data. In this work, we introduce PARC (Physics-based Augmentation with Reinforcement Learning for Character Controllers), a framework that leverages machine learning and physics-based simulation to iteratively augment motion datasets and expand the capabilities of terrain traversal controllers. PARC begins by training a motion generator on a small dataset consisting of core terrain traversal skills. The motion generator is then used to produce synthetic data for traversing new terrains. However, these generated motions often exhibit artifacts, such as incorrect contacts or discontinuities. To correct these artifacts, we train a physics-based tracking controller to imitate the motions in simulation. The corrected motions are then added to the dataset, which is used to continue training the motion generator in the next iteration. PARC’s iterative process jointly expands the capabilities of the motion generator and tracker, creating agile and versatile models for interacting with complex environments. PARC provides an effective approach to develop controllers for agile terrain traversal, which bridges the gap between the scarcity of motion data and the need for versatile character controllers.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "c5c01bf8db",
    "authors": "Jionghao Wang, Cheng Lin, Yuan Liu, Rui Xu, Zhiyang Dou, Xiaoxiao Long, Haoxiang Guo, Taku Komura, Xin Li, Wenping Wang",
    "title": "PDT: Point Distribution Transformation with Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730717",
    "pdf_link": null,
    "abstract": "Point-based representations have consistently played a vital role in geometric data structures. Most point cloud learning and processing methods typically leverage the unordered and unconstrained nature to represent the underlying geometry of 3D shapes. However, how to extract meaningful structural information from unstructured point cloud distributions and transform them into semantically meaningful point distributions remains an under-explored problem. We present PDT, a novel framework for point distribution transformation with diffusion models. Given a set of input points, PDT learns to transform the point set from its original geometric distribution into a target distribution that is semantically meaningful. Our method utilizes diffusion models with novel architecture and learning strategy, which effectively correlates the source and the target distribution through a denoising process. Through extensive experiments, we show that our method successfully transforms input point clouds into various forms of structured outputs - ranging from surface-aligned keypoints, and inner sparse joints to continuous feature lines. The results showcase our framework’s ability to capture both geometric and semantic features, offering a powerful tool for various 3D geometry processing tasks where structured point distributions are desired. Code will be available at this link: link.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "3ac471120a",
    "authors": "Jinseok Bae, Younghwan Lee, Donggeun Lim, Young Min Kim",
    "title": "PLT: Part-Wise Latent Tokens as Adaptable Motion Priors for Physically Simulated Characters",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730637",
    "pdf_link": null,
    "abstract": "Physically simulated characters can learn highly natural full-body motion guided by motion capture datasets. However, the range of motion is limited to the existing high-quality datasets, and cannot effectively adapt to challenging scenarios. We propose a novel policy architecture that learns part-wise motion skills, where individual parts can be separately extended and combined for unobserved settings. Our method employs a set of part-specific codebooks, which robustly capture motion dynamics without catastrophic collapse or forgetting. This structured decomposition allows intuitive control over the character’s behavior and dynamic exploration for a novel combination of part-wise motion. We further incorporate a refinement network compensating for subtle discrepancies in the disjoint discrete tokens, thus improving motion quality and stability. Our extensive evaluations show that our part-wise latent token achieves superior performance in imitating motions, even those from unseen distribution. We also validate our method in challenging tasks, including body tracking, navigation on complex terrains, and point-goal navigation with damaged body parts. Finally, we introduce a part-wise expansion of motion priors, where the physically simulated character incrementally adapts partial motion and produces unique combinations of whole-body motion, significantly diversifying motions.",
    "scholar_publication": "Proceedings of the Special Interest Group …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ff27aa5ca4",
    "authors": "Bingchen Yang, Haiyong Jiang, Hao Pan, Guosheng Lin, Jun Xiao, Peter Wonka, Bingchen Yang",
    "title": "PS-CAD: Local Geometry Guidance via Prompting and Selection for CAD Reconstruction",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3733595",
    "pdf_link": null,
    "abstract": "Reverse engineering CAD models from raw geometry is a classic but challenging research problem. In particular, reconstructing the CAD modeling sequence from point clouds provides great interpretability and convenience for editing. Analyzing previous work, we observed that a CAD modeling sequence represented by tokens and processed by a generative model does not have an immediate geometric interpretation. To improve upon this problem, we introduce geometric guidance into the reconstruction network. Our proposed model, PS-CAD, reconstructs the CAD modeling sequence one step at a time as illustrated in Figure . At each step, we provide three forms of geometric guidance. First, we provide the geometry of surfaces where the current reconstruction differs from the complete model as a point cloud. This helps the framework to focus on regions that still need work. Second, we use geometric analysis to extract a set of planar prompts, that correspond to candidate surfaces where a CAD extrusion step could be started. Third, we present a step-wise sampling to generate multiple complete candidate CAD modeling steps instead of single-tokens without direct geometric interpretation. Our framework has three major components. Geometric guidance computation extracts the first two types of geometric guidance. Single-step reconstruction computes a single candidate CAD modeling step for each provided prompt. Single-step selection selects among the candidate CAD modeling steps. The process continues until the reconstruction is completed. Our quantitative results show a significant improvement across all metrics. For example, on the dataset DeepCAD, PS-CAD improves upon the best published SOTA method by reducing the geometry errors (CD and HD) by 10%, and the structural error (ECD metric) by about 13%.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "47f99cd90a",
    "authors": "Kechun Wang, Renjie Chen",
    "title": "PaRas: A Rasterizer for Large-Scale Parametric Surfaces",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730658",
    "pdf_link": null,
    "abstract": "The advantages of higher-order surfaces, such as their ability to represent complex geometry compactly and smoothly, have led to their increasing use in computer graphics. This trend underscores the importance of developing efficient rendering algorithms tailored for these representations. We introduce PaRas, a highly performant rasterizer for real-time rendering of large-scale parametric surfaces with high precision. Unlike conventional graphics pipelines that rely on hardware tessellation to convert smooth surfaces into numerous flat triangles, our method provides a highly efficient and parallel approach to directly rasterize parametric surfaces. PaRas seamlessly integrates into existing workflows, enabling smooth surfaces to be handled with the same ease as triangle meshes. To accomplish this, we formulate the rasterization of parametric surfaces as a point inversion problem, employing a Newton-type iteration on the GPU to compute precise solutions. The framework’s effectiveness is demonstrated on quartic triangular Bézier patches and rational Bézier patches, both commonly used in high-precision modeling and industrial applications. Experimental results indicate that our rendering pipeline achieves higher efficiency and greater accuracy compared to traditional hardware tessellation techniques.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ba1444a454",
    "authors": "Magí Romanyà, Miguel A. Otaduy",
    "title": "Painless Differentiable Rotation Dynamics",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730944",
    "pdf_link": null,
    "abstract": "… of forward and differentiable rigid-body dynamics using Lie-algebra rotation derivatives. In … they evidence the painless implementation of both forward and differentiable dynamics. …",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "2df216c454",
    "authors": "Karran Pandey, Anita Hu, Clement Fuji Tsang, Or Perel, Karan Singh, Maria Shugrina",
    "title": "Painting with 3D Gaussian Splat Brushes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730724",
    "pdf_link": null,
    "abstract": "We explore interactive painting on 3D Gaussian splat scenes and other surfaces using 3D Gaussian splat brushes, each containing a chunk of realistic texture-geometry that make capture representations so appealing. The suite of brush capabilities we propose enables 3D artists to capture and then remix real world imagery and geometry with direct interactive control. In particular, we propose a set of algorithms for 1) selecting subsets of Gaussians as a brush pattern interactively, 2) applying the brush interactively to the same or other 3DGS scenes or other 3D surfaces using stamp-based painting, 3) using an inpainting Diffusion Model to adjust stamp seams for seamless and realistic appearance. We also present an ensemble of artistic brush parameters, resulting in a wide range of appearance options for the same brush. Our contribution is a judicious combination of algorithms, design features and creative affordances, that together enable the first prototype implementation of interactive brush-based painting with 3D Gaussian splats. We evaluate our system by showing compelling results on a diverse set of 3D scenes; and a user study with VFX/animation professionals, to validate system features, workflow, and potential for creative impact. Code and data for this paper can be accessed from splatpainting.github.io.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ab2af2cba2",
    "authors": "Aleksandar Cvejic, Abdelrahman Eldesokey, Peter Wonka",
    "title": "PartEdit: Fine-Grained Image Editing using Pre-Trained Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730747",
    "pdf_link": null,
    "abstract": "We present the first text-based image editing approach for object parts based on pre-trained diffusion models. Diffusion-based image editing approaches capitalized on the deep understanding of diffusion models of image semantics to perform a variety of edits. However, existing diffusion models lack sufficient understanding of many object parts, hindering fine-grained edits requested by users. To address this, we propose to expand the knowledge of pre-trained diffusion models to allow them to understand various object parts, enabling them to perform fine-grained edits. We achieve this by learning special textual tokens that correspond to different object parts through an efficient token optimization process. These tokens are optimized to produce reliable localization masks at each inference step to localize the editing region. Leveraging these masks, we design feature-blending and adaptive thresholding strategies to execute the edits seamlessly. To evaluate our approach, we establish a benchmark and an evaluation protocol for part editing. Experiments show that our approach outperforms existing editing methods on all metrics and is preferred by users of the time in conducted user studies.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "1041a5c3f1",
    "authors": "Guying Lin, Lei Yang, Congyi Zhang, Hao Pan, Yuhan Ping, Guodong Wei, Taku Komura, John Keyser, Wenping Wang, Guying Lin",
    "title": "Patch-Grid: An Efficient and Feature-Preserving Neural Implicit Surface Representation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3727142",
    "pdf_link": null,
    "abstract": "Neural implicit representations are increasingly used to depict three-dimensional (3D) shapes owing to their inherent smoothness and compactness, contrasting with traditional discrete representations. Yet, the multilayer perceptron–based neural representation, because of its smooth nature, rounds sharp corners or edges, rendering it unsuitable for representing objects with sharp features like computer-aided design (CAD) models. Moreover, neural implicit representations need long training times to fit 3D shapes. While previous works address these issues separately, we present a unified neural implicit representation called Patch-Grid, which efficiently fits complex shapes, preserves sharp features delineating different patches, and can also represent surfaces with open boundaries and thin geometric features. Patch-Grid learns a signed distance field (SDF) to approximate an encompassing surface patch of the shape with a learnable patch feature volume. To form sharp edges and corners in a CAD model, Patch-Grid merges the learned SDFs via the constructive solid geometry (CSG) approach. Core to the merging process is a novel merge grid design that organizes different patch feature volumes in a common octree structure. This design choice ensures robust merging of multiple learned SDFs by confining the CSG operations to localized regions. Additionally, it drastically reduces the complexity of the CSG operations in each merging cell, allowing the proposed method to be trained in seconds to fit a complex shape at high fidelity. Experimental results demonstrate that the proposed Patch-Grid representation is capable of accurately reconstructing shapes with complex sharp features, open boundaries, and thin geometric elements, achieving state-of-the-art reconstruction quality with high computational efficiency within seconds.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "4ba93b9d3a",
    "authors": "Zhaoyang Lv, Maurizio Monge, Ka Chen, Yufeng Zhu, Michael Goesele, Jakob Engel, Zhao Dong, Richard Newcombe",
    "title": "Photoreal Scene Reconstruction from an Egocentric Device",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730753",
    "pdf_link": null,
    "abstract": "In this paper, we investigate the challenges associated with using egocentric devices to photorealistic reconstruct the scene in high dynamic range. Existing methodologies typically assume using frame-rate 6DoF pose estimated from the device’s visual-inertial odometry system, which may neglect crucial details necessary for pixel-accurate reconstruction. This study presents two significant findings. Firstly, in contrast to mainstream work treating RGB camera as global shutter frame-rate camera, we emphasize the importance of employing visual-inertial bundle adjustment (VIBA) to calibrate the precise timestamps and movement of the rolling shutter RGB sensing camera in a high frequency trajectory format, which ensures an accurate calibration of the physical properties of the rolling-shutter camera. Secondly, we incorporate a physical image formation model based into Gaussian Splatting, which effectively addresses the sensor characteristics, including the rolling-shutter effect of RGB cameras and the dynamic ranges measured by sensors. Our proposed formulation is applicable to the widely-used variants of Gaussian Splats representation. We conduct a comprehensive evaluation of our pipeline using the open-source Project Aria device under diverse indoor and outdoor lighting conditions, and further validate it on a Meta Quest3 device. Across all experiments, we observe a consistent visual enhancement of +1 dB in PSNR by incorporating VIBA, with an additional +1 dB achieved through our proposed image formation model. Our complete implementation, evaluation datasets, and recording profile are available at https://www.projectaria.com/photoreal-reconstruction/",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "e1da18dd38",
    "authors": "Chris Careaga, Yağız Aksoy",
    "title": "Physically Controllable Relighting of Photographs",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730666",
    "pdf_link": null,
    "abstract": "We present a self-supervised approach to in-the-wild image relighting that enables fully controllable, physically based illumination editing. We achieve this by combining the physical accuracy of traditional rendering with the photorealistic appearance made possible by neural rendering. Our pipeline works by inferring a colored mesh representation of a given scene using monocular estimates of geometry and intrinsic components. This representation allows users to define their desired illumination configuration in 3D. The scene under the new lighting can then be rendered using a path-tracing engine. We send this approximate rendering of the scene through a feed-forward neural renderer to predict the final photorealistic relighting result. We develop a differentiable rendering process to reconstruct in-the-wild scene illumination, enabling self-supervised training of our neural renderer on raw image collections. Our method represents a significant step in bringing the explicit physical control over lights available in typical 3D computer graphics tools, such as Blender, to in-the-wild relighting.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "3dbfdc211e",
    "authors": "Tyrus Tracey, Stefan Diaconu, Sebastian Dille, S. Mahdi H. Miangoleh, Yağız Aksoy",
    "title": "Physically-Based Compositing of 2D Graphics",
    "paper_url": "https://dl.acm.org/doi/full/10.1145/3721250.3743006",
    "pdf_link": null,
    "abstract": "3 ConclusionWe demonstrate an interactive approach to physically-based 2D graphic insertion that can be completed from start to finish in a matter of minutes. Our tool provides an intuitive workflow to generate realistic composites with high fidelity.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "7d0812297d",
    "authors": "Diyang Zhang, Zhendong Wang, Zegao Liu, Xinming Pei, Weiwei Xu, Huamin Wang",
    "title": "Physics-inspired Estimation of Optimal Cloth Mesh Resolution",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730619",
    "pdf_link": null,
    "abstract": "In this paper, we tackle an important yet often overlooked question: What is the optimal mesh resolution for cloth simulation, without relying on preliminary simulations? The optimal resolution should be sufficient to capture fine details of all potential wrinkles, while avoiding an unnecessarily high resolution that wastes computational time and memory on excessive vertices. This challenge stems from the complex nature of wrinkle distribution, which varies spatially, temporally, and anisotropically across different orientations. To address this, we propose a method to estimate the optimal cloth mesh resolution, based on two key factors: material stiffness and boundary conditions.To determine the influence of material stiffness on wrinkle wavelength and amplitude, we apply the experimental theory presented by Cerda and Mahadevan to calculate the optimal mesh resolution for cloth fabrics. Similarly, for boundary conditions influencing local wrinkle formation, we use the same scaling law to determine the source resolution for stationary boundary conditions introduced by garment-making techniques such as shirring, folding, stitching, and down-filling, as well as predicted areas accounting for dynamic wrinkles introduced by collision compression caused by human motions. To ensure a smooth transition between different source resolutions, we apply another experimental theory from [Vandeparre et al. ] to compute the transition distance. A mesh sizing map is introduced to facilitate smooth transitions, ensuring precision in critical areas while maintaining efficiency in less important regions. Based on these sizing maps, triangular meshes with optimal resolution distribution are generated using Poisson sampling and Delaunay triangulation. The resulting method can not only enhance the realism and precision of cloth simulations but also support diverse application scenarios, making it a versatile solution for complex garment design.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "1afe77849e",
    "authors": "Minsu Kim, Eunho Jung, Yoonsang Lee",
    "title": "PhysicsFC: Learning User-Controlled Skills for a Physics-Based Football Player Controller",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731425",
    "pdf_link": null,
    "abstract": "We propose PhysicsFC, a method for controlling physically simulated football player characters to perform a variety of football skills-such as dribbling, trapping, moving, and kicking-based on user input, while seamlessly transitioning between these skills. Our skill-specific policies, which generate latent variables for each football skill, are trained using an existing physics-based motion embedding model that serves as a foundation for reproducing football motions. Key features include a tailored reward design for the Dribble policy, a two-phase reward structure combined with projectile dynamics-based initialization for the Trap policy, and a Data-Embedded Goal-Conditioned Latent Guidance (DEGCL) method for the Move policy. Using the trained skill policies, the proposed football player finite state machine (PhysicsFC FSM) allows users to interactively control the character. To ensure smooth and agile transitions between skill policies, as defined in the FSM, we introduce the Skill Transition-Based Initialization (STI), which is applied during the training of each skill policy. We develop several interactive scenarios to showcase PhysicsFC's effectiveness, including competitive trapping and dribbling, give-and-go plays, and 11v11 football games, where multiple PhysicsFC agents produce natural and controllable physics-based football player behaviors. Quantitative evaluations further validate the performance of individual skill policies and the transitions between them, using the presented metrics and experimental designs.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "3b05dd9e0a",
    "authors": "Yiling Pan, Zhixin Xu, Bin Wang, Bailin Deng",
    "title": "Piecewise Ruled Approximation for Freeform Mesh Surfaces",
    "paper_url": "https://arxiv.org/abs/2501.15258",
    "pdf_link": null,
    "abstract": "A ruled surface is a shape swept out by moving a line in 3D space. Due to their simple geometric forms, ruled surfaces have applications in various domains such as architecture and engineering. In the past, various approaches have been proposed to approximate a target shape using developable surfaces, which are special ruled surfaces with zero Gaussian curvature. However, methods for shape approximation using general ruled surfaces remain limited and often require the target shape to be either represented as parametric surfaces or have non-positive Gaussian curvature. In this paper, we propose a method to compute a piecewise ruled surface that approximates an arbitrary freeform mesh surface. We first use a group-sparsity formulation to optimize the given mesh shape into an approximately piecewise ruled form, in conjunction with a tangent vector field that indicates the ruling directions. Afterward, we utilize the optimization result to extract seams that separate smooth families of rulings, and use the seams to construct the initial rulings. Finally, we further optimize the positions and orientations of the rulings to improve the alignment with the input target shape. We apply our method to a variety of freeform shapes with different topologies and complexity, demonstrating its effectiveness in approximating arbitrary shapes.",
    "scholar_publication": "arXiv preprint arXiv:2501.15258, 2025 - arxiv.org"
  },
  {
    "paper_id": "50d65df3e8",
    "authors": "Eric Chen, Žiga Kovačič, Madhav Aggarwal, Abe Davis",
    "title": "Pocket Time-Lapse",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730594",
    "pdf_link": null,
    "abstract": "… , and visualizing panoramic time-lapse in uncontrolled settings … 𝑋, defines an image plane for our time-lapse, to which all other … lies in demonstrating pocket time-lapse as a novel ap…",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "7d60a84cab",
    "authors": "Michele Rocca, Sune Darkner, Kenny Erleben, Sheldon Andrews, Michele Rocca",
    "title": "Policy-Space Diffusion for Physics-Based Character Animation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3732285",
    "pdf_link": null,
    "abstract": "Adapting motion to new contexts in digital entertainment often demands fast agile prototyping. State-of-the-art techniques use reinforcement learning policies for simulating the underlined motion in a physics engine. Unfortunately, policies typically fail on unseen tasks and it is too time-consuming to fine-tune the policy for every new morphological, environmental, or motion change. We propose a novel point of view on using policy networks as a representation of motion for physics-based character animation. Our policies are compact, tailored to individual motion tasks, and preserve similarity with nearby tasks. This allows us to view the space of all motions as a manifold of policies where sampling substitutes training. We obtain memory-efficient encoding of motion that leverages the characteristics of control policies such as being generative, and robust to small environmental changes. With this perspective, we can sample novel motions by directly manipulating weights and biases through a Diffusion Model. Our newly generated policies can adapt to previously unseen characters, potentially saving time in rapid prototyping scenarios. Our contributions include the introduction of Common Neighbor Policy regularization to constrain policy similarity during motion imitation training making them suitable for generative modeling; a Diffusion Model adaptation for diverse morphology; and an open policy dataset. The results show that we can learn non-linear transformations in the policy space from labeled examples, and conditionally generate new ones. In a matter of seconds, we sample a batch of policies for different conditions that show comparable motion fidelity metrics as their respective trained ones.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ce1ec44942",
    "authors": "Shibo Liu, Tielin Dai, Ligang Liu, Xiao-Ming Fu",
    "title": "Polynomial 2D Biharmonic Coordinates for High-order Cages",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730887",
    "pdf_link": null,
    "abstract": "We derive closed-form expressions of biharmonic coordinates for 2D high-order cages, enabling the transformation of the input polynomial curves into polynomial curves of any order. Central to our derivation is the use of the high-order boundary element method. We demonstrate the practicality and effectiveness of our method on various 2D deformations. In practice, users can easily manipulate the Bézier control points to perform the desired intuitive deformation, as the biharmonic coordinates provide an enriched deformation space and encourage the alignment between the boundary cage and its interior geometry.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "9ddcf05eff",
    "authors": "Liwen Wu, Fujun Luan, Miloš Hašan, Ravi Ramamoorthi",
    "title": "Position-Normal Manifold for Efficient Glint Rendering on High-Resolution Normal Maps",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730633",
    "pdf_link": null,
    "abstract": "Detailed microstructures on specular objects often exhibit intriguing glinty patterns under high-frequency lighting, which is challenging to render using a conventional normal-mapped BRDF. In this paper, we present a manifold-based formulation of the glint normal distribution functions (NDF) that precisely captures the surface normal distributions over queried footprints. The manifold-based formulation transfers the integration for the glint NDF construction to a problem of mesh intersections. Compared to previous works that rely on complex numerical approximations, our integral solution is exact and much simpler to compute, which also allows an easy adaptation of a mesh clustering hierarchy to accelerate the NDF evaluation of large footprints. Our performance and quality analysis shows that our NDF formulation achieves similar glinty appearance compared to the baselines but is an order of magnitude faster. Within this framework, we further present a novel derivation of analytical shadow-masking for normal-mapped diffuse surfaces—a component that is often ignored in previous works.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "f86926e064",
    "authors": "Jiabao Brad Wang, Amir Vaxman",
    "title": "Power-Linear Polar Directional Fields",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730591",
    "pdf_link": null,
    "abstract": "We introduce a novel method for directional-field design on meshes, enabling users to specify singularities at any location on a mesh. Our method uses a piecewise power-linear representation for phase and scale, offering precise control over field topology. The resulting fields are smooth and accommodate any singularity index and field symmetry. With this representation, we mitigate the artifacts caused by coarse or uneven meshes. We showcase our approach on meshes with diverse topologies and triangle qualities.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "d46c1eb5b2",
    "authors": "Philippe Weier, Jérémy Riviere, Ruslan Guseinov, Stephan Garbin, Philipp Slusallek, Bernd Bickel, Thabo Beeler, Delio Vicini",
    "title": "Practical Inverse Rendering of Textured and Translucent Appearance",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730855",
    "pdf_link": null,
    "abstract": "Inverse rendering has emerged as a standard tool to reconstruct the parameters of appearance models from images (e.g., textured BSDFs). In this work, we present several novel contributions motivated by the practical challenges of recovering high-resolution surface appearance textures, including spatially-varying subsurface scattering parameters. First, we propose Laplacian mipmapping, which combines differentiable mipmapping and a Laplacian pyramid representation into an effective preconditioner. This seemingly simple technique significantly improves the quality of recovered surface textures on a set of challenging inverse rendering problems. Our method automatically adapts to the render and texture resolutions, only incurs moderate computational cost and achieves better quality than prior work while using fewer hyperparameters. Second, we introduce a specialized gradient computation algorithm for textured, path-traced subsurface scattering, which facilitates faithful reconstruction of translucent materials. By using path tracing, we enable the recovery of complex appearance while avoiding the approximations of the previously used diffusion dipole methods. Third, we demonstrate the application of both these techniques to reconstructing the textured appearance of human faces from sparse captures. Our method recovers high-quality relightable appearance parameters that are compatible with current production renderers.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "765c66576d",
    "authors": "Xiaochun Tong, Toshiya Hachisuka",
    "title": "Practical Stylized Nonlinear Monte Carlo Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730686",
    "pdf_link": null,
    "abstract": "The recent formulation of stylized rendering equation (SRE) models stylization by applying nonlinear functions to reflected radiance recursively at each bounce, allowing seamless blend between stylized and physically based light transport. A naive estimator has to branch at each stylized surface, resulting in exponential computation and storage cost. We propose a practical approach for rendering scenes with SRE at a tractable cost. We first propose nonlinear path filtering (NL-PF) that caches the radiance evaluations at intermediate bounces, reducing the exponential sampling cost of the branching estimator of SRE to polynomial. Despite the effectiveness of NL-PF, its high memory cost makes it less scalable. To further improve efficiency, we propose nonlinear radiance caching (NL-NRC) where we apply a compact neural network to store radiance fields. Our NL-NRC has the same linear time sampling cost as a non-branching path tracer and can solve SRE with a high number of bounces and recursive stylization. Our key insight is that, by allowing the network to learn outgoing radiance prior to applying any nonlinear function, the network converges to the correct solution, even when we only have access to biased gradients due to nonlinearity. Our NL-NRC enables rendering scenes with arbitrary, highly nonlinear stylization while achieving significant speedup over branching estimators.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "4b52ab74db",
    "authors": "EUN HYE JANG, MI CHANG, Woojin Kim, Daesub Yoon",
    "title": "Predicting Accidents in Conditional Autonomous Driving: A Multimodal Approach Integrating Human Misuse, Biometric Indicators, and Spatial Complexity",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743029",
    "pdf_link": null,
    "abstract": "Decreased attention, distraction, and complex environments are major contributors to accidents in Level 2 autonomous driving. This study examines how spatial complexity and human factors affect accident risk using scenario-based simulations. We analyzed subjective factors (workload, situation awareness) and biometric data (eye tracking, HRV). Logistic regression identified age, workload, and situation awareness as significant predictors, with 74.2% accuracy (5-fold cross-validation). High spatial complexity increased cognitive load and visual scanning, elevating accident risk. These results support the need for integrated prediction strategies and adaptive driver support systems to enhance safety.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "76ce8d383e",
    "authors": "Masahiro Kono, Akinobu Maejima, Yuki Koyama, Takeo Igarashi",
    "title": "Predicting Colors in Unpainted Gaps for Anime-Style Illustration",
    "paper_url": "https://dl.acm.org/doi/full/10.1145/3721250.3743027",
    "pdf_link": null,
    "abstract": "Anime-style digital illustrations, particularly those rooted in Japanese pop culture, have become a prominent form of creative production. The typical workflow involves drawing binary line art (Figure 2a), filling segmented regions with base colors, and then",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "46d202d506",
    "authors": "Mengqi Xia, Zhaoyang Zhang, Sumit Chaturvedi, Yutong Yi, Rundong Wu, Holly Rushmeier, Julie Dorsey",
    "title": "Predicting Fabric Appearance Through Thread Scattering and Inversion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731178",
    "pdf_link": null,
    "abstract": "The fashion industry has a real need to preview fabric designs using the actual threads they intend to use, ensuring that the designs they envisage can be physically realized. Unfortunately, today's fabric rendering relies on either hand-tuned parameters or parameters acquired from already fabricated cloth. Furthermore, existing curve-based scattering models are not suitable for this problem: they are either not naturally differentiable due to discrete fiber count parameters, or require a more detailed geometry representation, introducing extra complexity. In this work, we bridge this gap by presenting a novel pipeline that captures and digitizes physical threads and predicts the appearance of the fabric based on the weaving pattern. We develop a practical thread scattering model based on simulations of multiple fiber scattering within a thread. Using a cost-efficient multi-view setup, we capture threads of diverse colors and materials. We apply differentiable rendering to digitize threads, demonstrating that our model significantly improves the reconstruction accuracy compared to existing models, matching both reflection and transmission. We leverage a two-scale rendering technique to efficiently render woven cloth. We validate that our digital threads, combined with simulated woven yarn geometry, can accurately predict the fabric appearance by comparing to real samples. We show how our work can aid designs using diverse thread profiles, woven patterns, and textured design patterns.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "bec17bf94b",
    "authors": "Masood Masoodian, Inkeri Aula, Renata Vieira, Áurea Rodrigues, Ivo Santos, António Lacerda Diniz, Camila Campos, Rafael Prezado, Leonor Rocha",
    "title": "Preserving Intangible Cultural Heritage of Megalithic Sites Using Immersive Mobile XR",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743013",
    "pdf_link": null,
    "abstract": "This poster introduces the INT-ACT project which aims to investigate the use of immersive XR environments for presenting the emotional, experiential and environmental dimensions of Intangible Cultural Heritage (ICH) associated with tangible cultural heritage sites. It also presents a mobile XR demonstrator, developed as part of INT-ACT, that focuses on the ICH related to a megalithic site.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "f6680c055c",
    "authors": "Mizuki Akiyama, Christian Sandor, Yuki Igarashi",
    "title": "Pretraining Support for Cheerleading Stunts Using Virtual Reality",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743007",
    "pdf_link": null,
    "abstract": "Cheerleading stunts are group gymnastics performed by multiple people. As the skills involved become more challenging, it is necessary to devise better practice methods. Thus, in this paper, we propose a pretraining support system for cheerleading stunts using Virtual Reality (VR) technology. This system enables users to experience successfully performing a stunt in the virtual space by adopting the viewpoints of the cheerleaders performing various types of stunts. We evaluated our system through interviews with two experts. Our system has the potential to meaningfully augment the established training method of previsualization of stunts.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "8d3043b77f",
    "authors": "Jingwen Ye, Yuze He, Yanning Zhou, Yiqin Zhu, Kaiwen Xiao, Yong-Jin Liu, Wei Yang, Xiao Han",
    "title": "PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with Auto-Regressive transformer",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730732",
    "pdf_link": null,
    "abstract": "Shape primitive abstraction, which decomposes complex 3D shapes into simple geometric elements, plays a crucial role in human visual cognition and has broad applications in computer vision and graphics. While recent advances in 3D content generation have shown remarkable progress, existing primitive abstraction methods either rely on geometric optimization with limited semantic understanding or learn from small-scale, category-specific datasets, struggling to generalize across diverse shape categories. We present PrimitiveAnything, a novel framework that reformulates shape primitive abstraction as a primitive assembly generation task. PrimitiveAnything includes a shape-conditioned primitive transformer for auto-regressive generation and an ambiguity-free parameterization scheme to represent multiple types of primitives in a unified manner. The proposed framework directly learns the process of primitive assembly from large-scale human-crafted abstractions, enabling it to capture how humans decompose complex shapes into primitive elements. Through extensive experiments, we demonstrate that PrimitiveAnything can generate high-quality primitive assemblies that better align with human perception while maintaining geometric fidelity across diverse shape categories. It benefits various 3D applications and shows potential for enabling primitive-based user-generated content (UGC) in games. Project page: https://primitiveanything.github.io",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "7bd6d57f23",
    "authors": "Jiayi Eris Zhang, Doug James, Danny Kaufman",
    "title": "Progressive Dynamics++: A Framework for Stable, Continuous, and Consistent Animation Across Resolution and Time",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731202",
    "pdf_link": null,
    "abstract": "The recently developed Progressive Dynamics framework [Zhang et al. 2024] addresses the long-standing challenge in enabling rapid iterative design for high-fidelity cloth and shell animation. In this work, we identify fundamental limitations of the original method in terms of stability and temporal continuity. For robust progressive dynamics simulation we seek methods that provide: (1) stability across all levels of detail (LOD) and timesteps, (2) temporally continuous animations without jumps or jittering, and (3) user-controlled balancing between geometric consistency and enrichment at each timestep, thereby making it a practical previewing tool with high-quality results at the finest level to be used as the final output. We propose a general framework, Progressive Dynamics++, for constructing a family of progressive dynamics integration methods that advance physical simulation states forward in both time and spatial resolution, which includes Zhang et al. [2024]'s method as one member. We analyze necessary stability conditions for Progressive Dynamics integrators and introduce a novel, stable method that significantly improves temporal continuity, supported by a new quantitative measure. Additionally, we present a quantitative analysis of the trade-off between geometric consistency and enrichment, along with strategies for balancing between these aspects in transitions across resolution and time.",
    "scholar_publication": "ACM Transactions on Graphics …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "e63225e02d",
    "authors": "Hossein Baktash, Nicholas Sharp, Qingnan Zhou, Alec Jacobson, Keenan Crane",
    "title": "Putting Rigid Bodies to Rest",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731203",
    "pdf_link": null,
    "abstract": "… of a rigid body, without the use of physical simulation. In particular, given a rigid body in R 3 , we identify all possible stationary points, as well as the probability that the body will stop at …",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "af555bd0b9",
    "authors": "Hao Ni, Baiyu Chen, Zhaohan Wang, Zhiyong Chen, Wanyi Miao, Lyu Xin, Nan Cao",
    "title": "QRBTF - AI QR Code Generator",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743038",
    "pdf_link": null,
    "abstract": "… QRBTF generates artistic QR codes that maintain machine … QR modules to control signals; (2) Style-adaptive generation … The system has generated over 600,000 codes via qrbtf.com, …",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "6f97caa411",
    "authors": "Edward Lu, Anthony Rowe",
    "title": "QUASAR: Quad-based Adaptive Streaming And Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731213",
    "pdf_link": null,
    "abstract": "As AR/VR systems evolve to demand increasingly powerful GPUs, physically separating compute from display hardware emerges as a natural approach to enable a lightweight, comfortable form factor. Unfortunately, splitting the system into a client-server architecture leads to challenges in transporting graphical data. Simply streaming rendered images over a network suffers in terms of latency and reliability, especially given variable bandwidth. Although image-based reprojection techniques can help, they often do not support full motion parallax or disocclusion events. Instead, scene geometry can be streamed to the client, allowing local rendering of novel views. Traditionally, this has required a prohibitively large amount of interconnect bandwidth, excluding the use of practical networks. This paper presents a new quad-based geometry streaming approach that is designed with compression and the ability to adjust Quality-of-Experience (QoE) in response to target network bandwidths. Our approach advances previous work by introducing a more compact data structure and a temporal compression technique that reduces data transfer overhead by up to 15×, reducing bandwidth usage to as low as 100 Mbps. We optimized our design for hardware video codec compatibility and support an adaptive data streaming strategy that prioritizes transmitting only the most relevant geometry updates. Our approach achieves image quality comparable to, and in many cases exceeds, state-of-the-art techniques while requiring only a fraction of the bandwidth, enabling real-time geometry streaming on commodity headsets over WiFi.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "8cb5ede6bb",
    "authors": "Mariia Soroka, Christoph Peters, Steve Marschner",
    "title": "Quadric-Based Silhouette Sampling for Differentiable Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731146",
    "pdf_link": null,
    "abstract": "Physically based differentiable rendering has established itself as key to inverse rendering, in which scenes are recovered from images through gradient-based optimization. Taking the derivative of the rendering equation is made difficult by the presence of discontinuities in the integrand at object silhouettes. To obtain correct derivatives w.r.t. changing geometry, accounting e.g. for changing penumbras or silhouettes in glossy reflections, differentiable renderers must compute an integral over these silhouettes. Prior work proposed importance sampling of silhouette edges for a given shading point. The main challenge is to efficiently reject parts of the mesh without silhouettes during sampling, which has been done using top-down traversal of a tree. Inaccuracies of this existing rejection procedure result in many samples with zero contribution. Thus, variance remains high and subsequent work has focused on alternatives such as area sampling or path space differentiable rendering. We propose an improved rejection test. It reduces variance substantially, which makes edge sampling in a unidirectional path tracer competitive again. Our rejection test relies on two approximations to the triangle planes of a mesh patch: A bounding box in dual space and dual quadrics. Additionally, we improve the heuristics used for stochastic traversal of the tree. We evaluate our method in a unidirectional path tracer and achieve drastic improvements over the original edge sampling and outperform methods based on area sampling.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "e6907ba9a3",
    "authors": "Fumiya Narita, Nimiko Ochiai, Takashi Kanai, Ryoichi Ando",
    "title": "Quadtree Tall Cells for Eulerian Liquid Simulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730652",
    "pdf_link": null,
    "abstract": "This paper introduces a novel grid structure that extends tall cell methods for efficient deep water simulation. Unlike previous tall cell methods, which are designed to capture all the fine details around liquid surfaces, our approach subdivides tall cells horizontally, allowing for more aggressive adaptivity and a significant reduction in the number of cells. The foundation of our method lies in a new variational formulation of Poisson’s equations for pressure solve tailored for tall-cell grids, which naturally handles the transition of variable-sized cells. This variational view not only permits the use of the efficacy-proven conjugate gradient method but also facilitates monolithic two-way coupled rigid bodies. The key distinction between our method and previous general adaptive approaches, such as tetrahedral or octree grids, is the simplification of adaptive grid construction. Our method performs grid subdivision in a quadtree fashion, rather than an octree. These 2D cells are then simply extended vertically to complete the tall cell population. We demonstrate that this novel form of adaptivity, which we refer to as quadtree tall cells, delivers superior performance compared to traditional uniform tall cells.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "4fb305b47d",
    "authors": "Xiao-Lei Li, Hao-Xiang Chen, Yanni Zhang, Kai Ma, Alan Zhao, Tai-Jiang Mu, Haoxiang Guo, Ran Zhang",
    "title": "RELATE3D: REfocusing Latent Adapter for Targeted local Enhancement and Editing in 3D Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730648",
    "pdf_link": null,
    "abstract": "Recent advancements in 3D generation techniques have simplified the tedious manual process of 3D asset production. Among these methods, 3D native latent diffusion models are particularly effective in generating high-quality geometric details. However, achieving local enhancement and editing of the generated 3D models remains a challenge due to the limited understanding of the relationship between text,images,and 3D in terms of local semantics and feature space.We explore and reveal the characteristics of the native 3D latent space, make it decomposable and low-rank, thereby enabling efficient and effective learning for multimodal local alignment. Based on this, we introduce RELATE3D, a novel approach that combines a Refocusing Adapter with part-to-latent correspondence guided training for precise local enhancement and part-level editing of 3D geometry. The Refocusing Adapter incorporates partial image and caption signals, and, combined with part-to-latent mapping, directs modifications to the relevant latent dimensions during latent diffusion process. We validate the effectiveness of our approach through extensive experiments and ablation studies, showcasing the capabilities of our generative local enhancement and editing process, as well as global refinement.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "689c1bf9f2",
    "authors": "Krishna Mullia, Fujun Luan, Xin Sun, Miloš Hašan, Krishna Mullia",
    "title": "RNA: Relightable Neural Assets",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3695866",
    "pdf_link": null,
    "abstract": "… We propose a compact relightable neural 3D asset representation for geometries with … with a neural feature grid and an MLP decoder. This allows for view variation and full relightability, …",
    "scholar_publication": "ACM Transactions on Graphics, 2024 - dl.acm.org"
  },
  {
    "paper_id": "0a6c0106b1",
    "authors": "Ziyi Zhang, Nicolas Roussel, Thomas Muller, Tizian Zeltner, Merlin Nimier-David, Fabrice Rousselle, Wenzel Jakob",
    "title": "Radiance Surfaces: Optimizing Surface Representations with a 5D Radiance Field Loss",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730713",
    "pdf_link": null,
    "abstract": "We present a fast and simple technique to convert images into a radiance surface-based scene representation. Building on existing radiance volume reconstruction algorithms, we introduce a subtle yet impactful modification of the loss function requiring changes to only a few lines of code: instead of integrating the radiance field along rays and supervising the resulting images, we project the training images into the scene to directly supervise the spatio-directional radiance field.The primary outcome of this change is the complete removal of alpha blending and ray marching from the image formation model, instead moving these steps into the loss computation. In addition to promoting convergence to surfaces, this formulation assigns explicit semantic meaning to 2D subsets of the radiance field, turning them into well-defined radiance surfaces. We finally extract a level set from this representation, which results in a high-quality radiance surface model.Our method retains much of the speed and quality of the baseline algorithm. For instance, a suitably modified variant of Instant NGP maintains comparable computational efficiency, while achieving an average PSNR that is only 0.1 dB lower. Most importantly, our method generates explicit surfaces in place of an exponential volume, doing so with a level of simplicity not seen in prior work.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "af32bf6bac",
    "authors": "Anran Qi, Nico Pietroni, Maria Korosteleva, Olga Sorkine-Hornung, Adrien Bousseau",
    "title": "Rags2Riches: Computational Garment Reuse",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730703",
    "pdf_link": null,
    "abstract": "… algorithm to automatically compute sewing patterns for upcycling existing garments into new … a computational pipeline to convert an existing garment (a, source) into a new garment (b, …",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "b26689c447",
    "authors": "Liyuan Zhu, Shengqu Cai, Shengyu Huang, Gordon Wetzstein, Naji Khosravan, Iro Armeni",
    "title": "ReStyle3D: Scene-Level Appearance Transfer with Semantic Correspondences",
    "paper_url": "https://arxiv.org/abs/2502.10377",
    "pdf_link": null,
    "abstract": "We introduce ReStyle3D, a novel framework for scene-level appearance transfer from a single style image to a real-world scene represented by multiple views. The method combines explicit semantic correspondences with multi-view consistency to achieve precise and coherent stylization. Unlike conventional stylization methods that apply a reference style globally, ReStyle3D uses open-vocabulary segmentation to establish dense, instance-level correspondences between the style and real-world images. This ensures that each object is stylized with semantically matched textures. It first transfers the style to a single view using a training-free semantic-attention mechanism in a diffusion model. It then lifts the stylization to additional views via a learned warp-and-refine network guided by monocular depth and pixel-wise correspondences. Experiments show that ReStyle3D consistently outperforms prior methods in structure preservation, perceptual style similarity, and multi-view coherence. User studies further validate its ability to produce photo-realistic, semantically faithful results. Our code, pretrained models, and dataset will be publicly released, to support new applications in interior design, virtual staging, and 3D-consistent stylization.",
    "scholar_publication": "arXiv preprint arXiv …, 2025 - arxiv.org"
  },
  {
    "paper_id": "ecf5e35550",
    "authors": "Tao Huang, Haoyang Shi, Mengdi Wang, Yuxing Qiu, Yin Yang, Kui Wu",
    "title": "Real-Time Knit Deformation and Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731184",
    "pdf_link": null,
    "abstract": "The knit structure consists of interlocked yarns, with each yarn comprising multiple plies comprising tens to hundreds of twisted fibers. This intricate geometry and the large number of geometric primitives present substantial challenges for achieving high-fidelity simulation and rendering in real-time applications. In this work, we introduce the first real-time framework that takes an animated stitch mesh as input and enhances it with yarn-level simulation and fiber-level rendering. Our approach relies on a knot-based representation to model interlocked yarn contacts. The knot positions are interpolated from the underlying mesh, and associated yarn control points are optimized using a physically inspired energy formulation, which is solved through a GPU-based Gauss-Newton scheme for real-time performance. The optimized control points are sent to the GPU rasterization pipeline and rendered as yarns with fiber-level details. In real-time rendering, we introduce several decomposition strategies to enable realistic lighting effects on complex knit structures, even under environmental lighting, while maintaining computational and memory efficiency. Our simulation faithfully reproduces yarn-level structures under deformations, e.g., stretching and shearing, capturing interlocked yarn behaviors. The rendering pipeline achieves near-ground-truth visual quality while being 120,000× faster than path tracing reference with fiber-level geometries. The whole system provides real-time performance and has been evaluated through various application scenarios, including knit simulation for small patches and full garments and yarn-level relaxation in the design pipeline.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "b727a21333",
    "authors": "Xueming Yu, David George, John Millward, Paul Debevec",
    "title": "Real-Time Multispectral Lighting Reproduction",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743035",
    "pdf_link": null,
    "abstract": "We present a real-time algorithm for driving multispectral LED lights in a spherical lighting reproduction stage to achieve optimal color rendition for a dynamic lighting environment. Previous work has driven multispectral LED lights (ours include red, green, blue, white, and amber LEDs) by solving a nonnegative least squares (NNLS) problem for each light source; the solution ensures that each light appears to be the correct RGB color seen by the camera and also optimizes how closely the lights illuminate a color chart to appear as it should in the target lighting environment. We create a real-time version of this technique by pre-computing a lookup table of these NNLS solutions across the full range of input RGB values. Since the proper relative mix of LEDs depends on chrominance and not on luminance, our lookup table can be reduced to 2D saving both storage and computation. With this technique, we can drive several thousand multispectral LED lights at video frame rates with proper color matching and color rendition for a dynamic lighting environment.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ce2882d7f3",
    "authors": "Vinicius Hirono Gonçalves, Paula Dornhofer Paro Costa",
    "title": "Reassemble by Packing: Path-Valid Spectral Placement for 3D Fragment Assembly",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743043",
    "pdf_link": null,
    "abstract": "3D reassembly needs both tight alignment and a collision-free insertion path, but most methods enforce only the former. We recast the task as constrained packing, apply FFT correlation for coarse placement, and prune unreachable poses with a flood-fill path test on the correlation map. A final ICP stage then maximizes surface alignment (instead of minimizing contact as in classic packing). Assuming a known target envelope, this path-aware spectral pipeline yields high-fidelity, physically valid reconstructions, although accuracy still depends on the initial pose sampling.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "0b9860ed43",
    "authors": "Veeramanohar Avudaiappan, Ritwik Murali",
    "title": "Reconstructing Graphic Design Posters via Visual Decomposition and Semantic Layer Translation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743040",
    "pdf_link": null,
    "abstract": "This work presents a pipeline to convert rasterized graphic design posters into multi-layered, editable digital assets. It decomposes the input poster into core elements, categorizes them, and converts them into semantically meaningful formats. A novel strategy using Z-index addresses layer ordering and overlap. The pipeline’s accuracy was evaluated by comparing over 24,000 original and reconstructed posters of multiple widely used sizes and aspect ratios in print & digital media. Layer semantic accuracy was assessed using the LLaVA-7B model, which showed high confidence scores across image, text, and shape layers. A user-centered evaluation with 20 participants resulted in high satisfaction ratings, confirming the pipeline’s ability to accurately reproduce poster designs with excellent fidelity, layout, and overall quality. This pipeline contributes a refined approach to reconstructing rasterized graphic design posters, advancing beyond existing methods.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "fcec974d70",
    "authors": "Etienne Corman, Keenan Crane",
    "title": "Rectangular Surface Parameterization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731176",
    "pdf_link": null,
    "abstract": "… Parameterization of Arbitrary Surfaces We introduce hexagonal global parameterization, a new type of surface parameterization in … Such parameterizations enable the tiling of surfaces …",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "305ebe86fa",
    "authors": "Manuel Kansy, Jacek Naruniec, Christopher Schroers, Markus Gross, Romann Weber",
    "title": "Reenact Anything: Semantic Video Motion Transfer Using Motion-Textual Inversion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730668",
    "pdf_link": null,
    "abstract": "Recent years have seen a tremendous improvement in the quality of video generation and editing approaches. While several techniques focus on editing appearance, few address motion. Current approaches using text, trajectories, or bounding boxes are limited to simple motions, so we specify motions with a single motion reference video instead. We further propose to use a pre-trained image-to-video model rather than a text-to-video model. This approach allows us to preserve the exact appearance and position of a target object or scene and helps disentangle appearance from motion.Our method, called motion-textual inversion, leverages our observation that image-to-video models extract appearance mainly from the (latent) image input, while the text/image embedding injected via cross-attention predominantly controls motion. We thus represent motion using text/image embedding tokens. By operating on an inflated motion-text embedding containing multiple text/image embedding tokens per frame, we achieve a high temporal motion granularity. Once optimized on the motion reference video, this embedding can be applied to various target images to generate videos with semantically similar motions.Our approach does not require spatial alignment between the motion reference video and target image, generalizes across various domains, and can be applied to various tasks such as full-body and face reenactment, as well as controlling the motion of inanimate objects and the camera. We empirically demonstrate the effectiveness of our method in the semantic video motion transfer task, significantly outperforming existing methods in this context.Project website: https://mkansy.github.io/reenact-anything/",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "32c711b078",
    "authors": "Shaofei Wang, Tomas Simon, Igor Santesteban, Timur Bagautdinov, Junxuan Li, Vasu Agrawal, Fabian Prada, Shoou-I Yu, Pace Nalbone, Matt Gramlich, Roman Lubachersky, Chenglei Wu, Javier Romero, Jason Saragih, Michael Zollhoefer, Andreas Geiger, Siyu Tang, Shunsuke Saito",
    "title": "Relightable Full-Body Gaussian Codec Avatars",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730739",
    "pdf_link": null,
    "abstract": "We propose Relightable Full-Body Gaussian Codec Avatars, a new approach for modeling relightable full-body avatars with fine-grained details including face and hands. The unique challenge for relighting full-body avatars lies in the large deformations caused by body articulation and the resulting impact on appearance caused by light transport. Changes in body pose can dramatically change the orientation of body surfaces with respect to lights, resulting in both local appearance changes due to changes in local light transport functions, as well as non-local changes due to occlusion between body parts. To address this, we decompose the light transport into local and non-local effects. Local appearance changes are modeled using learnable zonal harmonics for diffuse radiance transfer. Unlike spherical harmonics, zonal harmonics are highly efficient to rotate under articulation. This allows us to learn diffuse radiance transfer in a local coordinate frame, which disentangles the local radiance transfer from the articulation of the body. To account for non-local appearance changes, we introduce a shadow network that predicts shadows given precomputed incoming irradiance on a base mesh. This facilitates the learning of non-local shadowing between the body parts. Finally, we use a deferred shading approach to model specular radiance transfer and better capture reflections and highlights such as eye glints. We demonstrate that our approach successfully models both the local and non-local light transport required for relightable full-body avatars, with a superior generalization ability under novel illumination conditions and unseen poses.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "a4f2ff39d5",
    "authors": "Chong Zeng, Yue Dong, Pieter Peers, Hongzhi Wu, Xin Tong",
    "title": "RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730595",
    "pdf_link": null,
    "abstract": "We present RenderFormer, a neural rendering pipeline that directly renders an image from a triangle-based representation of a scene with full global illumination effects and that does not require per-scene training or fine-tuning. Instead of taking a physics-centric approach to rendering, we formulate rendering as a sequence-to-sequence transformation where a sequence of tokens representing triangles with reflectance properties is converted to a sequence of output tokens representing small patches of pixels. RenderFormer follows a two stage pipeline: a view-independent stage that models triangle-to-triangle light transport, and a view-dependent stage that transforms a token representing a bundle of rays to the corresponding pixel values guided by the triangle-sequence from the the view-independent stage. Both stages are based on the transformer architecture and are learned with minimal prior constraints. We demonstrate and evaluate RenderFormer on scenes with varying complexity in shape and light transport.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "c5b16a74e0",
    "authors": "Jeffrey Liu, Daqi Lin, Markus Kettunen, Chris Wyman, Ravi Ramamoorthi",
    "title": "Reservoir Splatting for Temporal Path Resampling and Motion Blur",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730646",
    "pdf_link": null,
    "abstract": "Recent extensions to spatiotemporal path reuse, or ReSTIR, improve rendering efficiency in the presence of high-frequency content by augmenting path reservoirs to represent contributions over full pixel footprints. Still, if historical paths fail to contribute to future frames, these benefits disappear. Prior ReSTIR work backprojects to the prior frame to identify paths for reuse. Backprojection can fail to find relevant paths for many reasons, including moving cameras or subpixel geometry with differing motion. We introduce reservoir splatting to …",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ad86a9142d",
    "authors": "Peng Li, Zeyong Wei, Honghua Chen, Xuefeng Yan, Mingqiang Wei",
    "title": "Revisiting Tradition and Beyond: A Customized Bilateral Filtering Framework for Point Cloud Denoising",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730891",
    "pdf_link": null,
    "abstract": "Deep learning-based methods have become the dominant solution for point cloud denoising, offering strong generalization capabilities through data-driven training. However, traditional methods, despite their drawbacks of heavy parameter tuning and weak generalization, retain unique advantages in interpretability and theoretical robustness. This complementarity motivates us to explore a hybrid solution that leverages data-driven paradigms to overcome the performance constraints of traditional methods. In this paper, we revisit the classic bilateral filter (BF) as a case study and identify three key limitations hindering its performance: excessive parameter tuning, suboptimal neighborhood quality, and fixed parameters across the entire model. To address them, we propose CustomBF, a novel framework for customizing BF components at a per-point level. CustomBF employs multigraph encoders and a mutual guidance strategy to analyze local patches, enabling the customization of BF components including center point normal, neighborhood point coordinates, Gaussian function parameters, and neighborhood radius for each point. Experimental results demonstrate that this component-customized bilateral filter outperforms state-of-the-art methods and achieves robust denoising even in complex scenarios. It highlights the potential of hybrid methods to extend the applicability and effectiveness of traditional techniques.",
    "scholar_publication": "ACM Transactions on Graphics …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "8f941728a8",
    "authors": "Isabella Liu, Zhan Xu, Yifan Wang, Hao Tan, Zexiang Xu, Xiaolong Wang, Hao Su, Zifan Shi",
    "title": "RigAnything: Template-Free Autoregressive Rigging for Diverse 3D Assets",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731149",
    "pdf_link": null,
    "abstract": "We present RigAnything, a novel autoregressive transformer-based model, which makes 3D assets rig-ready by probabilistically generating joints and skeleton topologies and assigning skinning weights in a template-free manner. Unlike most existing auto-rigging methods, which rely on predefined skeleton templates and are limited to specific categories like humanoid, RigAnything approaches the rigging problem in an autoregressive manner, iteratively predicting the next joint based on the global input shape and the previous prediction. While autoregressive models are typically used to generate sequential data, RigAnything extends its application to effectively learn and represent skeletons, which are inherently tree structures. To achieve this, we organize the joints in a breadth-first search (BFS) order, enabling the skeleton to be defined as a sequence of 3D locations and the parent index. Furthermore, our model improves the accuracy of position prediction by leveraging diffusion modeling, ensuring precise and consistent placement of joints within the hierarchy. This formulation allows the autoregressive model to efficiently capture both spatial and hierarchical relationships within the skeleton. Trained end-to-end on both RigNet and Objaverse datasets, RigAnything demonstrates state-of-the-art performance across diverse object types, including humanoids, quadrupeds, marine creatures, insects, and many more, surpassing prior methods in quality, robustness, generalizability, and efficiency. It achieves significantly faster performance than existing auto-rigging methods, completing rigging in under a few seconds per shape.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "f261bbdf48",
    "authors": "Ryugo Morita, Sho Kuno, Ryunosuke Tanaka, Rongzhi Li, Hoang Dai Dinh, Issey Sukeda",
    "title": "SAWNA: Space-Aware Text to Image Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743023",
    "pdf_link": null,
    "abstract": "Layout-aware text-to-image generation allows users to synthesize images by specifying object positions through text prompts and layouts. This has proven useful in a variety of creative fields such as advertising, UI design, and animation, where structured scene control is essential. In real-world workflows, however, certain regions are often intentionally left empty—for instance, for headlines in advertisements, buttons in interface prototypes, or subtitles and speech bubbles in animation frames. Existing models lack the ability to explicitly preserve such negative spaces, often resulting in unwanted content and complicating downstream editing. We introduce Space-Controllable Text-to-Image Generation, a task that treats reserved areas as first-class constraints. To address this, we propose SAWNA (Space-Aware Text-to-Image Generation), a training-free diffusion framework that injects nonreactive noise into user-defined masked regions, ensuring they remain empty throughout generation. Our method maintains semantic integrity and visual fidelity without retraining and integrates seamlessly into layout-sensitive workflows in design, advertising, and animation. Experiments demonstrate that SAWNA reliably enforces spatial constraints and improves the practical usability of generated content.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "f6ed153965",
    "authors": "Rishit Dagli, Shivesh Prakash, Robert Wu, Houman Khosravani",
    "title": "SEE-2-SOUND: Zero-Shot Spatial Environment-to-Spatial Sound",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742965",
    "pdf_link": null,
    "abstract": "Generating combined visual and auditory sensory experiences is critical for immersive content. We introduce SEE-2-SOUND, a training-free pipeline that turns an image, GIF, or video into 5.1 spatial audio. SEE-2-SOUND sequentially: (i) segments visual sound sources; (ii) estimates their 3-D positions from monocular depth; (iii) synthesises mono audio for every source; and (iv) renders the mix with room acoustics. Built entirely from off-the-shelf models, the method needs no fine-tuning and runs in zero-shot mode on real or generated media. We demonstrate compelling results for generating spatial audio from videos, images, dynamic images, and media generated by learned approaches. Project page: https://see2sound.github.io/.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "7dbfb31587",
    "authors": "Tingting Liao, Yujian Zheng, Yuliang Xiu, Adilbek Karmanov, Liwen Hu, Leyang Jin, Hao Li",
    "title": "SOAP: Style-Omniscient Animatable Portraits",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730691",
    "pdf_link": null,
    "abstract": "Creating animatable 3D avatars from a single image remains challenging due to style limitations (realistic, cartoon, anime) and difficulties in handling accessories or hairstyles. While 3D diffusion models advance single-view reconstruction for general objects, outputs often lack animation controls or suffer from artifacts because of the domain gap. We propose SOAP, a style-omniscient framework to generate rigged, topology-consistent avatars from any portrait. Our method leverages a multiview diffusion model trained on 24K 3D heads with multiple styles and an adaptive optimization pipeline to deform the FLAME mesh while maintaining topology and rigging via differentiable rendering. The resulting textured avatars support FACS-based animation, integrate with eyeballs and teeth, and preserve details like braided hair or accessories. Extensive experiments demonstrate the superiority of our method over state-of-the-art techniques for both single-view head modeling and diffusion-based generation of Image-to-3D. Our code and data are publicly available for research purposes at github.com/TingtingLiao/soap.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "d84c3d4ad1",
    "authors": "Shivangi Aneja, Sebastian Weiss, Irene Baeza, Prashanth Chandran, Gaspard Zoss, Matthias Niessner, Derek Bradley",
    "title": "ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730729",
    "pdf_link": null,
    "abstract": "Generating high-fidelity real-time animated sequences of photorealistic 3D head avatars is important for many graphics applications, including immersive telepresence and movies. This is a challenging problem particularly when rendering digital avatar close-ups for showing character’s facial microfeatures and expressions. To capture the expressive, detailed nature of human heads, including skin furrowing and finer-scale facial movements, we propose to couple locally-defined facial expressions with 3D Gaussian splatting to enable creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In contrast to previous works that operate on a global expression space, we condition our avatar’s dynamics on patch-based local expression features and synthesize 3D Gaussians at a patch level. In particular, we leverage a patch-based geometric 3D face model to extract patch expressions and learn how to translate these into local dynamic skin appearance and motion by coupling the patches with anchor points of Scaffold-GS, a recent hierarchical scene representation. These anchors are then used to synthesize 3D Gaussians on-the-fly, conditioned by patch-expressions and viewing direction. We employ color-based densification and progressive training to obtain high-quality results and faster convergence for high resolution 3K training images. By leveraging patch-level expressions, ScaffoldAvatar consistently achieves state-of-the-art performance with visually natural motion, while encompassing diverse facial expressions and styles in real time.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "5e018bee0a",
    "authors": "Petr Strakos, Milan Jaros, Tomas Brzobohaty, Marketa Faltynkova, Ondrej Meca, Lubomir Riha",
    "title": "Scalable Volume Rendering of Billion-Cell CFD Simulations Using VFX Pipelines",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743008",
    "pdf_link": null,
    "abstract": "We present a modular, scalable workflow for high-fidelity volume rendering of large-scale CFD simulations. Designed with visual effects (VFX) techniques in mind, our workflow transforms unstructured CFD data into cinematic-quality visuals using parallel voxelization and sparse volume export. By leveraging CyclesPhi renderer and OpenVDB, we deliver performance, scalability, and expressive visualization on HPC infrastructure. Results on two large CFD cases demonstrate significant speedups over traditional tools with support for interactive rendering of volumes.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "e393e3e91b",
    "authors": "Wenyou Wang, Rex West, Toshiya Hachisuka",
    "title": "Segment-based Light Transport Simulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730847",
    "pdf_link": null,
    "abstract": "We propose a novel segment-based light transport framework that uses segments as the basic unit of light transport. Unlike vertex-based formulations, our segment-based formulation naturally accommodates the disconnected subpaths encountered in photon density estimation and path filtering methods, and opens the door to a wide range of new rendering methods that consider segments as a sampling primitive. To facilitate the development of segment-based rendering methods, we introduce several segment sampling techniques and estimation strategies, including a highly-performant recursive estimator. One of our key contributions is a general-purpose segment sampling framework based on marginal multiple importance sampling (MMIS). To demonstrate the practicality of our sampling framework, we show how it allows us to easily implement a robust bidirectional path filtering method — challenging under a vertex-based formulation — achieving superior filtering efficiency and convergence compared to state-of-the-art approaches.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "9c22046192",
    "authors": "Linjun Wu, Xiangjun Tang, Jingyuan Cong, He Wang, Bo Hu, Xu Gong, Songnan Li, Yuchen Liao, Yiqian Wu, Chen Liu, Xiaogang Jin",
    "title": "Semantically Consistent Text-to-Motion with Unsupervised Styles",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730641",
    "pdf_link": null,
    "abstract": "Text-to-stylized human motion generation leverages text descriptions for motion generation with fine-grained style control with respect to a reference motion. However, existing approaches typically rely on supervised style learning with labeled datasets, constraining their adaptability and generalization for effective diverse style control. Additionally, they have not fully explored the temporal correlations between motion, textual descriptions, and style, making it challenging to generate semantically consistent motion with precise style alignment. To address these limitations, we introduce a novel method that integrates unsupervised style from arbitrary references into a text-driven diffusion model to generate semantically consistent stylized human motion. The core innovation lies in leveraging text as a mediator to capture the temporal correspondences between motion and style, enabling the seamless integration of temporally dynamic style into motion features. Specifically, we first train a diffusion model on a text-motion dataset to capture the correlation between motion and text semantics. A style adapter then extracts temporally dynamic style features from reference motions and integrates a novel Semantic-Aware Style Injection (SASI) module to infuse these features into the diffusion model. The SASI module computes the semantic correlation between motion and style features based on text, selectively incorporating style features that align with motion content, ensuring semantic consistency and precise style alignment. Our style adapter does not require a labeled style dataset for training, enhancing adaptability and generalization of style control. Extensive evaluations show that our method outperforms previous approaches in terms of semantic consistency and style expressivity. Our webpage, https://fivezerojun.github.io/stylization.github.io/, includes links to the supplementary video and code.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "264116b170",
    "authors": "Yue Chang, Otman Benchekroun, Maurizio M. Chiaramonte, Peter Yichen Chen, Eitan Grinspun",
    "title": "Shape Space Spectra",
    "paper_url": "https://arxiv.org/abs/2408.10099",
    "pdf_link": null,
    "abstract": "… Our focus on shape space in turn requires new techniques for consistency of modes across … shapes in the shape space. such shape spaces in the graphics community include design …",
    "scholar_publication": "arXiv preprint arXiv …, 2024 - arxiv.org"
  },
  {
    "paper_id": "b72ac313f3",
    "authors": "Matthew Loges, Tomer Weiss",
    "title": "Simulating the Mechanics of Ant Swarm Aggregations",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742999",
    "pdf_link": null,
    "abstract": "Ants exhibit unique abilities to self-assemble into animate, living structures. Such structures display properties of both fluid and solid-like, deformable materials. Despite much progress in our understanding of ant aggregation dynamics, simulating such phenomena has been largely overlooked in real-time graphics and animation applications. We present a constraints-based approach for simulating the collective dynamics of ants. We demonstrate ant collective behaviors interactively with compelling physical realism.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "2c66215dc4",
    "authors": "Julian Knodt",
    "title": "Single Edge Collapse Quad-Dominant Mesh Reduction",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731143",
    "pdf_link": null,
    "abstract": "Mesh reduction using quadric error metrics is the industry standard for producing level-of-detail (LOD) geometry for meshes. Although industry tools produce visually excellent LODs, mesh topology is often ruined during decimation. This is because tools focus on triangle simplification and preserving rendered appearance, whereas artists often produce quad dominant meshes with clean edge topology. Artist created manual LODs preserve both appearance and quad topology. Furthermore, most existing tools for quad decimation only accept pure quad meshes and cannot handle any triangles. The gap between quad and triangular mesh decimation is because they are built on fundamentally different operations, triangle simplification uses single edge collapses, whereas quad decimation requires that entire sets of edges be collapsed atomically. In this work, we demonstrate that single edge collapse can be used to preserve most input quads without degrading geometric quality. Single edge collapse quad preservation is made possible by introducing dihedral-angle weighted quadrics for every edge, allowing optimization to evenly space edges while preserving features. It is further enabled by explicitly ordering edge collapses with nearly equivalent quadric error in a way that preserves quad topology. In addition to quad preservation, we demonstrate that by introducing weights for quadrics on certain edges, our framework can be used to preserve symmetry and joint influences. To demonstrate our approach is suitable for skinned mesh decimation on triangle meshes, we show that QEM with attributes can preserve joint influences better than prior work. We implement and test our approach on 67 static and 19 animated meshes from Sketchfab. On both static and animated meshes, our approach consistently outperforms prior work with lower Chamfer and Hausdorff distance, while preserving more quad topology when present.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "ad935a0aa4",
    "authors": "Ren Li, Cong Cao, Corentin Dumery, Yingxuan You, Hao Li, Pascal Fua",
    "title": "Single View Garment Reconstruction Using Diffusion Mapping Via Pattern Coordinates",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730651",
    "pdf_link": null,
    "abstract": "Reconstructing 3D clothed humans from images is fundamental to applications like virtual try-on, avatar creation, and mixed reality. While recent advances have enhanced human body recovery, accurate reconstruction of garment geometry—especially for loose-fitting clothing—remains an open challenge. We present a novel method for high-fidelity 3D garment reconstruction from single images that bridges 2D and 3D representations. Our approach combines Implicit Sewing Patterns (ISP) with a generative diffusion model to learn rich garment shape priors in a 2D UV space. A key innovation is our mapping model that establishes correspondences between 2D image pixels, UV pattern coordinates, and 3D geometry, enabling joint optimization of both 3D garment meshes and the corresponding 2D patterns by aligning learned priors with image observations. Despite training exclusively on synthetically simulated cloth data, our method generalizes effectively to real-world images, outperforming existing approaches on both tight- and loose-fitting garments. The reconstructed garments maintain physical plausibility while capturing fine geometric details, enabling downstream applications including garment retargeting and texture manipulation.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "b912a7a4c7",
    "authors": "Hao Jin, Haoran Xie",
    "title": "Sketch-based Fluid Video Generation Using Motion-Guided Diffusion Models in Still Landscape Images",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743033",
    "pdf_link": null,
    "abstract": "2 MethodsFigure 2 illustrates the workflow of our proposed framework for fluid video generation guided by hand-drawn sketches.",
    "scholar_publication": "Proceedings of the Special Interest Group on Computer …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "e47afe2a49",
    "authors": "Lei Zhong, Chuan Guo, Yiming Xie, Jiawei Wang, Changjian Li",
    "title": "Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731167",
    "pdf_link": null,
    "abstract": "Storyboarding is widely used for creating 3D animations. Animators use the 2D sketches in storyboards as references to craft the desired 3D animations through a trial-and-error process. The traditional approach requires exceptional expertise and is both labor-intensive and time-consuming. Consequently, there is a high demand for automated methods that can directly translate 2D storyboard sketches into 3D animations. This task is under-explored to date and inspired by the significant advancements of motion diffusion models, we propose to address it from the perspective of conditional motion synthesis. We thus present Sketch2Anim, composed of two key modules for sketch constraint understanding and motion generation. Specifically, due to the large domain gap between the 2D sketch and 3D motion, instead of directly conditioning on 2D inputs, we design a 3D conditional motion generator that simultaneously leverages 3D keyposes, joint trajectories, and action words, to achieve precise and fine-grained motion control. Then, we invent a neural mapper dedicated to aligning user-provided 2D sketches with their corresponding 3D keyposes and trajectories in a shared embedding space, enabling, for the first time, direct 2D control of motion generation. Our approach successfully transfers storyboards into high-quality 3D motions and inherently supports direct 3D animation editing, thanks to the flexibility of our multi-conditional motion generator. Comprehensive experiments and evaluations, and a user perceptual study demonstrate the effectiveness of our approach. The code, data, trained models, and sketch-based motion designing interface are at https://zhongleilz.github.io/Sketch2Anim/.",
    "scholar_publication": "ACM Transactions on Graphics …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "a8dfb5c9b9",
    "authors": "Feng-Lin Liu, Shi-Yang Li, Yan-Pei Cao, Hongbo Fu, Lin Gao",
    "title": "Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730623",
    "pdf_link": null,
    "abstract": "Recent video editing methods achieve attractive results in style transfer or appearance modification. However, editing the structural content of 3D scenes in videos remains challenging, particularly when dealing with significant viewpoint changes, such as large camera rotations or zooms. Key challenges include generating novel view content that remains consistent with the original video, preserving unedited regions, and translating sparse 2D inputs into realistic 3D video outputs. To address these issues, we propose Sketch3DVE, a sketch-based 3D-aware video editing method to enable detailed local manipulation of videos with significant viewpoint changes. To solve the challenge posed by sparse inputs, we employ image editing methods to generate edited results for the first frame, which are then propagated to the remaining frames of the video. We utilize sketching as an interaction tool for precise geometry control, while other mask-based image editing methods are also supported. To handle viewpoint changes, we perform a detailed analysis and manipulation of the 3D information in the video. Specifically, we utilize a dense stereo method to estimate a point cloud and the camera parameters of the input video. We then propose a point cloud editing approach that uses depth maps to represent the 3D geometry of newly edited components, aligning them effectively with the original 3D scene. To seamlessly merge the newly edited content with the original video while preserving the features of unedited regions, we introduce a 3D-aware mask propagation strategy and employ a video diffusion model to produce realistic edited videos. Extensive experiments demonstrate the superiority of Sketch3DVE in video editing.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "b7424571c7",
    "authors": "Runyi Yu, Yinhuai Wang, Qihan Zhao, Hok Wai Tsui, Jingbo Wang, Ping Tan, Qifeng Chen",
    "title": "SkillMimic-V2: Learning Robust and Generalizable Interaction Skills from Sparse and Noisy Demonstrations",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730640",
    "pdf_link": null,
    "abstract": "We address a fundamental challenge in Reinforcement Learning from Interaction Demonstration (RLID): demonstration noise and coverage limitations. While existing data collection approaches provide valuable interaction demonstrations, they often yield sparse, disconnected, and noisy trajectories that fail to capture the full spectrum of possible skill variations and transitions. Our key insight is that despite noisy and sparse demonstrations, there exist infinite physically feasible trajectories that naturally bridge between demonstrated skills or emerge from their neighboring states, forming a continuous space of possible skill variations and transitions. Building upon this insight, we present two data augmentation techniques: a Stitched Trajectory Graph (STG) that discovers potential transitions between demonstration skills, and a State Transition Field (STF) that establishes unique connections for arbitrary states within the demonstration neighborhood. To enable effective RLID with augmented data, we develop an Adaptive Trajectory Sampling (ATS) strategy for dynamic curriculum generation and a historical encoding mechanism for memory-dependent skill learning. Our approach enables robust skill acquisition that significantly generalizes beyond the reference demonstrations. Extensive experiments across diverse interaction tasks demonstrate substantial improvements over state-of-the-art methods in terms of convergence stability, generalization capability, and recovery robustness.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "182b9b8ea8",
    "authors": "Nick Shelton, Enoch Omale",
    "title": "Skylight: Real-Time Projection Mapping for Surgical Navigation Leveraging Skin-Adhered Fiducials",
    "paper_url": "https://dl.acm.org/doi/full/10.1145/3721250.3743042",
    "pdf_link": null,
    "abstract": "Surgical navigation systems have transformed spinal procedures by enhancing precision and reducing complications through realtime visualization of anatomy relative to tracked instruments [Suri, 2025][Bovonratwet, 2023]. However, widespread adoption remains limited due to invasive tracking methods, high costs, and complex user interfaces [Choo, 2008]. Spine surgery presents high risks—with readmission rates reaching 7.4%, reoperations 16% and complications 18%[Bernatz, 2015][Malter, 1998][Paul, 2016]—driving over $15 B in annual costs that Centers for Medicare and Medicaid Services (CMS) has identified as a target for reduction [Miller, 2008]. Skylight™ directly addresses these challenges with a non-invasive tracking system and real-time projection mapping, delivering navigation accuracy comparable to existing technologies while eliminating workflow barriers. This system demonstrates a clear path to reducing surgical risk, reoperation, and healthcare costs, making advanced navigation broadly accessible and clinically impactful [Vonlanthen, 2011][Mason, 2014].",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "c21d4acd81",
    "authors": "Tomohiro Kamide, Naoki Hashimoto",
    "title": "Smartphone-Based Simple HMD With Multiple Mirrors and Lenticular Lens for Ultra-Wide Field of View",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742980",
    "pdf_link": null,
    "abstract": "In this study, we propose a smartphone-based wide field-of-view HMD by expanding the display area using inexpensive mirrors and lenticular lenses. Lenticular lenses placed on the both edge of the display convert these areas into the multi-view displays. The expansion of the display area is achieved by observing multi-view images via the properly placed mirrors.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "f96b3fbd9b",
    "authors": "Nicolas Bonneel, David Coeurjolly, Jean-Claude Iehl, Victor Ostromoukhov",
    "title": "Sobol' Sequences with Guaranteed-Quality 2D Projections",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730821",
    "pdf_link": null,
    "abstract": "Low-discrepancy sequences, and more particularly Sobol' sequences are gold standard for drawing highly uniform samples for quasi-Monte Carlo applications. They produce so-called (t,s)-sequences, that is, sequences of s-dimensional samples whose uniformity is controlled by a non-negative integer quality factor t. The Monte Carlo integral estimator has a convergence rate that improves as t decreases. Sobol' construction in base 2 also provides extremely fast sampling point generation using efficient xor-based arithmetic. Computer graphics applications, such as rendering, often require high uniformity in consecutive 2D projections and in higher-dimensional projections at the same time. However, it can be shown that, in the classical Sobol' construction, only a single 2D sequence of points (up to scrambling), constructed using irreducible polynomials x and x + 1, achieves the ideal t = 0 property. Reusing this sequence in projections necessarily loses high dimensional uniformity. We prove the existence and construct many 2D Sobol' sequences having t = 1 using irreducible polynomials p and p2 + p + 1. They can be readily combined to produce higher-dimensional low discrepancy sequences with a high-quality t = 1, guaranteed in consecutive pairs of dimensions. We provide the initialization table that can be directly used with any existing Sobol' implementation, along with the corresponding generator matrices, for an optimized 692-dimensional Sobol' construction. In addition to guaranteeing the (1, 2)-sequence property for all consecutive pairs, we ensure that t ≤ 4 for consecutive 4D projections up to 215 points.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "f68db1dfc8",
    "authors": "Nathan King, Steven Ruuth, Christopher Batty",
    "title": "Spatial Adaptivity for Solving PDEs on Manifolds With the Closest Point Method",
    "paper_url": "https://dl.acm.org/doi/full/10.1145/3721250.3743012",
    "pdf_link": null,
    "abstract": "1 MethodThe closest point method (CPM) has recently been used in computer graphics for fluid simulations [Morgenroth et al. 2020] and geometry processing [King et al. 2024a; 2024b]. We propose the first framework to enable spatial adaptivity with CPM, providing a more efficient spatial discretization. To solve manifold PDEs with CPM an embedding PDE is constructed whose solution agrees with the solution of the manifold PDE at points. The embedding PDE is solved on a tubular neighbourhood of the manifold defined by",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ace251f4da",
    "authors": "Mutian Tong, Rundi Wu, Changxi Zheng",
    "title": "Spatiotemporally Consistent Indoor Lighting Estimation with Diffusion Priors",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730749",
    "pdf_link": null,
    "abstract": "Indoor lighting estimation from a single image or video remains a challenge due to its highly ill-posed nature, especially when the lighting condition of the scene varies spatially and temporally. We propose a method that estimates from an input video a continuous light field describing the spatiotemporally varying lighting of the scene. We leverage 2D diffusion priors for optimizing such light field represented as a MLP. To enable zero-shot generalization to in-the-wild scenes, we fine-tune a pre-trained image diffusion model to predict lighting at multiple locations by jointly inpainting multiple chrome balls as light probes. We evaluate our method on indoor lighting estimation from a single image or video and show superior performance over compared baselines. Most importantly, we highlight results on spatiotemporally consistent lighting estimation from in-the-wild videos, which is rarely demonstrated in previous works.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "707d65326c",
    "authors": "Hugo Schott, Théo Thonat, Thibaud Lambert, Eric Guérin, Eric Galin, Axel Paris",
    "title": "Sphere Carving: Bounding Volumes for Signed Distance Fields",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730845",
    "pdf_link": null,
    "abstract": "We introduce Sphere Carving, a novel method for automatically computing bounding volumes that closely bound a procedurally defined implicit surface. Starting from an initial bounding volume located far from the object, we iteratively approach the surface by leveraging the signed distance function information. Field function queries define a set of empty spheres, from which we extract intersection points that are used to compute a bounding volume. Our method is agnostic of the function representation and only requires a conservative signed distance field as input. This encompasses a large set of procedurally defined implicit surface models such as exact or Lipschitz functions, BlobTrees, or even neural representations. Sphere Carving is conceptually simple, independent of the function representation, requires a small number of function queries to create bounding volumes, and accelerates queries in Sphere Tracing and polygonization.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "f6fb968f07",
    "authors": "Kei Iwasaki, Yoshinori Dobashi",
    "title": "Spherical Lighting with Spherical Harmonics Hessian",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730689",
    "pdf_link": null,
    "abstract": "In this paper, we introduce a second-order derivative of spherical harmonics, spherical harmonics Hessian, and solid spherical harmonics, a variant of spherical harmonics, to the computer graphics community. These mathematical tools are used to develop an analytical representation of the Hessian matrix of spherical harmonics coefficients for spherical lights. We apply our analytic representation of the Hessian matrix to grid-based SH lighting rendering applications with many spherical lights that store the incident light field as spherical harmonics coefficients and their spatial gradient at sparse grid. We develop a Hessian-based error metric, with which our method automatically and adaptively subdivides the grid whether the interpolation using the spatial gradient is appropriate. Our method can be easily incorporated into the grid-based precomputed radiance transfer (PRT) framework with small additional storage. We demonstrate that our adaptive grid subdivided by using the Hessian-based error metric can substantially improve the rendering quality in equal-time grid construction.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "2b8b50111f",
    "authors": "Nicolas Violante, Andréas Meuleman, Alban Gauthier, Fredo Durand, Thibault Groueix, George Drettakis",
    "title": "Splat and Replace: 3D Reconstruction with Repetitive Elements",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730727",
    "pdf_link": null,
    "abstract": "We leverage repetitive elements in 3D scenes to improve novel view synthesis. Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly improved novel view synthesis but renderings of unseen and occluded parts remain low-quality if the training views are not exhaustive enough. Our key observation is that our environment is often full of repetitive elements. We propose to leverage those repetitions to improve the reconstruction of low-quality parts of the scene due to poor coverage and occlusions. We propose a method that segments each repeated instance in a 3DGS reconstruction, registers them together, and allows information to be shared among instances. Our method improves the geometry while also accounting for appearance variations across instances. We demonstrate our method on a variety of synthetic and real scenes with typical repetitive elements, leading to a substantial improvement in the quality of novel view synthesis.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "2e10432d43",
    "authors": "Minghao Yin, Yukang Cao, Songyou Peng, Kai Han",
    "title": "Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and Spatially Consistent Content Creation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730752",
    "pdf_link": null,
    "abstract": "Generating high-quality 4D content from monocular videos—for applications such as digital humans and AR/VR—poses challenges in ensuring temporal and spatial consistency, preserving intricate details, and incorporating user guidance effectively. To overcome these challenges, we introduce Splat4D, a novel framework enabling high-fidelity 4D content generation from a monocular video. Splat4D achieves superior performance while maintaining faithful spatial-temporal coherence, by leveraging multi-view rendering, inconsistency identification, a video diffusion model, and an asymmetric U-Net for refinement. Through extensive evaluations on public benchmarks, Splat4D consistently demonstrates state-of-the-art performance across various metrics, underscoring the efficacy of our approach. Additionally, the versatility of Splat4D is validated in various applications such as text/image conditioned 4D generation, 4D human generation, and text-guided content editing, producing coherent outcomes following user instructions. Project page: https://visual-ai.github.io/splat4d",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "46512e324a",
    "authors": "Mingyang Song, Yang Zhang, Marko Mihajlovic, Siyu Tang, Markus Gross, Tunc Ozan Aydin",
    "title": "Spline Deformation Field",
    "paper_url": "https://link.springer.com/article/10.1007/BF02410992",
    "pdf_link": null,
    "abstract": "… the complete, two-dimensional deformation field during the image correlation process on … In this work, a B-Spline function is used to represent the object deformation field throughout the …",
    "scholar_publication": "Experimental mechanics, 2002 - Springer"
  },
  {
    "paper_id": "7f52c2edf7",
    "authors": "Sara Sabour, Lily Goli, George Kopanas, Mark Matthews, Dmitry Lagun, Leonidas Guibas, Alec Jacobson, David Fleet, Andrea Tagliasacchi, Sara Sabour, Lily Goli",
    "title": "SpotLessSplats: Ignoring Distractors in 3D Gaussian Splatting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3727143",
    "pdf_link": null,
    "abstract": "Three-dimensional Gaussian Splatting (3DGS) is a promising technique for 3D reconstruction, offering efficient training and rendering speeds, making it suitable for real-time applications. However, current methods require highly controlled environments–no moving people or wind-blown elements, and consistent lighting–to meet the interview consistency assumption of 3DGS. This makes reconstruction of real-world captures problematic. We present SpotLessSplats, an approach that leverages pre-trained and general-purpose features coupled with robust optimization to effectively ignore transient distractors. Our method achieves state-of-the-art reconstruction quality both visually and quantitatively, on casual captures.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "0c84027bb6",
    "authors": "Forrest Iandola, Stanislav Pidhorskyi, Igor Santesteban, Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Frank Yu, Emanuel Garbin, Tomas Simon, Shunsuke Saito",
    "title": "SqueezeMe: Mobile-Ready Distillation of Gaussian Full-Body Avatars",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730599",
    "pdf_link": null,
    "abstract": "Gaussian-based human avatars have achieved an unprecedented level of visual fidelity. However, existing approaches based on high-capacity neural networks typically require a desktop GPU to achieve real-time performance for a single avatar, and it remains non-trivial to animate and render such avatars on mobile devices including a standalone VR headset due to substantially limited memory and computational bandwidth. In this paper, we present SqueezeMe, a simple and highly effective framework to convert high-fidelity 3D Gaussian full-body avatars into a lightweight representation that supports both animation and rendering with mobile-grade compute. Our key observation is that the decoding of pose-dependent Gaussian attributes from a neural network creates non-negligible memory and computational overhead. Inspired by blendshapes and linear pose correctives widely used in Computer Graphics, we address this by distilling the pose correctives learned with neural networks into linear layers. Moreover, we further reduce the parameters by sharing the correctives among nearby Gaussians. Combining them with a custom splatting pipeline based on Vulkan, we achieve, for the first time, simultaneous animation and rendering of 3 Gaussian avatars in real-time (72 FPS) on a Meta Quest 3 VR headset.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ef06f7c11f",
    "authors": "Jerry Hsu, Tongtong Wang, Kui Wu, Cem Yuksel",
    "title": "Stable Cosserat Rods",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730618",
    "pdf_link": null,
    "abstract": "… This solver significantly improves the numerical stability with Cosserat rods, … stable than alternative rod solvers, such as extended position-based dynamics and discrete elastic rods…",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "b23649de0d",
    "authors": "Yuxuan Zhang, Yirui Yuan, Yiren Song, Jiaming Liu",
    "title": "Stable-Makeup: When Real-World Makeup Transfer Meets Diffusion Model",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730702",
    "pdf_link": null,
    "abstract": "Current makeup transfer methods are limited to simple makeup styles, making them difficult to apply in real-world scenarios. In this paper, we introduce Stable-Makeup, a novel diffusion-based makeup transfer method capable of robustly transferring a wide range of real-world makeup, onto user-provided faces. Stable-Makeup is based on a pre-trained diffusion model and utilizes a Detail-Preserving (D-P) makeup encoder to encode makeup details. It also employs content and structural control modules to preserve the content and structural information of the source image. With the aid of our newly added makeup cross-attention layers in U-Net, we can accurately transfer the detailed makeup to the corresponding position in the source image. After content-structure decoupling training, Stable-Makeup can maintain the content and the facial structure of the source image. Moreover, our method has demonstrated strong robustness and generalizability, making it applicable to various tasks such as cross-domain makeup transfer, makeup-guided text-to-image generation, and so on. Extensive experiments have demonstrated that our approach delivers state-of-the-art results among existing makeup transfer methods and exhibits a highly promising with broad potential applications in various related fields.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "c6ff1c3cde",
    "authors": "Christina Shatford, Szymon Rusinkiewicz",
    "title": "Sticking Information in Plain Sight: Encoding and Detecting Hidden Stickers in the Real World",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743018",
    "pdf_link": null,
    "abstract": "While there are many techniques (e.g., QR codes) that convey information via visual patterns, many applications would benefit from having those codes be imperceptible to the human eye. We present a method for designing subtle code-conveying patterns that can be printed on transparent sticker paper, then applied to real-world surfaces. An image of a scene with an encoded sticker can be sent through our localization and decoding modules, where the sticker subsection is robustly localized and decoded. We jointly optimize the encoding, localization, and decoding modules end to end, taking into account both imperceptibility and accuracy. Notably, we also account for human error when placing stickers, as pixel-perfect alignment is not something that can be reliably expected. Our model encodes and decodes 100-bit secrets, which, with BCH error correction, means that a sticker could encode 56 data bits with 40 parity bits. Experimental results show that this method is robust to sticker placement errors while being easy to deploy in the real world.",
    "scholar_publication": "Proceedings of the Special Interest Group …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "2ea6787f62",
    "authors": "Pu Li, Wenhao Zhang, Jinglu Chen, Dongming Yan",
    "title": "Stitch-A-Shape: Bottom-up Learning for B-Rep Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730661",
    "pdf_link": null,
    "abstract": "Boundary representation (B-Rep) models serve as the primary representation format in modern CAD systems for describing 3D shapes. While deep learning has achieved success with various geometric representations, B-Reps remain challenging due to their hybrid nature of combining continuous geometry with discrete topological relationships. In this paper, we present Stitch-A-Shape, a B-Rep generation framework that directly models both topology and geometry. This strategy departs from prior work that focuses on either topology or geometry while recovering the other through post-processing. Our method consists of a geometry module that determines the spatial configuration of geometric elements (vertices, curves, and surface control points) and a topology module that establishes connectivity relationships and identifies boundary structures, including outer and inner loops. Our approach leverages a sequential \"stitching\" representation that mirrors the native data structure and inherent bottom-up organization of B-Rep, assembling geometric entities from vertices through curves to faces. We validate that our framework can handle topological and geometric ambiguities, as well as open surfaces and compound solids. Experiments show that Stitch-A-Shape achieves superior generation quality and computational efficiency compared to existing approaches in unconditional generation tasks, while exhibiting effective capabilities in class-conditional generation and B-Rep autocompletion applications.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ae90e7b312",
    "authors": "Abhishek Madan, Nicholas Sharp, Francis Williams, Ken Museth, David I.W. Levin",
    "title": "Stochastic Barnes-Hut Approximation for Fast Summation on the GPU",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730725",
    "pdf_link": null,
    "abstract": "We present a novel stochastic version of the Barnes-Hut approximation. Regarding the level-of-detail (LOD) family of approximations as control variates, we construct an unbiased estimator of the kernel sum being approximated. Through several examples in graphics applications such as winding number computation and smooth distance evaluation, we demonstrate that our method is well-suited for GPU computation, capable of outperforming a GPU-optimized implementation of the deterministic Barnes-Hut approximation by achieving equal median error in up to 9.4x less time.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "3727e59143",
    "authors": "Selena Ling, Merlin Nimier-David, Alec Jacobson, Nicholas Sharp",
    "title": "Stochastic Preconditioning for Neural Field Optimization",
    "paper_url": "https://arxiv.org/abs/2505.20473",
    "pdf_link": null,
    "abstract": "Neural fields are a highly effective representation across visual computing. This work observes that fitting these fields is greatly improved by incorporating spatial stochasticity during training, and that this simple technique can replace or even outperform custom-designed hierarchies and frequency space constructions. The approach is formalized as implicitly operating on a blurred version of the field, evaluated in-expectation by sampling with Gaussian-distributed offsets. Querying the blurred field during optimization greatly improves convergence and robustness, akin to the role of preconditioners in numerical linear algebra. This implicit, sampling-based perspective fits naturally into the neural field paradigm, comes at no additional cost, and is extremely simple to implement. We describe the basic theory of this technique, including details such as handling boundary conditions, and extending to a spatially-varying blur. Experiments demonstrate this approach on representations including coordinate MLPs, neural hashgrids, triplanes, and more, across tasks including surface reconstruction and radiance fields. In settings where custom-designed hierarchies have already been developed, stochastic preconditioning nearly matches or improves their performance with a simple and unified approach; in settings without existing hierarchies it provides an immediate boost to quality and robustness.",
    "scholar_publication": "arXiv preprint arXiv …, 2025 - arxiv.org"
  },
  {
    "paper_id": "1f9c13283a",
    "authors": "Luchuan Song, Yang Zhou, Zhan Xu, Yi Zhou, Deepali Aneja, Chenliang Xu",
    "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730635",
    "pdf_link": null,
    "abstract": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The StreamME synchronously records and reconstructs a head avatar from live video streams without any pre-cached data, enabling seamless integration of the reconstructed appearance into downstream applications. This exceptionally fast training strategy, which we refer to as on-the-fly training, is central to our approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating the reliance on MLPs in deformable 3DGS and relying solely on geometry, which significantly improves the adaptation speed to facial expression. To further ensure high efficiency in on-the-fly training, we introduced a simplification strategy based on primary points, which distributes the point clouds more sparsely across the facial surface, optimizing points number while maintaining rendering quality. Leveraging the on-the-fly training capabilities, our method protects the facial privacy and reduces communication bandwidth in VR system or online conference. Additionally, it can be directly applied to downstream application such as animation, toonify, and relighting. Please refer to our project page for more details: https://songluchuan.github.io/StreamME/.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "6c4b798e69",
    "authors": "Hangming Fan, Yuchi Huo, Chuankun Zheng, Chonghao Hu, Yazhen Yuan, Rui Wang",
    "title": "Streaming-Aware Neural Monte Carlo Rendering Framework with Unified Denoising-Compression and Client Collaboration",
    "paper_url": "https://s2025.conference-schedule.org/presentation/?id=papers_686&sess=sess133",
    "pdf_link": null,
    "abstract": null,
    "scholar_publication": null
  },
  {
    "paper_id": "4f6bfbcfbe",
    "authors": "Bosheng Li, Nikolas Schwarz, Wojtek Palubicki, Sören Pirk, Dominik L. Michels, Bedrich Benes",
    "title": "Stressful Tree Modeling: Breaking Branches with Strands",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730745",
    "pdf_link": null,
    "abstract": "We propose a novel approach for the computational modeling of lignified tissues, such as those found in tree branches and timber. We leverage a state-of-the-art strand-based representation for tree form, which we extend to describe biophysical processes at short and long time scales. Simulations at short time scales enable us to model different breaking patterns due to branch bending, twisting, and breaking. On long timescales, our method enables the simulation of realistic branch shapes under the influence of plausible biophysical processes, such as the development of compression and tension wood. We specifically focus on computationally fast simulations of woody material, enabling the interactive exploration of branches and wood breaking. By leveraging Cosserat rod physics, our method enables the generation of a wide variety of breaking patterns. We showcase the capabilities of our method by performing and visualizing numerous experiments.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "8b4237151c",
    "authors": "Yuqing Liu, Rinchong Kim, Kyunghee Kim",
    "title": "Stroke Imprint: Knitting Reassurance Into Anxious Moments",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743045",
    "pdf_link": null,
    "abstract": "Imagine if, during moments of heightened anxiety, you could once again feel the gentle, familiar touch of a loved one’s hand. Stroke Imprint is a knitted wearable that simulates stroking sensations to comfort young women experiencing anxiety through pressure sensing and SMA-based actuation. Paired with a digital interface, the glove allows users to record personalized tactile sensation. Through user interviews, design iterations, and user testing, the study demonstrates its potential as an anxiety tracking, therapeutic wearable within a closed biofeedback loop.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "c097941436",
    "authors": "Naoto Shirashima, Hideki Todo, Yuki Yamaoka, Shizuo Kaji, Kunihiko Kobayashi, Haruna Shimotahira, Yonghao Yue",
    "title": "Stroke Transfer for Participating Media",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730603",
    "pdf_link": null,
    "abstract": "We present a method for generating stroke-based painterly drawings of participating media, such as smoke, fire, and clouds, by transferring stroke attributes—color, width, length, and orientation—from exemplar to animation frames. Building on the stroke transfer framework, we introduce features and basis fields capturing lighting-, view-, and geometry-dependent information, extending surface-based ones (e.g., intensity, apparent normals and curvatures, and distance from silhouettes) to volumetric scenes while supporting traditional surface objects. Novel features, including apparent relative velocity and mean free-path, address non-rigid flow and dynamic scenes. Our system combines automated exemplar selection, user-guided style learning, and temporally coherent stroke generation, enabling artistic and expressive visualizations of dynamic media.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "a625467b11",
    "authors": "Zhenglin Pan, Haoran Xie",
    "title": "StructInbet: Integrating Explicit Structural Guidance Into Inbetween Frame Generation",
    "paper_url": "https://dl.acm.org/doi/full/10.1145/3721250.3743032",
    "pdf_link": null,
    "abstract": "2 MethodThe overall model architecture is shown in Figure 2. StructInbet is implemented on a pre-trained Stable Diffusion UNet [Rombach et al. 2021], extended with two conditioning branches. The first is a ControlNet [Zhang et al. 2023] encoder that processes structural guidance and trajectory maps, guiding gesture generation in the diffusion UNet through zero-convolution modules. The second is a character reference UNet, architecturally similar to the diffusion UNet, that extracts appearance features from the start and end frames to ensure visual consistency of the character’s appearance.",
    "scholar_publication": "Proceedings of the Special Interest Group on Computer …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "9d259949ee",
    "authors": "Jiepeng Wang, Hao Pan, Yang Liu, Xin Tong, Taku Komura, Wenping Wang, Jiepeng Wang",
    "title": "StructRe: Rewriting for Structured Shape Modeling",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3732934",
    "pdf_link": null,
    "abstract": "Man-made 3D shapes are naturally organized in parts and hierarchies; such structures provide important constraints for shape reconstruction and generation. Modeling shape structures is difficult, because there can be multiple hierarchies for a given shape, causing ambiguity, and across different categories, the shape structures are correlated with semantics, limiting generalization. We present StructRe, a structure rewriting system, as a novel approach to structured shape modeling. Given a 3D object represented by points and …",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "7054494440",
    "authors": "Peiying Zhang, Nanxuan Zhao, Jing Liao",
    "title": "Style Customization of Text-to-Vector Generation with Image Diffusion Priors",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730707",
    "pdf_link": null,
    "abstract": "Scalable Vector Graphics (SVGs) are highly favored by designers due to their resolution independence and well-organized layer structure. Although existing text-to-vector (T2V) generation methods can create SVGs from text prompts, they often overlook an important need in practical applications: style customization, which is vital for producing a collection of vector graphics with consistent visual appearance and coherent aesthetics.Extending existing T2V methods for style customization poses certain challenges. Optimization-based T2V models can utilize the priors of text-to-image (T2I) models for customization, but struggle with maintaining structural regularity. On the other hand, feed-forward T2V models can ensure structural regularity, yet they encounter difficulties in disentangling content and style due to limited SVG training data.To address these challenges, we propose a novel two-stage style customization pipeline for SVG generation, making use of the advantages of both feed-forward T2V models and T2I image priors. In the first stage, we train a T2V diffusion model with a path-level representation to ensure the structural regularity of SVGs while preserving diverse expressive capabilities. In the second stage, we customize the T2V diffusion model to different styles by distilling customized T2I models. By integrating these techniques, our pipeline can generate high-quality and diverse SVGs in custom styles based on text prompts in an efficient feed-forward manner. The effectiveness of our method has been validated through extensive experiments. The project page is https://customsvg.github.io.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "691a3710f7",
    "authors": "Haowei Xiong, Kexin Nie, Jiachen Zeng, Shujing Shen, Mengyao Guo",
    "title": "SugART: Mixed Reality Sugar Painting for Intangible Cultural Heritage Learning at Home",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742997",
    "pdf_link": null,
    "abstract": "This paper introduces SugART, a Mixed Reality (MR) project that enables users to learn and recreate traditional sugar painting at home. By combining hand tracking, virtual guidance, and real-time feedback, our project supports creative expression and cultural education, thereby lowering barriers to participation in intangible cultural heritage through accessible and interactive digital experiences.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "e7bf577c8a",
    "authors": "Volodymyr Karpenko, Taimoor Tariq, Jorge Condor, Piotr Didyk",
    "title": "Super Resolution for Humans",
    "paper_url": "https://www.sciencedirect.com/science/article/pii/S0031320310000853",
    "pdf_link": null,
    "abstract": "… resolution images share similar intrinsic geometries, various recent super-resolution methods reconstruct high resolution … the local embedding of low resolution images. These methods …",
    "scholar_publication": "Pattern Recognition, 2010 - Elsevier"
  },
  {
    "paper_id": "72576bfa73",
    "authors": "Ruipeng Wang, Zhen Ren, Jinxiang Wang",
    "title": "SurfelPlus: A Surfel-Based Global Illumination Solution Optimized for Low-End Graphics Hardware",
    "paper_url": "https://dl.acm.org/doi/full/10.1145/3721250.3742969",
    "pdf_link": null,
    "abstract": "2 MethodSurfelPlus is a real-time global illumination renderer built on NVIDIA’s vk_raytrace framework, leveraging Vulkan ray tracing and surfel-based GI methods. Our approach dynamically discretizes scene geometry into surfels—point primitives storing position, normal, color, and material properties [Pfister et al. 2000]—to efficiently compute indirect lighting.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "8fe1eb9386",
    "authors": "Ellie Arar, Yarden Frenkel, Daniel Cohen-Or, Ariel Shamir, Yael Vinker",
    "title": "SwiftSketch: A Diffusion Model for Image-to-Vector Sketch Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730612",
    "pdf_link": null,
    "abstract": "Recent advancements in large vision-language models have enabled highly expressive and diverse vector sketch generation. However, state-of-the-art methods rely on a time-consuming optimization process involving repeated feedback from a pretrained model to determine stroke placement. Consequently, despite producing impressive sketches, these methods are limited in practical applications. In this work, we introduce SwiftSketch, a diffusion model for image-conditioned vector sketch generation that can produce high-quality sketches in less than a second. SwiftSketch operates by progressively denoising stroke control points sampled from a Gaussian distribution. Its transformer-decoder architecture is designed to effectively handle the discrete nature of vector representation and capture the inherent global dependencies between strokes. To train SwiftSketch, we construct a synthetic dataset of image-sketch pairs, addressing the limitations of existing sketch datasets, which are often created by non-artists and lack professional quality. For generating these synthetic sketches, we introduce ControlSketch, a method that enhances SDS-based techniques by incorporating precise spatial control through a depth-aware ControlNet. We demonstrate that SwiftSketch generalizes across diverse concepts, efficiently producing sketches that combine high fidelity with a natural and visually appealing style.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "c9378e2887",
    "authors": "Cédric Zanni, Cédric Zanni",
    "title": "Synchronized tracing of primitive-based implicit volumes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3702227",
    "pdf_link": null,
    "abstract": "Implicit volumes are known for their ability to represent smooth shapes of arbitrary topology thanks to hierarchical combinations of primitives using a structure called a blobtree. We present a new tile-based rendering pipeline well suited for modeling scenarios, i.e., no preprocessing is required when primitive parameters are updated. When using approximate signed distance fields (fields with Lipschitz bound close to 1), we rely on compact, smooth CSG operators - extended from standard bounded operators - to compute a tight augmented bounding volume for all primitives of the blobtree. The pipeline relies on a low-resolution A-buffer storing the primitives of interest of a given screen tile. The A-buffer is then used during ray processing to synchronize threads within a subfrustum. This allows coherent field evaluation within workgroups. We use a sparse bottom-up tree traversal to prune the blobtree on-the-fly which allows us to decorrelate field evaluation complexity from the full blobtree size. The ray processing itself is done using the sphere tracing algorithm. The pipeline scales well to volumes consisting of thousands of primitives.",
    "scholar_publication": "ACM Transactions on Graphics, 2024 - dl.acm.org"
  },
  {
    "paper_id": "ef462e65bb",
    "authors": "Aleksander Plocharski, Jan Swidzinski, Przemyslaw Musialski",
    "title": "Taking Control: Procedural Diffusion Guidance for Architectural Facade Editing",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742991",
    "pdf_link": null,
    "abstract": "Our training-free method enables photorealistic facade editing by combining hierarchical procedural structure control with diffusion models. Starting from a facade image, we reconstruct, edit, and guide generation to produce high-fidelity, photorealistic variations. The method ensures structural consistency and appearance preservation, demonstrating the power of symbolic modeling for controllable image synthesis.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "3e70ed34eb",
    "authors": "Gengyan S. Li, Paulo Gotardo, Timo Bolkart, Stephan Garbin, Kripasindhu Sarkar, Abhimitra Meka, Alexandros Lattas, Thabo Beeler",
    "title": "TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head Modeling",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730710",
    "pdf_link": null,
    "abstract": "Sparse volumetric reconstruction and rendering via 3D Gaussian splatting have recently enabled animatable 3D head avatars that are rendered under arbitrary viewpoints with impressive photorealism. Today, such photoreal avatars are seen as a key component in emerging applications in telepresence, extended reality, and entertainment. Building a photoreal avatar requires estimating the complex non-rigid motion of different facial components as seen in input video images; due to inaccurate motion estimation, animatable models typically present a loss of fidelity and detail when compared to their non-animatable counterparts, built from an individual facial expression. Also, recent state-of-the-art models are often affected by memory limitations that reduce the number of 3D Gaussians used for modeling, leading to lower detail and quality. To address these problems, we present a new high-detail 3D head avatar model that improves upon the state of the art, largely increasing the number of 3D Gaussians and modeling quality for rendering at 4K resolution. Our high-quality model is reconstructed from multiview input video and builds on top of a mesh-based 3D morphable model, which provides a coarse deformation layer for the head. Photoreal appearance is modelled by 3D Gaussians embedded within the continuous UVD tangent space of this mesh, allowing for more effective densification where most needed. Additionally, these Gaussians are warped by a novel UVD deformation field to capture subtle, localized motion. Our key contribution is the novel deformable Gaussian encoding and overall fitting procedure that allows our head model to preserve appearance detail, while capturing facial motion and other transient high-frequency features such as skin wrinkling.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "7a555b716b",
    "authors": "Alexandre Binninger, Ruben Wiersma, Philipp Herholz, Olga Sorkine-Hornung",
    "title": "TetWeave: Isosurface Extraction using On-The-Fly Delaunay Tetrahedral Grids for Gradient-Based Mesh Optimization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730851",
    "pdf_link": null,
    "abstract": "We introduce TetWeave, a novel isosurface representation for gradient-based mesh optimization that jointly optimizes the placement of a tetrahedral grid used for Marching Tetrahedra and a novel directional signed distance at each point. TetWeave constructs tetrahedral grids on-the-fly via Delaunay triangulation, enabling increased flexibility compared to predefined grids. The extracted meshes are guaranteed to be watertight, two-manifold and intersection-free. The flexibility of TetWeave enables a resampling strategy that places new points where reconstruction error is high and allows to encourage mesh fairness without compromising on reconstruction error. This leads to high-quality, adaptive meshes that require minimal memory usage and few parameters to optimize. Consequently, TetWeave exhibits near-linear memory scaling relative to the vertex count of the output mesh — a substantial improvement over predefined grids. We demonstrate the applicability of TetWeave to a broad range of challenging tasks in computer graphics and vision, such as multi-view 3D reconstruction, mesh compression and geometric texture generation. Our code is available at https://github.com/AlexandreBinninger/TetWeave.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "50666ca6f7",
    "authors": "Yiqian Wu, Malte Prinzler, Xiaogang Jin, Siyu Tang",
    "title": "Text-based Animatable 3D Avatars with Morphable Model Alignment",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730680",
    "pdf_link": null,
    "abstract": "The generation of high-quality, animatable 3D head avatars from text has enormous potential in content creation applications such as games, movies, and embodied virtual assistants. Current text-to-3D generation methods typically combine parametric head models with 2D diffusion models using score distillation sampling to produce 3D-consistent results. However, they struggle to synthesize realistic details and suffer from misalignments between the appearance and the driving parametric model, resulting in unnatural animation results. We discovered that these limitations stem from ambiguities in the 2D diffusion predictions during 3D avatar distillation, specifically: i) the avatar’s appearance and geometry is underconstrained by the text input, and ii) the semantic alignment between the predictions and the parametric head model is insufficient because the diffusion model alone cannot incorporate information from the parametric model. In this work, we propose a novel framework, AnimPortrait3D, for text-based realistic animatable 3DGS avatar generation with morphable model alignment, and introduce two key strategies to address these challenges. First, we tackle appearance and geometry ambiguities by utilizing prior information from a pretrained text-to-3D model to initialize a 3D avatar with robust appearance, geometry, and rigging relationships to the morphable model. Second, we refine the initial 3D avatar for dynamic expressions using a ControlNet that is conditioned on semantic and normal maps of the morphable model to ensure accurate alignment. As a result, our method outperforms existing approaches in terms of synthesis quality, alignment, and animation fidelity. Our experiments show that the proposed method advances the state of the art in text-based, animatable 3D head avatar generation. Code and model for this paper are at AnimPortrait3D.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "8da6585bf8",
    "authors": "Julian Knodt, Xifeng Gao, Julian Knodt",
    "title": "Texture Size Reduction Through Symmetric Overlap and Texture Carving",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3714408",
    "pdf_link": null,
    "abstract": "Maintaining memory-efficient 3D assets is critical for game development due to size constraints for applications, as well as runtime costs such as GPU data transfers. While most prior work on 3D modeling focuses on reducing triangle count, few works focus on reducing texture sizes. We propose an automatic approach to reduce the texture size for 3D models while maintaining the rendered appearance of the original input. The two core components of our approach are (1) overlapping identical UV charts and folding mirrored regions within charts through an optimal transport optimization, and (2) carving redundant and void texels in a UV-aware and texture-aware way without inverting the UV mesh. The first component creates additional void space, whereas the second removes void space, and their combination can greatly increase texels utilized by the UV mesh at lower texture resolutions. Our method is robust and general, and can process a 3D model with arbitrary UV layout and multiple textures without modifying the 3D mesh. We evaluate our approach on 110 models from the Google Scanned Object dataset and 64 models from Sketchfab. Compared to other approaches, ours has on average 1 to 3 dB PSNR higher rendering similarity and reduces pixelation in visual comparisons.",
    "scholar_publication": "ACM Transactions on Graphics, 2025 - dl.acm.org"
  },
  {
    "paper_id": "d852e3580f",
    "authors": "YI JEN LIN, Wei-Chen Yen, Chun-Cheng Hsu",
    "title": "The Gesture Lives On: A VR-Driven Puppet Performance in Immersive Space",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742981",
    "pdf_link": null,
    "abstract": "The Gesture Lives On is a real-time VR performance system that reimagines traditional Taiwanese glove puppetry through immersive, interactive means. Rooted in precise gestural movement, this art form faces decline amid shifting audience engagement. Using VR-based gesture recognition, a performer co-creates a digital duet with a virtual puppet. This hybrid space transforms tradition into contemporary expression, offering audiences a new way to experience puppetry within a shared virtual–physical environment.",
    "scholar_publication": "Proceedings of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "595675cc70",
    "authors": "Maria Larsson, Hodaka Yamaguchi, Ehsan Pajouheshgar, I-Chao Shen, Kenji Tojo, Chia-Ming Chang, Lars Hansson, Olof Broman, Takashi Ijiri, Ariel Shamir, Wenzel Jakob, Takeo Igarashi",
    "title": "The Mokume Dataset and Inverse Modeling of Solid Wood Textures",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730874",
    "pdf_link": null,
    "abstract": "We present the Mokume dataset for solid wood texturing consisting of 190 cube-shaped samples of various hard and softwood species documented by high-resolution exterior photographs, annual ring annotations, and volumetric computed tomography (CT) scans. A subset of samples further includes photographs along slanted cuts through the cube for validation purposes. Using this dataset, we propose a three-stage inverse modeling pipeline to infer solid wood textures using only exterior photographs. Our method begins by evaluating a neural model to localize year rings on the cube face photographs. We then extend these exterior 2D observations into a globally consistent 3D representation by optimizing a procedural growth field using a novel iso-contour loss. Finally, we synthesize a detailed volumetric color texture from the growth field. For this last step, we propose two methods with different efficiency and quality characteristics: a fast inverse procedural texture method, and a neural cellular automaton (NCA). We demonstrate the synergy between the Mokume dataset and the proposed algorithms through comprehensive comparisons with unseen captured data. We also present experiments demonstrating the efficiency of our pipeline's components against ablations and baselines. Our code, the dataset, and reconstructions are available via https://mokumeproject.github.io/.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "db64d6e54e",
    "authors": "Zhen Han, Mattias Teye, Derek Yadgaroff, Judith Bütepage",
    "title": "Tiny is not small enough: High quality, low-resource facial animation models through hybrid knowledge distillation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730929",
    "pdf_link": null,
    "abstract": "The training of high-quality, robust machine learning models for speech-driven 3D facial animation requires a large, diverse dataset of high-quality audio-animation pairs. To overcome the lack of such a dataset, recent work has introduced large pre-trained speech encoders that are robust to variations in the input audio and, therefore, enable the facial animation model to generalize across speakers, audio quality, and languages. However, the resulting facial animation models are prohibitively large and lend themselves only to offline inference on a dedicated machine. In this work, we explore on-device, real-time facial animation models in the context of game development. We overcome the lack of large datasets by using hybrid knowledge distillation with pseudo-labeling. Given a large audio dataset, we employ a high-performing teacher model to train very small student models. In contrast to the pre-trained speech encoders, our student models only consist of convolutional and fully-connected layers, removing the need for attention context or recurrent updates. In our experiments, we demonstrate that we can reduce the memory footprint to up to 3.4 MB and required future audio context to up to 81 ms while maintaining high-quality animations. This paves the way for on-device inference, an important step towards realistic, model-driven digital characters.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "69cf00859e",
    "authors": "Daniele Della Pietra, Gino Lanzo Hahn, Nicola Garau",
    "title": "To Infinity and Beyond: a GPU-Driven Memory Sharing Pipeline to Generate and Process Infinite Synthetic Data",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742983",
    "pdf_link": null,
    "abstract": "What if data generation, manipulation, and training could all happen entirely on the GPU, without ever touching the RAM or the CPU? In this work, we present a novel pipeline based on Unreal Engine 5, which allows us to generate, render, and process graphics data entirely on the GPU. By keeping the data stored in GPU memory throughout all the steps, we bypass the traditional bottlenecks related to CPU-GPU transfers, significantly accelerating data manipulation and enabling fast training of deep learning algorithms. Traditional storage systems impose latency and capacity limitations, which become increasingly problematic as data volume increases. Our method demonstrates substantial performance improvements on multiple benchmarks, offering a new paradigm for integrating game engines with data-driven applications. More information on our project page: https://mmlab-cv.github.io/Infinity/",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "1161110a79",
    "authors": "Daniel Garibi, Shahar Yadin, Roni Paiss, Omer Tov, Shiran Zada, Ariel Ephrat, Tomer Michaeli, Inbar Mosseri, Tali Dekel",
    "title": "TokenVerse: Versatile Multi-concept Personalization in Token Modulation Space",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730843",
    "pdf_link": null,
    "abstract": "We present TokenVerse - a method for multi-concept personalization, leveraging a pre-trained text-to-image diffusion model. Our framework can disentangle complex visual elements and attributes from as little as a single image, while enabling seamless plug-and-play generation of combinations of concepts extracted from multiple images. As opposed to existing works, TokenVerse can handle multiple images with multiple concepts each, and supports a wide-range of concepts, including objects, accessories, materials, pose, and lighting. Our work exploits a DiT-based text-to-image model, in which the input text affects the generation through both attention and modulation (shift and scale). We observe that the modulation space is semantic and enables localized control over complex concepts. Building on this insight, we devise an optimization-based framework that takes as input an image and a text description, and finds for each word a distinct direction in the modulation space. These directions can then be used to generate new images that combine the learned concepts in a desired configuration. We demonstrate the effectiveness of TokenVerse in challenging personalization settings, and showcase its advantages over existing methods.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "ab9d033b40",
    "authors": "Daniel Zint, Zhouyuan Chen, Yifei Zhu, Denis Zorin, Teseo Schneider, Daniele Panozzo",
    "title": "Topological Offsets",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731157",
    "pdf_link": null,
    "abstract": "… , watertight, and self-intersection-free offset surface strictly enclosing the input, while doing … topological offset around the input with purely combinatorial operations. The topological offset …",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "1812c90a60",
    "authors": "Hidehito Ohba, Tatsuya Yatagawa, Shigeo Morishima",
    "title": "Towards Accelerating Polarization Path tracing of Multi-Bounce Smith Microfacet BSDFs",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742996",
    "pdf_link": null,
    "abstract": "The polarization state of light is described in a local coordinate frame, where the oscillation of the electronic and magnetic fields occurs. In physics, this frame is rotated according to the surface normal of the object. In this study, we investigate the effect of this frame rotation while evaluating multi-bounce Smith microfacet BSDFs. We show evidence that we can speed up the evaluation if the frame rotation does not significantly matter. Then, we experimentally show the acceleration can be practically feasible.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "53dc9a35dd",
    "authors": "Zilin Xu, Xiang Chen, Chen Liu, Beibei Wang, Lu Wang, Zahra Montazeri, Ling-Qi Yan",
    "title": "Towards Comprehensive Neural Materials: Dynamic Structure-Preserving Synthesis with Accurate Silhouette at Instant Inference Speed",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730626",
    "pdf_link": null,
    "abstract": "Photorealistic rendering aims to accurately replicate real-world appearances. Traditional methods, like microfacet-based models, often struggle with complex visuals. Consequently, neural material techniques have emerged, typically offering improved performance over traditional approaches. However, these neural material approaches only attempt to address one or a few essential aspects of the complete appearance while neglecting others (quality, parallax & silhouette, synthesis, performance). Although these aspects may seem separate, they are inherently intertwined as part of the complete appearance which cannot be isolated. In this paper, we challenge the comprehensive neural material representation by thoroughly considering the essential aspects of the complete appearance. We introduce an int8-quantized neural network that keeps high fidelity (quality) while achieving an order of magnitude speedup (performance) compared to previous methods. We also present a controllable structure-preserving synthesis strategy (synthesis), along with accurate displacement effects (parallax & silhouette) through a dynamic two-step displacement tracing technique.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "e4c9cfc3eb",
    "authors": "Sophie Kergaßner, Taimoor Tariq, Piotr Didyk",
    "title": "Towards Understanding Depth Perception in Foveated Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730609",
    "pdf_link": null,
    "abstract": "The true vision for real-time virtual and augmented reality is reproducing our visual reality in its entirety on immersive displays. To this end, foveated rendering leverages the limitations of spatial acuity in human peripheral vision to allocate computational resources to the fovea while reducing quality in the periphery. Such methods are often derived from studies on the spatial resolution of the human visual system and its ability to perceive blur in the periphery, enabling the potential for high spatial quality in real-time. However, the effects of blur on other visual cues that depend on luminance contrast, such as depth, remain largely unexplored. It is critical to understand this interplay, as accurate depth representation is a fundamental aspect of visual realism. In this paper, we present the first evaluation exploring the effects of foveated rendering on stereoscopic depth perception. We design a psychovisual experiment to quantitatively study the effects of peripheral blur on depth perception. Our analysis demonstrates that stereoscopic acuity remains unaffected (or even improves) by high levels of peripheral blur. Based on our studies, we derive a simple perceptual model that determines the amount of foveation that does not affect stereoacuity. Furthermore, we analyze the model in the context of common foveation practices reported in literature. The findings indicate that foveated rendering does not impact stereoscopic depth perception, and stereoacuity remains unaffected with up to 2 × stronger foveation than commonly used. Finally, we conduct a validation experiment and show that our findings hold for complex natural stimuli.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "f5f483e82b",
    "authors": "Hyunsoo Kim, Jinah Park",
    "title": "Train Once, Generate Anywhere: Discretization Agnostic Neural Cellular Automata Using SPH Method",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3742998",
    "pdf_link": null,
    "abstract": "2 MethodsThis section details the SPH-NCA method, focusing on our key contributions: SPH perception, gated adaptation, and the progressive growing training scheme. Figure 2 provides an overview of the architecture and its main components, while additional details can be found in the supplemental material.",
    "scholar_publication": "Proceedings of the Special Interest Group on Computer …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "03420472b4",
    "authors": "Chengxu Zuo, Jiawei Huang, Xiao Jiang, Yuan Yao, Xiangren Shi, Rui Cao, Xinyu Yi, Feng Xu, Shihui Guo, Yipeng Qin",
    "title": "Transformer IMU Calibrator: Dynamic On-body IMU Calibration for Inertial Motion Capture",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730937",
    "pdf_link": null,
    "abstract": "In this paper, we propose a novel dynamic calibration method for sparse inertial motion capture systems, which is the first to break the restrictive absolute static assumption in IMU calibration, i.e., the coordinate drift RG′ G and measurement offset RBS remain constant during the entire motion, thereby significantly expanding their application scenarios. Specifically, we achieve real-time estimation of RG′ G and RBS under two relaxed assumptions: i) the matrices change negligibly in a short time window; ii) the human movements/IMU readings are diverse in such a time window. Intuitively, the first assumption reduces the number of candidate matrices, and the second assumption provides diverse constraints, which greatly reduces the solution space and allows for accurate estimation of RG′ G and RBS from a short history of IMU readings in real time. To achieve this, we created synthetic datasets of paired RG′ G, RBS matrices and IMU readings, and learned their mappings using a Transformer-based model. We also designed a calibration trigger based on the diversity of IMU readings to ensure that assumption ii) is met before applying our method. To our knowledge, we are the first to achieve implicit IMU calibration (i.e., seamlessly putting IMUs into use without the need for an explicit calibration process), as well as the first to enable long-term and accurate motion capture using sparse IMUs. The code and dataset are available at https://github.com/ZuoCX1996/TIC.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "37935be047",
    "authors": "Wesley Chang, Andrew Russell, Stephane Grabli, Matt Chiang, Christophe Hery, Doug Roble, Ravi Ramamoorthi, Tzu-Mao Li, Olivier Maury",
    "title": "Transforming Unstructured Hair Strands into Procedural Hair Grooms",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731168",
    "pdf_link": null,
    "abstract": "In recent years, reconstruction methods have been developed that can recover strand-level hair geometry from images. However, these methods recover a vast number of individual hair strands that are difficult to edit and simulate. Many methods also rely on neural priors to infer non-visible inner hair, which can result in poor inner hair structure for complex hairstyles, such as curly hair. We propose an inverse hair grooming pipeline that transforms the imperfect 3D strands from these reconstruction methods into procedural hair grooms that consist of a small set of guide strands and hair grooming operators, inspired by pipelines used by artists in popular 3D modeling tools such as Blender and Houdini. We take a probabilistic view of these hair grooms and design various optimization strategies and loss functions to optimize for the guide strands and operator parameters. Due to the proceduralism, our resulting grooms can naturally represent challenging hairstyles, have structurally sound inner hair, and are easily editable.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "4b6ad528c4",
    "authors": "Letian Huang, Dongwei Ye, Jialin Dan, Chengzhi Tao, Huiwen Liu, Kun Zhou, Bo Ren, Yuanqi Li, Yanwen Guo, Jie Guo",
    "title": "TransparentGS: Fast Inverse Rendering of Transparent Objects with Gaussians",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730892",
    "pdf_link": null,
    "abstract": "The emergence of neural and Gaussian-based radiance field methods has led to considerable advancements in novel view synthesis and 3D object reconstruction. Nonetheless, specular reflection and refraction continue to pose significant challenges due to the instability and incorrect overfitting of radiance fields to high-frequency light variations. Currently, even 3D Gaussian Splatting (3D-GS), as a powerful and efficient tool, falls short in recovering transparent objects with nearby contents due to the existence of apparent secondary ray effects. To address this issue, we propose TransparentGS, a fast inverse rendering pipeline for transparent objects based on 3D-GS. The main contributions are three-fold. Firstly, an efficient representation of transparent objects, transparent Gaussian primitives, is designed to enable specular refraction through a deferred refraction strategy. Secondly, we leverage Gaussian light field probes (GaussProbe) to encode both ambient light and nearby contents in a unified framework. Thirdly, a depth-based iterative probes query (IterQuery) algorithm is proposed to reduce the parallax errors in our probe-based framework. Experiments demonstrate the speed and accuracy of our approach in recovering transparent objects from complex environments, as well as several applications in computer graphics and vision.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "117de71949",
    "authors": "Hengyuan Chang, Xiaoxuan Xie, Syuhei Sato, Haoran Xie",
    "title": "Two-Stage Sketch-Based Smoke Illustration Generation Using Stream Function",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743034",
    "pdf_link": null,
    "abstract": "2 MethodsFigure 2 illustrates the workflow of our proposed framework for fluid video generation guided by hand-drawn sketches.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "f08b733911",
    "authors": "Huadong Zhang, Lizhou Cao, Chao Peng",
    "title": "UltraMeshRenderer: Efficient Structure and Management of GPU Out-of-core Memory for Real-time Rendering of Gigantic 3D Meshes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731186",
    "pdf_link": null,
    "abstract": "GPUs can encounter memory capacity constraints, which pose challenges for achieving real-time rendering performance when processing large 3D models that exceed available memory. State-of-the-art out-of-core rendering frameworks have leveraged Level of Detail (LOD) and frame-to-frame coherence data management techniques to optimize memory usage and minimize CPU-to-GPU data transfer costs. However, the size of view-dependently selected data may still exceed GPU memory capacity, and data transfer remains the most significant bottleneck in overall performance costs. To address these, we introduce a new GPU out-of-core rendering approach that includes a LOD selection method that takes into account both memory and coherence constraints and a parallel in-place GPU memory management algorithm that efficiently assembles the data of the current frame with GPU-resident data from the previous frame and transferred data. Our approach bounds memory usage and data transfer costs, prioritizes and schedules the transfer of essential data, incrementally refining the LOD over subsequent frames to converge toward the desired visual fidelity. Our parallel memory management algorithm consolidates frame-different and reusable data, dynamically reallocating GPU memory slots for efficient in-place operations. Hierarchical LOD representations remain a core component, and we emphasize their role in supporting adaptive data transfer and coherence management, characterized by a uniform depth and near-equal patch size at all levels. Our approach adapts seamlessly to scenarios with varying levels of coherence by balancing real-time performance with visual consistency. Experimental results demonstrate that our system achieves significant performance improvements, rendering scenes with billions of triangles in real-time, outperforming existing methods while maintaining consistent visual quality during dynamic interactions.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "b71c74e762",
    "authors": "Lifan Wu, Nathan Morrical, Sai Praveen Bangaru, Rohan Sawhney, Shuang Zhao, Chris Wyman, Ravi Ramamoorthi, Aaron Lefohn",
    "title": "Unbiased Differential Visibility Using Fixed-Step Walk-on-Spherical-Caps And Closest Silhouettes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731174",
    "pdf_link": null,
    "abstract": "Computing derivatives of path integrals under evolving scene geometry is a fundamental problem in physics-based differentiable rendering, which requires differentiating discontinuities in the visibility function. Warped-area reparameterization is a powerful technique to compute differential visibility, and key is construction of a velocity field that is continuous in the domain interior and agrees with defined velocities on boundaries. Robustly and efficiently constructing such fields remains challenging. We present a novel velocity field construction for differential visibility. Inspired by recent Monte Carlo solvers for partial differential equations (PDEs), we formulate the velocity field via Laplace's equation and solve it with a walk-on-spheres (WoS) algorithm. To improve efficiency, we introduce a fixed-step WoS that terminates random walks after a fixed step count, resulting in a continuous but non-harmonic velocity field still valid for warped-area reparameterization. Furthermore, to practically apply our method to complex 3D scenes, we propose an efficient cone query to find the closest silhouettes on a boundary. Our cone query finds the closest point under the geodesic distance on a unit sphere, and is analogous to the closest point query by WoS to compute Euclidean distance. As a result, our method generalizes WoS to perform random walks on spherical caps over the unit sphere. We demonstrate that this enables a more robust and efficient unbiased estimator for differential visibility.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "38ae396612",
    "authors": "Ruben Wiersma, Julien Philip, Miloš Hašan, Krishna Mullia, Fujun Luan, Elmar Eisemann, Valentin Deschaintre",
    "title": "Uncertainty for SVBRDF Acquisition using Frequency Analysis",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730592",
    "pdf_link": null,
    "abstract": "This paper aims to quantify uncertainty for SVBRDF acquisition in multi-view captures. Under uncontrolled illumination and unstructured viewpoints, there is no guarantee that the observations contain enough information to reconstruct the appearance properties of a captured object. We study this ambiguity, or uncertainty, using entropy and accelerate the analysis by using the frequency domain, rather than the domain of incoming and outgoing viewing angles. The result is a method that computes a map of uncertainty over an entire object within a millisecond. We find that the frequency model allows us to recover SVBRDF parameters with competitive performance, that the accelerated entropy computation matches results with a physically-based path tracer, and that there is a positive correlation between error and uncertainty. We then show that the uncertainty map can be applied to improve SVBRDF acquisition using capture guidance, sharing information on the surface, and using a diffusion model to inpaint uncertain regions. Our code is available at https://github.com/rubenwiersma/svbrdf_uncertainty.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "6064e86e33",
    "authors": "Timo Probst, Matthias Teschner, Timo Probst",
    "title": "Unified Pressure, Surface Tension and Friction for SPH Fluids",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3708034",
    "pdf_link": null,
    "abstract": "Fluid droplets behave significantly different from larger fluid bodies. At smaller scales, surface tension and friction between fluids and the boundary play an essential role and are even able to counteract gravitational forces. There are quite a few existing approaches that model surface tension forces within an SPH environment. However, as often as not, physical correctness and simulation stability are still major concerns with many surface tension formulations. We propose a new approach to compute surface tension that is both robust and produces the right amount of surface tension. Conversely, less attention was given to friction forces at the fluid-boundary interface. Recent experimental research indicates that Coulomb friction can be used to describe the behavior of droplets resting on a slope. Motivated by this, we develop a novel friction force formulation at the fluid-boundary interface following the Coulomb model, which allows us to replicate a new range of well known fluid behavior such as the motion of rain droplets on a window pane. Both forces are combined with an IISPH variant into one unified solver that is able to simultaneously compute strongly coupled surface tension, friction and pressure forces.",
    "scholar_publication": "ACM Transactions on Graphics, 2024 - dl.acm.org"
  },
  {
    "paper_id": "f29030419c",
    "authors": "Chunyi Sun, Junlin Han, Runjia Li, Weijian Deng, Dylan Campbell, Stephen Gould",
    "title": "Unsupervised Decomposition of 3D Shapes into Expressive and Editable Extruded Profile Primitives",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730704",
    "pdf_link": null,
    "abstract": "Transforming 3D shapes into representations that support part-level editing, flexible redesign, and efficient compression is vital for asset customization, content creation, and optimization in digital design. Despite its importance, achieving a representation that balances expressivity, editability, compactness, and interpretability remains a challenge. We introduce 3D2EP, a novel method for 3D shape decomposition that represents objects as a collection of differentiable, parametric primitives. Given a 3D shape represented by a voxel grid, 3D2EP decomposes this into a set of primitive parts, each generated by extruding a scaled 2D profile along a 3D curve, with the requisite components being predicted in a feedforward manner. That is, each primitive is constrained to have a single cross-section profile, up to scale. This enables the primitives to adapt to the data, capturing the geometry with precision but without excess degrees-of-freedom that would stymie editability.Extensive evaluations highlight 3D2EP’s ability to reconstruct complex shapes with a compact and interpretable representation, emphasizing its suitability for a wide range of 3D modeling applications.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "9726025549",
    "authors": "Zhaofeng Luo, Zhitong Cui, Shijian Luo, Mengyu Chu, Minchen Li",
    "title": "VR-Doh: Hands-on 3D Modeling in Virtual Reality",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731154",
    "pdf_link": null,
    "abstract": "We introduce VR-Doh, an open-source, hands-on 3D modeling system that enables intuitive creation and manipulation of elastoplastic objects in Virtual Reality (VR). By customizing the Material Point Method (MPM) for real-time simulation of hand-induced large deformations and enhancing 3D Gaussian Splatting for seamless rendering, VR-Doh provides an interactive and immersive 3D modeling experience. Users can naturally sculpt, deform, and edit objects through both contact- and gesture-based hand-object interactions. To achieve real-time performance, our system incorporates localized simulation techniques, particle-level collision handling, and the decoupling of physical and appearance representations, ensuring smooth and responsive interactions. VR-Doh supports both object creation and editing, enabling diverse modeling tasks such as designing food items, characters, and interlocking structures, all resulting in simulation-ready assets. User studies with both novice and experienced participants highlight the system's intuitive design, immersive feedback, and creative potential. Compared to existing geometric modeling tools, VR-Doh offers enhanced accessibility and natural interaction, making it a powerful tool for creative exploration in VR.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "40e0c4ad24",
    "authors": "Yucheol Jung, Hyomin Kim, Hyejeong Yoon, Yoonha Hwang, Seungyong Lee",
    "title": "Variable Shared Template for Consistent Non-rigid ICP",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731428",
    "pdf_link": null,
    "abstract": "Non-rigid registration of 3D shape collections using a template mesh is essential for constructing 3D datasets. Traditional non-rigid Iterative Closest Point (ICP) methods rely on manually selected template meshes, which can result in inconsistent registrations when applied to diverse shape collections. This inconsistency arises particularly when the template lacks common shape features with the input instances or when landmark annotations are sparse. To overcome this limitation, we propose a novel ICP framework that jointly optimizes a shared template shape and its instance-wise deformations. Our joint optimization framework assigns distinct roles to the shared template and instance-wise deformations: the template captures common shape features, while instance-wise deformations handle residual registration errors. We use stronger smoothness regularization on the instance-wise deformations in early iterations to prioritize the accumulation of common details on the template. Additionally, a distortion alignment energy minimizes interinstance map distortions, promoting consistent instance-wise deformations. On challenging 3D datasets with large shape variations, our method achieves state-of-the-art fitting accuracy and consistent results in shape averaging and deformation transfer. By removing the need for a carefully selected preset template, our method extends the capability of extrinsic non-rigid registration frameworks, offering a more robust and flexible solution for challenging registration scenarios.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "8d0387b38b",
    "authors": "Leticia Mattos Da Silva, Silvia Sellán, Natalia Pacheco-Tallaj, Justin Solomon",
    "title": "Variational Elastodynamic Simulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730726",
    "pdf_link": null,
    "abstract": "… used in variational integrators for elastodynamic simulation. In … Thanks to our use of variational integration, it preserves … that are not preserved exactly by variational integration still are …",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "64d1700fcc",
    "authors": "Elie Michel, Alec Jacobson, Siddhartha Chaudhuri, Jean-Marc Thiery",
    "title": "Variational Green and Biharmonic Coordinates for 2D Polynomial Cages",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731421",
    "pdf_link": null,
    "abstract": "We present closed-form expressions for Green and biharmonic coordinates with respect to polynomial curved 2D cages, enabling reliable cage-based image deformation both to and from a curved cage. We further provide closed-form expressions for first- and second-order derivatives of these coordinates with respect to the encoded position. This enables the use of variational solvers for interacting with the 2D shape at arbitrary points while keeping the fast decoding strength of cage-based deformation, which we illustrate for a variety of elastic deformation energies.",
    "scholar_publication": "ACM Transactions on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "79bb823e17",
    "authors": "Jianjun Xia, Tao Ju",
    "title": "Variational Surface Reconstruction Using Natural Neighbors",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731191",
    "pdf_link": null,
    "abstract": "Surface reconstruction from points is a fundamental problem in computer graphics. While numerous methods have been proposed, it remains challenging to reconstruct from sparse and non-uniform point distributions, particularly when normals are absent. We present a robust and scalable method for reconstructing an implicit surface from points without normals. By exploring the locality of natural neighborhoods, we propose local reformulations of a previous global method, known for its ability to surface sparse points but high computational cost, thereby significantly improving its scalability while retaining its robustness. Experiments show that our method achieves comparable speed to existing reconstruction methods on large inputs while producing fewer artifacts in under-sampled regions.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "c8935408ea",
    "authors": "Haolin Lu, Delio Vicini, Wesley Chang, Tzu-Mao Li",
    "title": "Vector-Valued Monte Carlo Integration Using Ratio Control Variates",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731175",
    "pdf_link": null,
    "abstract": "Variance reduction techniques are widely used for reducing the noise of Monte Carlo integration. However, these techniques are typically designed with the assumption that the integrand is scalar-valued. Recognizing that rendering and inverse rendering broadly involve vector-valued integrands, we identify the limitations of classical variance reduction methods in this context. To address this, we introduce ratio control variates, an estimator that leverages a ratio-based approach instead of the conventional difference-based control variates. Our analysis and experiments demonstrate that ratio control variables can significantly reduce the mean squared error of vector-valued integration compared to existing methods and are broadly applicable to various rendering and inverse rendering tasks.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "9ee89f642c",
    "authors": "Minseok Kim, Wonjeong Seo, Sung-Hee Lee, Jungdam Won",
    "title": "ViSA: Physics-based Virtual Stunt Actors for Ballistic Stunts",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3731424",
    "pdf_link": null,
    "abstract": "We introduce ViSA (Virtual Stunt Actors), an interactive animation system designed to create realistic ballistic stunt actions frequently seen in filmmaking and TV production. By providing spatial constraints suitable for the desired stunt scene, our system generates physically plausible motions satisfying the given constraints. The problem is formulated as a deep reinforcement learning task, incorporating a novel state and action spaces, as well as straightforward yet effective rewards for ballistic stunt actions. Users can receive a fast response within several minutes and continue to choreograph complex stunt scenes in an interactive manner. We demonstrate ballistic stunt scenes resembling those in various films and TV dramas, such as traffic accidents, falling down stairs, and falls from buildings. The effectiveness of the technical components and design choices in our system is demonstrated through extensive comparisons, analyses, and ablation studies.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "dc3a40d8e1",
    "authors": "Yuanpeng Tu, Luo Hao, Chen Xi, Sihui Ji, Xiang Bai, Zhao Hengshuang",
    "title": "VideoAnydoor: High-fidelity Video Object Insertion with Precise Motion Control",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730647",
    "pdf_link": null,
    "abstract": "Despite significant advancements in video generation, inserting a given object into videos remains a challenging task. The difficulty lies in preserving the appearance details of the reference object and accurately modeling coherent motion at the same time. In this paper, we propose VideoAnydoor, a zero-shot video object insertion framework with high-fidelity detail preservation and precise motion control. Starting from a text-to-video model, we utilize an ID extractor to inject the global identity and leverage a box sequence to control the overall motion. To preserve the detailed appearance and meanwhile support fine-grained motion control, we design a pixel warper. It takes the reference image with arbitrary key-points and the corresponding key-point trajectories as inputs. It warps the pixel details according to the trajectories and fuses the warped features with the diffusion U-Net, thus improving detail preservation and supporting users in manipulating the motion trajectories. In addition, we propose a training strategy involving both videos and static images with a reweight reconstruction loss to enhance insertion quality. VideoAnydoor demonstrates significant superiority over existing methods and naturally supports various downstream applications (e.g., talking head generation, video virtual try-on, multi-region editing) without task-specific fine-tuning.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "16d124df0b",
    "authors": "Yuxuan Bian, Zhaoyang Zhang, Xuan Ju, Mingdeng Cao, Liangbin Xie, Ying Shan, Qiang Xu",
    "title": "VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730673",
    "pdf_link": null,
    "abstract": "Video inpainting, crucial for the media industry, aims to restore corrupted content. However, current methods relying on limited pixel propagation or single-branch image inpainting architectures face challenges with generating fully masked objects, balancing background preservation with foreground generation, and maintaining ID consistency over long video. To address these issues, we propose VideoPainter, an efficient dual-branch framework featuring a lightweight context encoder. This plug-and-play encoder processes masked videos and injects background guidance into any pre-trained video diffusion transformer, generalizing across arbitrary mask types, enhancing background integration and foreground generation, and enabling user-customized control. We further introduce a strategy to resample inpainting regions for maintaining ID consistency in any-length video inpainting. Additionally, we develop a scalable dataset pipeline using advanced vision models and construct VPData and VPBench—the largest video inpainting dataset with segmentation masks and dense caption (>390K clips) —to support large-scale training and evaluation. We also show VideoPainter’s promising potential in downstream applications such as video editing. Extensive experiments demonstrate VideoPainter’s state-of-the-art performance in any-length video inpainting and editing across 8 key metrics, including video quality, mask region preservation, and textual coherence.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "01c3548d55",
    "authors": "Qingqin Liu, Ziqi Fang, Jiayi Wu, Shaoyu Cai, Jianhui Yan, Tiande Mo, Shuk Ching CHAN, Kening Zhu",
    "title": "VirCHEW Reality: On-Face Kinesthetic Feedback for Enhancing Food-Intake Experience in Virtual Reality",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730694",
    "pdf_link": null,
    "abstract": "While haptic interfaces for virtual reality (VR) has received extensive research attention, on-face haptics in VR remained less explored, especially for virtual food intake. In this paper, we introduce VirCHEW Reality, a face-worn haptic device designed to provide on-face kinesthetic force feedback, to enhance the virtual food-chewing experience in VR. Leveraging a pneumatic actuation system, VirCHEW Reality controlled the process of air inflation and deflation, to simulate the mechanical properties of food textures, such as hardness, cohesiveness, and stickiness. We evaluated the system through three user studies. First, a just-noticeable difference (JND) study examined users’ sensitivity to and the system’s capability of rendering different levels of on-face pneumatic-based kinesthetic feedback while users performing chewing action. Building upon the user-distinguishable signal ranges found in the first study, we further conducted a matching study to explore the correspondence between the kinesthetic stimuli provided by our device and user-perceived food textures, revealing the capability of simulating food texture properties during chewing (e.g., hardness, cohesiveness, stickiness). Finally, a user study in a VR eating scenario showed that VirCHEW Reality could significantly improve the users’ ratings on the sense of presence, compared to the condition without haptic feedback. Our findings further highlighted possible applications in virtual/remote dining, healthcare, and immersive entertainment.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "354cb05d05",
    "authors": "Xijie Yang, Linning Xu, Lihan Jiang, Dahua Lin, Bo Dai",
    "title": "Virtualized 3D Gaussians: Flexible Cluster-based Level-of-Detail System for Real-Time Rendering of Composed Scenes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730602",
    "pdf_link": null,
    "abstract": "3D Gaussian Splatting (3DGS) enables the reconstruction of intricate digital 3D assets from multi-view images by leveraging a set of 3D Gaussian primitives for rendering. Its explicit and discrete representation facilitates the seamless composition of complex digital worlds, offering significant advantages over previous neural implicit methods. However, when applied to large-scale compositions, such as crowd-level scenes, it can encompass numerous 3D Gaussians, posing substantial challenges for real-time rendering. To address this, inspired by Unreal Engine 5’s Nanite system, we propose Virtualized 3D Gaussians (V3DG), a cluster-based LOD solution that constructs hierarchical 3D Gaussian clusters and dynamically selects only the necessary ones to accelerate rendering speed. Our approach consists of two stages: (1) Offline Build, where hierarchical clusters are generated using a local splatting method to minimize visual differences across granularities, and (2) Online Selection, where footprint evaluation determines perceptible clusters for efficient rasterization during rendering. We curate a dataset of synthetic and real-world scenes, including objects, trees, people, and buildings, each requiring 0.1 billion 3D Gaussians to capture fine details. Experiments show that our solution balances rendering efficiency and visual quality across user-defined tolerances, facilitating downstream interactive applications that compose extensive 3DGS assets for consistent rendering performance.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "d7bf49ee81",
    "authors": "Mie Sato, Kazuki Takeyama, Naoki Hashimoto",
    "title": "Weight Illusion Induced by AR Visual Effects on the Arm",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743028",
    "pdf_link": null,
    "abstract": "This study investigates the effective range of the weight illusion induced by AR visual effects displayed on the arm. The results show that AR visual effects on the arm can create a “strong” impression and that using such visual effects can induce a weight illusion in which weights ranging from 100 g to 500 g are perceived as lighter when lifted with the arm augmented by these visual effects.",
    "scholar_publication": "… of the Special Interest Group on …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "c0fc929257",
    "authors": "Kenneth Chen, Nathan Matsuda, Jon McElvain, Yang Zhao, Thomas Wan, Qi Sun, Alexandre Chapiro",
    "title": "What is HDR? Perceptual Impact of Luminance and Contrast in Immersive Displays",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730629",
    "pdf_link": null,
    "abstract": "The contrast and luminance capabilities of a display are central to the quality of the image. High dynamic range (HDR) displays have high luminance and contrast, but it can be difficult to ascertain whether a given set of characteristics qualifies for this label. This is especially unclear for new display modes, such as virtual reality (VR). This paper studies the perceptual impact of peak luminance and contrast of a display, including characteristics and use cases representative of VR. To achieve this goal, we first developed a haploscope testbed prototype display capable of achieving 1,000 nits peak luminance and 1,000,000:1 contrast with high precision. We then collected a novel HDR video dataset targetting VR-relevant content types. We also implemented custom tone mapping operators to map between display parameter sets. Finally, we collected subjective preference data spanning 3 orders of magnitude in each dimension. Our data was used to fit a model, which was validated using a subjective study on an HDR VR prototype headmounted display (HMD). Our model helps provide guidance for future display design, and helps standardize the understanding of HDR.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "a63945636e",
    "authors": "Keyang Ye, Tianjia Shao, Kun Zhou",
    "title": "When Gaussian Meets Surfel: Ultra-fast High-fidelity Radiance Field Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730925",
    "pdf_link": null,
    "abstract": "We introduce Gaussian-enhanced Surfels (GESs), a bi-scale representation for radiance field rendering, wherein a set of 2D opaque surfels with view-dependent colors represent the coarse-scale geometry and appearance of scenes, and a few 3D Gaussians surrounding the surfels supplement fine-scale appearance details. The rendering with GESs consists of two passes - surfels are first rasterized through a standard graphics pipeline to produce depth and color maps, and then Gaussians are splatted with depth testing and color accumulation on each pixel order independently. The optimization of GESs from multi-view images is performed through an elaborate coarse-to-fine procedure, faithfully capturing rich scene appearance. The entirely sorting-free rendering of GESs not only achieves very fast rates, but also produces view-consistent images, successfully avoiding popping artifacts under view changes. The basic GES representation can be easily extended to achieve antialiasing in rendering (Mip-GES), boosted rendering speeds (Speedy-GES) and compact storage (Compact-GES), and reconstruct better scene geometries by replacing 3D Gaussians with 2D Gaussians (2D-GES). Experimental results show that GESs advance the state-of-the-arts as a compelling representation for ultra-fast high-fidelity radiance field rendering.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2025 - dl.acm.org"
  },
  {
    "paper_id": "8b499ce94d",
    "authors": "Mengyao Guo, Junfeng Meng",
    "title": "When Mud Toys Meet Digital M(B)uddies: How \"Play With Earth\" Bridges Traditional Craftsmanship and AI-Assisted Creation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743000",
    "pdf_link": null,
    "abstract": "\"Play with Earth\" introduces a novel project that addresses the preservation and innovation of intangible cultural heritage (ICH), with a focus on traditional mud toys from China’s Yellow River. Based on a comprehensive documentation of 15,686 photographs of mud toys and interviews with inheritors, our project achieved an interactive platform combining traditional craftsmanship with AI-assisted creativity.",
    "scholar_publication": "Proceedings of the Special Interest Group on Computer …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "34caa8f13b",
    "authors": "Junke Zhu, Zehan Wu, Qixing Zhang, Cheng Liao, Zhangjin Huang",
    "title": "WishGI: Lightweight Static Global Illumination Baking via Spherical Harmonics Fitting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3730935",
    "pdf_link": null,
    "abstract": "Global illumination combines direct and indirect lighting to create realistic lighting effects, bringing virtual scenes closer to reality. Static global illumination is a crucial component of virtual scene rendering, leveraging precomputation and baking techniques to significantly reduce runtime computational costs. Unfortunately, many existing works prioritize visual quality by relying on extensive texture storage and massive pixel-level texture sampling, leading to large performance overhead. In this paper, we introduce an illumination reconstruction method that effectively reduces sampling in fragment shader and avoids additional render passes, making it well-suited for low-end platforms. To achieve high-quality global illumination with reduced memory usage, we adopt a spherical harmonics fitting approach for baking effective illumination information and propose an inverse probe distribution method that generates unique probe associations for each mesh. This association, which can be generated offline in the local space, ensures consistent lighting quality across all instances of the same mesh. As a consequence, our method delivers highly competitive lighting effects while using only approximately 5% of the memory required by mainstream industry techniques.",
    "scholar_publication": "ACM Transactions on Graphics …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "3a54269482",
    "authors": "Khin Yuupar Myat, Hope Jo, Cindy Nakhammouane, Gaeun Lee",
    "title": "You Can Grow Here: A Therapeutic VR Journey for Anxiety Management",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721250.3743037",
    "pdf_link": null,
    "abstract": "You Can Grow Here is an immersive VR experience developed for the CAVE2™ environment, aligning with the UN Sustainable Development Goal of Good Health and Well-Being. In response to the mental health challenges intensified by the COVID-19 pandemic, the project explores how interactive storytelling, ambient sound, and 3D typography can support emotional reflection and teach anxiety coping strategies. Built in Unity with custom assets from Blender and Maya, the experience differs from most clinical VR programs, allowing users to independently explore emotions, manage anxiety, and practice evidence-based calming techniques within a safe, narrative-driven space that builds emotional resilience.",
    "scholar_publication": "Proceedings of the Special …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "b3d31b8bed",
    "authors": "Elad Richardson, Yuval Alaluf, Ali Mahdavi-Amiri, Daniel Cohen-Or",
    "title": "pOps: Photo-Inspired Diffusion Operators",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730615",
    "pdf_link": null,
    "abstract": "… , we introduce pOps, a framework that trains specific semantic operators directly on CLIP … We show that pOps can be used to learn a variety of photo-inspired operators with distinct …",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  },
  {
    "paper_id": "e1f84950c6",
    "authors": "Sarah Taylor, Salvador Medina, Jonathan Windle, Erica Alcusa Sáez, Iain Matthews",
    "title": "xADA: Controllable and Expressive Audio-Driven Animation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3721238.3730711",
    "pdf_link": null,
    "abstract": "We introduce xADA, a generative model for creating expressive and realistic animation of the face, tongue, and head directly from speech audio. Our approach leverages the pretrained Whisper audio encoder to extract rich speech features which are decoded into face and head animation using a series of gated recurrent unit (GRU) networks. The generated animation maps directly onto MetaHuman compatible rig controls enabling seamless integration into industry-standard content creation pipelines. xADA operates fully automatically, with an option for users to override the detected emotion and/or blink timings. xADA generalizes across languages, and voice styles, and can animate non-verbal sounds. Quantitative evaluation and a user study demonstrate that xADA produces state-of-the-art animation with high realism, frequently indistinguishable from ground truth performance. Additionally, we outline a comprehensive data capture protocol designed to collect an extensive range of speech and non-verbal sounds for training animation models.",
    "scholar_publication": "Proceedings of the …, 2025 - dl.acm.org"
  }
]