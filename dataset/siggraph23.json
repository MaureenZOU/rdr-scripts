[
  {
    "paper_id": "papers_685",
    "authors": "Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler, George Drettakis",
    "title": "3D Gaussian Splatting for Real-time Radiance Field Rendering",
    "paper_url": "https://paoloalfano.com/assets/updates/Gaussian_splatting_and_NeRF.pdf",
    "pdf_link": null,
    "abstract": "… Set of 3D gaussian … Set of 3D gaussian … Set of 3D gaussian …",
    "scholar_publication": "ACM Trans. Graph., 2023 - paoloalfano.com"
  },
  {
    "paper_id": "papers_722",
    "authors": "Biao Zhang, Jiapeng Tang, Matthias Nießner, Peter Wonka",
    "title": "3DShape2VecSet: A 3D Shape Representation for Neural Fields and Generative Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592442",
    "pdf_link": null,
    "abstract": "We introduce 3DShape2VecSet, a novel shape representation for neural fields designed for generative diffusion models. Our shape representation can encode 3D shapes given as surface models or point clouds, and represents them as neural fields. The concept of neural fields has previously been combined with a global latent vector, a regular grid of latent vectors, or an irregular grid of latent vectors. Our new representation encodes neural fields on top of a set of vectors. We draw from multiple concepts, such as the radial basis function representation, and the cross attention and self-attention function, to design a learnable representation that is especially suitable for processing with transformers. Our results show improved performance in 3D shape encoding and 3D shape generative modeling tasks. We demonstrate a wide variety of generative applications: unconditioned generation, category-conditioned generation, text-conditioned generation, point-cloud completion, and image-conditioned generation. Code: https://1zb.github.io/3DShape2VecSet/.",
    "scholar_publication": "ACM Transactions On …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_237",
    "authors": "Tianyi Xie, Minchen Li, Yin Yang, Chenfanfu Jiang",
    "title": "A Contact Proxy Splitting Method for Lagrangian Solid-fluid Coupling",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592115",
    "pdf_link": null,
    "abstract": "We present a robust and efficient method for simulating Lagrangian solid-fluid coupling based on a new operator splitting strategy. We use variational formulations to approximate fluid properties and solid-fluid interactions, and introduce a unified two-way coupling formulation for SPH fluids and FEM solids using interior point barrier-based frictional contact. We split the resulting optimization problem into a fluid phase and a solid-coupling phase using a novel time-splitting approach with augmented contact proxies, and propose efficient custom linear solvers. Our technique accounts for fluids interaction with nonlinear hyperelastic objects of different geometries and codimensions, while maintaining an algorithmically guaranteed non-penetrating criterion. Comprehensive benchmarks and experiments demonstrate the efficacy of our method.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_489",
    "authors": "Michal Edelstein, Nestor Guillen, Justin Solomon, Mirela Ben-Chen",
    "title": "A Convex Optimization Framework for Regularized Geodesic Distances",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591523",
    "pdf_link": null,
    "abstract": "We propose a general convex optimization problem for computing regularized geodesic distances. We show that under mild conditions on the regularizer the problem is well posed. We propose three different regularizers and provide analytical solutions in special cases, as well as corresponding efficient optimization algorithms. Additionally, we show how to generalize the approach to the all pairs case by formulating the problem on the product manifold, which leads to symmetric distances. Our regularized distances compare favorably to existing methods, in terms of robustness and ease of calibration.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_337",
    "authors": "Ruben Timotheues Wiersma, Ahmad Nasikun, Elmar Eisemann, Klaus Hildebrandt",
    "title": "A Fast Geometric Multigrid Method for Curved Surfaces",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591502",
    "pdf_link": null,
    "abstract": "We introduce a geometric multigrid method for solving linear systems arising from variational problems on surfaces in geometry processing, Gravo MG. Our scheme uses point clouds as a reduced representation of the levels of the multigrid hierarchy to achieve a fast hierarchy construction and to extend the applicability of the method from triangle meshes to other surface representations like point clouds, nonmanifold meshes, and polygonal meshes. To build the prolongation operators, we associate each point of the hierarchy to a triangle constructed from points in the next coarser level. We obtain well-shaped candidate triangles by computing graph Voronoi diagrams centered around the coarse points and determining neighboring Voronoi cells. Our selection of triangles ensures that the connections of each point to points at adjacent coarser and finer levels are balanced in the tangential directions. As a result, we obtain sparse prolongation matrices with three entries per row and fast convergence of the solver. Code is available at https://graphics.tudelft.nl/gravo_mg.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_559",
    "authors": "Yunchen Yu, Mengqi Xia, Bruce Walter, Eric Michielssen, Steve Marschner",
    "title": "A Full-wave Reference Simulator for Computing Surface Reflectance",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592414",
    "pdf_link": null,
    "abstract": "Computing light reflection from rough surfaces is an important topic in computer graphics. Reflection models developed based on geometric optics fail to capture wave effects such as diffraction and interference, while existing models based on physical optics approximations give erroneous predictions under many circumstances (eg when multiple scattering from the surface cannot be ignored). We present a scalable 3D full-wave simulator for computing reference solutions to surface scattering problems, which can be used to evaluate and guide …",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_833",
    "authors": "Dann Mensah, Nam Hee Kim, Miika Aittala, Samuli Laine, Jaakko Lehtinen",
    "title": "A Hybrid Generator Architecture for Controllable Face Synthesis",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591563",
    "pdf_link": null,
    "abstract": "Modern data-driven image generation models often surpass traditional graphics techniques in quality. However, while traditional modeling and animation tools allow precise control over the image generation process in terms of interpretable quantities — e.g., shapes and reflectances — endowing learned models with such controls is generally difficult. In the context of human faces, we seek a data-driven generator architecture that simultaneously retains the photorealistic quality of modern generative adversarial networks (GAN) and allows explicit, disentangled controls over head shapes, expressions, identity, background, and illumination. While our high-level goal is shared by a large body of previous work, we approach the problem with a different philosophy: We treat the problem as an unconditional synthesis task, and engineer interpretable inductive biases into the model that make it easy for the desired behavior to emerge. Concretely, our generator is a combination of learned neural networks and fixed-function blocks, such as a 3D morphable head model and texture-mapping rasterizer, and we leave it up to the training process to figure out how they should be used together. This greatly simplifies the training problem by removing the need for labeled training data; we learn the distributions of the independent variables that drive the model instead of requiring that their values are known for each training image. Furthermore, we need no contrastive or imitation learning for correct behavior. We show that our design successfully encourages the generative model to make use of the internal, interpretable representations in a semantically meaningful manner. This allows sampling of different aspects of the image independently, as well as precise control of the results by manipulating the internal state of the interpretable blocks within the generator. This enables, for instance, facial animation using traditional animation tools.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_127",
    "authors": "Harrison Jesse Smith, Qingyuan Zheng, Yifei Li, Somya Jain, Jessica Hodgins, Harrison Jesse Smith",
    "title": "A Method for Animating Children's Drawings of the Human Figure",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592788",
    "pdf_link": null,
    "abstract": "Children's drawings have a wonderful inventiveness, creativity, and variety to them. We present a system that automatically animates children's drawings of the human figure, is robust to the variance inherent in these depictions, and is simple and straightforward enough for anyone to use. We demonstrate the value and broad appeal of our approach by building and releasing the Animated Drawings Demo, a freely available public website that has been used by millions of people around the world. We present a set of experiments …",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_217",
    "authors": "Ryusuke Sugimoto, Terry Chen, Yiti Jiang, Christopher Batty, Toshiya Hachisuka",
    "title": "A Practical Walk-on-Boundary Method for Boundary Value Problems",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592109",
    "pdf_link": null,
    "abstract": "We introduce the walk-on-boundary (WoB) method for solving boundary value problems to computer graphics. WoB is a grid-free Monte Carlo solver for certain classes of second order partial differential equations. A similar Monte Carlo solver, the walk-on-spheres (WoS) method, has been recently popularized in computer graphics due to its advantages over traditional spatial discretization-based alternatives. We show that WoB's intrinsic properties yield further advantages beyond those of WoS. Unlike WoS, WoB naturally supports various boundary conditions (Dirichlet, Neumann, Robin, and mixed) for both interior and exterior domains. WoB builds upon boundary integral formulations, and it is mathematically more similar to light transport simulation in rendering than the random walk formulation of WoS. This similarity between WoB and rendering allows us to implement WoB on top of Monte Carlo ray tracing, and to incorporate advanced rendering techniques (e.g., bidirectional estimators with multiple importance sampling, the virtual point lights method, and Markov chain Monte Carlo) into WoB. WoB does not suffer from the intrinsic bias of WoS near the boundary and can estimate solutions precisely on the boundary. Our numerical results highlight the advantages of WoB over WoS as an attractive alternative to solve boundary value problems based on Monte Carlo.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_755",
    "authors": "Mengqi Xia, Bruce Walter, Christophe Hery, Olivier Maury, Eric Michielssen, Steve Marschner",
    "title": "A Practical Wave Optics Reflection Model for Hair and Fur",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592446",
    "pdf_link": null,
    "abstract": "Traditional fiber scattering models, based on ray optics, are missing some important visual aspects of fiber appearance. Previous work [Xia et al. 2020] on wave scattering from ideal extrusions demonstrated that diffraction produces strong forward scattering and colorful effects that are missing from ray-based models. However, that work was unable to include some important surface characteristics such as surface roughness and tilted cuticle scales, which are known to be important for fiber appearance. In this work, we take an important step to study wave effects from rough fibers with arbitrary 3D microgeometry. While the full-wave simulation of realistic 3D fibers remains intractable, we developed a 3D wave optics simulator based on a physical optics approximation, using a GPU-based hierarchical algorithm to greatly accelerate the calculation. It simulates surface reflection and diffractive scattering, which are present in all fibers and typically dominate for darkly pigmented fibers. The simulation provides a detailed picture of first order scattering, but it is not practical to use for production rendering as this would require tabulation per fiber geometry. To practically handle geometry variations in the scene, we propose a model based on wavelet noise, capturing the important statistical features in the simulation results that are relevant for rendering. Both our simulation and practical model show similar granular patterns to those observed in optical measurement. Our compact noise model can be easily combined with existing scattering models to render hair and fur of various colors, introducing visually important colorful glints that were missing from all previous models.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_765",
    "authors": "Junqiu Zhu, Adrian Jarabo, Carlos Aliaga, Ling-Qi Yan, Matt Jen-Yuan Chiang",
    "title": "A Realistic Surface-based Cloth Rendering Model",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591554",
    "pdf_link": null,
    "abstract": "We propose a surface-based cloth shading model that generates realistic cloth appearance with ply-level details. It generalizes previous surface-based models to a broader set of cloth including knitted and thin woven cloth. Our model takes into account the most dominant visual features of cloth, including anisotropic S-shaped reflection highlight, cross-shaped transmission highlights, delta transmission, and shadowing masking. We model these elements via a comprehensive micro-scale BSDF and a meso-scale effective BSDF formulation. Then, we propose an implementation that leverages the Monte Carlo sampler of path tracing for reducing precomputation to the bare minimum, by evaluating the effective BSDF as a Monte Carlo estimate, and encoding visibility using anisotropic spherical Gaussians. We demonstrate our model by replicating a set of woven and knitted fabrics, showing good match with respect to captured photographs.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_102",
    "authors": "Yuxing Qiu, Samuel Temple Reeve, Minchen Li, Yin Yang, Stuart Slattery, Chenfanfu Jiang, Yuxing Qiu",
    "title": "A Sparse Distributed Gigascale Resolution Material Point Method",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3570160",
    "pdf_link": null,
    "abstract": "In this article, we present a four-layer distributed simulation system and its adaptation to the Material Point Method (MPM). The system is built upon a performance portable C++ programming model targeting major High-Performance-Computing (HPC) platforms. A key ingredient of our system is a hierarchical block-tile-cell sparse grid data structure that is distributable to an arbitrary number of Message Passing Interface (MPI) ranks. We additionally propose strategies for efficient dynamic load balance optimization to maximize the efficiency of MPI tasks. Our simulation pipeline can easily switch among backend programming models, including OpenMP and CUDA, and can be effortlessly dispatched onto supercomputers and the cloud. Finally, we construct benchmark experiments and ablation studies on supercomputers and consumer workstations in a local network to evaluate the scalability and load balancing criteria. We demonstrate massively parallel, highly scalable, and gigascale resolution MPM simulations of up to 1.01 billion particles for less than 323.25 seconds per frame with 8 OpenSSH-connected workstations.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_110",
    "authors": "Tanaboon Tongbuasirilai, Jonas Unger, Christine Guillemot, Ehsan Miandji, Tanaboon Tongbuasirilai",
    "title": "A Sparse Non-parametric BRDF Model",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3533427",
    "pdf_link": null,
    "abstract": "… novel sparse non-parametric Bidirectional Reflectance Distribution Function (BRDF) model derived using a machine learning approach to represent the space of possible BRDFs using …",
    "scholar_publication": "ACM Transactions on …, 2022 - dl.acm.org"
  },
  {
    "paper_id": "papers_195",
    "authors": "Ziqi Wang, Florian Kennel-Maushart, Yijiang Huang, Bernhard Thomaszewski, Stelian Coros",
    "title": "A Temporal Coherent Topology Optimization Approach for Assembly Planning of Bespoke Frame Structures",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592102",
    "pdf_link": null,
    "abstract": "We present a computational framework for planning the assembly sequence of bespoke frame structures. Frame structures are one of the most commonly used structural systems in modern architecture, providing resistance to gravitational and external loads. Building frame structures requires traversing through several partially built states. If the assembly sequence is planned poorly, these partial assemblies can exhibit substantial deformation due to self-weight, slowing down or jeopardizing the assembly process. Finding a good assembly sequence that minimizes intermediate deformations is an interesting yet challenging combinatorial problem that is usually solved by heuristic search algorithms. In this paper, we propose a new optimization-based approach that models sequence planning using a series of topology optimization problems. Our key insight is that enforcing temporal coherent constraints in the topology optimization can lead to sub-structures with small deformations while staying consistent with each other to form an assembly sequence. We benchmark our algorithm on a large data set and show improvements in both performance and computational time over greedy search algorithms. In addition, we demonstrate that our algorithm can be extended to handle assembly with static or dynamic supports. We further validate our approach by generating a series of results in multiple scales, including a real-world prototype with a mixed reality assistant using our computed sequence and a simulated example demonstrating a multi-robot assembly application.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_402",
    "authors": "Kartik Chandra, Tzu-Mao Li, Joshua Tenenbaum, Jonathan Ragan-Kelley",
    "title": "Acting as Inverse Inverse Planning",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591510",
    "pdf_link": null,
    "abstract": "Great storytellers know how to take us on a journey. They direct characters to act—not necessarily in the most rational way—but rather in a way that leads to interesting situations, and ultimately creates an impactful experience for audience members looking on. If audience experience is what matters most, then can we help artists and animators directly craft such experiences, independent of the concrete character actions needed to evoke those experiences? In this paper, we offer a novel computational framework for such tools. Our key idea is to optimize animations with respect to simulated audience members’ experiences. To simulate the audience, we borrow an established principle from cognitive science: that human social intuition can be modeled as “inverse planning,” the task of inferring an agent’s (hidden) goals from its (observed) actions. Building on this model, we treat storytelling as “inverse inverse planning,” the task of choosing actions to manipulate an inverse planner’s inferences. Our framework is grounded in literary theory, naturally capturing many storytelling elements from first principles. We give a series of examples to demonstrate this, with supporting evidence from human subject studies.",
    "scholar_publication": "Acm siggraph 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_152",
    "authors": "Hui Ying, Tianjia Shao, He Wang, Yin Yang, Kun Zhou",
    "title": "Adaptive Local Basis Functions for Shape Completion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591485",
    "pdf_link": null,
    "abstract": "In this paper, we focus on the task of 3D shape completion from partial point clouds using deep implicit functions. Existing methods seek to use voxelized basis functions or the ones from a certain family of functions (e.g., Gaussians), which leads to high computational costs or limited shape expressivity. On the contrary, our method employs adaptive local basis functions, which are learned end-to-end and not restricted in certain forms. Based on those basis functions, a local-to-local shape completion framework is presented. Our algorithm learns sparse parameterization with a small number of basis functions while preserving local geometric details during completion. Quantitative and qualitative experiments demonstrate that our method outperforms the state-of-the-art methods in shape completion, detail preservation, generalization to unseen geometries, and computational cost. Code and data for this paper are at https://github.com/yinghdb/Adaptive-Local-Basis-Functions.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_720",
    "authors": "Ryan Capouellez, Jiacheng Dai, Aaron Hertzmann, Denis Zorin",
    "title": "Algebraic Smooth Occluding Contours",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591547",
    "pdf_link": null,
    "abstract": "… mesh, and renders smooth occluding contours for a given … show the occluding contours can be computed algebraically under … to the quadratic coefficients, contours and visibility can be …",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_457",
    "authors": "Florine Hartwig, Josua Sassen, Omri Azencot, Martin Rumpf, Mirela Ben-Chen",
    "title": "An Elastic Basis for Spectral Shape Correspondence",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591518",
    "pdf_link": null,
    "abstract": "Finding correspondences between shapes is a central task in geometry processing with applications such as texture or deformation transfer and shape interpolation. We develop a spectral method for finding correspondences between non-isometric shapes that aligns extrinsic features. For this, we propose a novel crease aware spectral basis, that is derived from the Hessian of an elastic thin shell energy. We incorporate this basis in a functional map framework and demonstrate the effectiveness of our approach for mapping non-isometric shapes such that prominent features are put in correspondence. Finally, we describe the necessary adaptations to the functional map framework for working with non-orthogonal basis functions, thus considerably widening the scope of future uses of spectral shape correspondence.",
    "scholar_publication": "ACM SIGGRApH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_637",
    "authors": "Brennan Shacklett, Luc Guy Rosenzweig, Bidipta Sarkar, Zhiqiang Xie, Andrew Szot, Erik Wijmans, Vladlen Koltun, Dhruv Batra, Kayvon Fatahalian",
    "title": "An Extensible, Data-oriented Architecture for High-performance, Many-world Simulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592427",
    "pdf_link": null,
    "abstract": "Training AI agents to perform complex tasks in simulated worlds requires millions to billions of steps of experience. To achieve high performance, today's fastest simulators for training AI agents adopt the idea of batch simulation: using a single simulation engine to simultaneously step many environments in parallel. We introduce a framework for productively authoring novel training environments (including custom logic for environment generation, environment time stepping, and generating agent observations and rewards) that execute as high-performance, GPU-accelerated batched simulators. Our key observation is that the entity-component-system (ECS) design pattern, popular for expressing CPU-side game logic today, is also well-suited for providing the structure needed for high-performance batched simulators. We contribute the first fully-GPU accelerated ECS implementation that natively supports batch environment simulation. We demonstrate how ECS abstractions impose structure on a training environment's logic and state that allows the system to efficiently manage state, amortize work, and identify GPU-friendly coherent parallel computations within and across different environments. We implement several learning environments in this framework, and demonstrate GPU speedups of two to three orders of magnitude over open source CPU baselines and 5-33× over strong baselines running on a 32-thread CPU. An implementation of the OpenAI hide and seek 3D environment written in our framework, which performs rigid body physics and ray tracing in each simulator step, achieves over 1.9 million environment steps per second on a single GPU.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_606",
    "authors": "Seunghwan Lee, Yifeng Jiang, C. Karen Liu",
    "title": "Anatomically Detailed Simulation of Human Torso",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592425",
    "pdf_link": null,
    "abstract": "Many existing digital human models approximate the human skeletal system using rigid bodies connected by rotational joints. While the simplification is considered acceptable for legs and arms, it significantly lacks fidelity to model rich torso movements in common activities such as dancing, Yoga, and various sports. Research from biomechanics provides more detailed modeling for parts of the torso, but their models often operate in isolation and are not fast and robust enough to support computationally heavy applications and large-scale data generation for full-body digital humans. This paper proposes a new torso model that aims to achieve high fidelity both in perception and in functionality, while being computationally feasible for simulation and optimal control tasks. We build a detailed human torso model consisting of various anatomical components, including facets, ligaments, and intervertebral discs, by coupling efficient finite-element and rigid-body simulations. Given an existing motion capture sequence without dense markers placed on the torso, our new model is able to recover the underlying torso bone movements. Our method is remarkably robust that it can be used to automatically \"retrofit\" the entire Mixamo motion database of highly diverse human motions without user intervention. We also show that our model is computationally efficient for solving trajectory optimization of highly dynamic full-body movements, without relying any reference motion. Physiological validity of the model is validated against established literature.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_727",
    "authors": "Zhengyu Huang, Haoran Xie, Tsukasa Fukusato, Kazunori Miyata",
    "title": "AniFaceDrawing: Anime Portrait Exploration During Your Sketching",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591548",
    "pdf_link": null,
    "abstract": "This paper focuses on how artificial intelligence (AI) can be used to assist general users in the creation of professional portraits, that is, consistently converting rough sketches into high-quality anime portraits during their sketching process. The input to this task is a sequence of incomplete human freehand sketches that are gradually refined stroke by stroke, while the output is a sequence of high-quality anime portraits that correspond to the input sketches as guidance. Although recent GANs can generate high quality images, it is a challenging problem to maintain the high quality of generated images from sketches with a low degree of completion due to ill-posed problems in conditional image generation. Even with the latest sketch-to-image (S2I) technology, it is still difficult to create high-quality images from incomplete rough sketches for anime portraits because the lines in anime style tend to be more abstract than in realistic style. In this paper, we addressed this problem using the latent space exploration of StyleGAN with a two-stage training strategy. Specifically, we consider the input strokes of a freehand sketch to correspond to edge information-related attributes in the latent structural code of StyleGAN, and term the matching between strokes and these attributes “stroke-level disentanglement.” In the first stage, we trained an image encoder with the pre-trained StyleGAN model as a teacher encoder. In the second stage, we simulated the drawing process of the generated images and trained the sketch encoder for incomplete progressive sketches to generate high-quality portrait images with feature alignment to the disentangled representations at the stroke level in the teacher encoder. We verified the proposed progressive S2I system with both qualitative and quantitative evaluations and achieved high-quality anime portraits from incomplete progressive sketches. What’s more, our user study proved its effectiveness in art creation assistance for the anime style.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_295",
    "authors": "Jingwei Huang, Shanshan Zhang, Bo Duan, Yanfeng Zhang, Xiaoyang Guo, Mingwei Sun, Li Yi",
    "title": "ArrangementNet: Learning Scene Arrangements for Vectorized Indoor Scene Modeling",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592122",
    "pdf_link": null,
    "abstract": "We present a novel vectorized indoor modeling approach that converts point clouds into building information models (BIM) with concise and semantically segmented polygonal meshes. Existing methods detect planar shapes and connect them to complete the scene. Some focus on floor plan reconstruction as a simplified problem to better analyze connectivity between planes of floors and walls. However, the connectivity analysis is still challenging and ill-posed with incomplete point clouds as input. We propose ArrangementNet to estimate scene arrangements from an incomplete point cloud, which we can easily convert into a BIM model. ArrangementNet is a novel graph neural network that consumes noisy over-partitioned initial arrangements extracted through non-learning techniques and outputs high-quality scene arrangement. The core of ArrangementNet is an extended graph convolution that leverages co-linear and co-face relationships in the arrangement and improves the quality of prediction in complex scenes. We apply ArrangementNet to improve floor plan and ceiling arrangements and enrich them with semantic objects as scene arrangements for scene generation. Our approach faithfully models challenging scenes obtained from laser scans or multiview stereo and shows significant improvement in BIM model reconstruction compared to the state-of-the-art. Our code is available at https://github.com/zssjh/ArrangementNet.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_111",
    "authors": "Fanchao Zhong, Yonglai Xu, Haisen Zhao, Lin Lu, Fanchao Zhong",
    "title": "As-continuous-as-possible Extrusion-based Fabrication of Surface Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3575859",
    "pdf_link": null,
    "abstract": "In this study, we propose a computational framework for optimizing the continuity of the toolpath in fabricating surface models on an extrusion-based 3D printer. Toolpath continuity is a critical issue that influences both the quality and the efficiency of extrusion-based fabrication. Transfer moves lead to rough and bumpy surfaces, where this phenomenon worsens for materials with large viscosity, like clay. The effects of continuity on the surface models are even more severe in terms of the quality of the surface and the stability of the model. We introduce a criterion called the one–path patch (OPP) to represent a patch on the surface of the shell that can be traversed along one path by considering the constraints on fabrication. We study the properties of the OPPs and their merging operations to propose a bottom-up OPP merging procedure to decompose the given shell surface into a minimal number of OPPs, and to generate the “as-continuous-as-possible” (ACAP) toolpath. Furthermore, we augment the path planning algorithm with a curved-layer printing scheme that reduces staircase defects and improves the continuity of the toolpath by connecting multiple segments. We evaluated the ACAP algorithm on ceramic and thermoplastic materials, and the results showed that it improves the fabrication of surface models in terms of both efficiency and surface quality.",
    "scholar_publication": "ACM Transactions on Graphics, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_241",
    "authors": "Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, Daniel Cohen-Or",
    "title": "Attend-and-Excite: Attention-based Semantic Guidance for Text-to-image Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592116",
    "pdf_link": null,
    "abstract": "Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of catastrophic neglect, where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes (e.g., colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of Generative Semantic Nursing (GSN), where we seek to intervene in the generative process on the fly during inference time to improve the faithfulness of the generated images. Using an attention-based formulation of GSN, dubbed Attend-and-Excite, we guide the model to refine the cross-attention units to attend to all subject tokens in the text prompt and strengthen --- or excite --- their activations, encouraging the model to generate all subjects described in the text prompt. We compare our approach to alternative approaches and demonstrate that it conveys the desired concepts more faithfully across a range of text prompts. Code is available at our project page: https://attendandexcite.github.io/Attend-and-Excite/.",
    "scholar_publication": "ACM transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_882",
    "authors": "Yuelang Xu, Lizhen Wang, Xiaochen Zhao, Hongwen Zhang, Yebin Liu",
    "title": "AvatarMAV: Fast 3D Head Avatar Reconstruction Using Motion-aware Neural Voxels",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591567",
    "pdf_link": null,
    "abstract": "With NeRF widely used for facial reenactment, recent methods can recover photo-realistic 3D head avatar from just a monocular video. Unfortunately, the training process of the NeRF-based methods is quite time-consuming, as MLP used in the NeRF-based methods is inefficient and requires too many iterations to converge. To overcome this problem, we propose AvatarMAV, a fast 3D head avatar reconstruction method using Motion-Aware Neural Voxels. AvatarMAV is the first to model both the canonical appearance and the decoupled expression motion by neural voxels for head avatar. In particular, the motion-aware neural voxels is generated from the weighted concatenation of multiple 4D tensors. The 4D tensors semantically correspond one-to-one with 3DMM expression basis and share the same weights as 3DMM expression coefficients. Benefiting from our novel representation, the proposed AvatarMAV can recover photo-realistic head avatars in just 5 minutes (implemented with pure PyTorch), which is significantly faster than the state-of-the-art facial reenactment methods. Project page: https://www.liuyebin.com/avatarmav.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_193",
    "authors": "Zerong Zheng, Xiaochen Zhao, Hongwen Zhang, Boning Liu, Yebin Liu",
    "title": "AvatarReX: Real-time Expressive Full-body Avatars",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592101",
    "pdf_link": null,
    "abstract": "We present AvatarReX, a new method for learning NeRF-based full-body avatars from video data. The learnt avatar not only provides expressive control of the body, hands and the face together, but also supports real-time animation and rendering. To this end, we propose a compositional avatar representation, where the body, hands and the face are separately modeled in a way that the structural prior from parametric mesh templates is properly utilized without compromising representation flexibility. Furthermore, we disentangle the geometry and appearance for each part. With these technical designs, we propose a dedicated deferred rendering pipeline, which can be executed at a real-time framerate to synthesize high-quality free-view images. The disentanglement of geometry and appearance also allows us to design a two-pass training strategy that combines volume rendering and surface rendering for network training. In this way, patch-level supervision can be applied to force the network to learn sharp appearance details on the basis of geometry estimation. Overall, our method enables automatic construction of expressive full-body avatars with real-time rendering capability, and can generate photo-realistic images with dynamic details for novel body motions and facial expressions.",
    "scholar_publication": "ACM Transactions on Graphics …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_312",
    "authors": "Benjamin Jones, James Noeckel, Milin Kodnongbua, Ilya Baran, Adriana Schulz",
    "title": "B-rep Matching for Collaborating Across CAD Systems",
    "paper_url": "https://arxiv.org/abs/2306.03169",
    "pdf_link": null,
    "abstract": "Large Computer-Aided Design (CAD) projects usually require collaboration across many different CAD systems as well as applications that interoperate with them for manufacturing, visualization, or simulation. A fundamental barrier to such collaborations is the ability to refer to parts of the geometry (such as a specific face) robustly under geometric and/or topological changes to the model. Persistent referencing schemes are a fundamental aspect of most CAD tools, but models that are shared across systems cannot generally make use of these internal referencing mechanisms, creating a challenge for collaboration. In this work, we address this issue by developing a novel learning-based algorithm that can automatically find correspondences between two CAD models using the standard representation used for sharing models across CAD systems: the Boundary-Representation (B-rep). Because our method works directly on B-reps it can be generalized across different CAD applications enabling collaboration.",
    "scholar_publication": "arXiv preprint arXiv …, 2023 - arxiv.org"
  },
  {
    "paper_id": "papers_595",
    "authors": "Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P. Srinivasan, Richard Szeliski, Jonathan T. Barron, Ben Mildenhall",
    "title": "BakedSDF: Meshing Neural SDFs for Real-time View Synthesis",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591536",
    "pdf_link": null,
    "abstract": "We present a method for reconstructing high-quality meshes of large unbounded real-world scenes suitable for photorealistic novel view synthesis. We first optimize a hybrid neural volume-surface scene representation designed to have well-behaved level sets that correspond to surfaces in the scene. We then bake this representation into a high-quality triangle mesh, which we equip with a simple and fast view-dependent appearance model based on spherical Gaussians. Finally, we optimize this baked representation to best reproduce the captured viewpoints, resulting in a model that can leverage accelerated polygon rasterization pipelines for real-time view synthesis on commodity hardware. Our approach outperforms previous scene representations for real-time rendering in terms of accuracy, speed, and power consumption, and produces high quality meshes that enable applications such as appearance editing and physical simulation.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_234",
    "authors": "Pengbin Tang, Stelian Coros, Bernhard Thomaszewski",
    "title": "Beyond Chainmail: Computational Modeling of Discrete Interlocking Materials",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592112",
    "pdf_link": null,
    "abstract": "We present a method for computational modeling, mechanical characterization, and macro-scale simulation of discrete interlocking materials (DIM)---3D-printed chainmail fabrics made of quasi-rigid interlocking elements. Unlike conventional elastic materials for which deformation and restoring force are directly coupled, the mechanics of DIM are governed by contacts between individual elements that give rise to anisotropic deformation constraints. To model the mechanical behavior of these materials, we propose a computational approach that builds on three key components. (a): we explore the space of feasible deformations using native-scale simulations at the per-element level. (b): based on this simulation data, we introduce the concept of strain-space boundaries to represent deformation limits for in- and out-of-plane deformations, and (c): we use the strain-space boundaries to drive an efficient macro-scale simulation model based on homogenized deformation constraints. We evaluate our method on a set of representative discrete interlocking materials and validate our findings against measurements on physical prototypes.",
    "scholar_publication": "ACM Transactions on Graphics …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_271",
    "authors": "Jungnam Park, Moon Seok Park, Jehee Lee, Jungdam Won",
    "title": "Bidirectional GaitNet: A Bidirectional Prediction Model of Human Gait and Anatomical Conditions",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591492",
    "pdf_link": null,
    "abstract": "We present a novel generative model, called Bidirectional GaitNet, that learns the relationship between human anatomy and its gait. The simulation model of human anatomy is a comprehensive, full-body, simulation-ready, musculoskeletal model with 304 Hill-type musculotendon units. The Bidirectional GaitNet consists of forward and backward models. The forward model predicts a gait pattern of a person with specific physical conditions, while the backward model estimates the physical conditions of a person when his/her gait pattern is provided. Our simulation-based approach first learns the forward model by distilling the simulation data generated by a state-of-the-art predictive gait simulator and then constructs a Variational Autoencoder (VAE) with the learned forward model as its decoder. Once it is learned its encoder serves as the backward model. We demonstrate our model on a variety of healthy/impaired gaits and validate it in comparison with physical examination data of real patients.",
    "scholar_publication": "ACM SIGGRAPH 2023 Conference …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_790",
    "authors": "Omri Avrahami, Ohad Fried, Dani Lischinski",
    "title": "Blended Latent Diffusion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592450",
    "pdf_link": null,
    "abstract": "… local image edits by blending the latents at each step, similarly to Blended Diffusion. Next we … publicly available models that address our setting are Blended Diffusion and GLIDEfiltered. …",
    "scholar_publication": "ACM transactions on graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_862",
    "authors": "Kunkun Pang, Dafei Qin, Yingruo Fan, Julian Habekost, Takaaki Shiratori, Junichi Yamagishi, Taku Komura",
    "title": "Bodyformer: Semantics-guided 3D Body Gesture Synthesis With Transformer",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592456",
    "pdf_link": null,
    "abstract": "Automatic gesture synthesis from speech is a topic that has attracted researchers for applications in remote communication, video games and Metaverse. Learning the mapping between speech and 3D full-body gestures is difficult due to the stochastic nature of the problem and the lack of a rich cross-modal dataset that is needed for training. In this paper, we propose a novel transformer-based framework for automatic 3D body gesture synthesis from speech. To learn the stochastic nature of the body gesture during speech, we propose a variational transformer to effectively model a probabilistic distribution over gestures, which can produce diverse gestures during inference. Furthermore, we introduce a mode positional embedding layer to capture the different motion speeds in different speaking modes. To cope with the scarcity of data, we design an intra-modal pre-training scheme that can learn the complex mapping between the speech and the 3D gesture from a limited amount of data. Our system is trained with either the Trinity speech-gesture dataset or the Talking With Hands 16.2M dataset. The results show that our system can produce more realistic, appropriate, and diverse body gestures compared to existing state-of-the-art approaches.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_498",
    "authors": "Bailey Miller, Rohan Sawhney, Keenan Crane, Ioannis Gkioulekas",
    "title": "Boundary Value Caching for Walk on Spheres",
    "paper_url": "https://arxiv.org/abs/2302.11825",
    "pdf_link": null,
    "abstract": "Grid-free Monte Carlo methods such as walk on spheres can be used to solve elliptic partial differential equations without mesh generation or global solves. However, such methods independently estimate the solution at every point, and hence do not take advantage of the high spatial regularity of solutions to elliptic problems. We propose a fast caching strategy which first estimates solution values and derivatives at randomly sampled points along the boundary of the domain (or a local region of interest). These cached values then provide cheap, output-sensitive evaluation of the solution (or its gradient) at interior points, via a boundary integral formulation. Unlike classic boundary integral methods, our caching scheme introduces zero statistical bias and does not require a dense global solve. Moreover we can handle imperfect geometry (e.g., with self-intersections) and detailed boundary/source terms without repairing or resampling the boundary representation. Overall, our scheme is similar in spirit to virtual point light methods from photorealistic rendering: it suppresses the typical salt-and-pepper noise characteristic of independent Monte Carlo estimates, while still retaining the many advantages of Monte Carlo solvers: progressive evaluation, trivial parallelization, geometric robustness, etc. We validate our approach using test problems from visual and geometric computing.",
    "scholar_publication": "arXiv preprint arXiv …, 2023 - arxiv.org"
  },
  {
    "paper_id": "papers_483",
    "authors": "Chaoyang Lyu, Kai Bai, Yiheng Wu, Mathieu Desbrun, Changxi Zheng, Xiaopei Liu",
    "title": "Building a Virtual Weakly-compressible Wind Tunnel Testing Facility",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592394",
    "pdf_link": null,
    "abstract": "Virtual wind tunnel testing is a key ingredient in the engineering design process for the automotive and aeronautical industries as well as for urban planning: through visualization and analysis of the simulation data, it helps optimize lift and drag coefficients, increase peak speed, detect high pressure zones, and reduce wind noise at low cost prior to manufacturing. In this paper, we develop an efficient and accurate virtual wind tunnel system based on recent contributions from both computer graphics and computational fluid …",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_670",
    "authors": "Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, Xue Bin Peng",
    "title": "CALM: Conditional Adversarial Latent Models for Directable Virtual Characters",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591541",
    "pdf_link": null,
    "abstract": "In this work, we present Conditional Adversarial Latent Models (CALM), an approach for generating diverse and directable behaviors for user-controlled interactive virtual characters. Using imitation learning, CALM learns a representation of movement that captures the complexity and diversity of human motion, and enables direct control over character movements. The approach jointly learns a control policy and a motion encoder that reconstructs key characteristics of a given motion without merely replicating it. The results show that CALM learns a semantic motion representation, enabling control over the generated motions and style-conditioning for higher-level task training. Once trained, the character can be controlled using intuitive interfaces, akin to those found in video games.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_566",
    "authors": "Chenliang Zhou, Fangcheng Zhong, Cengiz Oztireli",
    "title": "CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable and Controllable Text-Guided Face Manipulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591532",
    "pdf_link": null,
    "abstract": "Recently introduced Contrastive Language-Image Pre-Training (CLIP) [Radford et al. 2021] bridges images and text by embedding them into a joint latent space. This opens the door to ample literature that aims to manipulate an input image by providing a textual explanation. However, due to the discrepancy between image and text embeddings in the joint space, using text embeddings as the optimization target often introduces undesired artifacts in the resulting images. Disentanglement, interpretability, and controllability are also hard to guarantee for manipulation. To alleviate these problems, we propose to define corpus subspaces spanned by relevant prompts to capture specific image characteristics. We introduce CLIP projection-augmentation embedding (PAE) as an optimization target to improve the performance of text-guided image manipulation. Our method is a simple and general paradigm that can be easily computed and adapted, and smoothly incorporated into any CLIP-based image manipulation algorithm. To demonstrate the effectiveness of our method, we conduct several theoretical and empirical studies. As a case study, we utilize the method for text-guided semantic face editing. We quantitatively and qualitatively demonstrate that PAE facilitates a more disentangled, interpretable, and controllable face image manipulation with state-of-the-art quality and accuracy.",
    "scholar_publication": "ACM SIGGRAPH 2023 Conference …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_817",
    "authors": "wamiq R. Para, Paul Guerrero, Niloy Mitra, Peter Wonka",
    "title": "COFS COntrolable Furniture Layout Synthesis",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591561",
    "pdf_link": null,
    "abstract": "Realistic, scalable, and controllable generation of furniture layouts is essential for many applications in virtual reality, augmented reality, game development and synthetic data generation. The most successful current methods tackle this problem as a sequence generation problem which imposes a specific ordering on the elements of the layout, making it hard to exert fine-grained control over the attributes of a generated scene. Existing methods provide control through object-level conditioning, or scene completion, where …",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_209",
    "authors": "Yuefan Shen, Shunsuke Saito, Ziyan Wang, Olivier Maury, Chenglei Wu, Jessica Hodgins, Youyi Zheng, Giljoo Nam",
    "title": "CT2Hair: High-fidelity 3D Hair Modeling Using Computed Tomography",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592106",
    "pdf_link": null,
    "abstract": "We introduce CT2Hair, a fully automatic framework for creating high-fidelity 3D hair models that are suitable for use in downstream graphics applications. Our approach utilizes real-world hair wigs as input, and is able to reconstruct hair strands for a wide range of hair styles. Our method leverages computed tomography (CT) to create density volumes of the hair regions, allowing us to see through the hair unlike image-based approaches which are limited to reconstructing the visible surface. To address the noise and limited resolution of the input density volumes, we employ a coarse-to-fine approach. This process first recovers guide strands with estimated 3D orientation fields, and then populates dense strands through a novel neural interpolation of the guide strands. The generated strands are then refined to conform to the input density volumes. We demonstrate the robustness of our approach by presenting results on a wide variety of hair styles and conducting thorough evaluations on both real-world and synthetic datasets. Code and data for this paper are at github.com/facebookresearch/CT2Hair.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_875",
    "authors": "Shivangi Aneja, Justus Thies, Angela Dai, Matthias Niessner",
    "title": "ClipFace: Text-guided Editing of Textured 3D Morphable Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591566",
    "pdf_link": null,
    "abstract": "We propose ClipFace, a novel self-supervised approach for text-guided editing of textured 3D morphable model of faces. Specifically, we employ user-friendly language prompts to enable control of the expressions as well as appearance of 3D faces. We leverage the geometric expressiveness of 3D morphable models, which inherently possess limited controllability and texture expressivity, and develop a self-supervised generative model to jointly synthesize expressive, textured, and articulated faces in 3D. We enable high-quality texture generation for 3D faces by adversarial self-supervised training, guided by differentiable rendering against collections of real RGB images. Controllable editing and manipulation are given by language prompts to adapt texture and expression of the 3D morphable model. To this end, we propose a neural network that predicts both texture and expression latent codes of the morphable model. Our model is trained in a self-supervised fashion by exploiting differentiable rendering and losses based on a pre-trained CLIP model. Once trained, our model jointly predicts face textures in UV-space, along with expression parameters to capture both geometry and texture changes in facial expressions in a single forward pass. We further show the applicability of our method to generate temporally changing textures for a given animation sequence.",
    "scholar_publication": "ACM SIGGRAPH 2023 Conference …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_517",
    "authors": "Cheng-Kang (Ted) Chao, Jason Klein, Jianchao Tan, Jose Echevarria, Yotam Gingold",
    "title": "ColorfulCurves: Palette-aware Lightness Control and Color Editing via Sparse Optimization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592405",
    "pdf_link": null,
    "abstract": "Color editing in images often consists of two main tasks: changing hue and saturation, and editing lightness or tone curves. State-of-the-art palette-based recoloring approaches entangle these two tasks. A user's only lightness control is changing the lightness of individual palette colors. This is inferior to state-of-the-art commercial software, where lightness editing is based on flexible tone curves that remap lightness. However, tone curves are only provided globally or per color channel (e.g., RGB). They are unrelated to the image content. Neither tone curves nor palette-based approaches support direct image-space edits---changing a specific pixel to a desired hue, saturation, and lightness. ColorfulCurves solves both of these problems by uniting palette-based and tone curve editing. In ColorfulCurves, users directly edit palette colors' hue and saturation, per-palette tone curves, or image pixels (hue, saturation, and lightness). ColorfulCurves solves an L2,1 optimization problem in real-time to find a sparse edit that satisfies all user constraints. Our expert study found overwhelming support for ColorfulCurves over experts' preferred tools.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_494",
    "authors": "Zhen Chen, Danny Kaufman, Mélina Skouras, Etienne Vouga",
    "title": "Complex Wrinkle Field Evolution",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592397",
    "pdf_link": null,
    "abstract": "… Complex Wrinkle Fields (CWFs), a new discrete wrinkle model that enables the resolution of highly detailed wrinkle … the wrinkle amplitude, a one-form 𝜔 per edge to model wrinkle …",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_763",
    "authors": "Pei Xu, Xiumin Shang, Victor Zordan, Ioannis Karamouzas",
    "title": "Composite Motion Learning With Task Control",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592447",
    "pdf_link": null,
    "abstract": "We present a deep learning method for composite and task-driven motion control for physically simulated characters. In contrast to existing data-driven approaches using reinforcement learning that imitate full-body motions, we learn decoupled motions for specific body parts from multiple reference motions simultaneously and directly by leveraging the use of multiple discriminators in a GAN-like setup. In this process, there is no need of any manual work to produce composite reference motions for learning. Instead, the control policy explores by itself how the composite motions can be combined automatically. We further account for multiple task-specific rewards and train a single, multi-objective control policy. To this end, we propose a novel framework for multi-objective learning that adaptively balances the learning of disparate motions from multiple sources and multiple goal-directed control objectives. In addition, as composite motions are typically augmentations of simpler behaviors, we introduce a sample-efficient method for training composite control policies in an incremental manner, where we reuse a pre-trained policy as the meta policy and train a cooperative policy that adapts the meta one for new composite tasks. We show the applicability of our approach on a variety of challenging multi-objective tasks involving both composite motion imitation and multiple goal-directed control. Code is available at https://motion-lab.github.io/CompositeMotion.",
    "scholar_publication": "ACM Transactions on Graphics …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_497",
    "authors": "Michele Vidulis, Yingying Ren, Julian Panetta, Eitan Grinspun, Mark Pauly",
    "title": "Computational Exploration of Multistable Elastic Knots",
    "paper_url": "https://infoscience.epfl.ch/bitstreams/e5e057ed-4561-48ac-a095-594fb86bfb52/download",
    "pdf_link": null,
    "abstract": "Knots are a fundamental concept in mathematics and physics. As mathematical objects, knots are mainly studied as topological equivalence classes of embeddings of a circle into 3D space [Adams 2004]. As physical objects, knots can be classi ed into two main categories: tight knots and loose knots. Tight knots, commonly used in medical sutures, climbing, or boating, leverage friction to block sliding. Their geometric con gurations alter the mechanical properties of the rope in which they are tied, such as its tensile strength [Stasiak et al. 1999].On the other hand, one can tie a knot in a thin elastic rod or wire, and join its ends to form a loose knot (Figure 1). The resulting elastic knot will relax into a con guration that minimizes its elastic energy subject to topology-preserving non-interpenetration constraints. Despite the simplicity of the material system, even a single knot can exhibit a surprising variety of geometrically distinct equilibrium shapes. In other words, loose elastic knots are multistable (see Figure 2 and Figure 8).",
    "scholar_publication": "ACM Trans …, 2023 - infoscience.epfl.ch"
  },
  {
    "paper_id": "papers_305",
    "authors": "Eric Tabellion, Nikhil Karnad, Noa Glaser, Ben Weiss, David Jacobs, Yael Pritch",
    "title": "Computational Long Exposure Mobile Photography",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592124",
    "pdf_link": null,
    "abstract": "Long exposure photography produces stunning imagery, representing moving elements in a scene with motion-blur. It is generally employed in two modalities, producing either a foreground or a background blur effect. Foreground blur images are traditionally captured on a tripod-mounted camera and portray blurred moving foreground elements, such as silky water or light trails, over a perfectly sharp background landscape. Background blur images, also called panning photography, are captured while the camera is tracking a moving subject, to produce an image of a sharp subject over a background blurred by relative motion. Both techniques are notoriously challenging and require additional equipment and advanced skills. In this paper, we describe a computational burst photography system that operates in a hand-held smartphone camera app, and achieves these effects fully automatically, at the tap of the shutter button. Our approach first detects and segments the salient subject. We track the scene motion over multiple frames and align the images in order to preserve desired sharpness and to produce aesthetically pleasing motion streaks. We capture an under-exposed burst and select the subset of input frames that will produce blur trails of controlled length, regardless of scene or camera motion velocity. We predict inter-frame motion and synthesize motion-blur to fill the temporal gaps between the input frames. Finally, we composite the blurred image with the sharp regular exposure to protect the sharpness of faces or areas of the scene that are barely moving, and produce a final high resolution and high dynamic range (HDR) photograph. Our system democratizes a capability previously reserved to professionals, and makes this creative style accessible to most casual photographers.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_121",
    "authors": "Xiaohong Jia, Falai Chen, Shanshan Yao, Shanshan Yao",
    "title": "Computing the Singularities of Rational Parametric Surfaces Using Moving Planes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3551387",
    "pdf_link": null,
    "abstract": "… In this article, we present an efficient and robust algorithm for computing all the singularities (including their orders) of rational parametric surfaces using the technique of moving planes. …",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2022 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_112",
    "authors": "Chenxi Liu, Pierre Bénard, Aaron Hertzmann, Shayan Hoshyari, Chenxi Liu",
    "title": "ConTesse: Accurate Occluding Contours for Subdivision Surfaces",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3544778",
    "pdf_link": null,
    "abstract": "This article proposes a method for computing the visible occluding contours of subdivision surfaces. The article first introduces new theory for contour visibility of smooth surfaces. Necessary and sufficient conditions are introduced for when a sampled occluding contour is valid, that is, when it may be assigned consistent visibility. Previous methods do not guarantee these conditions, which helps explain why smooth contour visibility has been such a challenging problem in the past. The article then proposes an algorithm that, given a subdivision surface, finds sampled contours satisfying these conditions, and then generates a new triangle mesh matching the given occluding contours. The contours of the output triangle mesh may then be rendered with standard non-photorealistic rendering algorithms, using the mesh for visibility computation. The method can be applied to any triangle mesh, by treating it as the base mesh of a subdivision surface.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_116",
    "authors": "Antonin Bernardin, Eulalie Coevoet, Paul Kry, Sheldon Andrews, Christian Duriez, Maud Marchal, Maud Marchal",
    "title": "Constraint-based Simulation of Passive Suction Cups",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3551889",
    "pdf_link": null,
    "abstract": "In this paper, we propose a physics-based model of suction phenomenon to achieve simulation of deformable objects like suction cups. Our model uses a constraint-based formulation to simulate the variations of pressure inside suction cups. The respective internal pressures are represented as pressure constraints which are coupled with anti-interpenetration and friction constraints. Furthermore, our method is able to detect multiple air cavities using information from collision detection. We solve the pressure constraints based on the ideal gas law while considering several cavity states. We test our model with a number of scenarios reflecting a variety of uses, for instance, a spring loaded jumping toy, a manipulator performing a pick and place task, and an octopus tentacle grasping a soda can. We also evaluate the ability of our model to reproduce the physics of suction cups of varying shapes, lifting objects of different masses, and sliding on a slippery surface. The results show promise for various applications such as the simulation in soft robotics and computer animation.",
    "scholar_publication": "ACM Transactions on …, 2022 - dl.acm.org"
  },
  {
    "paper_id": "papers_512",
    "authors": "Maxine Perroni-Scharf, Szymon Rusinkiewicz",
    "title": "Constructing Printable Surfaces With View-dependent Appearance",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591526",
    "pdf_link": null,
    "abstract": "We present a method for the digital fabrication of surfaces whose appearance varies based on viewing direction. The surfaces are constructed from a mesh of bars arranged in a self-occluding colored heightfield that creates the desired view-dependent effects. At the heart of our method is a novel and simple differentiable rendering algorithm specifically designed to render colored 3D heightfields and enable efficient calculation of the gradient of appearance with respect to heights and colors. This algorithm forms the basis of a coarse-to-fine ML-based optimization process that adjusts the heights and colors of the strips to minimize the loss between the desired and real surface appearance from each viewpoint, deriving meshes that can then be fabricated using a 3D printer. Using our method, we demonstrate both synthetic and real-world fabricated results with view-dependent appearance.",
    "scholar_publication": "ACM SIGGRAPH 2023 conference …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_244",
    "authors": "Arjun Lakshmipathy, Nicole Feng, Yu Xi Lee, Moshe Mahler, Nancy Pollard",
    "title": "Contact Edit: Artist Tools for Intuitive Modeling of Hand-object Interactions",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592117",
    "pdf_link": null,
    "abstract": "Posing high-contact interactions is challenging and time-consuming, with hand-object interactions being especially difficult due to the large number of degrees of freedom (DOF) of the hand and the fact that humans are experts at judging hand poses. This paper addresses this challenge by elevating contact areas to first-class primitives. We provide end-to-end art-directable (EAD) tools to model interactions based on contact areas, directly manipulate contact areas, and compute corresponding poses automatically. To make these operations intuitive and fast, we present a novel axis-based contact model that supports real-time approximately isometry-preserving operations on triangulated surfaces, permits movement between surfaces, and is both robust and scalable to large areas. We show that use of our contact model facilitates high quality posing even for unconstrained, high-DOF custom rigs intended for traditional keyframe-based animation pipelines. We additionally evaluate our approach with comparisons to prior art, ablation studies, user studies, qualitative assessments, and extensions to full-body interaction.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_294",
    "authors": "Mégane Bati, Stéphane Blanco, Christophe Coustet, Vincent Eymet, Vincent Forest, Richard Fournier, Jacques Gautrais, Nicolas Mellado, Mathias Paulin, Benjamin Piaud",
    "title": "Coupling Conduction, Convection, and Radiative Transfer in a Single Path-space: Application to Infrared Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592121",
    "pdf_link": null,
    "abstract": "In the past decades, Monte Carlo methods have shown their ability to solve PDEs, independently of the dimensionality of the integration domain and for different use-cases (e.g. light transport, geometry processing, physics simulation). Specifically, the path-space formulation of transport equations is a key ingredient to define tractable and scalable solvers, and we observe nowadays a strong interest in the definition of simulation systems based on Monte Carlo algorithms. We also observe that, when simulating combined physics (e.g. thermal rendering from a heat transfer simulation), there is a lack of coupled Monte Carlo algorithms allowing to solve all the physics at once, in the same path space, rather than combining several independent MC estimators, a combination that would make the global solver critically sensitive to the complexity of each simulation space. This brings to our proposal: a coupled, single path-space, Monte Carlo algorithm for efficient multi-physics problems solving. In this work, we combine our understanding and knowledge of Physics and Computer Graphics to demonstrate how to formulate and arrange different simulation spaces into a single path space. We define a tractable formalism for coupled heat transfer simulation using Monte Carlo, and we leverage the path-space construction to interactively compute multiple simulations with different conditions in the same scene, in terms of boundary conditions and observation time. We validate our proposal in the context of infrared rendering with different thermal simulation scenarios: e.g., room temperature simulation, visualization of heat paths within materials (detection of thermal bridges), heat diffusion capacity of thermal exchanger. We expect that our theoretical framework will foster collaboration and multidisciplinary studies. The perspectives this framework opens are detailed and we suggest a research agenda towards the resolution of coupled PDEs at the interface of Physics and Computer Graphics.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_541",
    "authors": "SooBin Lim, SeungWon Seo, HyeongYeop Kang",
    "title": "DARAM: Dynamic Avatar-human Motion Remapping Technique for Realistic Virtual Stair Ascending Motions",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591527",
    "pdf_link": null,
    "abstract": "This paper introduces DARAM, a dynamic avatar-human motion remapping technique that enables VR users to ascend virtual stairs. The primary design goal is to provide a realistic sensation of virtual stair walking while accounting for discrepancies between the user’s real body motion and the avatar’s motion, arising due to the virtual stairs present only in the virtual environment. Another design goal is to make DARAM applicable to dynamic multi-user environments. To this end, DARAM is designed to achieve motion remapping dynamically without requiring prior information about virtual stairs or environments, simplifying implementation in diverse VR applications. Furthermore, DARAM aims to synthesize avatar motion that delivers not only a realistic first-person experience but also a believable third-person experience for surrounding observers, making it applicable to multi-user VR applications. Two user studies demonstrate that the proposed technique successfully serves our design goals.",
    "scholar_publication": "ACM SIGGRAPH 2023 Conference Proceedings, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_128",
    "authors": "Tong Wu, Jia-Mu Sun, Yu-Kun Lai, Lin Gao",
    "title": "DE-NeRF: DEcoupled Neural Radiance Fields for View-consistent Appearance Editing and High-frequency Environmental Relighting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591483",
    "pdf_link": null,
    "abstract": "Neural Radiance Fields (NeRF) have shown promising results in novel view synthesis. While achieving state-of-the-art rendering results, NeRF usually encodes all properties related to geometry and appearance of the scene together into several MLP (Multi-Layer Perceptron) networks, which hinders downstream manipulation of geometry, appearance and illumination. Recently researchers made attempts to edit geometry, appearance and lighting for NeRF. However, they fail to render view-consistent results after editing the …",
    "scholar_publication": "ACM SIGGRAPH 2023 conference …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_836",
    "authors": "Ruben Grandia, Farbod Farshidian, Espen Knoop, Christian Schumacher, Marco Hutter, Moritz Bächer",
    "title": "DOC: Differentiable Optimal Control for Retargeting Motions Onto Legged Robots",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592454",
    "pdf_link": null,
    "abstract": "Legged robots are designed to perform highly dynamic motions. However, it remains challenging for users to retarget expressive motions onto these complex systems. In this paper, we present a Differentiable Optimal Control (DOC) framework that facilitates the transfer of rich motions from either animals or animations onto these robots. Interfacing with either motion capture or animation data, we formulate retargeting objectives whose parameters make them agnostic to differences in proportions and numbers of degrees of freedom between input and robot. Optimizing these parameters over the manifold spanned by optimal state and control trajectories, we minimize the retargeting error. We demonstrate the utility and efficacy of our modeling by applying DOC to a Model-Predictive Control (MPC) formulation, showing retargeting results for a family of robots of varying proportions and mass distribution. With a hardware deployment, we further show that the retargeted motions are physically feasible, while MPC ensures that the robots retain their capability to react to unexpected disturbances.",
    "scholar_publication": "ACM Transactions On …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_106",
    "authors": "Haocheng Ren, Hangming Fan, Rui Wang, Yuchi Huo, Rui Tang, Lei Wang, Hujun Bao, Haocheng Ren",
    "title": "Data-driven Digital Lighting Design for Residential Indoor Spaces",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3582001",
    "pdf_link": null,
    "abstract": "Conventionally, interior lighting design is technically complex yet challenging and requires professional knowledge and aesthetic disciplines of designers. This article presents a new digital lighting design framework for virtual interior scenes, which allows novice users to automatically obtain lighting layouts and interior rendering images with visually pleasing lighting effects. The proposed framework utilizes neural networks to retrieve and learn underlying design guidelines and the principles beneath the existing lighting designs, e.g., a newly constructed dataset of 6k 3D interior scenes from professional designers with dense annotations of lights. With a 3D furniture-populated indoor scene as the input, the framework takes two stages to perform lighting design: (1) lights are iteratively placed in the room; (2) the colors and intensities of the lights are optimized by an adversarial scheme, resulting in lighting designs with aesthetic lighting effects. Quantitative and qualitative experiments show that the proposed framework effectively learns the guidelines and principles and generates lighting designs that are preferred over the rule-based baseline and comparable to those of professional human designers.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_478",
    "authors": "Nicholas Sharp, Cristian Romero, Alec Jacobson, Etienne Vouga, Paul Kry, David I.W. Levin, Justin Solomon",
    "title": "Data-free Learning of Reduced-order Kinematics",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591521",
    "pdf_link": null,
    "abstract": "Physical systems ranging from elastic bodies to kinematic linkages are defined on high-dimensional configuration spaces, yet their typical low-energy configurations are concentrated on much lower-dimensional subspaces. This work addresses the challenge of identifying such subspaces automatically: given as input an energy function for a high-dimensional system, we produce a low-dimensional map whose image parameterizes a diverse yet low-energy submanifold of configurations. The only additional input needed is a …",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_122",
    "authors": "Steve Bako, Pradeep Sen, Anton Kaplanyan, Steve Bako",
    "title": "Deep Appearance Prefiltering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3570327",
    "pdf_link": null,
    "abstract": "… for prefiltering 3D environments with complex geometry and materials (eg, the Disney BRDF), while maintaining the appearance … -driven prefiltering step to obtain an appearance phase …",
    "scholar_publication": "ACM Transactions on Graphics, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_274",
    "authors": "Jinkai Hu, Chengzhong Yu, Hongli Liu, Lingqi Yan, Yiqian Wu, Xiaogang Jin",
    "title": "Deep Real-time Volumetric Rendering Using Multi-feature Fusion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591493",
    "pdf_link": null,
    "abstract": "We present Multi-feature Radiance-Predicting Neural Networks (MRPNN), a practical framework with a lightweight feature fusion neural network for rendering high-order scattered radiance of participating media in real time. By reformulating the Radiative Transfer Equation (RTE) through theoretical examination, we propose transmittance fields, generated at a low cost, as auxiliary information to help the network better approximate the RTE, drastically reducing the size of the neural network. The light weight network efficiently estimates the difficult-to-solve in-scattering term and allows for configurable shading parameters while improving prediction accuracy. In addition, we propose a frequency-sensitive stencil design in order to handle non-cloud shapes, resulting in accurate shadow boundaries. Results show that our MRPNN is able to synthesize indistinguishable output compared to the ground truth. Most importantly, MRPNN achieves a speedup of two orders of magnitude compared to the state-of-the-art, and is able to render high-quality participating material in real time.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_810",
    "authors": "Lianghao Zhang, Fangzhou Gao, Li Wang, Minjing Yu, Jiamin Cheng, Jiawan Zhang",
    "title": "Deep SVBRDF Estimation From Single Image Under Learned Planar Lighting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591559",
    "pdf_link": null,
    "abstract": "Estimating spatially varying BRDF from a single image without complicated acquisition devices is a challenging problem. In this paper, a deep learning based method was proposed to improve the capture efficiency of single image significantly by learning the lighting pattern of a planar light source, and reconstruct high-quality SVBRDF by learning the global correlation prior of the input image. In our framework, the lighting pattern optimization is embedded in the training process of the network by introducing an online rendering process. The rendering process not only renders images online as the input of network, but also efficiently back propagates gradients from the network to optimize the lighting pattern. Once trained, the network can estimate SVBRDFs from real photographs captured under the learned lighting pattern. Additionally, we describe an onsite capture setup that needs no careful calibration to capture the material sample efficiently. In particular, even a cell phone can be used for illumination. We demonstrate on synthetic and real data that our method could recover a wide range of materials from a single image casually captured under the learned lighting pattern.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_615",
    "authors": "Arthur Firmino, Jeppe Revall Frisvad, Henrik Wann Jensen",
    "title": "Denoising-aware Adaptive Sampling for Monte Carlo Ray Tracing",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591537",
    "pdf_link": null,
    "abstract": "Monte Carlo rendering is a computationally intensive task, but combined with recent deep-learning based advances in image denoising it is possible to achieve high quality images in a shorter amount of time. We present a novel adaptive sampling technique that further improves the efficiency of Monte Carlo rendering combined with deep-learning based denoising. Our proposed technique is general, can be combined with existing pre-trained denoisers, and, in contrast with previous techniques, does not itself require any additional neural networks or learning. A key contribution of our work is a general method for estimating the variance of the outputs of a neural network whose inputs are random variables. Our method iteratively renders additional samples and uses this novel variance estimate to compute the sample distribution for each subsequent iteration. Compared to uniform sampling and previous adaptive sampling techniques, our method achieves better equal-time error in all scenes tested, and when combined with a recent denoising post-correction technique, significantly faster error convergence is realized.",
    "scholar_publication": "ACM SIGGRAPH 2023 Conference …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_313",
    "authors": "Qiaodong Cui, Victor Rong, Desai Chen, Wojciech Matusik",
    "title": "Dense, Interlocking-free and Scalable Spectral Packing of Generic 3D Objects",
    "paper_url": "https://www.voxelmatters.com/wp-content/uploads/2023/06/spectralPacking_optimized.pdf",
    "pdf_link": null,
    "abstract": "1: 2• Cui et al. its practical importance, such as in robotic packaging [Wang and Hauser 2019], 3D printing [Chen et al. 2015; Yao et al. 2015], transportation [Egeblad et al. 2010], and layout and pattern generation [Fanni et al. 2022; Reinert et al. 2013]. We focus on the problem of maximum density packing given a fixed container and a list of 3D objects. Packing density is the total volume of all packed objects divided by container volume. In this scenario, there may be more objects than what can fit into a single container. We later extend our base algorithm to multi-tray packing where the goal is to pack all objects using as few containers as possible. We also require the packing to be interlocking-free: parts must be physically disassembled without breaking or deforming them. The disassembly is allowed to move a part or a group of parts through one or more steps along possibly different directions. We gear our algorithm toward 3D printing [Sitthi-Amorn et al. 2015; Vanek et al. 2014]. In this case, the container is virtual and only serves to specify the printing volume. Objects can be disassembled in any direction after printing. This is different from packing in robotics such as Hu et al.[2020], where the objects are inserted through a physical opening.The first challenge of packing generic 3D objects is addressing the complicated collision constraints that stem from arbitrary geometries. Many algorithms have been proposed previously [Lamas-Fernandez et al. 2022; Liu et al. 2015; Ma et al. 2018; Romanova et al. 2018], but it remains difficult to scale these algorithms to pack more than a few dozen complex objects. Inspired by works in protein docking [Katchalski-Katzir et al. 1992], we formulate collision constraints as correlations between discretized objects, which are computed in the spectral domain with FFT. This is efficient because a single convolution using FFT computes collision detection results for an object at all voxel locations. This algorithm is scalable to thousands of complex 3D objects. We then introduce a proximity metric at every voxel location, also computed using FFT. The resulting algorithm is extremely efficient at finding object placements for tight packing.",
    "scholar_publication": "ACM Trans. Graph., 2023 - voxelmatters.com"
  },
  {
    "paper_id": "papers_480",
    "authors": "Daoming Liu, Davide Pellis, Yu-Chou Chiang, Florian Rist, Johannes Wallner, Helmut Pottmann",
    "title": "Deployable Strip Structures",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592393",
    "pdf_link": null,
    "abstract": "… area identifies a class of deployable structures which minimizes the … Our deployable quad structures are related to K-surfaces which … In particular, we show that deployable structures are …",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_366",
    "authors": "Anpei Chen, Zexiang Xu, Xinyue Wei, Siyu Tang, Hao Su, Andreas Geiger",
    "title": "Dictionary Fields: Learning a Neural Basis Decomposition",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592135",
    "pdf_link": null,
    "abstract": "We present Dictionary Fields, a novel neural representation which decomposes a signal into a product of factors, each represented by a classical or neural field representation, operating on transformed input coordinates. More specifically, we factorize a signal into a coefficient field and a basis field, and exploit periodic coordinate transformations to apply the same basis functions across multiple locations and scales. Our experiments show that Dictionary Fields lead to improvements in approximation quality, compactness, and training time when compared to previous fast reconstruction methods. Experimentally, our representation achieves better image approximation quality on 2D image regression tasks, higher geometric quality when reconstructing 3D signed distance fields, and higher compactness for radiance field reconstruction tasks. Furthermore, Dictionary Fields enable generalization to unseen images/3D scenes by sharing bases across signals during training which greatly benefits use cases such as image regression from partial observations and few-shot radiance field reconstruction.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_552",
    "authors": "Xiaochun Tong, Hsueh-Ti Derek Liu, Yotam Gingold, Alec Jacobson",
    "title": "Differentiable Heightfield Path Tracing With Accelerated Discontinuities",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591530",
    "pdf_link": null,
    "abstract": "We investigate the problem of accelerating a physically-based differentiable renderer for heightfields based on path tracing with global illumination. On a heightfield with 1 million vertices (1024 × 1024 resolution), our differentiable renderer requires only 4 ms per sample per pixel when differentiating direct illumination, orders of magnitude faster than most existing general 3D mesh differentiable renderers. It is well-known that one can leverage spatial hierarchical data structures (e.g., the maximum mipmaps) to accelerate the forward pass of heightfield rendering. The key idea of our approach is to further utilize the hierarchy to speed up the backward pass—differentiable heightfield rendering. Specifically, we use the maximum mipmaps to accelerate the process of identifying scene discontinuities, which is crucial for obtaining accurate derivatives. Our renderer supports global illumination. we are able to optimize global effects, such as shadows, with respect to the geometry and the material parameters. Our differentiable renderer achieves real-time frame rates and unlocks interactive inverse rendering applications. We demonstrate the flexibility of our method with terrain optimization, geometric illusions, shadow optimization, and text-based shape generation.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_236",
    "authors": "Juan Sebastian Montes Maestre, Yinwei Du, Ronan Hinchet, Stelian Coros, Bernhard Thomaszewski",
    "title": "Differentiable Stripe Patterns for Inverse Design of Structured Surfaces",
    "paper_url": "https://arxiv.org/abs/2305.13841",
    "pdf_link": null,
    "abstract": "Stripe patterns are ubiquitous in nature and everyday life. While the synthesis of these patterns has been thoroughly studied in the literature, their potential to control the mechanics of structured materials remains largely unexplored. In this work, we introduce Differentiable Stripe Patterns--a computational approach for automated design of physical surfaces structured with stripe-shaped bi-material distributions. Our method builds on the work by Knoppel and colleagues for generating globally-continuous and equally-spaced …",
    "scholar_publication": "arXiv preprint arXiv …, 2023 - arxiv.org"
  },
  {
    "paper_id": "paperstog_114",
    "authors": "Yunpu Hu, Masatoshi Ishikawa, Leo Miyashita, Yunpu Hu",
    "title": "Differential Frequency Heterodyne Time-of-flight Imaging for Instantaneous Depth and Velocity Estimation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3546939",
    "pdf_link": null,
    "abstract": "In this study, we discuss the imaging of depth and velocity using heterodyne-mode time-of-flight (ToF) cameras. In particular, Doppler ToF (D-ToF) imaging utilizes heterodyne modulation to measure the velocity from the Doppler frequency shift, which uniquely facilitates the instantaneous radial velocity estimation. However, theoretical discussion on D-ToF is limited to orthogonal frequency and sinusoidal waveform modulation. This study extends the formulation of the D-ToF imaging, and proposes an arbitrary-frequency, arbitrary-waveform framework considering a phase-compensated, symmetrical two-dimensional correlation map. With the proposed framework, the optimal heterodyne frequency for frequency decoding is found. A differential frequency sampling and decoding method is then proposed, which computes the frequency and phase from as few as four simultaneously captured images. With an experiment platform we built, it is confirmed that the minimum velocity sensing error is half that of the orthogonal frequency method, and the sensible phase range is approximately 2.5 times larger. The conclusions in this study allow the ToF velocity imaging to be applied at the optimal sample frequencies for a wide range of ToF sensors. This pushes one step further to the practical use of ToF velocity imaging.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2022 - dl.acm.org"
  },
  {
    "paper_id": "papers_582",
    "authors": "Mariia Myronova, William Neveu, Mikhail Bessmeltsev",
    "title": "Differential Operators on Sketches via Alpha Contours",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592420",
    "pdf_link": null,
    "abstract": "A vector sketch is a popular and natural geometry representation depicting a 2D shape. When viewed from afar, the disconnected vector strokes of a sketch and the empty space around them visually merge into positive space and negative space, respectively. Positive and negative spaces are the key elements in the composition of a sketch and define what we perceive as the shape. Nevertheless, the notion of positive or negative space is mathematically ambiguous: While the strokes unambiguously indicate the interior or boundary of a 2D shape, the empty space may or may not belong to the shape's exterior. For standard discrete geometry representations, such as meshes or point clouds, some of the most robust pipelines rely on discretizations of differential operators, such as Laplace-Beltrami. Such discretizations are not available for vector sketches; defining them may enable numerous applications of classical methods on vector sketches. However, to do so, one needs to define the positive space of a vector sketch, or the sketch shape. Even though extracting this 2D sketch shape is mathematically ambiguous, we propose a robust algorithm, Alpha Contours, constructing its conservative estimate: a 2D shape containing all the input strokes, which lie in its interior or on its boundary, and aligning tightly to a sketch. This allows us to define popular differential operators on vector sketches, such as Laplacian and Steklov operators. We demonstrate that our construction enables robust tools for vector sketches, such as As-Rigid-As-Possible sketch deformation and functional maps between sketches, as well as solving partial differential equations on a vector sketch.",
    "scholar_publication": "ACM Transactions on Graphics …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_803",
    "authors": "Adéla Šubrtová, Michal Lukáč, Jan Čech, David Futschik, Eli Shechtman, Daniel Sýkora",
    "title": "Diffusion Image Analogies",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591558",
    "pdf_link": null,
    "abstract": "… difficulties by elevating the original concept of image analogies into a next level where also … power of diffusion models and demonstrate how to achieve image-based analogy without the …",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_316",
    "authors": "Xingang Pan, Ayush Tewari, Thomas Leimkühler, Lingjie Liu, Abhimitra Meka, Christian Theobalt",
    "title": "Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591500",
    "pdf_link": null,
    "abstract": "… This work aims to develop an interactive image manipulation method for GANs where users only need to click on the images to define some pairs of (handle point, target point) and drive …",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_120",
    "authors": "Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang, Cheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, Jingyi Yu",
    "title": "DreamFace: Progressive Generation of Animatable 3D Faces Under Text Guidance",
    "paper_url": "https://arxiv.org/abs/2304.03117",
    "pdf_link": null,
    "abstract": "Emerging Metaverse applications demand accessible, accurate, and easy-to-use tools for 3D digital human creations in order to depict different cultures and societies as if in the physical world. Recent large-scale vision-language advances pave the way to for novices to conveniently customize 3D content. However, the generated CG-friendly assets still cannot represent the desired facial traits for human characteristics. In this paper, we present DreamFace, a progressive scheme to generate personalized 3D faces under text guidance. It enables layman users to naturally customize 3D facial assets that are compatible with CG pipelines, with desired shapes, textures, and fine-grained animation capabilities. From a text input to describe the facial traits, we first introduce a coarse-to-fine scheme to generate the neutral facial geometry with a unified topology. We employ a selection strategy in the CLIP embedding space, and subsequently optimize both the details displacements and normals using Score Distillation Sampling from generic Latent Diffusion Model. Then, for neutral appearance generation, we introduce a dual-path mechanism, which combines the generic LDM with a novel texture LDM to ensure both the diversity and textural specification in the UV space. We also employ a two-stage optimization to perform SDS in both the latent and image spaces to significantly provides compact priors for fine-grained synthesis. Our generated neutral assets naturally support blendshapes-based facial animations. We further improve the animation ability with personalized deformation characteristics by learning the universal expression prior using the cross-identity hypernetwork. Notably, DreamFace can generate of realistic 3D facial assets with physically-based rendering quality and rich animation ability from video footage, even for fashion icons or exotic characters in cartoons and fiction movies.",
    "scholar_publication": "arXiv preprint arXiv …, 2023 - arxiv.org"
  },
  {
    "paper_id": "papers_580",
    "authors": "Ruicheng Xiong, Yang Lu, Cong Chen, Jiaming Zhu, Yajun Zeng, Ligang Liu",
    "title": "ETER: Elastic Tessellation for Real-time Pixel-accurate Rendering of Large-scale NURBS Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592419",
    "pdf_link": null,
    "abstract": "We present ETER, an elastic tessellation framework for rendering large-scale NURBS models with pixel-accurate and crack-free quality at real-time frame rates. We propose a highly parallel adaptive tessellation algorithm to achieve pixel accuracy, measured by the screen space error between the exact surface and its triangulation. To resolve a bottleneck in NURBS rendering, we present a novel evaluation method based on uniform sampling grids and accelerated by GPU Tensor Cores. Compared to evaluation based on hardware tessellation, our method has achieved a significant speedup of 2.9 to 16.2 times depending on the degrees of the patches. We develop an efficient crack-filling algorithm based on conservative rasterization and visibility buffer to fill the tessellation-induced cracks while greatly reducing the jagged effect introduced by conservative rasterization. We integrate all our novel algorithms, implemented in CUDA, into a GPU NURBS rendering pipeline based on Mesh Shaders and hybrid software/hardware rasterization. Our performance data on a commodity GPU show that the rendering pipeline based on ETER is capable of rendering up to 3.7 million patches (0.25 billion tessellated triangles) in real-time (30FPS). With its advantages in performance, scalability, and visual quality in rendering large-scale NURBS models, a real-time tessellation solution based on ETER can be a powerful alternative or even a potential replacement for the existing pre-tessellation solution in CAD systems.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_672",
    "authors": "Alexander Weinrauch, Wolfgang Tatzgern, Pascal Stadlbauer, Alexis Crickx, Jozef Hladky, Arno Coomans, Martin Winter, Joerg H. Mueller, Markus Steinberger",
    "title": "Effect-based Multi-viewer Caching for Cloud-native Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592431",
    "pdf_link": null,
    "abstract": "With cloud computing becoming ubiquitous, it appears as virtually everything can be offered as-a-service. However, real-time rendering in the cloud forms a notable exception, where the cloud adoption stops at running individual game instances in compute centers. In this paper, we explore whether a cloud-native rendering architecture is viable and scales to multi-client rendering scenarios. To this end, we propose world-space and on-surface caches to share rendering computations among viewers placed in the same virtual world. We discuss how caches can be utilized on an effect-basis and demonstrate that a large amount of computations can be saved as the number of viewers in a scene increases. Caches can easily be set up for various effects, including ambient occlusion, direct illumination, and diffuse global illumination. Our results underline that the image quality using cached rendering is on par with screen-space rendering and due to its simplicity and inherent coherence, cached rendering may even have advantages in single viewer setups. Analyzing the runtime and communication costs, we show that cached rendering is already viable in multi-GPU systems. Building on top of our research, cloud-native rendering may be just around the corner.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_749",
    "authors": "Ugo Finnendahl, Dimitrios Bogiokas, Pablo Robles Cervantes, Marc Alexa",
    "title": "Efficient Embeddings in Exact Arithmetic",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592445",
    "pdf_link": null,
    "abstract": "We provide a set of tools for generating planar embeddings of triangulated topological spheres. The algorithms make use of Schnyder labelings and realizers. A new representation of the realizer based on dual trees leads to a simple linear time algorithm mapping from weights per triangle to barycentric coordinates and, more importantly, also in the reverse direction. The algorithms can be implemented so that all coefficients involved are 1 or -1. This enables integer computation, making all computations exact. Being a Schnyder realizer, mapping from positive triangle weights guarantees that the barycentric coordinates form an embedding. The reverse direction enables an algorithm for fixing flipped triangles in planar realizations, by mapping from coordinates to weights and adjusting the weights (without forcing them to be positive). In a range of experiments, we demonstrate that all algorithms are orders of magnitude faster than existing robust approaches.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_384",
    "authors": "Kaisiyuan Wang, Hang Zhou, Qianyi Wu, Jiaxiang Tang, Zhiliang Xu, Borong Liang, Tianshu Hu, Errui Ding, Jingtuo Liu, Ziwei Liu, Jingdong Wang",
    "title": "Efficient Video Portrait Reenactment via Grid-based Codebook",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591509",
    "pdf_link": null,
    "abstract": "While progress has been made in the field of portrait reenactment, the problem of how to efficiently produce high-fidelity and accurate videos remains. Recent studies build direct mappings between driving signals and their predictions, leading to failure cases when synthesizing background textures and detailed local motions. In this paper, we propose the Video Portrait via Grid-based Codebook (VPGC) framework, which achieves efficient and high-fidelity portrait modeling. Our key insight is to query driving signals in a position-aware textural codebook with an explicit grid structure. The grid-based codebook stores delicate textural information locally according to our observations on video portraits, which can be learned efficiently and precisely. We subsequently design a Prior-Guided Driving Module to predict reliable features from the driving signals, which can be later decoded back to high-quality video portraits by querying the codebook. Comprehensive experiments are conducted to validate the effectiveness of our approach.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_168",
    "authors": "Xinyu Yi, Yuxiao Zhou, Marc Habermann, Vladislav Golyanik, Shaohua Pan, Christian Theobalt, Feng Xu",
    "title": "EgoLocate: Real-time Motion Capture, Localization, and Mapping With Sparse Body-mounted Sensors",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592099",
    "pdf_link": null,
    "abstract": "Human and environment sensing are two important topics in Computer Vision and Graphics. Human motion is often captured by inertial sensors, while the environment is mostly reconstructed using cameras. We integrate the two techniques together in EgoLocate, a system that simultaneously performs human motion capture (mocap), localization, and mapping in real time from sparse body-mounted sensors, including 6 inertial measurement units (IMUs) and a monocular phone camera. On one hand, inertial mocap suffers from large translation drift due to the lack of the global positioning signal. EgoLo-cate leverages image-based simultaneous localization and mapping (SLAM) techniquesto locate the human in the reconstructed scene. Onthe other hand, SLAM often fails when the visual feature is poor. EgoLocate involves inertial mocap to provide a strong prior for the camera motion. Experiments show that localization, a key challenge for both two fields, is largely improved by our technique, compared with the state of the art of the two fields. Our codes are available for research at https://xinyu-yi.github.io/EgoLocate/.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_347",
    "authors": "Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano, Gal Chechik, Daniel Cohen-Or",
    "title": "Encoder-based Domain Tuning for Fast Personalization of Text-to-image Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592133",
    "pdf_link": null,
    "abstract": "Text-to-image personalization aims to teach a pre-trained diffusion model to reason about novel, user provided concepts, embedding them into new scenes guided by natural language prompts. However, current personalization approaches struggle with lengthy training times, high storage requirements or loss of identity. To overcome these limitations, we propose an encoder-based domain-tuning approach. Our key insight is that by underfitting on a large set of concepts from a given domain, we can improve generalization and create a model that is more amenable to quickly adding novel concepts from the same domain. Specifically, we employ two components: First, an encoder that takes as an input a single image of a target concept from a given domain, e.g. a specific face, and learns to map it into a word-embedding representing the concept. Second, a set of regularized weight-offsets for the text-to-image model that learn how to effectively injest additional concepts. Together, these components are used to guide the learning of unseen concepts, allowing us to personalize a model using only a single image and as few as 5 training steps --- accelerating personalization from dozens of minutes to seconds, while preserving quality. Code and trained encoders will be available at our project page.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_343",
    "authors": "Beichen Li, Liang Shi, Wojciech Matusik",
    "title": "End-to-end Procedural Material Capture With Proxy-free Mixed-integer Optimization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592132",
    "pdf_link": null,
    "abstract": "Node-graph-based procedural materials are vital to 3D content creation within the computer graphics industry. Leveraging the expressive representation of procedural materials, artists can effortlessly generate diverse appearances by altering the graph structure or node parameters. However, manually reproducing a specific appearance is a challenging task that demands extensive domain knowledge and labor. Previous research has sought to automate this process by converting artist-created material graphs into differentiable programs and optimizing node parameters against a photographed material appearance using gradient descent. These methods involve implementing differentiable filter nodes [Shi et al. 2020] and training differentiable neural proxies for generator nodes to optimize continuous and discrete node parameters [Hu et al. 2022a] jointly. Nevertheless, Neural Proxies exhibits critical limitations, such as long training times, inaccuracies, fixed resolutions, and confined parameter ranges, which hinder their scalability towards the broad spectrum of production-grade material graphs. These constraints fundamentally stem from the absence of faithful and efficient implementations of generic noise and pattern generator nodes, both differentiable and non-differentiable. Such deficiency prevents the direct optimization of continuous and discrete generator node parameters without relying on surrogate models. We present Diffmat v2, an improved differentiable procedural material library, along with a fully-automated, end-to-end procedural material capture framework that combines gradient-based optimization and gradient-free parameter search to match existing production-grade procedural materials against user-taken flash photos. Diffmat v2 expands the range of differentiable material graph nodes in Diffmat [Shi et al. 2020] by adding generic noise/pattern generator nodes and user-customizable per-pixel filter nodes. This allows for the complete translation and optimization of procedural materials across various categories without the need for external proprietary tools or pre-cached noise patterns. Consequently, our method can capture a considerably broader array of materials, encompassing those with highly regular or stochastic geometries. We demonstrate that our end-to-end approach yields a closer match to the target than MATch [Shi et al. 2020] and Neural Proxies [Hu et al. 2022a] when starting from initially unmatched continuous and discrete parameters.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_255",
    "authors": "Jiatian Sun, Longxiulin Deng, Triantafyllos Afouras, Andrew Owens, Abe Davis",
    "title": "Eventfulness for Interactive Video Alignment",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592118",
    "pdf_link": null,
    "abstract": "Humans are remarkably sensitive to the alignment of visual events with other stimuli, which makes synchronization one of the hardest tasks in video editing. A key observation of our work is that most of the alignment we do involves salient localizable events that occur sparsely in time. By learning how to recognize these events, we can greatly reduce the space of possible synchronizations that an editor or algorithm has to consider. Furthermore, by learning descriptors of these events that capture additional properties of visible motion, we can build active tools that adapt their notion of eventfulness to a given task as they are being used. Rather than learning an automatic solution to one specific problem, our goal is to make a much broader class of interactive alignment tasks significantly easier and less time-consuming. We show that a suitable visual event descriptor can be learned entirely from stochastically-generated synthetic video. We then demonstrate the usefulness of learned and adaptive eventfulness by integrating it in novel interactive tools for applications including audio-driven time warping of video and the extraction and application of sound effects across different videos.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_406",
    "authors": "Zheng-Yu Zhao, Mo Li, Zheng Zhang, Qing Fang, Ligang Liu, Xiao-Ming Fu",
    "title": "Evolutionary Piecewise Developable Approximations",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592140",
    "pdf_link": null,
    "abstract": "We propose a novel method to compute high-quality piecewise developable approximations for triangular meshes. Central to our approach is an evolutionary genetic algorithm for optimizing the combinatorial and discontinuous fitness function, including the approximation error, the number of patches, the patch boundary length, and the penalty for small patches and narrow regions within patches. The genetic algorithm's operations (i.e., initialization, selection, mutation, and crossover) are explicitly designed to minimize the fitness function. The main challenge is evaluating the fitness function's approximation error as it requires developable patches, which are difficult or time-consuming to obtain. Resolving the challenge is based on a critical observation: the approximation error and the mapping distortion between an input surface and its developable approximation are positively correlated empirically. To efficiently measure distortion without explicitly generating developable shapes, we creatively use conformal mapping techniques. Then, we control the mapping distortion at a relatively low level to achieve high shape similarity in the genetic algorithm. The feasibility and effectiveness of our method are demonstrated over 240 complex examples. Compared with the state-of-the-art methods, our results have much smaller approximation errors, fewer patches, shorter patch boundaries, and fewer small patches and narrow regions.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_485",
    "authors": "Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, Baoquan Chen",
    "title": "Example-based Motion Synthesis via Generative Motion Matching",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592395",
    "pdf_link": null,
    "abstract": "We present GenMM, a generative model that \"mines\" as many diverse motions as possible from a single or few example sequences. In stark contrast to existing data-driven methods, which typically require long offline training time, are prone to visual artifacts, and tend to fail on large and complex skeletons, GenMM inherits the training-free nature and the superior quality of the well-known Motion Matching method. GenMM can synthesize a high-quality motion within a fraction of a second, even with highly complex and large skeletal structures. At the heart of our generative framework lies the generative motion matching module, which utilizes the bidirectional visual similarity as a generative cost function to motion matching, and operates in a multi-stage framework to progressively refine a random guess using exemplar motion matches. In addition to diverse motion generation, we show the versatility of our generative framework by extending it to a number of scenarios that are not possible with motion matching alone, including motion completion, key frame-guided generation, infinite looping, and motion reassembly.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_261",
    "authors": "Paul Merrell",
    "title": "Example-based Procedural Modeling Using Graph Grammars",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592119",
    "pdf_link": null,
    "abstract": "We present a method for automatically generating polygonal shapes from an example using a graph grammar. Most procedural modeling techniques use grammars with manually created rules, but our method can create them automatically from an example. Our graph grammars generate graphs that are locally similar to a given example. We disassemble the input into small pieces called primitives and then reassemble the primitives into new graphs. We organize all possible locally similar graphs into a hierarchy and find matching graphs within the hierarchy. These matches are used to create a graph grammar that can construct every locally similar graph. Our method generates graphs using the grammar and then converts them into a planar graph drawing to produce the final shape.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_583",
    "authors": "Valentin Zenon Nigolian, Marcel Campen, David Bommes",
    "title": "Expansion Cones: A Progressive Volumetric Mapping Framework",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592421",
    "pdf_link": null,
    "abstract": "Volumetric mapping is a ubiquitous and difficult problem in Geometry Processing and has been the subject of research in numerous and various directions. While several methods show encouraging results, the field still lacks a general approach with guarantees regarding map bijectivity. Through this work, we aim at opening the door to a new family of methods by providing a novel framework based on the concept of progressive expansion. Starting from an initial map of a tetrahedral mesh whose image may contain degeneracies but no …",
    "scholar_publication": "ACM Transactions on Graphics …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_604",
    "authors": "Zeqi Gu, Wenqi Xian, Noah Snavely, Abe Davis",
    "title": "FactorMatte: Redefining Video Matting for Re-composition Tasks",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592423",
    "pdf_link": null,
    "abstract": "We propose Factor Matting, an alternative formulation of the video matting problem in terms of counterfactual video synthesis that is better suited for re-composition tasks. The goal of factor matting is to separate the contents of a video into independent components, each representing a counterfactual version of the scene where the contents of other components have been removed. We show that factor matting maps well to a more general Bayesian framing of the matting problem that accounts for complex conditional interactions between layers. Based on this observation, we present a method for solving the factor matting problem that learns augmented patch-based appearance priors to produce useful decompositions even for video with complex cross-layer interactions like splashes, shadows, and reflections. Our method is trained per-video and does not require external training data or any knowledge about the 3D structure of the scene. Through extensive experiments, we show that it is able to produce useful decompositions of scenes with such complex interactions while performing competitively on classical matting tasks as well. We also demonstrate the benefits of our approach on a wide range of downstream video editing tasks. Our project website is at: https://factormatte.github.io/.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_890",
    "authors": "Anran Lin, Nanxuan Zhao, Shuliang Ning, Yuda Qiu, Baoyuan Wang, Xiaoguang Han",
    "title": "FashionTex: Controllable Virtual Try-on With Text and Texture",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591568",
    "pdf_link": null,
    "abstract": "Virtual try-on attracts increasing research attention as a promising way for enhancing the user experience for online cloth shopping. Though existing methods can generate impressive results, users need to provide a well-designed reference image containing the target fashion clothes that often do not exist. To support user-friendly fashion customization in full-body portraits, we propose a multi-modal interactive setting by combining the advantages of both text and texture for multi-level fashion manipulation. With the carefully designed fashion editing module and loss functions, FashionTex framework can semantically control cloth types and local texture patterns without annotated pairwise training data. We further introduce an ID recovery module to maintain the identity of input portrait. Extensive experiments have demonstrated the effectiveness of our proposed pipeline. Code for this paper are at https://github.com/picksh/FashionTex.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_502",
    "authors": "Otman Benchekroun, Jiayi Eris Zhang, Siddhartha Chaudhuri, Eitan Grinspun, Yi Zhou, Alec Jacobson",
    "title": "Fast Complementary Dynamics via Skinning Eigenmodes",
    "paper_url": "https://arxiv.org/abs/2303.11886",
    "pdf_link": null,
    "abstract": "We propose a reduced-space elasto-dynamic solver that is well suited for augmenting rigged character animations with secondary motion. At the core of our method is a novel deformation subspace based on Linear Blend Skinning that overcomes many of the shortcomings prior subspace methods face. Our skinning subspace is parameterized entirely by a set of scalar weights, which we can obtain through a small, material-aware and rig-sensitive generalized eigenvalue problem. The resulting subspace can easily capture rotational motion and guarantees that the resulting simulation is rotation equivariant. We further propose a simple local-global solver for linear co-rotational elasticity and propose a clustering method to aggregate per-tetrahedra non-linear energetic quantities. The result is a compact simulation that is fully decoupled from the complexity of the mesh.",
    "scholar_publication": "arXiv preprint arXiv …, 2023 - arxiv.org"
  },
  {
    "paper_id": "paperstog_124",
    "authors": "Tianyu Wang, Jiong Chen, Dongping Li, Xiaowei Liu, Huamin Wang, Kun Zhou, Tianyu Wang",
    "title": "Fast GPU-based Two-way Continuous Collision Handling",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3604551",
    "pdf_link": null,
    "abstract": "Step-and-project is a popular method to simulate non-penetrating deformable bodies in physically based animation. The strategy is to first integrate the system in time without considering contacts and then resolve potential intersections, striking a good balance between plausibility and efficiency. However, existing methods can be defective and unsafe when using large time steps, taking risks of failure or demanding repetitive collision testing and resolving that severely degrade performance. In this article, we propose a novel two-way method for fast and reliable continuous collision handling. Our method launches an optimization from both ends of the intermediate time-integrated state and the previous intersection-free state. It progressively generates a piecewise linear path and eventually obtains a feasible solution for the next time step. The algorithm efficiently alternates between a forward step and a backward step until the result is conditionally converged. Thanks to a set of unified volume-based contact constraints, our method offers flexible and reliable handling of various codimensional deformable bodies, including volumetric bodies, cloth, hair, and sand. Experimental results demonstrate the safety, robustness, physical fidelity, and numerical efficiency of our method, making it particularly suitable for scenarios involving large deformations or large time steps.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_315",
    "authors": "kaixuan zhang, Jingxian Wang, Daizong Tian, Thrasyvoulos Pappas",
    "title": "Film Grain Rendering and Parameter Estimation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592127",
    "pdf_link": null,
    "abstract": "We propose a realistic film grain rendering algorithm based on statistics derived analytically from a physics-based Boolean model that Newson et al. adopted for Monte Carlo simulations of film grain. We also propose formulas for estimation of the model parameters from scanned film grain images. The proposed rendering is computationally efficient and can be used for real-time film grain simulation for a wide range of film grain parameters when the individual film grains are not visible. Experimental results demonstrate the effectiveness of the proposed approach for both constant and real-world images, for a six orders of magnitude speed-up compared with the Monte Carlo simulations of the Newson et al. approach.",
    "scholar_publication": "ACM Transactions on Graphics …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_662",
    "authors": "Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangxue Yin, Zian Wang, Wenzheng Chen, Zan Gojcic, Sanja Fidler, Nicholas Sharp, Jun Gao",
    "title": "Flexible Isosurface Extraction for Gradient-based Mesh Optimization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592430",
    "pdf_link": null,
    "abstract": "This work considers gradient-based mesh optimization, where we iteratively optimize for a 3D surface mesh by representing it as the isosurface of a scalar field, an increasingly common paradigm in applications including photogrammetry, generative modeling, and inverse physics. Existing implementations adapt classic isosurface extraction algorithms like Marching Cubes or Dual Contouring; these techniques were designed to extract meshes from fixed, known fields, and in the optimization setting they lack the degrees of freedom to represent high-quality feature-preserving meshes, or suffer from numerical instabilities. We introduce FlexiCubes, an isosurface representation specifically designed for optimizing an unknown mesh with respect to geometric, visual, or even physical objectives. Our main insight is to introduce additional carefully-chosen parameters into the representation, which allow local flexible adjustments to the extracted mesh geometry and connectivity. These parameters are updated along with the underlying scalar field via automatic differentiation when optimizing for a downstream task. We base our extraction scheme on Dual Marching Cubes for improved topological properties, and present extensions to optionally generate tetrahedral and hierarchically-adaptive meshes. Extensive experiments validate FlexiCubes on both synthetic benchmarks and real-world applications, showing that it offers significant improvements in mesh quality and geometric fidelity.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_500",
    "authors": "Hang Yin, Mohammad Sina Nabizadeh, Baichuan Wu, Stephanie Wang, Albert Chern",
    "title": "Fluid Cohomology",
    "paper_url": "https://books.google.com/books?hl=en&lr=&id=OEyDAwAAQBAJ&oi=fnd&pg=PA1&dq=Fluid+Cohomology&ots=vh3OipVSOY&sig=tNZyDI8GkiGN6tUMeidYqSZ4XCE",
    "pdf_link": null,
    "abstract": "… In part I we define a “finite dimensional 3D fluid algebra,” write its Euler equation and derive … on cohomology. For the flat torus these are used in numerical calculation for fluids. …",
    "scholar_publication": "Low-dimensional and symplectic topology, 2011 - books.google.com"
  },
  {
    "paper_id": "papers_392",
    "authors": "Wei Li, Mathieu Desbrun",
    "title": "Fluid-solid Coupling in Kinetic Two-phase Flow Simulation",
    "paper_url": "https://inria.hal.science/hal-04174289/",
    "pdf_link": null,
    "abstract": "Real-life flows exhibit complex and visually appealing behaviors such as bubbling, splashing, glugging and wetting that simulation techniques in graphics have attempted to capture for years. While early approaches were not capable of reproducing multiphase flow phenomena due to their excessive numerical viscosity and low accuracy, kinetic solvers based on the lattice Boltzmann method have recently demonstrated the ability to simulate water-air interaction at high Reynolds numbers in a massively-parallel fashion. However, robust and accurate handling of fluid-solid coupling has remained elusive: be it for CG or CFD solvers, as soon as the motion of immersed objects is too fast or too sudden, pressures near boundaries and interfacial forces exhibit spurious oscillations leading to blowups. Built upon a phase-field and velocity-distribution based lattice-Boltzmann solver for multiphase flows, this paper spells out a series of numerical improvements in momentum exchange, interfacial forces, and two-way coupling to drastically reduce these typical artifacts, thus significantly expanding the types of fluid-solid coupling that we can efficiently simulate. We highlight the numerical benefits of our solver through various challenging simulation results, including comparisons to previous work and real footage.",
    "scholar_publication": "ACM Transactions on Graphics, 2023 - inria.hal.science"
  },
  {
    "paper_id": "papers_689",
    "authors": "Alexander Rath, Ömercan Yazici, Philipp Slusallek",
    "title": "Focal Path Guiding for Light Transport Simulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591543",
    "pdf_link": null,
    "abstract": "Focal points are fascinating effects that emerge from various constellations, for example when light passes through narrow gaps or when objects are seen through lenses or mirrors. These effects can be challenging to render, as paths need to pass through small regions that are not always known beforehand and can occur freely in space. Specialized algorithms exist for some effects, but many of them rely on Markov chain Monte Carlo integration, which is known to suffer from uneven convergence undesirable in practice. Path guiding methods are a promising alternative, but existing techniques only handle a subset of focal effects. We propose a novel form of guiding that is specifically tailored to identify focal points and sample them in accordance to their image contribution. Our technique is the first to unify all focal effects in a single framework and we demonstrate that it can render effects that previous state-of-the-art techniques are unable to handle.",
    "scholar_publication": "ACM SIGGRAPH 2023 Conference …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_584",
    "authors": "Guillaume Cordonnier, Guillaume Jouvet, Adrien Peytavie, Jean Braun, Marie-Paule Cani, Bedrich Benes, Eric Galin, Eric Guerin, James Gain",
    "title": "Forming Terrains by Glacial Erosion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592422",
    "pdf_link": null,
    "abstract": "We introduce the first solution for simulating the formation and evolution of glaciers, together with their attendant erosive effects, for periods covering the combination of glacial and inter-glacial cycles. Our efficient solution includes both a fast yet accurate deep learning-based estimation of highorder ice flows and a new, multi-scale advection scheme enabling us to account for the distinct time scales at which glaciers reach equilibrium compared to eroding the terrain. We combine the resulting glacial erosion model with finer-scale erosive phenomena to account for the transport of debris flowing from cliffs. This enables us to model the formation of terrain shapes not previously adequately modeled in Computer Graphics, ranging from U-shaped and hanging valleys to fjords and glacial lakes.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_885",
    "authors": "Panayiotis Charalambous, Julien Pettre, Vassilis Vassiliades, Yiorgos Chrysanthou, Nuria Pelechano",
    "title": "GREIL-Crowds: Crowd Simulation With Deep Reinforcement Learning and Examples",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592459",
    "pdf_link": null,
    "abstract": "Simulating crowds with realistic behaviors is a difficult but very important task for a variety of applications. Quantifying how a person balances between different conflicting criteria such as goal seeking, collision avoidance and moving within a group is not intuitive, especially if we consider that behaviors differ largely between people. Inspired by recent advances in Deep Reinforcement Learning, we propose Guided REinforcement Learning (GREIL) Crowds, a method that learns a model for pedestrian behaviors which is guided by reference crowd data. The model successfully captures behaviors such as goal seeking, being part of consistent groups without the need to define explicit relationships and wandering around seemingly without a specific purpose. Two fundamental concepts are important in achieving these results: (a) the per agent state representation and (b) the reward function. The agent state is a temporal representation of the situation around each agent. The reward function is based on the idea that people try to move in situations/states in which they feel comfortable in. Therefore, in order for agents to stay in a comfortable state space, we first obtain a distribution of states extracted from real crowd data; then we evaluate states based on how much of an outlier they are compared to such a distribution. We demonstrate that our system can capture and simulate many complex and subtle crowd interactions in varied scenarios. Additionally, the proposed method generalizes to unseen situations, generates consistent behaviors and does not suffer from the limitations of other data-driven and reinforcement learning approaches.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_546",
    "authors": "Steffen Hinderink, Marcel Campen",
    "title": "Galaxy Maps: Localized Foliations for Bijective Volumetric Mapping",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592410",
    "pdf_link": null,
    "abstract": "A method is presented to compute volumetric maps and parametrizations of objects over 3D domains. As a key feature, continuity and bijectivity are ensured by construction. Arbitrary objects of ball topology, represented as tetrahedral meshes, are supported. Arbitrary convex as well as star-shaped domains are supported. Full control over the boundary mapping is provided. The method is based on the technique of simplicial foliations, generalized to a broader class of domain shapes and applied adaptively in a novel localized manner. This increases flexibility as well as efficiency over the state of the art, while maintaining reliability in guaranteeing map bijectivity.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_167",
    "authors": "Stefan Jeschke, Chris Wojtan",
    "title": "Generalizing Shallow Water Simulations With Dispersive Surface Waves",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592098",
    "pdf_link": null,
    "abstract": "This paper introduces a novel method for simulating large bodies of water as a height field. At the start of each time step, we partition the waves into a bulk flow (which approximately satisfies the assumptions of the shallow water equations) and surface waves (which approximately satisfy the assumptions of Airy wave theory). We then solve the two wave regimes separately using appropriate state-of-the-art techniques, and re-combine the resulting wave velocities at the end of each step. This strategy leads to the first heightfield wave model capable of simulating complex interactions between both deep and shallow water effects, like the waves from a boat wake sloshing up onto a beach, or a dam break producing wave interference patterns and eddies. We also analyze the numerical dispersion created by our method and derive an exact correction factor for waves at a constant water depth, giving us a numerically perfect re-creation of theoretical water wave dispersion patterns.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_133",
    "authors": "Changyang Li, Lap-Fai Yu",
    "title": "Generating Activity Snippets by Learning Human-scene Interactions",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592096",
    "pdf_link": null,
    "abstract": "We present an approach to generate virtual activity snippets, which comprise sequenced keyframes of multi-character, multi-object interaction scenarios in 3D environments, by learning from recordings of human-scene interactions. The generation consists of two stages. First, we use a sequential deep graph generative model with a temporal module to iteratively generate keyframe descriptions, which represent abstract interactions using graphs, while preserving spatial-temporal relations through the activities. Second, we devise an optimization framework to instantiate the activity snippets in virtual 3D environments guided by the generated keyframe descriptions. Our approach optimizes the poses of character and object instances encoded by the graph nodes to satisfy the relations and constraints encoded by the graph edges. The instantiation process includes a coarse 2D optimization followed by a fine 3D optimization to effectively explore the complex solution space for placing and posing the instances. Through experiments and a perceptual study, we applied our approach to generate plausible activity snippets under different settings.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_462",
    "authors": "Yiwei Hu, Paul Guerrero, Milos Hasan, Holly Rushmeier, Valentin Deschaintre",
    "title": "Generating Procedural Materials From Text or Image Prompts",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591520",
    "pdf_link": null,
    "abstract": "Node graph systems are used ubiquitously for material design in computer graphics. They allow the use of visual programming to achieve desired effects without writing code. As high-level design tools they provide convenience and flexibility, but mastering the creation of node graphs usually requires professional training. We propose an algorithm capable of generating multiple node graphs from different types of prompts, significantly lowering the bar for users to explore a specific design space. Previous work [Guerrero et al. 2022] was limited to unconditional generation of random node graphs, making the generation of an envisioned material challenging. We propose a multi-modal node graph generation neural architecture for high-quality procedural material synthesis which can be conditioned on different inputs (text or image prompts), using a CLIP-based encoder. We also create a substantially augmented material graph dataset, key to improving the generation quality. Finally, we generate high-quality graph samples using a regularized sampling process and improve the matching quality by differentiable optimization for top-ranked samples. We compare our methods to CLIP-based database search baselines (which are themselves novel) and achieve superior or similar performance without requiring massive data storage. We further show that our model can produce a set of material graphs unconditionally, conditioned on images, text prompts or partial graphs, serving as a tool for automatic visual programming completion.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_731",
    "authors": "Amir Barda, guy tevet, adriana Schulz, Amit Haim Bermano",
    "title": "Generative Design of Sheet Metal Structures",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592444",
    "pdf_link": null,
    "abstract": "Sheet Metal (SM) fabrication is perhaps one of the most common metalworking technique. Despite its prevalence, SM design is manual and costly, with rigorous practices that restrict the search space, yielding suboptimal results. In contrast, we present a framework for the first automatic design of SM parts. Focusing on load bearing applications, our novel system generates a high-performing manufacturable SM that adheres to the numerous constraints that SM design entails: The resulting part minimizes manufacturing costs while adhering to structural, spatial, and manufacturing constraints. In other words, the part should be strong enough, not disturb the environment, and adhere to the manufacturing process. These desiderata sum up to an elaborate, sparse, and expensive search space. Our generative approach is a carefully designed exploration process, comprising two steps. In Segment Discovery connections from the input load to attachable regions are accumulated, and during Segment Composition the most performing valid combination is searched for. For Discovery, we define a slim grammar, and sample it for parts using a Markov-Chain Monte Carlo (MCMC) approach, ran in intercommunicating instances (i.e, chains) for diversity. This, followed by a short continuous optimization, enables building a diverse and high-quality library of substructures. During Composition, a valid and minimal cost combination of the curated substructures is selected. To improve compliance significantly without additional manufacturing costs, we reinforce candidate parts onto themselves --- a unique SM capability called self-riveting. we provide our code and data in https://github.com/amir90/AutoSheetMetal. We show our generative approach produces viable parts for numerous scenarios. We compare our system against a human expert and observe improvements in both part quality and design time. We further analyze our pipeline's steps with respect to resulting quality, and have fabricated some results for validation. We hope our system will stretch the field of SM design, replacing costly expert hours with minutes of standard CPU, making this cheap and reliable manufacturing method accessible to anyone.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_103",
    "authors": "Roman Poya, Rogelio Ortigosa, Theodore Kim, Theodore Kim",
    "title": "Geometric Optimisation via Spectral Shifting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3585003",
    "pdf_link": null,
    "abstract": "We present a geometric optimisation framework that can recover fold-over free maps from non-injective initial states using popular flip-preventing distortion energies. Since flip-preventing energies are infinite for folded configurations, we propose a new regularisation scheme that shifts the singular values of the deformation gradient. This allow us to re-use many existing algorithms, especially locally injective methods for initially folded maps. Our regularisation is suitable for both singular value- and invariant-based formulations, and systematically contributes multiple stabilisers to the Hessian. In contrast to proxy-based techniques, we maintain second-order convergence. Compact expressions for the energy eigensystems can be obtained for our extended stretch invariants, enabling the use of fast projected Newton solvers. Although spectral shifting in general has no theoretical guarantees that the global minimum is an injection, extensive experiments show that our framework is fast and extremely robust in practice, and capable of generating high-quality maps from severely distorted, degenerate and folded initialisations.",
    "scholar_publication": "ACM Transactions on Graphics, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_158",
    "authors": "Tenglong Ao, Zeyi Zhang, Libin Liu",
    "title": "GestureDiffuCLIP: Gesture Diffusion Model With CLIP Latents",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592097",
    "pdf_link": null,
    "abstract": "The automatic generation of stylized co-speech gestures has recently received increasing attention. Previous systems typically allow style control via predefined text labels or example motion clips, which are often not flexible enough to convey user intent accurately. In this work, we present GestureDiffuCLIP, a neural network framework for synthesizing realistic, stylized co-speech gestures with flexible style control. We leverage the power of the large-scale Contrastive-Language-Image-Pre-training (CLIP) model and present a novel CLIP-guided mechanism that extracts efficient style representations from multiple input modalities, such as a piece of text, an example motion clip, or a video. Our system learns a latent diffusion model to generate high-quality gestures and infuses the CLIP representations of style into the generator via an adaptive instance normalization (AdaIN) layer. We further devise a gesture-transcript alignment mechanism that ensures a semantically correct gesture generation based on contrastive learning. Our system can also be extended to allow fine-grained style control of individual body parts. We demonstrate an extensive set of examples showing the flexibility and generalizability of our model to a variety of style descriptions. In a user study, we show that our system outperforms the state-of-the-art approaches regarding human likeness, appropriateness, and style correctness.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_322",
    "authors": "Rui Xu, Zhiyang Dou, Ningna Wang, Shiqing Xin, Shuangmin Chen, Mingyan Jiang, Xiaohu Guo, Wenping Wang, Changhe Tu",
    "title": "Globally Consistent Normal Orientation for Point Clouds by Regularizing the Winding-number Field",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592129",
    "pdf_link": null,
    "abstract": "Estimating normals with globally consistent orientations for a raw point cloud has many downstream geometry processing applications. Despite tremendous efforts in the past decades, it remains challenging to deal with an unoriented point cloud with various imperfections, particularly in the presence of data sparsity coupled with nearby gaps or thin-walled structures. In this paper, we propose a smooth objective function to characterize the requirements of an acceptable winding-number field, which allows one to find the globally consistent normal orientations starting from a set of completely random normals. By taking the vertices of the Voronoi diagram of the point cloud as examination points, we consider the following three requirements: (1) the winding number is either 0 or 1, (2) the occurrences of 1 and the occurrences of 0 are balanced around the point cloud, and (3) the normals align with the outside Voronoi poles as much as possible. Extensive experimental results show that our method outperforms the existing approaches, especially in handling sparse and noisy point clouds, as well as shapes with complex geometry/topology.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_717",
    "authors": "Jorge Condor, Michal Piovarci, Bernd Bickel, Piotr Didyk",
    "title": "Gloss-aware Color Correction for 3D Printing",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591546",
    "pdf_link": null,
    "abstract": "Color and gloss are fundamental aspects of surface appearance. State-of-the-art fabrication techniques can manipulate both properties of the printed 3D objects. However, in the context of appearance reproduction, perceptual aspects of color and gloss are usually handled separately, even though previous perceptual studies suggest their interaction. Our work is motivated by previous studies demonstrating a perceived color shift due to a change in the object’s gloss, i.e., two samples with the same color but different surface gloss appear as they have different colors. In this paper, we conduct new experiments which support this observation and provide insights into the magnitude and direction of the perceived color change. We use the observations as guidance to design a new method that estimates and corrects the color shift enabling the fabrication of objects with the same perceived color but different surface gloss. We formulate the problem as an optimization procedure solved using differentiable rendering. We evaluate the effectiveness of our method in perceptual experiments with 3D objects fabricated using a multi-material 3D printer and demonstrate potential applications.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_815",
    "authors": "Shuangbing Song, Fan Zhong, Tianju Wang, Xueying Qin, Changhe Tu",
    "title": "Guided Linear Upsampling",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592453",
    "pdf_link": null,
    "abstract": "… Guided upsampling is an effective approach for accelerating high-resolution image … guided upsampling method. Each pixel in the high-resolution image is represented as a linear …",
    "scholar_publication": "ACM Transactions on Graphics …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_119",
    "authors": "Longwen Zhang, Zijun Zhao, Xinzhou Cong, Qixuan Zhang, Shuqi Gu, Yuchong Gao, Rui Zheng, Wei Yang, Lan Xu, Jingyi Yu",
    "title": "HACK: Learning a Parametric Head and Neck Model for High-fidelity Animation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592093",
    "pdf_link": null,
    "abstract": "Significant advancements have been made in developing parametric models for digital humans, with various approaches concentrating on parts such as the human body, hand, or face. Nevertheless, connectors such as the neck have been overlooked in these models, with rich anatomical priors often unutilized. In this paper, we introduce HACK (Head-And-neCK), a novel parametric model for constructing the head and cervical region of digital humans. Our model seeks to disentangle the full spectrum of neck and larynx motions, facial expressions, and appearance variations, providing personalized and anatomically consistent controls, particularly for the neck regions. To build our HACK model, we acquire a comprehensive multi-modal dataset of the head and neck under various facial expressions. We employ a 3D ultrasound imaging scheme to extract the inner biomechanical structures, namely the precise 3D rotation information of the seven vertebrae of the cervical spine. We then adopt a multi-view photometric approach to capture the geometry and physically-based textures of diverse subjects, who exhibit a diverse range of static expressions as well as sequential head-and-neck movements. Using the multi-modal dataset, we train the parametric HACK model by separating the 3D head and neck depiction into various shape, pose, expression, and larynx blendshapes from the neutral expression and the rest skeletal pose. We adopt an anatomically-consistent skeletal design for the cervical region, and the expression is linked to facial action units for artist-friendly controls. We also propose to optimize the mapping from the identical shape space to the PCA spaces of personalized blendshapes to augment the pose and expression blendshapes, providing personalized properties within the framework of the generic model. Furthermore, we use larynx blendshapes to accurately control the larynx deformation and force the larynx slicing motions along the vertical direction in the UV-space for precise modeling of the larynx beneath the neck skin. HACK addresses the head and neck as a unified entity, offering more accurate and expressive controls, with a new level of realism, particularly for the neck regions. This approach has significant benefits for numerous applications, including geometric fitting and animation, and enables inter-correlation analysis between head and neck for fine-grained motion synthesis and transfer.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_861",
    "authors": "Rahul Mitra, Liane Makatura, Emily Whiting, Edward Chien",
    "title": "Helix-free Stripes for Knit Graph Design",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591564",
    "pdf_link": null,
    "abstract": "The problem of placing evenly-spaced stripes on a triangular mesh mirrors that of having evenly-spaced course rows and wale columns in a knit graph for a given geometry. This work presents strategies for producing helix-free stripe patterns and traces them to produce helix-free knit graphs suitable for machine knitting. We optimize directly for the discrete differential (1-form) of the stripe texture function, i.e., the spinning form, and demonstrate the knitting-specific advantages of this framework. In particular, we note how simple linear constraints allow us to place stitch irregularities, align course rows and wale columns to boundary/feature curves, and eliminate helical stripes. Two mixed-integer optimization strategies using these constraints are presented and applied to several mesh models. The results are smooth, globally-informed, helix-free stripe patterns that we trace to produce machine-knittable graphs. We further provide an explicit characterization of helical stripes and a theoretical analysis of their elimination constraints.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_118",
    "authors": "Ji Gan, Weiqiang Wang, Jiaxu Leng, Xinbo Gao, Weiqiang Wang",
    "title": "HiGAN+: Handwriting Imitation GAN With Disentangled Representations",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3550070",
    "pdf_link": null,
    "abstract": "Humans remain far better than machines at learning, where humans require fewer examples to learn new concepts and can use those concepts in richer ways. Take handwriting as an example, after learning from very limited handwriting scripts, a person can easily imagine what the handwritten texts would like with other arbitrary textual contents (even for unseen words or texts). Moreover, humans can also hallucinate to imitate calligraphic styles from just a single reference handwriting sample (that even have never seen before). Humans can do such hallucinations, perhaps because they can learn to disentangle the textual contents and calligraphic styles from handwriting images. Inspired by this, we propose a novel handwriting imitation generative adversarial network (HiGAN+) for realistic handwritten text synthesis based on disentangled representations. The proposed HiGAN+ can achieve a precise one-shot handwriting style transfer by introducing the writer-specific auxiliary loss and contextual loss, and it also attains a good global & local consistency by refining local details of synthetic handwriting images. Extensive experiments, including human evaluations, on the benchmark dataset validate our superiority in terms of visual quality, scalability, compactness, and style transferability compared with the state-of-the-art GANs for handwritten text synthesis.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2022 - dl.acm.org"
  },
  {
    "paper_id": "papers_231",
    "authors": "Zachary Ferguson, Pranav Jain, Denis Zorin, Teseo Schneider, Daniele Panozzo",
    "title": "High-order Incremental Potential Contact for Elastodynamic Simulation on Curved Meshes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591488",
    "pdf_link": null,
    "abstract": "High-order bases provide major advantages over linear ones in terms of efficiency, as they provide (for the same physical model) higher accuracy for the same running time, and reliability, as they are less affected by locking artifacts and mesh quality. Thus, we introduce a high-order finite element (FE) formulation (high-order bases) for elastodynamic simulation on high-order (curved) meshes with contact handling based on the recently proposed Incremental Potential Contact (IPC) model. Our approach is based on the observation that each IPC optimization step used to minimize the elasticity, contact, and friction potentials leads to linear trajectories even in the presence of nonlinear meshes or nonlinear FE bases. It is thus possible to retain the strong non-penetration guarantees and large time steps of the original formulation while benefiting from the high-order bases and high-order geometry. We accomplish this by mapping displacements and resulting contact forces between a linear collision proxy and the underlying high-order representation. We demonstrate the effectiveness of our approach in a selection of problems from graphics, computational fabrication, and scientific computing.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_109",
    "authors": "Alex Adkins, Aline Normoyle, Lorraine Lin, Yu Sun, Yuting Ye, Massimiliano Di Luca, Sophie Jörg, Sophie Jörg",
    "title": "How Important Are Detailed Hand Motions for Communication for a Virtual Character Through the Lens of Charades?",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3578575",
    "pdf_link": null,
    "abstract": "Detailed hand motions play an important role in face-to-face communication to emphasize points, describe objects, clarify concepts, or replace words altogether. While shared virtual reality (VR) spaces are becoming more popular, these spaces do not, in most cases, capture and display accurate hand motions. In this article, we investigate the consequences of such errors in hand and finger motions on comprehension, character perception, social presence, and user comfort. We conduct three perceptual experiments where participants guess words and movie titles based on motion captured movements. We introduce errors and alterations to the hand movements and apply techniques to synthesize or correct hand motions. We collect data from more than 1000 Amazon Mechanical Turk participants in two large experiments, and conduct a third experiment in VR. As results might differ depending on the virtual character used, we investigate all effects on two virtual characters of different levels of realism. We furthermore investigate the effects of clip length in our experiments. Amongst other results, we show that the absence of finger motion significantly reduces comprehension and negatively affects people’s perception of a virtual character and their social presence. Adding some hand motions, even random ones, does attenuate some of these effects when it comes to the perception of the virtual character or social presence, but it does not necessarily improve comprehension. Slightly inaccurate or erroneous hand motions are sufficient to achieve the same level of comprehension as with accurate hand motions. They might however still affect the viewers’ impression of a character. Finally, jittering hand motions should be avoided as they significantly decrease user comfort.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_560",
    "authors": "Mustafa Işık, Martin Rünz, Markos Georgopoulos, Taras Khakhulin, Jonathan Starck, Lourdes Agapito, Matthias Nießner",
    "title": "HumanRF: High-fidelity Neural Radiance Fields for Humans in Motion",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592415",
    "pdf_link": null,
    "abstract": "Representing human performance at high-fidelity is an essential building block in diverse applications, such as film production, computer games or videoconferencing. To close the gap to production-level quality, we introduce HumanRF1, a 4D dynamic neural scene representation that captures full-body appearance in motion from multi-view video input, and enables playback from novel, unseen viewpoints. Our novel representation acts as a dynamic video encoding that captures fine details at high compression rates by factorizing space-time into a temporal matrix-vector decomposition. This allows us to obtain temporally coherent reconstructions of human actors for long sequences, while representing high-resolution details even in the context of challenging motion. While most research focuses on synthesizing at resolutions of 4MP or lower, we address the challenge of operating at 12MP. To this end, we introduce ActorsHQ, a novel multi-view dataset that provides 12MP footage from 160 cameras for 16 sequences with high-fidelity, per-frame mesh reconstructions2. We demonstrate challenges that emerge from using such high-resolution data and show that our newly introduced HumanRF effectively leverages this data, making a significant step towards production-level quality novel view synthesis.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_317",
    "authors": "Zheng-Jun Du, Liang-Fu Kang, Jianchao Tan, Yotam Gingold, Kun Xu",
    "title": "Image Vectorization and Editing via Linear Gradient Layer Decomposition",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592128",
    "pdf_link": null,
    "abstract": "A key advantage of vector graphics over raster graphics is their editability. For example, linear gradients define a spatially varying color fill with a few intuitive parameters, which are ubiquitously supported in standard vector graphics formats and libraries. By layering regions filled with linear gradients, complex appearances can be created. We propose an automatic method to convert a raster image into layered regions of linear gradients. Given an input raster image segmented into regions, our approach decomposes the resulting regions into opaque and semi-transparent linear gradient fills. Our approach is fully automatic (e.g., users do not identify a background as in previous approaches) and exhaustively considers all possible decompositions that satisfy perceptual cues. Experiments on a variety of images demonstrate that our method is robust and effective.",
    "scholar_publication": "ACM Transactions on Graphics …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_605",
    "authors": "Kangrui Xue, Doug L. James, Jui-Hsien Wang, Ryan Aronson, Timothy Langlois",
    "title": "Improved Water Sound Synthesis Using Coupled Bubbles",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592424",
    "pdf_link": null,
    "abstract": "We introduce a practical framework for synthesizing bubble-based water sounds that captures the rich inter-bubble coupling effects responsible for low-frequency acoustic emissions from bubble clouds. We propose coupled-bubble oscillator models with regularized singularities, and techniques to reduce the computational cost of time stepping with dense, time-varying mass matrices. Airborne acoustic emissions are estimated using finite-difference time-domain (FDTD) methods. We propose a simple, analytical surface-acceleration model, and a sample-and-hold GPU wavesolver that is simple and faster than prior CPU wavesolvers. Sound synthesis results are demonstrated using bubbly flows from incompressible, two-phase simulations, as well as procedurally generated examples using single-phase FLIP fluid animations. Our results demonstrate sound simulations with hundreds of thousands of bubbles, and perceptually significant frequency transformations with fuller low-frequency content.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_418",
    "authors": "Haiwei Zhang, Jiqing Zhang, Bo Dong, Pieter Peers, Wenwei Wu, Xiaopeng Wei, Felix Heide, Xin Yang",
    "title": "In the Blink of an Eye: Event-based Emotion Recognition",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591511",
    "pdf_link": null,
    "abstract": "We introduce a wearable single-eye emotion recognition device and a real-time approach to recognizing emotions from partial observations of an emotion that is robust to changes in lighting conditions. At the heart of our method is a bio-inspired event-based camera setup and a newly designed lightweight Spiking Eye Emotion Network (SEEN). Compared to conventional cameras, event-based cameras offer a higher dynamic range (up to 140 dB vs. 80 dB) and a higher temporal resolution (in the order of μ s vs. 10s of ms). Thus, the …",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_640",
    "authors": "Zachary Ferguson, Teseo Schneider, Danny Kaufman, Daniele Panozzo",
    "title": "In-Timestep Remeshing for Contacting Elastodynamics",
    "paper_url": "https://par.nsf.gov/biblio/10463901",
    "pdf_link": null,
    "abstract": "We propose In-Timestep Remeshing, a fully coupled, adaptive meshing algorithm for contacting elastodynamics where remeshing steps are tightly integrated, implicitly, within the timestep solve. Our algorithm refines and coarsens the domain automatically by measuring physical energy changes within each ongoing timestep solve. This provides consistent, degree-of-freedom-efficient, productive remeshing that, by construction, is physics-aware and so avoids the errors, over-refinements, artifacts, per-example hand-tuning, and instabilities commonly encountered when remeshing with timestepping methods. Our in-timestep computation then ensures that each simulation step's output is both a converged stable solution on the updated mesh and a temporally consistent trajectory with respect to the model and solution of the last timestep. At the same time, the output is guaranteed safe (intersection- and inversion-free) across all operations. We demonstrate applications across a wide range of extreme stress tests with challenging contacts, sharp geometries, extreme compressions, large timesteps, and wide material stiffness ranges - all scenarios well-appreciated to challenge existing remeshing methods.",
    "scholar_publication": "ACM Transactions on …, 2023 - par.nsf.gov"
  },
  {
    "paper_id": "papers_534",
    "authors": "Koya Narumi, Kazuki Koyama, Kai Suto, Yuta Noma, Hiroki Sato, Tomohiro Tachi, Masaaki Sugimoto, Takeo Igarashi, Yoshihiro Kawahara",
    "title": "Inkjet 4D Print: Self-folding Tessellated Origami Objects by Inkjet UV Printing",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592409",
    "pdf_link": null,
    "abstract": "We propose Inkjet 4D Print, a self-folding fabrication method of 3D origami tessellations by printing 2D patterns on both sides of a heat-shrinkable base sheet, using a commercialized inkjet ultraviolet (UV) printer. Compared to the previous folding-based 4D printing approach using fused deposition modeling (FDM) 3D printers [An et al. 2018], our method has merits in (1) more than 1200 times higher resolution in terms of the number of self-foldable facets, (2) 2.8 times faster printing speed, and (3) optional full-color decoration. This paper describes the material selection, the folding mechanism, the heating condition, and the printing patterns to self-fold both known and freeform tessellations. We also evaluated the self-folding resolution, the printing and transformation speed, and the shape accuracy of our method. Finally, we demonstrated applications enabled by our self-foldable tessellated objects.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_750",
    "authors": "Gilles Daviet",
    "title": "Interactive Hair Simulation on the GPU Using ADMM",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591551",
    "pdf_link": null,
    "abstract": "We devise a local–global solver dedicated to the simulation of Discrete Elastic Rods (DER) with Coulomb friction that can fully leverage the massively parallel compute capabilities of moderns GPUs. We verify that our simulator can reproduce analytical results on recently published cantilever, bend–twist, and stick–slip experiments, while drastically decreasing iteration times for high-resolution hair simulations. Being able to handle contacting assemblies of several thousand elastic rods in real-time, our fast solver paves the ways for new workflows such as interactive physics-based editing of digital grooms.",
    "scholar_publication": "ACM SIGGRAPH 2023 Conference Proceedings, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_762",
    "authors": "Saeed Hadadan, Geng Lin, Jan Novák, Fabrice Rousselle, Matthias Zwicker",
    "title": "Inverse Global Illumination Using a Neural Radiometric Prior",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591553",
    "pdf_link": null,
    "abstract": "Inverse rendering methods that account for global illumination are becoming more popular, but current methods require evaluating and automatically differentiating millions of path integrals by tracing multiple light bounces, which remains expensive and prone to noise. Instead, this paper proposes a radiometric prior as a simple alternative to building complete path integrals in a traditional differentiable path tracer, while still correctly accounting for global illumination. Inspired by the Neural Radiosity technique, we use a neural network as a radiance function, and we introduce a prior consisting of the norm of the residual of the rendering equation in the inverse rendering loss. We train our radiance network and optimize scene parameters simultaneously using a loss consisting of both a photometric term between renderings and the multi-view input images, and our radiometric prior (the residual term). This residual term enforces a physical constraint on the optimization that ensures that the radiance field accounts for global illumination. We compare our method to a vanilla differentiable path tracer, and more advanced techniques such as Path Replay Backpropagation. Despite the simplicity of our approach, we can recover scene parameters with comparable and in some cases better quality, at considerably lower computation times.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_632",
    "authors": "Eric Heitz, Laurent Belcour, Thomas Chambon",
    "title": "Iterative alpha-(de)Blending: A Minimalist Deterministic Diffusion Model",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591540",
    "pdf_link": null,
    "abstract": "We derive a minimalist but powerful deterministic denoising-diffusion model. While denoising diffusion has shown great success in many domains, its underlying theory remains largely inaccessible to non-expert users. Indeed, an understanding of graduate-level concepts such as Langevin dynamics or score matching appears to be required to grasp how it works. We propose an alternative approach that requires no more than undergrad calculus and probability. We consider two densities and observe what happens when random samples from these densities are blended (linearly interpolated). We show that iteratively blending and deblending samples produces random paths between the two densities that converge toward a deterministic mapping. This mapping can be evaluated with a neural network trained to deblend samples. We obtain a model that behaves like deterministic denoising diffusion: it iteratively maps samples from one density (e.g., Gaussian noise) to another (e.g., cat images). However, compared to the state-of-the-art alternative, our model is simpler to derive, simpler to implement, more numerically stable, achieves higher quality results in our experiments, and has interesting connections to computer graphics.",
    "scholar_publication": "ACM SIGGRAPH 2023 Conference …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_698",
    "authors": "Karran Pandey, Fanny Chevalier, Karan Singh",
    "title": "Juxtaform: Interactive Visual Summarization for Exploratory Shape Design",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592436",
    "pdf_link": null,
    "abstract": "We present juxtaform, a novel approach to the interactive summarization of large shape collections for conceptual shape design. We conduct a formative study to ascertain design goals for creative shape exploration tools. Motivated by a mathematical formulation of these design goals, juxtaform integrates the exploration, analysis, selection, and refinement of large shape collections to support an interactive divergence-convergence shape design workflow. We exploit sparse, segmented sketch-stroke visual abstractions of shape and a novel visual summarization algorithm to balance the needs of shape understanding, in-situ shape juxtaposition, and visual clutter. Our evaluation is three-fold: we show that existing shape and stroke clustering algorithms do not address our design goals compared to our proposed shape corpus summarization algorithm; we compare juxtaform against a structured image gallery interface for various shape design and analysis tasks; and we present multiple compelling 2D/3D applications using juxtaform.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_302",
    "authors": "Karlis Martins Briedis, Abdelaziz Djelouah, Raphaël Ortiz, Mark Meyer, Markus Gross, Christopher Schroers",
    "title": "Kernel-based Frame Interpolation for Spatio-temporally Adaptive Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591497",
    "pdf_link": null,
    "abstract": "Recently, there has been exciting progress in frame interpolation for rendered content. In this offline rendering setting, additional inputs, such as albedo and depth, can be extracted from a scene at a very low cost and, when integrated in a suitable fashion, can significantly improve the quality of the interpolated frames. Although existing approaches have been able to show good results, most high-quality interpolation methods use a synthesis network for direct color prediction. In complex scenarios, this can result in unpredictable behavior and …",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_367",
    "authors": "Yoad Tewel, Rinon Gal, Gal Chechik, Yuval Atzmon",
    "title": "Key-locked Rank One Editing for Text-to-image Personalization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591506",
    "pdf_link": null,
    "abstract": "Text-to-image models (T2I) offer a new level of flexibility by allowing users to guide the creative process through natural language. However, personalizing these models to align with user-provided visual concepts remains a challenging problem. The task of T2I personalization poses multiple hard challenges, such as maintaining high visual fidelity while allowing creative control, combining multiple personalized concepts in a single image, and keeping a small model size. We present Perfusion, a T2I personalization method that addresses these challenges using dynamic rank-1 updates to the underlying T2I model. Perfusion avoids overfitting by introducing a new mechanism that “locks” new concepts’ cross-attention Keys to their superordinate category. Additionally, we develop a gated rank-1 approach that enables us to control the influence of a learned concept during inference time and to combine multiple concepts. This allows runtime efficient balancing of visual-fidelity and textual-alignment with a single 100KB trained model. Importantly, it can span different operating points across the Pareto front without additional training. We compare our approach to strong baselines and demonstrate its qualitative and quantitative strengths.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_226",
    "authors": "Zhenwei Wang, Nanxuan Zhao, Gerhard Hancke, Rynson W.H. Lau",
    "title": "Language-based Photo Color Adjustment for Graphic Designs",
    "paper_url": "https://3dvar.com/Wang2023Language.pdf",
    "pdf_link": null,
    "abstract": "Graphic designs (eg, posters, webpages, slides and advertisements) have become a prevailing communication tool nowadays. As photos play an essential role in graphic designs, they are often recolored to associate and harmonize with other elements (eg, text, background and shape) in the graphic designs in real applications [GraphicsZoo 2020; Huang et al. 2018; Jordá-Albiñana et al. 2009; Mcguire 2019]. This helps the graphic design deliver the message effectively, engage viewers and evoke emotions. Prior works have been conducted to help automate this task [Cohen-Or et al. 2006; Kim and Suk 2018; Nguyen et al. 2017; Zhao et al. 2021]. However, when used by novices, existing methods and commercial software (eg, Photoshop, Affinity Photo, and GIMP) often face a dilemma between the ease of use and level of expressiveness. Commercial software allows users to indicate arbitrary color modifications, but requires them to have design knowledge and rich experience. The work of Cohen-Or et al.[2006] allows users to adjust the image color to match with the color of the context elements through different harmonic schemes, but it requires users to understand these schemes and it is often difficult for them to control the color change within the specified local regions. The framework from Nguyen et al.[2017] allows users to recolor a group of images based on the theme color of a design (eg, brochure), but the color can be changed in undesirable ways (eg, skin tone turns green) without considering the semantic contents of the images. Methods such as Kim and Suk [2018] and Zhao et al.[2021] allow users to recolor a photo with source colors from the design by a single click, but these methods can only generate deterministic results that may not be the desired outcome.In this work, we aim to seek an interactive approach that is user friendly and has a broad range of expressiveness. We follow the basic setting of existing works [Kim and Suk 2018; Zhao et al. 2021] by recoloring the target local region (s) in the photo with a source color extracted from the design. However, we take advantage of the recent success of language-based interactions used in various computer vision tasks [Bahng et al. 2018; Chen et al. 2018; Jiang et al. 2021; Lüddecke and Ecker 2022; Ma et al. 2018; Weng et al. 2022; Zou et al. 2019], and present a language-based system for photo color adjustment in the context of graphic designs (Fig. 1). This system interaction comes natural to people and is intuitive to use. It can also be combined with voice input and is effective to the literacy education for children [Zou et al. 2019]. The system allows users to simply state their goals via concise verbal terms, without the need to learn a new interface or hunt through menus [Laput et al. 2013; Woolfe et al. 2007; Zou et al. 2019]. Although there are some works on language-based image editing and colorization, designing such a system specifically for our task has several unique challenges. 1) Color Accuracy: Rather than recoloring using an arbitrary color, the source color should be exactly the same as the one obtained from the design itself, as specified by the user. Existing works [Li et al. 2020a; Liu et al. 2020; Zou et al. 2019] that rely on specifying a vague color attribute (eg,“blue”) with a wide range of possible mapped values may not be accurate enough. How to interpret the instruction to extract the source colors",
    "scholar_publication": "ACM Trans. Graph., 2023 - 3dvar.com"
  },
  {
    "paper_id": "paperstog_101",
    "authors": "Hugo Schott, Axel Paris, Lucie Fournier, Éric Guérin, Éric Galin, Hugo Schott",
    "title": "Large-scale Terrain",
    "paper_url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20309",
    "pdf_link": null,
    "abstract": "… approach for representing large-scale terrain using GPs. Specifically, this … large-scale terrain data, taking into account the local smoothness while preserving spatial features in the terrain…",
    "scholar_publication": "Journal of Field …, 2009 - Wiley Online Library"
  },
  {
    "paper_id": "papers_711",
    "authors": "Yuelang Xu, Hongwen Zhang, Lizhen Wang, Xiaochen Zhao, Han Huang, Guojun Qi, Yebin Liu",
    "title": "LatentAvatar: Learning Latent Expression Code for Expressive Neural Head Avatar",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591545",
    "pdf_link": null,
    "abstract": "Existing approaches to animatable NeRF-based head avatars are either built upon face templates or use the expression coefficients of templates as the driving signal. Despite the promising progress, their performances are heavily bound by the expression power and the tracking accuracy of the templates. In this work, we present LatentAvatar, an expressive neural head avatar driven by latent expression codes. Such latent expression codes are learned in an end-to-end and self-supervised manner without templates, enabling our method to get rid of expression and tracking issues. To achieve this, we leverage a latent head NeRF to learn the person-specific latent expression codes from a monocular portrait video, and further design a Y-shaped network to learn the shared latent expression codes of different subjects for cross-identity reenactment. By optimizing the photometric reconstruction objectives in NeRF, the latent expression codes are learned to be 3D-aware while faithfully capturing the high-frequency detailed expressions. Moreover, by learning a mapping between the latent expression code learned in shared and person-specific settings, LatentAvatar is able to perform expressive reenactment between different subjects. Experimental results show that our LatentAvatar is able to capture challenging expressions and the subtle movement of teeth and even eyeballs, which outperforms previous state-of-the-art solutions in both quantitative and qualitative comparisons. Project page: https://www.liuyebin.com/latentavatar.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_533",
    "authors": "Haotian Zhang, Ye Yuan, Viktor Makoviychuk, Yunrong Guo, Sanja Fidler, Xue Bin Peng, Kayvon Fatahalian",
    "title": "Learning Physically Simulated Tennis Skills From Broadcast Videos",
    "paper_url": "https://openreview.net/pdf?id=zLazLiPyoJ",
    "pdf_link": null,
    "abstract": "Developing controllers for physics-based character simulation and control is one of the core challenges of computer animation. In recent years, techniques that combine deep reinforcement learning (DRL) and motion imitation have produced simulated characters that exhibit impressive lifelike motions and perform a range of athletic skills. The vast majority of systems use motion capture (mocap) data as the source of kinematic motions to imitate. Unfortunately, it is costly to acquire large amounts of high quality mocap animation. In contrast, video of athletic events is widely available and provides a",
    "scholar_publication": "ACM Trans …, 2023 - openreview.net"
  },
  {
    "paper_id": "papers_879",
    "authors": "Simon Alexanderson, Rajmund Nagy, Jonas Beskow, Gustav Eje Henter",
    "title": "Listen, Denoise, Action! Audio-Driven Motion Synthesis with Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592458",
    "pdf_link": null,
    "abstract": "Diffusion models have experienced a surge of interest as highly expressive yet efficiently trainable probabilistic models. We show that these models are an excellent fit for synthesising human motion that co-occurs with audio, e.g., dancing and co-speech gesticulation, since motion is complex and highly ambiguous given audio, calling for a probabilistic description. Specifically, we adapt the DiffWave architecture to model 3D pose sequences, putting Conformers in place of dilated convolutions for improved modelling power. We also demonstrate control over motion style, using classifier-free guidance to adjust the strength of the stylistic expression. Experiments on gesture and dance generation confirm that the proposed method achieves top-of-the-line motion quality, with distinctive styles whose expression can be made more or less pronounced. We also synthesise path-driven locomotion using the same model architecture. Finally, we generalise the guidance procedure to obtain product-of-expert ensembles of diffusion models and demonstrate how these may be used for, e.g., style interpolation, a contribution we believe is of independent interest.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_334",
    "authors": "Honglin Chen, Changxi Zheng, Kevin Wampler",
    "title": "Local Deformation for Interactive Shape Editing",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591501",
    "pdf_link": null,
    "abstract": "We introduce a novel regularization for localizing an elastic-energy-driven deformation to only those regions being manipulated by the user. Our local deformation features a natural region of influence, which is automatically adaptive to the geometry of the shape, the size of the deformation and the elastic energy in use. We further propose a three-block ADMM-based optimization to efficiently minimize the energy and achieve interactive frame rates. Our approach avoids the artifacts of other alternative methods, is simple and easy to implement, does not require tedious control primitive setup and generalizes across different dimensions and elastic energies. We demonstrates the effectiveness and efficiency of our localized deformation tool through a variety of local editing scenarios, including 1D, 2D, 3D elasticity and cloth deformation.",
    "scholar_publication": "ACM SIGGRAPH 2023 Conference …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_198",
    "authors": "Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong, Yang Liu, Heung-Yeung Shum",
    "title": "Locally Attentional SDF Diffusion for Controllable 3D Shape Generation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592103",
    "pdf_link": null,
    "abstract": "Although the recent rapid evolution of 3D generative neural networks greatly improves 3D shape generation, it is still not convenient for ordinary users to create 3D shapes and control the local geometry of generated shapes. To address these challenges, we propose a diffusion-based 3D generation framework --- locally attentional SDF diffusion, to model plausible 3D shapes, via 2D sketch image input. Our method is built on a two-stage diffusion model. The first stage, named occupancy-diffusion, aims to generate a low-resolution occupancy field to approximate the shape shell. The second stage, named SDF-diffusion, synthesizes a high-resolution signed distance field within the occupied voxels determined by the first stage to extract fine geometry. Our model is empowered by a novel view-aware local attention mechanism for image-conditioned shape generation, which takes advantage of 2D image patch features to guide 3D voxel feature learning, greatly improving local controllability and model generalizability. Through extensive experiments in sketch-conditioned and category-conditioned 3D shape generation tasks, we validate and demonstrate the ability of our method to provide plausible and diverse 3D shapes, as well as its superior controllability and generalizability over existing work.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_871",
    "authors": "Heng Liu, David Bommes",
    "title": "Locally Meshable Frame Fields",
    "paper_url": "https://www.algohex.eu/publications/locally-meshable-frame-fields/locally-meshable-frame-fields-sg-2023-lowres.pdf",
    "pdf_link": null,
    "abstract": "… meshability of a frame eld are identi ed, and we discuss an algorithm to turn a given frame eld into a locally meshable, and with further processing into a (globally) meshable one. Sec. 4 …",
    "scholar_publication": "ACM Trans. Graph., 2023 - algohex.eu"
  },
  {
    "paper_id": "papers_623",
    "authors": "Christian Reiser, Rick Szeliski, Dor Verbin, Pratul Srinivasan, Ben Mildenhall, Andreas Geiger, Jon Barron, Peter Hedman",
    "title": "MERF: Memory-Efficient Radiance Fields for Real-time View Synthesis in Unbounded Scenes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592426",
    "pdf_link": null,
    "abstract": "Neural radiance fields enable state-of-the-art photorealistic view synthesis. However, existing radiance field representations are either too compute-intensive for real-time rendering or require too much memory to scale to large scenes. We present a Memory-Efficient Radiance Field (MERF) representation that achieves real-time rendering of large-scale scenes in a browser. MERF reduces the memory consumption of prior sparse volumetric radiance fields using a combination of a sparse feature grid and high-resolution 2D feature planes. To support large-scale unbounded scenes, we introduce a novel contraction function that maps scene coordinates into a bounded volume while still allowing for efficient ray-box intersection. We design a lossless procedure for baking the parameterization used during training into a model that achieves real-time rendering while still preserving the photorealistic view synthesis quality of a volumetric radiance field.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_121",
    "authors": "Rulin Chen, Pengyun Qiu, Peng Song, Bailin Deng, Ziqi Wang, Ying He",
    "title": "Masonry Shell Structures With Discrete Equivalence Classes",
    "paper_url": "https://orca.cardiff.ac.uk/id/eprint/160032/",
    "pdf_link": null,
    "abstract": "This paper proposes a method to model masonry shell structures where the shell elements fall into a set of discrete equivalence classes. Such shell structure can reduce the fabrication cost and simplify the physical construction due to reuse of a few template shell elements. Given a freeform surface, our goal is to generate a small set of template shell elements that can be reused to produce a seamless and buildable structure that closely resembles the surface. The major technical challenge in this process is balancing the desire for high reusability of template elements with the need for a seamless and buildable final structure. To address the challenge, we define three error metrics to measure the seamlessness and buildability of shell structures made from discrete equivalence classes and develop a hierarchical cluster-and-optimize approach to generate a small set of template elements that produce a structure closely approximating the surface with low error metrics. We demonstrate the feasibility of our approach on various freeform surfaces and geometric patterns, and validate buildability of our results with four physical prototypes. Code and data of this paper are at https://github.com/Linsanity81/TileableShell.",
    "scholar_publication": "ACM Transactions on …, 2023 - orca.cardiff.ac.uk"
  },
  {
    "paper_id": "papers_460",
    "authors": "Prafull Sharma, Julien Philip, Michaël Gharbi, Bill Freeman, Fredo Durand, Valentin Deschaintre",
    "title": "Materialistic: Selecting Similar Materials in Images",
    "paper_url": "https://par.nsf.gov/biblio/10505452",
    "pdf_link": null,
    "abstract": "Separating an image into meaningful underlying components is a crucial first step for both editing and understanding images. We present a method capable of selecting the regions of a photograph exhibiting the same material as an artist-chosen area. Our proposed approach is robust to shading, specular highlights, and cast shadows, enabling selection in real images. As we do not rely on semantic segmentation (different woods or metal should not be selected together), we formulate the problem as a similarity-based grouping problem based …",
    "scholar_publication": "ACM Transactions on …, 2023 - par.nsf.gov"
  },
  {
    "paper_id": "papers_307",
    "authors": "Yucheol Jung, Hyomin Kim, Gyeongha Hwang, Seung-Hwan Baek, Seungyong Lee",
    "title": "Mesh Density Adaptation for Template-based Shape Reconstruction",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591498",
    "pdf_link": null,
    "abstract": "In 3D shape reconstruction based on template mesh deformation, a regularization, such as smoothness energy, is employed to guide the reconstruction into a desirable direction. In this paper, we highlight an often overlooked property in the regularization: the vertex density in the mesh. Without careful control on the density, the reconstruction may suffer from under-sampling of vertices near shape details. We propose a novel mesh density adaptation method to resolve the under-sampling problem. Our mesh density adaptation energy …",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_387",
    "authors": "Lubna Abu Rmaileh, Alan Brunton",
    "title": "Meso-facets for Goniochromatic 3D Printing",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592137",
    "pdf_link": null,
    "abstract": "… 3D printers to produce goniochromatic effects on arbitrary surfaces by procedurally augmenting the input surface with meso-facets, … 3D print processing, and for goniochromatic quality. …",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_293",
    "authors": "Élie Michel, Tamy Boubekeur",
    "title": "MesoGen: Designing Procedural On-surface Stranded Mesostructures",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591496",
    "pdf_link": null,
    "abstract": "Three-dimensional mesostructures enrich coarse macrosurfaces with complex features, which are 3D geometry with arbitrary topology in essence, but are expected to be self-similar with no tiling artifacts, just like texture-based material models. This is a challenging task, as no existing modeling tool provides the right constraints in the design phase to ensure such properties while maintaining real-time editing capabilities. In this paper, we propose MesoGen, a novel tile-centric authoring approach for the design of procedural mesostructures featuring non-periodic self-similarity while being represented as a compact and GPU-friendly model. We ensure by construction the continuity of the mesostructure: the user designs a set of atomic tiles by drawing 2D cross-sections on the interfaces between tiles, and selecting pairs of cross-sections to be connected as strands, i.e., 3D sweep surfaces. In parallel, a tiling engine continuously fills the shell space of the macrosurface with the so-defined tile set while ensuring that only matching interfaces are in contact. Moreover, the engine suggests to the user the addition of new tiles whenever the problem happens to be over-constrained. As a result, our method allows for the rapid creation of complex, seamless procedural mesostructure and is particularly adapted for wicker-like ones, often impossible to achieve with scattering-based mesostructure synthesis methods.",
    "scholar_publication": "ACM SIGGRAPH 2023 Conference …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_864",
    "authors": "Mark van de Ruit, Elmar Eisemann",
    "title": "Metameric: Spectral Uplifting via Controllable Color Constraints",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591565",
    "pdf_link": null,
    "abstract": "Spectral rendering is a crucial solution for photorealistic rendering. However, most available texture assets are RGB-only, and access to spectral content is limited. Uplifting methods that recover full spectral representations from RGB inputs have therefore received much attention. Yet, most methods are deterministic, while, in reality, there is no one-to-one mapping. As a consequence, the appearance of uplifted textures is fully determined under all illuminants. Hereby, metamers, which are materials with differing spectral responses that appear identical under a specific illumination, are excluded. We propose a method which makes this uplifting process controllable. Hereby, a user can define texture appearance under various lighting conditions, leading to a greatly increased flexibility for content design. Our method determines the space of possible metameric manipulations and enables interactive adjustments, while maintaining a set of user-specified appearance constraints. To achieve this goal, we formulate the problem as a constrained optimization, building upon an interpolation scheme and data-based reflectance generation, which maintain plausibility. Besides its value for artistic control, our solution is lightweight and can be executed on the fly, which keeps its memory consumption low and makes it easy to integrate into existing frameworks.",
    "scholar_publication": "ACM SIGGRAPH 2023 Conference …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_718",
    "authors": "Andrea Maggiordomo, Henry Moreton, Marco Tarini, Marco Tarini",
    "title": "Micro-Mesh Construction",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592440",
    "pdf_link": null,
    "abstract": "Micro-meshes (μ-meshes) are a new structured graphics primitive supporting a large increase in geometric fidelity, without commensurate memory and run-time processing costs, …",
    "scholar_publication": "ACM Transactions on Graphics …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_160",
    "authors": "Eugene d'Eon, Benedikt Bitterli, Andrea Weidlich, Tizian Zeltner",
    "title": "Microfacet Theory for Non-uniform Heightfields",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591486",
    "pdf_link": null,
    "abstract": "We propose new methods for combining NDFs in microfacet theory, enabling a wider range of surface statistics. The new BSDFs that follow allow for independent adjustment of appearance at grazing angles, and can’t be represented by linear blends of single-NDF BSDFs. We derive importance sampling for a symmetric operator that blends NDFs uniformly, and introduce a new asymmetric operator that supports NDF variation with elevation. We also extend Smith’s model to support piecewise-constant NDF and material variations with elevation, and demonstrate accuracy via Monte Carlo simulations.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_702",
    "authors": "Martin Heistermann, Jethro Warnett, David Bommes",
    "title": "Min-Deviation-Flow in Bi-directed Graphs for T-Mesh Quantization",
    "paper_url": "https://www.algohex.eu/publications/bimdf-quantization/bimdf-quantization-lowres.pdf",
    "pdf_link": null,
    "abstract": "Several state-of-the-art field-based quadrangular re-meshing methods generate non-conforming T-mesh layouts as an intermediate step towards the final quad mesh. This approach was initially conceived as a way to split the expensive mixed-integer problem of generating an integer-grid map into two easier problems [Campen et al. 2015]: A seamless map can be computed for an input surface from a frame field via purely continuous optimization. Its motorcycle graph [Eppstein et al. 2008] yields a T-mesh–a patch layout that may contain T-junctions, ie, vertices that are corners of some but not all incident patches (cf. Figure 2). The final integer-grid map can then be obtained from this by solving a purely discrete optimization problem, commonly called T-mesh quantization. Later approaches forgo the seamless map generation and operate on the frame field directly to obtain a T-mesh [Pietroni et al. 2021]. In this work, we focus on the T-mesh quantization problem and frame it as an instance of a broader class of combinatorial problems, a generalized minimum-cost flow problem on bi-directed graphs.",
    "scholar_publication": "ACM Trans. Graph., 2023 - algohex.eu"
  },
  {
    "paper_id": "papers_729",
    "authors": "Cusuh Ham, James Hays, Jingwan Lu, Krishna Kumar Singh, Zhifei Zhang, Tobias Hinz",
    "title": "Modulating Pretrained Diffusion Models for Multimodal Image Synthesis",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591549",
    "pdf_link": null,
    "abstract": "We present multimodal conditioning modules (MCM) for enabling conditional image synthesis using pretrained diffusion models. Previous multimodal synthesis works rely on training networks from scratch or fine-tuning pretrained networks, both of which are computationally expensive for large, state-of-the-art diffusion models. Our method uses pretrained networks but does not require any updates to the diffusion network’s parameters. MCM is a small module trained to modulate the diffusion network’s predictions during sampling using 2D modalities (e.g., semantic segmentation maps, sketches) that were unseen during the original training of the diffusion model. We show that MCM enables user control over the spatial layout of the image and leads to increased control over the image generation process. Training MCM is cheap as it does not require gradients from the original diffusion net, consists of only ∼ 1% of the number of parameters of the base diffusion model, and is trained using only a limited number of training examples. We evaluate our method on unconditional and text-conditional models to demonstrate the improved control over the generated images and their alignment with respect to the conditioning inputs.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_616",
    "authors": "Simeng Qiu, Hadi Amata, Wolfgang Heidrich",
    "title": "MoiréTag: Angular Measurement and Tracking With a Passive Marker",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591538",
    "pdf_link": null,
    "abstract": "Stable, low-cost, and precise visual measurement of directional information has many applications in domains such as virtual and augmented reality, visual odometry, or industrial computer vision. Conventional approaches like checkerboard patterns require careful pre-calibration, and can therefore not be operated in snapshot mode. Other optical methods like autocollimators offer very high precision but require controlled environments and are hard to take outside the lab. Non-optical methods like IMUs are low cost and widely available, but suffer from high drift errors. To overcome these challenges, we propose a novel snapshot method for angular measurement and tracking with Moiré patterns that are generated by binary structures printed on both sides of a glass plate. The Moiré effect amplifies minute angular shifts and translates them into spatial phase shifts that can be readily measured with a camera, effectively implementing an optical Vernier scale. We further extend this principle from a simple phase shift to a chirp model, which allows for full 6D tracking as well as estimation of camera intrinsics like the field of view. Simulation and experimental results show that the proposed non-contact object tracking framework is computationally efficient and the average angular accuracy of 0.17° outperforms the state-of-the-arts.",
    "scholar_publication": "ACM SIGGRAPH 2023 Conference …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_572",
    "authors": "Oliver Gross, Yousuf Soliman, Marcel Padilla, Felix Knöppel, Ulrich Pinkall, Peter Schröder",
    "title": "Motion From Shape Change",
    "paper_url": "https://www.researchgate.net/profile/Oliver_Gross5/publication/372661561_Motion_from_Shape_Change/links/6538e7d673a2865c7ad1b818/Motion-from-Shape-Change.pdf",
    "pdf_link": null,
    "abstract": "… We consider motion e ected by shape change. Such motions are ubiquitous in nature and the human made environment, ranging from single cells to platform divers and jelly sh …",
    "scholar_publication": "ACM Trans …, 2023 - researchgate.net"
  },
  {
    "paper_id": "papers_253",
    "authors": "Yunuo Chen, Tianyi Xie, Cem Yuksel, Danny Kaufman, Yin Yang, Chenfanfu Jiang, Minchen Li",
    "title": "Multi-layer Thick Shells",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591489",
    "pdf_link": null,
    "abstract": "… [2011], we design a reduced prism finite element to explicitly track the thickness evolution of multi-layer shells. With dual-quadrature reduced integration, shear locking can be avoided …",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_786",
    "authors": "Wangbo Yu, Yanbo Fan, Yong Zhang, Xuan Wang, Fei Yin, Yunpeng Bai, Yan-Pei Cao, Ying Shan, Yang Wu, Zhongqian Sun, Baoyuan Wu",
    "title": "NOFA: NeRF-based One-shot Facial Avatar Reconstruction",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591555",
    "pdf_link": null,
    "abstract": "3D facial avatar reconstruction has been a significant research topic in computer graphics and computer vision, where photo-realistic rendering and flexible controls over poses and expressions are necessary for many related applications. Recently, its performance has been greatly improved with the development of neural radiance fields (NeRF). However, most existing NeRF-based facial avatars focus on subject-specific reconstruction and reenactment, requiring multi-shot images containing different views of the specific subject for training, and the learned model cannot generalize to new identities, limiting its further applications. In this work, we propose a one-shot 3D facial avatar reconstruction framework that only requires a single source image to reconstruct a high-fidelity 3D facial avatar. For the challenges of lacking generalization ability and missing multi-view information, we leverage the generative prior of 3D GAN and develop an efficient encoder-decoder network to reconstruct the canonical neural volume of the source image, and further propose a compensation network to complement facial details. To enable fine-grained control over facial dynamics, we propose a deformation field to warp the canonical volume into driven expressions. Through extensive experimental comparisons, we achieve superior synthesis results compared to several state-of-the-art methods.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_135",
    "authors": "Yi-Hua Huang, Yan-Pei Cao, Yu-Kun Lai, Ying Shan, Lin Gao",
    "title": "NeRF-Texture: Texture Synthesis With Neural Radiance Fields",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591484",
    "pdf_link": null,
    "abstract": "Texture synthesis is a fundamental problem in computer graphics that would benefit various applications. Existing methods are effective in handling 2D image textures. In contrast, many real-world textures contain meso-structure in the 3D geometry space, such as grass, leaves, and fabrics, which cannot be effectively modeled using only 2D image textures. We propose a novel texture synthesis method with Neural Radiance Fields (NeRF) to capture and synthesize textures from given multi-view images. In the proposed NeRF texture representation, a scene with fine geometric details is disentangled into the meso-structure textures and the underlying base shape. This allows textures with meso-structure to be effectively learned as latent features situated on the base shape, which are fed into a NeRF decoder trained simultaneously to represent the rich view-dependent appearance. Using this implicit representation, we can synthesize NeRF-based textures through patch matching of latent features. However, inconsistencies between the metrics of the reconstructed content space and the latent feature space may compromise the synthesis quality. To enhance matching performance, we further regularize the distribution of latent features by incorporating a clustering constraint. Experimental results and evaluations demonstrate the effectiveness of our approach.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_348",
    "authors": "Yuan Liu, Peng Wang, Cheng Lin, Xiaoxiao Long, Jiepeng Wang, Lingjie Liu, Taku Komura, Wenping Wang",
    "title": "NeRO: Neural Geometry and BRDF Reconstruction of Reflective Objects From Multiview Images",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592134",
    "pdf_link": null,
    "abstract": "We present a neural rendering-based method called NeRO for reconstructing the geometry and the BRDF of reflective objects from multiview images captured in an unknown environment. Multiview reconstruction of reflective objects is extremely challenging because specular reflections are view-dependent and thus violate the multiview consistency, which is the cornerstone for most multiview reconstruction methods. Recent neural rendering techniques can model the interaction between environment lights and the object surfaces to fit the view-dependent reflections, thus making it possible to reconstruct reflective objects from multiview images. However, accurately modeling environment lights in the neural rendering is intractable, especially when the geometry is unknown. Most existing neural rendering methods, which can model environment lights, only consider direct lights and rely on object masks to reconstruct objects with weak specular reflections. Therefore, these methods fail to reconstruct reflective objects, especially when the object mask is not available and the object is illuminated by indirect lights. We propose a two-step approach to tackle this problem. First, by applying the split-sum approximation and the integrated directional encoding to approximate the shading effects of both direct and indirect lights, we are able to accurately reconstruct the geometry of reflective objects without any object masks. Then, with the object geometry fixed, we use more accurate sampling to recover the environment lights and the BRDF of the object. Extensive experiments demonstrate that our method is capable of accurately reconstructing the geometry and the BRDF of reflective objects from only posed RGB images without knowing the environment lights and the object masks. Codes and datasets are available at https://github.com/liuyuan-pal/NeRO.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_848",
    "authors": "Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter, Matthias Nießner",
    "title": "NeRSemble: Multi-view Radiance Field Reconstruction of Human Heads",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592455",
    "pdf_link": null,
    "abstract": "We focus on reconstructing high-fidelity radiance fields of human heads, capturing their animations over time, and synthesizing re-renderings from novel viewpoints at arbitrary time steps. To this end, we propose a new multi-view capture setup composed of 16 calibrated machine vision cameras that record time-synchronized images at 7.1 MP resolution and 73 frames per second. With our setup, we collect a new dataset of over 4700 high-resolution, high-framerate sequences of more than 220 human heads, from which we introduce a new human head reconstruction benchmark1. The recorded sequences cover a wide range of facial dynamics, including head motions, natural expressions, emotions, and spoken language. In order to reconstruct high-fidelity human heads, we propose Dynamic Neural Radiance Fields using Hash Ensembles (NeRSemble). We represent scene dynamics by combining a deformation field and an ensemble of 3D multi-resolution hash encodings. The deformation field allows for precise modeling of simple scene movements, while the ensemble of hash encodings helps to represent complex dynamics. As a result, we obtain radiance field representations of human heads that capture motion over time and facilitate re-rendering of arbitrary novel viewpoints. In a series of experiments, we explore the design choices of our method and demonstrate that our approach outperforms state-of-the-art dynamic radiance field approaches by a significant margin.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_439",
    "authors": "Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Terrance Wang, Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, David McAllister, Justin Kerr, Angjoo Kanazawa",
    "title": "Nerfstudio: A Modular Framework for Neural Radiance Field Development",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591516",
    "pdf_link": null,
    "abstract": "Neural Radiance Fields (NeRF) are a rapidly growing area of research with wide-ranging applications in computer vision, graphics, robotics, and more. In order to streamline the development and deployment of NeRF research, we propose a modular PyTorch framework, Nerfstudio. Our framework includes plug-and-play components for implementing NeRF-based methods, which make it easy for researchers and practitioners to incorporate NeRF into their projects. Additionally, the modular design enables support for extensive real-time visualization tools, streamlined pipelines for importing captured in-the-wild data, and tools for exporting to video, point cloud and mesh representations. The modularity of Nerfstudio enables the development of Nerfacto, our method that combines components from recent papers to achieve a balance between speed and quality, while also remaining flexible to future modifications. To promote community-driven development, all associated code and data are made publicly available with open-source licensing.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_492",
    "authors": "Bing Xu, Liwen Wu, Milos Hasan, Fujun Luan, Iliyan Georgiev, Zexiang Xu, Ravi Ramamoorthi",
    "title": "NeuSample: Importance Sampling for Neural Materials",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591524",
    "pdf_link": null,
    "abstract": "Neural material representations have recently been proposed to augment the material appearance toolbox used in realistic rendering. These models are successful at tasks ranging from measured BTF compression, through efficient rendering of synthetic displaced materials with occlusions, to BSDF layering. However, importance sampling has been an after-thought in most neural material approaches, and has been handled by inefficient cosine-hemisphere sampling or mixing it with an additional simple analytic lobe. In this paper we fill that gap, by evaluating and comparing various pdf-learning approaches for sampling spatially varying neural materials, and proposing new variations of these approaches. We investigate three sampling approaches: analytic-lobe mixtures, normalizing flows, and histogram prediction. Within each type, we introduce improvements beyond previous work, and we extensively evaluate and compare these approaches in terms of sampling rate, wall-clock time, and final visual quality. Our versions of normalizing flows and histogram mixtures perform well and can be used in practical rendering systems, potentially facilitating the broader adoption of neural material models in production.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_360",
    "authors": "Jiahui Fan, Beibei Wang, Milos Hasan, Jian Yang, Ling-Qi Yan",
    "title": "Neural Biplane Representation for BTF Rendering and Acquisition",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591505",
    "pdf_link": null,
    "abstract": "Bidirectional Texture Functions (BTFs) are able to represent complex materials with greater generality than traditional analytical models. This holds true for both measured real materials and synthetic ones. Recent advancements in neural BTF representations have significantly reduced storage costs, making them more practical for use in rendering. These representations typically combine spatial feature (latent) textures with neural decoders that handle angular dimensions per spatial location. However, these models have yet to combine fast compression and inference, accuracy, and generality. In this paper, we propose a biplane representation for BTFs, which uses a feature texture in the half-vector domain as well as the spatial domain. This allows the learned representation to encode high-frequency details in both the spatial and angular domains. Our decoder is small yet general, meaning it is trained once and fixed. Additionally, we optionally combine this representation with a neural offset module for parallax and masking effects. Our model can represent a broad range of BTFs and has fast compression and inference due to its lightweight architecture. Furthermore, it enables a simple way to capture BTF data. By taking about 20 cell phone photos with a collocated camera and flash, our model can plausibly recover the entire BTF, despite never observing function values with differing view and light directions. We demonstrate the effectiveness of our model in the acquisition of many measured materials, including challenging materials such as fabrics.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_797",
    "authors": "Dafei Qin, Jun Saito, Noam Aigerman, Thibault Groueix, Taku Komura",
    "title": "Neural Face Rigging for Animating and Retargeting Facial Meshes in the Wild",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591556",
    "pdf_link": null,
    "abstract": "We propose an end-to-end deep-learning approach for automatic rigging and retargeting of 3D models of human faces in the wild. Our approach, called Neural Face Rigging (NFR), holds three key properties: (i) NFR’s expression space maintains human-interpretable editing parameters for artistic controls; (ii) NFR is readily applicable to arbitrary facial meshes with different connectivity and expressions; (iii) NFR can encode and produce fine-grained details of complex expressions performed by arbitrary subjects. To the best of our knowledge, NFR is the first approach to provide realistic and controllable deformations of in-the-wild facial meshes, without the manual creation of blendshapes or correspondence. We design a deformation autoencoder and train it through a multi-dataset training scheme, which benefits from the unique advantages of two data sources: a linear 3DMM with interpretable control parameters as in FACS and 4D captures of real faces with fine-grained details. Through various experiments, we show NFR’s ability to automatically produce realistic and accurate facial deformations across a wide range of existing datasets and noisy facial scans in-the-wild, while providing artist-controlled, editable parameters.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_569",
    "authors": "Honghao Dong, Guoping Wang, Sheng Li",
    "title": "Neural Parametric Mixtures for Path Guiding",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591533",
    "pdf_link": null,
    "abstract": "Previous path guiding techniques typically rely on spatial subdivision structures to approximate directional target distributions, which may cause failure to capture spatio-directional correlations and introduce parallax issue. In this paper, we present Neural Parametric Mixtures (NPM), a neural formulation to encode target distributions for path guiding algorithms. We propose to use a continuous and compact neural implicit representation for encoding parametric models while decoding them via lightweight neural networks. We then derive a gradient-based optimization strategy to directly train the parameters of NPM with noisy Monte Carlo radiance estimates. Our approach efficiently models the target distribution (incident radiance or the product integrand) for path guiding, and outperforms previous guiding methods by capturing the spatio-directional correlations more accurately. Moreover, our approach is more training efficient and is practical for parallelization on modern GPUs.",
    "scholar_publication": "ACM SIGGRAPH 2023 Conference Proceedings, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_831",
    "authors": "Martin Balint, Krzysztof Wolski, Karol Myszkowski, Hans-Peter Seidel, Rafał Mantiuk",
    "title": "Neural Partitioning Pyramids for Denoising Monte Carlo Renderings",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591562",
    "pdf_link": null,
    "abstract": "Recent advancements in hardware-accelerated raytracing made it possible to achieve interactive framerates even for algorithms previously considered offline, such as path tracing. Interactive path tracing pipelines rely heavily on spatiotemporal denoising to produce a high-quality output from low-sample-count renderings. Such denoising is typically implemented as multiscale-kernel-based filters driven by lightweight U-Nets operating on pixels, and encoders operating on samples. In this work, we present a novel kernel …",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_728",
    "authors": "Philippe Weier, Tobias Zirr, Anton Kaplanyan, Ling-Qi Yan, Philipp Slusallek",
    "title": "Neural Prefiltering for Correlation-aware Levels of Detail",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592443",
    "pdf_link": null,
    "abstract": "We introduce a practical general-purpose neural appearance filtering pipeline for physically-based rendering. We tackle the previously difficult challenge of aggregating visibility across many levels of detail from local information only, without relying on learning visibility for the entire scene. The high adaptivity of neural representations allows us to retain geometric correlations along rays and thus avoid light leaks. Common approaches to prefiltering decompose the appearance of a scene into volumetric representations with physically-motivated parameters, where the inflexibility of the fitted models limits rendering accuracy. We avoid assumptions on particular types of geometry or materials, bypassing any special-case decompositions. Instead, we directly learn a compressed representation of the intra-voxel light transport. For such high-dimensional functions, neural networks have proven to be useful representations. To satisfy the opposing constraints of prefiltered appearance and correlation-preserving point-to-point visibility, we use two small independent networks on a sparse multi-level voxel grid. Each network requires 10--20 minutes of training to learn the appearance of an asset across levels of detail. Our method achieves 70--95% compression ratios and around 25% of quality improvements over previous work. We reach interactive to real-time framerates, depending on the level of detail.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_562",
    "authors": "Yun-Chun Chen, Vladimir Kim, Noam Aigerman, Alec Jacobson",
    "title": "Neural Progressive Meshes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591531",
    "pdf_link": null,
    "abstract": "… Neural Progressive Meshes. We present a framework that learns a progressive compressed representation of meshes … Given a high-resolution mesh in the database, the server trains a …",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_414",
    "authors": "Albert Reed, Juhyeon Kim, Thomas Blanford, Adithya Pediredla, Daniel Brown, Suren Jayasuriya",
    "title": "Neural Volumetric Reconstruction for Coherent Synthetic Aperture Sonar",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592141",
    "pdf_link": null,
    "abstract": "Synthetic aperture sonar (SAS) measures a scene from multiple views in order to increase the resolution of reconstructed imagery. Image reconstruction methods for SAS coherently combine measurements to focus acoustic energy onto the scene. However, image formation is typically under-constrained due to a limited number of measurements and bandlimited hardware, which limits the capabilities of existing reconstruction methods. To help meet these challenges, we design an analysis-by-synthesis optimization that leverages recent …",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_107",
    "authors": "Simon Duenser, Bernhard Thomaszewski, Roi Poranne, Stelian Coros, Simon Duenser",
    "title": "Nonlinear Compliant Modes for Large-deformation Analysis of Flexible Structures",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3568952",
    "pdf_link": null,
    "abstract": "Many flexible structures are characterized by a small number of compliant modes, i.e., large-deformation paths that can be traversed with little mechanical effort, whereas resistance to other deformations is much stiffer. Predicting the compliant modes for a given flexible structure, however, is challenging. While linear eigenmodes capture the small-deformation behavior, they quickly divert into states of unrealistically high energy for larger displacements. Moreover, they are inherently unable to predict nonlinear phenomena such as buckling, stiffening, multistability, and contact. To address this limitation, we propose Nonlinear Compliant Modes—a physically principled extension of linear eigenmodes for large-deformation analysis. Instead of constraining the entire structure to deform along a given eigenmode, our method only prescribes the projection of the system’s state onto the linear mode while all other degrees of freedom follow through energy minimization. We evaluate the potential of our method on a diverse set of flexible structures, ranging from compliant mechanisms to topology-optimized joints and structured materials. As validated through experiments on physical prototypes, our method correctly predicts a broad range of nonlinear effects that linear eigenanalysis fails to capture.",
    "scholar_publication": "ACM Transactions on …, 2022 - dl.acm.org"
  },
  {
    "paper_id": "papers_335",
    "authors": "Peng-Shuai Wang",
    "title": "OctFormer: Octree-based Transformers for 3D Point Clouds",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592131",
    "pdf_link": null,
    "abstract": "We propose octree-based transformers, named OctFormer, for 3D point cloud learning. OctFormer can not only serve as a general and effective backbone for 3D point cloud segmentation and object detection but also have linear complexity and is scalable for large-scale point clouds. The key challenge in applying transformers to point clouds is reducing the quadratic, thus overwhelming, computation complexity of attentions. To combat this issue, several works divide point clouds into non-overlapping windows and constrain attentions in each local window. However, the point number in each window varies greatly, impeding the efficient execution on GPU. Observing that attentions are robust to the shapes of local windows, we propose a novel octree attention, which leverages sorted shuffled keys of octrees to partition point clouds into local windows containing a fixed number of points while permitting shapes of windows to change freely. And we also introduce dilated octree attention to expand the receptive field further. Our octree attention can be implemented in 10 lines of code with open-sourced libraries and runs 17 times faster than other point cloud attentions when the point number exceeds 200k. Built upon the octree attention, OctFormer can be easily scaled up and achieves state-of-the-art performances on a series of 3D semantic segmentation and 3D object detection benchmarks, surpassing previous sparse-voxel-based CNNs and point cloud transformers in terms of both efficiency and effectiveness. Notably, on the challenging ScanNet200 dataset, OctFormer outperforms sparse-voxel-based CNNs by 7.3 in mIoU. Our code and trained models are available at https://wang-ps.github.io/octformer.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_105",
    "authors": "Roberto Montano Murillo, Ryuji Hirayama, Diego Martínez Plasencia, Diego Martínez Plasencia",
    "title": "OpenMPD: A Low-level Presentation Engine for Multimodal Particle-based Displays",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3572896",
    "pdf_link": null,
    "abstract": "Phased arrays of transducers have been quickly evolving in terms of software and hardware with applications in haptics (acoustic vibrations), display (levitation), and audio. Most recently, Multimodal Particle-based Displays (MPDs) have even demonstrated volumetric content that can be seen, heard, and felt simultaneously, without additional instrumentation. However, current software tools only support individual modalities and they do not address the integration and exploitation of the multi-modal potential of MPDs. This is because there is no standardized presentation pipeline tackling the challenges related to presenting such kind of multi-modal content (e.g., multi-modal support, multi-rate synchronization at 10 KHz, visual rendering or synchronization and continuity). This article presents OpenMPD, a low-level presentation engine that deals with these challenges and allows structured exploitation of any type of MPD content (i.e., visual, tactile, audio). We characterize OpenMPD’s performance and illustrate how it can be integrated into higher-level development tools (i.e., Unity game engine). We then illustrate its ability to enable novel presentation capabilities, such as support of multiple MPD contents, dexterous manipulations of fast-moving particles, or novel swept-volume MPD content.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_555",
    "authors": "Xavier Chermain, Cédric Zanni, Jonàs Martínez, Pierre-Alexandre Hugron, Sylvain Lefebvre",
    "title": "Orientable Dense Cyclic Infill for Anisotropic Appearance Fabrication",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592412",
    "pdf_link": null,
    "abstract": "We present a method to 3D print surfaces exhibiting a prescribed varying field of anisotropic appearance using only standard fused filament fabrication printers. This enables the fabrication of patterns triggering reflections similar to that of brushed metal with direct control over the directionality of the reflections. Our key insight, on which we ground the method, is that the direction of the deposition paths leads to a certain degree of surface roughness, which yields a visual anisotropic appearance. Therefore, generating dense cyclic infills aligned with a line field allows us to grade the anisotropic appearance of the printed surface. To achieve this, we introduce a highly parallelizable algorithm for optimizing oriented, cyclic paths. Our algorithm outperforms existing approaches regarding efficiency, robustness, and result quality. We demonstrate the effectiveness of our technique in conveying an anisotropic appearance on several challenging test cases, ranging from patterns to photographs reinterpreted as anisotropic appearances.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_716",
    "authors": "Chen Zong, Jiacheng Xu, Jiantao Song, Shuangmin Chen, Shiqing Xin, Wenping Wang, Changhe Tu",
    "title": "P2M: A Fast Solver for Querying Distance From Point to Mesh Surface",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592439",
    "pdf_link": null,
    "abstract": "Most of the existing point-to-mesh distance query solvers, such as Proximity Query Package (PQP), Embree and Fast Closest Point Query (FCPW), are based on bounding volume hierarchy (BVH). The hierarchical organizational structure enables one to eliminate the vast majority of triangles that do not help find the closest point. In this paper, we develop a totally different algorithmic paradigm, named P2M, to speed up point-to-mesh distance queries. Our original intention is to precompute a KD tree (KDT) of mesh vertices to approximately encode the geometry of a mesh surface containing vertices, edges and faces. However, it is very likely that the closest primitive to the query point is an edge e (resp., a face f), but the KDT reports a mesh vertex υ instead. We call υ an interceptor of e (resp., f). The main contribution of this paper is to invent a simple yet effective interception inspection rule and an efficient flooding interception inspection algorithm for quickly finding out all the interception pairs. Once the KDT and the interception table are precomputed, the query stage proceeds by first searching the KDT and then looking up the interception table to retrieve the closest geometric primitive. Statistics show that our query algorithm runs many times faster than the state-of-the-art solvers.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_553",
    "authors": "Marco Freire, Manas Bhargava, Camille Schreck, Pierre-Alexandre Hugron, Bernd Bickel, Sylvain Lefebvre",
    "title": "PCBend: Light Up Your 3D Shapes With Foldable Circuit Boards",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592411",
    "pdf_link": null,
    "abstract": "We propose a computational design approach for covering a surface with individually addressable RGB LEDs, effectively forming a low-resolution surface screen. To achieve a low-cost and scalable approach, we propose creating designs from flat PCB panels bent in-place along the surface of a 3D printed core. Working with standard rigid PCBs enables the use of established PCB manufacturing services, allowing the fabrication of designs with several hundred LEDs. Our approach optimizes the PCB geometry for folding, and then jointly optimizes the LED packing, circuit and routing, solving a challenging layout problem under strict manufacturing requirements. Unlike paper, PCBs cannot bend beyond a certain point without breaking. Therefore, we introduce parametric cut patterns acting as hinges, designed to allow bending while remaining compact. To tackle the joint optimization of placement, circuit and routing, we propose a specialized algorithm that splits the global problem into one sub-problem per triangle, which is then individually solved. Our technique generates PCB blueprints in a completely automated way. After being fabricated by a PCB manufacturing service, the boards are bent and glued by the user onto the 3D printed support. We demonstrate our technique on a range of physical models and virtual examples, creating intricate surface light patterns from hundreds of LEDs. The code and data for this paper are available at https://github.com/mfremer/pcbend.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_166",
    "authors": "Jinseok Bae, Jungdam Won, Donggeun Lim, Cheol-Hui Min, Young Min Kim",
    "title": "PMP: Learning to Physically Interact With Environments Using Part-wise Motion Priors",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591487",
    "pdf_link": null,
    "abstract": "We present a method to animate a character incorporating multiple part-wise motion priors (PMP). While previous works allow creating realistic articulated motions from reference data, the range of motion is largely limited by the available samples. Especially for the interaction-rich scenarios, it is impractical to attempt acquiring every possible interacting motion, as the combination of physical parameters increases exponentially. The proposed PMP allows us to assemble multiple part skills to animate a character, creating a diverse set of motions with different combinations of existing data. In our pipeline, we can train an agent with a wide range of part-wise priors. Therefore, each body part can obtain a kinematic insight of the style from the motion captures, or at the same time extract dynamics-related information from the additional part-specific simulation. For example, we can first train a general interaction skill, e.g. grasping, only for the dexterous part, and then combine the expert trajectories from the pre-trained agent with the kinematic priors of other limbs. Eventually, our whole-body agent learns a novel physical interaction skill even with the absence of the object trajectories in the reference motion sequence.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_427",
    "authors": "Wesley Chang, Venkataram Sivaram, Derek Nowrouzezahrai, Toshiya Hachisuka, Ravi Ramamoorthi, Tzu-Mao Li",
    "title": "Parameter-space ReSTIR for Differentiable and Inverse Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591512",
    "pdf_link": null,
    "abstract": "Differentiable rendering is frequently used in gradient descent-based inverse rendering pipelines to solve for scene parameters – such as reflectance or lighting properties – from target image inputs. Efficient computation of accurate, low variance gradients is critical for rapid convergence. While many methods employ variance reduction strategies, they operate independently on each gradient descent iteration, requiring large sample counts and computation. Gradients may however vary slowly between iterations, leading to unexplored potential benefits when reusing sample information to exploit this coherence. We develop an algorithm to reuse Monte Carlo gradient samples between gradient iterations, motivated by reservoir-based temporal importance resampling in forward rendering. Direct application of this method is not feasible, as we are computing many derivative estimates (i.e., one per optimization parameter) instead of a single pixel intensity estimate; moreover, each of these gradient estimates can affect multiple pixels, and gradients can take on negative values. We address these challenges by reformulating differential rendering integrals in parameter space, developing a new resampling estimator that treats negative functions, and combining these ideas into a reuse algorithm for inverse texture optimization. We significantly reduce gradient error compared to baselines, and demonstrate faster inverse rendering convergence in settings involving complex direct lighting and material textures.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_117",
    "authors": "Zhansheng Li, Yangyang Xu, Nanxuan Zhao, Yang Zhou, Yongtuo Liu, Dahua Lin, Shengfeng He, Nanxuan Zhao",
    "title": "Parsing-conditioned Anime Translation: A New Dataset and Method",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3585002",
    "pdf_link": null,
    "abstract": "Anime is an abstract art form that is substantially different from the human portrait, leading to a challenging misaligned image translation problem that is beyond the capability of existing methods. This can be boiled down to a highly ambiguous unconstrained translation between two domains. To this end, we design a new anime translation framework by deriving the prior knowledge of a pre-trained StyleGAN model. We introduce disentangled encoders to separately embed structure and appearance information into the same latent code, governed by four tailored losses. Moreover, we develop a FaceBank aggregation method that leverages the generated data of the StyleGAN, anchoring the prediction to produce in-domain animes. To empower our model and promote the research of anime translation, we propose the first anime portrait parsing dataset, Danbooru-Parsing, containing 4,921 densely labeled images across 17 classes. This dataset connects the face semantics with appearances, enabling our new constrained translation setting. We further show the editability of our results, and extend our method to manga images, by generating the first manga parsing pseudo data. Extensive experiments demonstrate the values of our new dataset and method, resulting in the first feasible solution on anime translation.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_579",
    "authors": "Xingchang Huang, Tobias Ritschel, Hans-Peter Seidel, Pooran Memari, Gurprit Singh",
    "title": "Patternshop: Editing Point Patterns by Image Manipulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592418",
    "pdf_link": null,
    "abstract": "Point patterns are characterized by their density and correlation. While spatial variation of density is well-understood, analysis and synthesis of spatially-varying correlation is an open challenge. No tools are available to intuitively edit such point patterns, primarily due to the lack of a compact representation for spatially varying correlation. We propose a low-dimensional perceptual embedding for point correlations. This embedding can map point patterns to common three-channel raster images, enabling manipulation with off-the-shelf …",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_120",
    "authors": "Cara Tursun, Piotr Didyk, Cara Tursun",
    "title": "Perceptual Visibility Model for Temporal Contrast Changes in Periphery",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3564241",
    "pdf_link": null,
    "abstract": "Modeling perception is critical for many applications and developments in computer graphics to optimize and evaluate content generation techniques. Most of the work to date has focused on central (foveal) vision. However, this is insufficient for novel wide-field-of-view display devices, such as virtual and augmented reality headsets. Furthermore, the perceptual models proposed for the fovea do not readily extend to the off-center, peripheral visual field, where human perception is drastically different. In this article, we focus on modeling the temporal aspect of visual perception in the periphery. We present new psychophysical experiments that measure the sensitivity of human observers to different spatio-temporal stimuli across a wide field of view. We use the collected data to build a perceptual model for the visibility of temporal changes at different eccentricities in complex video content. Finally, we discuss, demonstrate, and evaluate several problems that can be addressed using our technique. First, we show how our model enables injecting new content into the periphery without distracting the viewer, and we discuss the link between the model and human attention. Second, we demonstrate how foveated rendering methods can be evaluated and optimized to limit the visibility of temporal aliasing.",
    "scholar_publication": "ACM Transactions on Graphics, 2022 - dl.acm.org"
  },
  {
    "paper_id": "papers_575",
    "authors": "Grace Kuo, Eric Penner, Seth Moczydlowski, Alex Ching, Douglas Lanman, Nathan Matsuda",
    "title": "Perspective-correct VR Passthrough Without Reprojection",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591534",
    "pdf_link": null,
    "abstract": "Virtual reality (VR) passthrough uses external cameras on the front of a headset to allow the user to see their environment. However, passthrough cameras cannot physically be co-located with the user’s eyes, so the passthrough images have a different perspective than what the user would see without the headset. Although the images can be computationally reprojected into the desired view, errors in depth estimation, view-dependent effects, and missing information at occlusion boundaries can lead to undesirable artifacts. We propose a novel computational camera that directly samples the rays that would have gone into the user’s eye, several centimeters behind the sensor. Our design contains an array of lenses with an aperture behind each lens, and the apertures are strategically placed to allow through only the desired rays. The resulting thin, flat architecture has suitable form factor for VR, and the image reconstruction is computationally lightweight, enabling low-latency passthrough. We demonstrate our approach experimentally in a fully functional binocular passthrough prototype with practical calibration and real-time image reconstruction. Finally, we experimentally validate that our camera captures the correct perspective for VR passthrough, even in the presence of transparent objects, specular highlights, and complex occluding structures.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_577",
    "authors": "Xilong Zhou, Milos Hasan, Valentin Deschaintre, Paul Guerrero, Yannick Hold-Geoffroy, Kalyan Sunkavalli, Nima Kalantari",
    "title": "PhotoMat: A Material Generator Learned From Single Flash Photos",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591535",
    "pdf_link": null,
    "abstract": "Authoring high-quality digital materials is key to realism in 3D rendering. Previous generative models for materials have been trained exclusively on synthetic data; such data is limited in availability and has a visual gap to real materials. We circumvent this limitation by proposing PhotoMat: the first material generator trained exclusively on real photos of material samples captured using a cell phone camera with flash. Supervision on individual material maps is not available in this setting. Instead, we train a generator for a neural material representation that is rendered with a learned relighting module to create arbitrarily lit RGB images; these are compared against real photos using a discriminator. We train PhotoMat with a new dataset of 12,000 material photos captured with handheld phone cameras under flash lighting. We demonstrate that our generated materials have better visual quality than previous material generators trained on synthetic data. Moreover, we can fit analytical material models to closely match these generated neural materials, thus allowing for further editing and use in 3D rendering.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_453",
    "authors": "Jonathan Panuelos, Ryan Goldade, Eitan Grinspun, David Levin, Christopher Batty",
    "title": "PolyStokes: A Polynomial Model Reduction Method for Viscous Fluid Simulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592146",
    "pdf_link": null,
    "abstract": "Standard liquid simulators apply operator splitting to independently solve for pressure and viscous stresses, a decoupling that induces incorrect free surface boundary conditions. Such methods are unable to simulate fluid phenomena reliant on the balance of pressure and viscous stresses, such as the liquid rope coil instability exhibited by honey. By contrast, unsteady Stokes solvers retain coupling between pressure and viscosity, thus resolving these phenomena, albeit using a much larger and thus more computationally expensive linear system compared to the decoupled approach. To accelerate solving the unsteady Stokes problem, we propose a reduced fluid model wherein interior regions are represented with incompressible polynomial vector fields. Sets of standard grid cells are consolidated into super-cells, each of which are modelled using a quadratic field of 26 degrees of freedom. We demonstrate that the reduced field must necessarily be at least quadratic, with the affine model being unable to correctly capture viscous forces. We reproduce the liquid rope coiling instability, as well as other simulated examples, to show that our reduced model is able to reproduce the same fluid phenomena at a smaller computational cost. Futhermore, we performed a crowdsourced user survey to verify that our method produces imperceptible differences compared to the full unsteady Stokes method.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_309",
    "authors": "Élie Michel, Jean-Marc Thiery",
    "title": "Polynomial 2D Green Coordinates for Polygonal Cages",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591499",
    "pdf_link": null,
    "abstract": "Cage coordinates are a powerful means to define 2D deformation fields from sparse control points. We introduce Conformal polynomial Coordinates for closed polyhedral cages, enabling segments to be transformed into polynomial curves of any order. Extending classical 2D Green coordinates, our coordinates result in conformal harmonic deformations that are cage-aware. We demonstrate the usefulness of our technique on a variety of 2D deformation scenarios where curves allow artists to perform intuitive deformations with few input parameters. Our method combines the texture preservation property of conformal deformations together with the expressiveness offered by Bezier controls.",
    "scholar_publication": "ACM SIGGRAPH 2023 Conference Proceedings, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_267",
    "authors": "Zhe Li, Zerong Zheng, Yuxiao Liu, Boyao Zhou, Yebin Liu",
    "title": "PoseVocab: Learning Joint-structured Pose Embeddings for Human Avatar Modeling",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591490",
    "pdf_link": null,
    "abstract": "Creating pose-driven human avatars is about modeling the mapping from the low-frequency driving pose to high-frequency dynamic human appearances, so an effective pose encoding method that can encode high-fidelity human details is essential to human avatar modeling. To this end, we present PoseVocab, a novel pose encoding method that encourages the network to discover the optimal pose embeddings for learning the dynamic human appearance. Given multi-view RGB videos of a character, PoseVocab constructs key poses and latent embeddings based on the training poses. To achieve pose generalization and temporal consistency, we sample key rotations in so(3) of each joint rather than the global pose vectors, and assign a pose embedding to each sampled key rotation. These joint-structured pose embeddings not only encode the dynamic appearances under different key poses, but also factorize the global pose embedding into joint-structured ones to better learn the appearance variation related to the motion of each joint. To improve the representation ability of the pose embedding while maintaining memory efficiency, we introduce feature lines, a compact yet effective 3D representation, to model more fine-grained details of human appearances. Furthermore, given a query pose and a spatial position, a hierarchical query strategy is introduced to interpolate pose embeddings and acquire the conditional pose feature for dynamic human synthesis. Overall, PoseVocab effectively encodes the dynamic details of human appearance and enables realistic and generalized animation under novel poses. Experiments show that our method outperforms other state-of-the-art baselines both qualitatively and quantitatively in terms of synthesis quality. Code is available at https://github.com/lizhe00/PoseVocab.",
    "scholar_publication": "ACM SIGGRAPH 2023 conference …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_214",
    "authors": "Janghun Kim, Sungkil Lee",
    "title": "Potentially Visible Hidden-volume Rendering for Multi-view Warping",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592108",
    "pdf_link": null,
    "abstract": "This paper presents the model and rendering algorithm of Potentially Visible Hidden Volumes (PVHVs) for multi-view image warping. PVHVs are 3D volumes that are occluded at a known source view, but potentially visible at novel views. Given a bound of novel views, we define PVHVs using the edges of foreground fragments from the known view and the bound of novel views. PVHVs can be used to batch-test the visibilities of source fragments without iterating individual novel views in multi-fragment rendering, and thereby, cull redundant fragments prior to warping. We realize the model of PVHVs in Depth Peeling (DP). Our Effective Depth Peeling (EDP) can reduce the number of completely hidden fragments, capture important fragments early, and reduce warping cost. We demonstrate the benefit of our PVHVs and EDP in terms of memory, quality, and performance in multi-view warping.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_126",
    "authors": "Liane Makatura, Bohan Wang, Yi-Lu Chen, Bolei Deng, Chris Wojtan, Bernd Bickel, Wojciech Matusik, Liane Makatura",
    "title": "Procedural Metamaterials: A Unified Procedural Graph for Metamaterial Design",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3605389",
    "pdf_link": null,
    "abstract": "We introduce a compact, intuitive procedural graph representation for cellular metamaterials, which are small-scale, tileable structures that can be architected to exhibit many useful material properties. Because the structures’ “architectures” vary widely—with elements such as beams, thin shells, and solid bulks—it is difficult to explore them using existing representations. Generic approaches like voxel grids are versatile, but it is cumbersome to represent and edit individual structures; architecture-specific approaches address these issues, but are incompatible with one another. By contrast, our procedural graph succinctly represents the construction process for any structure using a simple skeleton annotated with spatially varying thickness. To express the highly constrained triply periodic minimal surfaces (TPMS) in this manner, we present the first fully automated version of the conjugate surface construction method, which allows novices to create complex TPMS from intuitive input. We demonstrate our representation’s expressiveness, accuracy, and compactness by constructing a wide range of established structures and hundreds of novel structures with diverse architectures and material properties. We also conduct a user study to verify our representation’s ease-of-use and ability to expand engineers’ capacity for exploration.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_798",
    "authors": "Zackary Misso, Yining Karl Li, Brent Burley, Daniel Teece, Wojciech Jarosz",
    "title": "Progressive Null-tracking for Volumetric Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591557",
    "pdf_link": null,
    "abstract": "Null-collision approaches for estimating transmittance and sampling free-flight distances are the current state-of-the-art for unbiased rendering of general heterogeneous participating media. However, null-collision approaches have a strict requirement for specifying a tightly bounding total extinction in order to remain both robust and performant; in practice this requirement restricts the use of null-collision techniques to only participating media where the density of the medium at every possible point in space is known a-priori. In production …",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_272",
    "authors": "Qing Zhang, Hao Jiang, Yongwei Nie, Wei-Shi Zheng",
    "title": "Pyramid Texture Filtering",
    "paper_url": "https://dl.acm.org/doi/pdf/10.1145/218380.218446",
    "pdf_link": null,
    "abstract": "… all discrete filters, there are tradeoffs in the design of the steerable pyramid filters (eg, filter size … There are two aspects of these images that the pyramid texture model misses. First, these …",
    "scholar_publication": "Proceedings of the 22nd annual conference on …, 1995 - dl.acm.org"
  },
  {
    "paper_id": "papers_352",
    "authors": "Sunmin Lee, Sebastian Starke, Yuting Ye, Jungdam Won, Alexander Winkler",
    "title": "QuestEnvSim: Environment-aware Simulated Motion Tracking From Sparse Sensor Input",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591504",
    "pdf_link": null,
    "abstract": "Replicating a user’s pose from only wearable sensors is important for many AR/VR applications. Most existing methods for motion tracking avoid environment interaction apart from foot-floor contact due to their complex dynamics and hard constraints. However, in daily life people regularly interact with their environment, e.g. by sitting on a couch or leaning on a desk. Using Reinforcement Learning, we show that headset and controller pose, if combined with physics simulation and environment observations can generate realistic full-body poses even in highly constrained environments. The physics simulation automatically enforces the various constraints necessary for realistic poses, instead of manually specifying them as in many kinematic approaches. These hard constraints allow us to achieve high-quality interaction motions without typical artifacts such as penetration or contact sliding. We discuss three features, the environment representation, the contact reward and scene randomization, crucial to the performance of the method. We demonstrate the generality of the approach through various examples, such as sitting on chairs, a couch and boxes, stepping over boxes, rocking a chair and turning an office chair. We believe these are some of the highest-quality results achieved for motion tracking from sparse sensor with scene interaction.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_434",
    "authors": "Xiangjun Tang, Linjun Wu, He Wang, Bo Hu, Xu Gong, Yuchen Liao, Songnan Li, Qilong Kou, Xiaogang Jin",
    "title": "RSMT: Real-time Stylized Motion Transition for Characters",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591514",
    "pdf_link": null,
    "abstract": "Styled online in-between motion generation has important application scenarios in computer animation and games. Its core challenge lies in the need to satisfy four critical requirements simultaneously: generation speed, motion quality, style diversity, and synthesis controllability. While the first two challenges demand a delicate balance between simple fast models and learning capacity for generation quality, the latter two are rarely investigated together in existing methods, which largely focus on either control without style or uncontrolled stylized motions. To this end, we propose a Real-time Stylized Motion Transition method (RSMT) to achieve all aforementioned goals. Our method consists of two critical, independent components: a general motion manifold model and a style motion sampler. The former acts as a high-quality motion source and the latter synthesizes styled motions on the fly under control signals. Since both components can be trained separately on different datasets, our method provides great flexibility, requires less data, and generalizes well when no/few samples are available for unseen styles. Through exhaustive evaluation, our method proves to be fast, high-quality, versatile, and controllable. The code and data are available at https://github.com/yuyujunjun/RSMT-Realtime-Stylized-Motion-Transition.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_522",
    "authors": "Karthik Vaidyanathan, Marco Salvi, Bartlomiej Wronski, Tomas Akenine-Moller, Pontus Ebelin, Aaron Lefohn",
    "title": "Random-access Neural Compression of Material Textures",
    "paper_url": "https://arxiv.org/abs/2305.17105",
    "pdf_link": null,
    "abstract": "The continuous advancement of photorealism in rendering is accompanied by a growth in texture data and, consequently, increasing storage and memory demands. To address this issue, we propose a novel neural compression technique specifically designed for material textures. We unlock two more levels of detail, i.e., 16x more texels, using low bitrate compression, with image quality that is better than advanced image compression techniques, such as AVIF and JPEG XL. At the same time, our method allows on-demand, real-time decompression with random access similar to block texture compression on GPUs, enabling compression on disk and memory. The key idea behind our approach is compressing multiple material textures and their mipmap chains together, and using a small neural network, that is optimized for each material, to decompress them. Finally, we use a custom training implementation to achieve practical compression speeds, whose performance surpasses that of general frameworks, like PyTorch, by an order of magnitude.",
    "scholar_publication": "arXiv preprint arXiv …, 2023 - arxiv.org"
  },
  {
    "paper_id": "papers_896",
    "authors": "Alex Trevithick, Matthew Chan, Michael Stengel, Eric Chan, Chao Liu, Zhiding Yu, Sameh Khamis, Manmohan Chandraker, Ravi Ramamoorthi, Koki Nagano",
    "title": "Real-time Radiance Fields for Single-image Portrait View Synthesis",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592460",
    "pdf_link": null,
    "abstract": "We present a one-shot method to infer and render a photorealistic 3D representation from a single unposed image (eg, face portrait) in real-time. Given a single RGB input, our image encoder directly predicts a canonical triplane representation of a neural radiance field for 3D-aware novel view synthesis via volume rendering. Our method is fast (24 fps) on consumer hardware, and produces higher quality results than strong GAN-inversion baselines that require test-time optimization. To train our triplane encoder pipeline, we use only synthetic …",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_400",
    "authors": "Baptiste Nicolet, Fabrice Rousselle, Jan Novak, Alexander Keller, Wenzel Jakob, Thomas Müller",
    "title": "Recursive Control Variates for Inverse Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592139",
    "pdf_link": null,
    "abstract": "We present a method for reducing errors---variance and bias---in physically based differentiable rendering (PBDR). Typical applications of PBDR repeatedly render a scene as part of an optimization loop involving gradient descent. The actual change introduced by each gradient descent step is often relatively small, causing a significant degree of redundancy in this computation. We exploit this redundancy by formulating a gradient estimator that employs a recursive control variate, which leverages information from previous optimization steps. The control variate reduces variance in gradients, and, perhaps more importantly, alleviates issues that arise from differentiating loss functions with respect to noisy inputs, a common cause of drift to bad local minima or divergent optimizations. We experimentally evaluate our approach on a variety of path-traced scenes containing surfaces and volumes and observe that primal rendering efficiency improves by a factor of up to 10.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_113",
    "authors": "Chong Zeng, Guojun Chen, Yue Dong, Pieter Peers, Hongzhi Wu, Xin Tong",
    "title": "Relighting Neural Radiance Fields With Shadow and Highlight Hints",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591482",
    "pdf_link": null,
    "abstract": "This paper presents a novel neural implicit radiance representation for free viewpoint relighting from a small set of unstructured photographs of an object lit by a moving point light source different from the view position. We express the shape as a signed distance function modeled by a multi layer perceptron. In contrast to prior relightable implicit neural representations, we do not disentangle the different light transport components, but model both the local and global light transport at each point by a second multi layer perceptron that, in addition, to density features, the current position, the normal (from the signed distance function), view direction, and light position, also takes shadow and highlight hints to aid the network in modeling the corresponding high frequency light transport effects. These hints are provided as a suggestion, and we leave it up to the network to decide how to incorporate these in the final relit result. We demonstrate and validate our neural implicit representation on synthetic and real scenes exhibiting a wide variety of shapes, material properties, and global illumination light transport.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_115",
    "authors": "Pengfei Wang, Zixiong Wang, Shiqing Xin, Xifeng Gao, Wenping Wang, Changhe Tu, Pengfei Wang",
    "title": "Restricted Delaunay Triangulation for Explicit Surface Reconstruction",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3533768",
    "pdf_link": null,
    "abstract": "The task of explicit surface reconstruction is to generate a surface mesh by interpolating a given point cloud. Explicit surface reconstruction is necessary when the point cloud is required to appear exactly on the surface. However, for a non-perfect input, such as lack of normals, low density, irregular distribution, thin and tiny parts, and high genus, a robust explicit reconstruction method that can generate a high-quality manifold triangulation is missing. We propose a robust explicit surface reconstruction method that starts from an initial …",
    "scholar_publication": "ACM Transactions on …, 2022 - dl.acm.org"
  },
  {
    "paper_id": "papers_690",
    "authors": "Qingqin Hua, Pascal Grittmann, Philipp Slusallek",
    "title": "Revisiting Controlled Mixture Sampling for Rendering Applications",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592435",
    "pdf_link": null,
    "abstract": "Monte Carlo rendering makes heavy use of mixture sampling and multiple importance sampling (MIS). Previous work has shown that control variates can be used to make such mixtures more efficient and more robust. However, the existing approaches failed to yield practical applications, chiefly because their underlying theory is based on the unrealistic assumption that a single mixture is optimized for a single integral. This is in stark contrast with rendering reality, where millions of integrals are computed---one per pixel---and each is infinitely recursive. We adapt and extend the theory introduced by previous work to tackle the challenges of real-world rendering applications. We achieve robust mixture sampling and (approximately) optimal MIS weighting for common applications such as light selection, BSDF sampling, and path guiding.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_449",
    "authors": "Bosheng Li, Jonathan Klein, Dominik L. Michels, Bedrich Benes, Sören Pirk, Wojtek Pałubicki",
    "title": "Rhizomorph: The Coordinated Function of Shoots and Roots",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592145",
    "pdf_link": null,
    "abstract": "Computer graphics has dedicated a considerable amount of effort to generating realistic models of trees and plants. Many existing methods leverage procedural modeling algorithms - that often consider biological findings - to generate branching structures of individual trees. While the realism of tree models generated by these algorithms steadily increases, most approaches neglect to model the root system of trees. However, the root system not only adds to the visual realism of tree models but also plays an important role in the development of trees. In this paper, we advance tree modeling in the following ways: First, we define a physically-plausible soil model to simulate resource gradients, such as water and nutrients. Second, we propose a novel developmental procedural model for tree roots that enables us to emergently develop root systems that adapt to various soil types. Third, we define long-distance signaling to coordinate the development of shoots and roots. We show that our advanced procedural model of tree development enables - for the first time - the generation of trees with their root systems.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_493",
    "authors": "Zhen Chen, Zherong Pan, Kui Wu, Etienne Vouga, Xifeng Gao",
    "title": "Robust Low-poly Meshing for General 3D Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592396",
    "pdf_link": null,
    "abstract": "We propose a robust re-meshing approach that can automatically generate visual-preserving low-poly meshes for any high-poly models found in the wild. Our method can be seamlessly integrated into current mesh-based 3D asset production pipelines. Given an input high-poly, our method proceeds in two stages: 1) Robustly extracting an offset surface mesh that is feature-preserving, and guaranteed to be watertight, manifold, and self-intersection free; 2) Progressively simplifying and flowing the offset mesh to bring it close to the input. The simplicity and the visual-preservation of the generated low-poly is controlled by a user-required target screen size of the input: decreasing the screen size reduces the element count of the low-poly but enlarges its visual difference from the input. We have evaluated our method on a subset of the Thingi10K dataset that contains models created by practitioners in different domains, with varying topological and geometric complexities. Compared to state-of-the-art approaches and widely used software, our method demonstrates its superiority in terms of the element count, visual preservation, geometry, and topology guarantees of the generated low-polys.",
    "scholar_publication": "ACM Transactions on Graphics …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_444",
    "authors": "Jerry Hsu, Tongtong Wang, Zherong Pan, Xifeng Gao, Cem Yuksel, Kui Wu",
    "title": "Sag-free Initialization for Strand-based Hybrid Hair Simulation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592143",
    "pdf_link": null,
    "abstract": "Lagrangian/Eulerian hybrid strand-based hair simulation techniques have quickly become a popular approach in VFX and real-time graphics applications. With Lagrangian hair dynamics, the inter-hair contacts are resolved in the Eulerian grid using the continuum method, i.e., the MPM scheme with the granular Drucker-Prager rheology, to avoid expensive collision detection and handling. This fuzzy collision handling makes the authoring process significantly easier. However, although current hair grooming tools provide a wide range of strand-based modeling tools for this simulation approach, the crucial sag-free initialization functionality remains often ignored. Thus, when the simulation starts, gravity would cause any artistic hairstyle to sag and deform into unintended and undesirable shapes. This paper proposes a novel four-stage sag-free initialization framework to solve stable quasistatic configurations for hybrid strand-based hair dynamic systems. These four stages are split into two global-local pairs. The first one ensures static equilibrium at every Eulerian grid node with additional inequality constraints to prevent stress from exiting the yielding surface. We then derive several associated closed-form solutions in the local stage to compute segment rest lengths, orientations, and particle deformation gradients in parallel. The second global-local step solves along each hair strand to ensure all the bend and twist constraints produce zero net torque on every hair segment, followed by a local step to adjust the rest Darboux vectors to a unit quaternion. We also introduce an essential modification for the Darboux vector to eliminate the ambiguity of the Cosserat rod rest pose in both initialization and simulation. We evaluate our method on a wide range of hairstyles, and our approach can only take a few seconds to minutes to get the rest quasistatic configurations for hundreds of hair strands. Our results show that our method successfully prevents sagging and has minimal impact on the hair motion during simulation.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_235",
    "authors": "Hezhi Cao, Xia Xi, Guan Wu, Ruizhen Hu, Ligang Liu",
    "title": "ScanBot: Autonomous Reconstruction via Deep Reinforcement Learning",
    "paper_url": "https://hezhicao.github.io/Scanbot/scanbot_files/scanbot_supplementary.pdf",
    "pdf_link": null,
    "abstract": "Algorithm. Starting from the initial location, the robot tries to explore and scan the unknown environment with quality reconstruction S. Then, a ROI with insu cient exploration or incomplete objects is generated by the global scanning policy Π () after taking in the 2D quality map M built from S. The ROI is rst represented by a goal point (), accessed through the path P generated by D* lite and ends up to be, where the robot actually arrived. All unscanned objects in the designated ROI are progressively selected with group of and completed by scanning the viewpoint () dedicated by the masked local policy Π (). The local policy takes the current voxelized ROI region V and selected objects V as input to generate the viewpoint for completion until it is content with the reconstruction and chooses STOP action, or no feasible candidate viewpoints left. The robot iterates the above steps until the whole scene has been completely explored and scanned. The entire autoscanning process is shown in Algorithm 1, where B and B denotes unscanned objects bank in designated ROI and global scanned objects bank, respectively.",
    "scholar_publication": "ACM Trans. Graph., 2023 - hezhicao.github.io"
  },
  {
    "paper_id": "papers_422",
    "authors": "Pengfei Shen, Ruizeng Li, Beibei Wang, Ligang Liu",
    "title": "Scratch-based Reflection Art via Differentiable Rendering",
    "paper_url": "https://wangningbei.github.io/2023/DiffGlints_files/paper_diffGlints_compressed.pdf",
    "pdf_link": null,
    "abstract": "The 3D visual optical arts can produce impressive special effects by designing interactions between objects and light sources, displaying images in shadows, caustics, or reflectors. The design of such effects is challenging due to the complex interactions between the lights and objects. Therefore, computational 3D visual arts [Glasner et al. 2014; Levin et al. 2013; Mitra and Pauly 2009; Sakurai et al. 2018; Weyrich et al. 2009; Wu et al. 2022b; Yue et al. 2014] have attracted a great deal of attention to creating these special effects automatically. Among various kinds of computational 3D visual arts, we focus on the 3D reflection arts [Wu et al. 2022b]. This type of art aims to create reflectors that can display different images when viewed from different directions. The existing works achieve this goal by modifying different properties on the surface, as shown in Figure 2. Some works manipulate the height field by placing small mirrors [Weyrich et al. 2009] or raised stripes [Sakurai et al. 2018; Snelgrove et al. 2013]. The others control the reflectance [Lan et al. 2013; Matusik et al. 2009], or colors [Pjanic and Hersch 2015a, b]. However, these works discretize reflector surfaces with regular grids/facets, leading to a huge parameter space, which raises difficulties for optimization and fabrication. Moreover, previous works use the genetic algorithm",
    "scholar_publication": "ACM Trans. Graph., 2023 - wangningbei.github.io"
  },
  {
    "paper_id": "papers_199",
    "authors": "Lei Lan, Minchen Li, Chenfanfu Jiang, Huamin Wang, Yin Yang",
    "title": "Second-order Stencil Descent for Interior-point Hyperelasticity",
    "paper_url": "https://wanghmin.github.io/publication/lan-2023-sos/Lan-2023-SOS.pdf",
    "pdf_link": null,
    "abstract": "Newton’s method has been a popular choice [Baraff and Witkin 1998] for solving the variational form [Kane et al. 2000; Martin et al. 2011] associated with various deformable models. Recently, many contributions suggest that more efficient simulation is possible using non-Newton or quasi-Newton solvers [Hecht et al. 2012; Li et al. 2019; Liu et al. 2017; Narain et al. 2016; Wang and Yang 2016; Wang",
    "scholar_publication": "ACM Trans. Graph, 2023 - wanghmin.github.io"
  },
  {
    "paper_id": "papers_703",
    "authors": "Sizhuo Ma, Varun Sundar, Paul Mos, Claudio Bruschini, Edoardo Charbon, Mohit Gupta",
    "title": "Seeing Photons in Color",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592438",
    "pdf_link": null,
    "abstract": "… While the previous section proposed computational and algorithmic approaches for processing the raw single-photon color frames, in this section, we consider the problem of raw frame …",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_787",
    "authors": "Jenny Lin, Vidya Narayanan, Yuka Ikarashi, Jonathan Ragan-Kelley, Gilbert Bernstein, James McCann",
    "title": "Semantics and Scheduling for Machine Knitting Compilers",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592449",
    "pdf_link": null,
    "abstract": "Machine knitting is a well-established fabrication technique for complex soft objects, and both companies and researchers have developed tools for generating machine knitting patterns. However, existing representations for machine knitted objects are incomplete (do not cover the complete domain of machine knittable objects) or overly specific (do not account for symmetries and equivalences among knitting instruction sequences). This makes it difficult to define correctness in machine knitting, let alone verify the correctness of a given program or program transformation. The major contribution of this work is a formal semantics for knitout, a low-level Domain Specific Language for knitting machines. We accomplish this by using what we call the fenced tangle, which extends concepts from knot theory to allow for a mathematical definition of knitting program equivalence that matches the intuition behind knit objects. Finally, using this formal representation, we prove the correctness of a sequence of rewrite rules; and demonstrate how these rewrite rules can form the foundation for higher-level tasks such as compiling a program for a specific machine and optimizing for time/reliability, all while provably generating the same knit object under our proposed semantics. By establishing formal definitions of correctness, this work provides a strong foundation for compiling and optimizing knit programs.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_479",
    "authors": "Chang Wook Seo, Amirsaman Ashtari, Junyong Noh",
    "title": "Semi-supervised Reference-based Sketch Extraction Using a Contrastive Learning Framework",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592392",
    "pdf_link": null,
    "abstract": "Sketches reflect the drawing style of individual artists; therefore, it is important to consider their unique styles when extracting sketches from color images for various applications. Unfortunately, most existing sketch extraction methods are designed to extract sketches of a single style. Although there have been some attempts to generate various style sketches, the methods generally suffer from two limitations: low quality results and difficulty in training the model due to the requirement of a paired dataset. In this paper, we propose a novel multi-modal sketch extraction method that can imitate the style of a given reference sketch with unpaired data training in a semi-supervised manner. Our method outperforms state-of-the-art sketch extraction methods and unpaired image translation methods in both quantitative and qualitative evaluations.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_563",
    "authors": "R. Kenny Jones, Paul Guerrero, Niloy J. Mitra, Daniel Ritchie",
    "title": "ShapeCoder: Discovering Abstractions for Visual Programs From Unstructured Primitives",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592416",
    "pdf_link": null,
    "abstract": "We introduce ShapeCoder, the first system capable of taking a dataset of shapes, represented with unstructured primitives, and jointly discovering (i) useful abstraction functions and (ii) programs that use these abstractions to explain the input shapes. The discovered abstractions capture common patterns (both structural and parametric) across a dataset, so that programs rewritten with these abstractions are more compact, and suppress spurious degrees of freedom. ShapeCoder improves upon previous abstraction discovery methods, finding better abstractions, for more complex inputs, under less stringent input assumptions. This is principally made possible by two methodological advancements: (a) a shape-to-program recognition network that learns to solve sub-problems and (b) the use of e-graphs, augmented with a conditional rewrite scheme, to determine when abstractions with complex parametric expressions can be applied, in a tractable manner. We evaluate ShapeCoder on multiple datasets of 3D shapes, where primitive decompositions are either parsed from manual annotations or produced by an unsupervised cuboid abstraction method. In all domains, ShapeCoder discovers a library of abstractions that captures high-level relationships, removes extraneous degrees of freedom, and achieves better dataset compression compared with alternative approaches. Finally, we investigate how programs rewritten to use discovered abstractions prove useful for downstream tasks.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_373",
    "authors": "He Chen, Elie Diaz, Cem Yuksel",
    "title": "Shortest Path to Boundary for Self-intersecting Meshes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592136",
    "pdf_link": null,
    "abstract": "We introduce a method for efficiently computing the exact shortest path to the boundary of a mesh from a given internal point in the presence of self-intersections. We provide a formal definition of shortest boundary paths for self-intersecting objects and present a robust algorithm for computing the actual shortest boundary path. The resulting method offers an effective solution for collision and self-collision handling while simulating deformable volumetric objects, using fast simulation techniques that provide no guarantees on collision resolution. Our evaluation includes complex self-collision scenarios with a large number of active contacts, showing that our method can successfully handle them by introducing a relatively minor computational overhead.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_270",
    "authors": "Yunbo Zhang, Deepak Gopinath, Yuting Ye, Jessica Hodgins, Greg Turk, Jungdam Won",
    "title": "Simulation and Retargeting of Complex Multi-character Interactions",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591491",
    "pdf_link": null,
    "abstract": "We present a method for reproducing complex multi-character interactions for physically simulated humanoid characters using deep reinforcement learning. Our method learns control policies for characters that imitate not only individual motions, but also the interactions between characters, while maintaining balance and matching the complexity of reference data. Our approach uses a novel reward formulation based on an interaction graph that measures distances between pairs of interaction landmarks. This reward encourages control policies to efficiently imitate the character’s motion while preserving the spatial relationships of the interactions in the reference motion. We evaluate our method on a variety of activities, from simple interactions such as a high-five greeting to more complex interactions such as gymnastic exercises, Salsa dancing, and box carrying and throwing. This approach can be used to “clean-up” existing motion capture data to produce physically plausible interactions or to retarget motion to new characters with different sizes, kinematics or morphologies while maintaining the interactions in the original data.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_436",
    "authors": "James Bieron, Xin Tong, Pieter Peers",
    "title": "Single Image Neural Material Relighting",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591515",
    "pdf_link": null,
    "abstract": "This paper presents a novel neural material relighting method for revisualizing a photograph of a planar spatially-varying material under novel viewing and lighting conditions. Our approach is motivated by the observation that the plausibility of a spatially varying material is judged purely on the visual appearance, not on the underlying distribution of appearance parameters. Therefore, instead of using an intermediate parametric representation (e.g., SVBRDF) that requires a rendering stage to visualize the spatially-varying material for novel viewing and lighting conditions, neural material relighting directly generates the target visual appearance. We explore and evaluate two different use cases where the relit results are either used directly, or where the relit images are used to enhance the input in existing multi-image spatially varying reflectance estimation methods. We demonstrate the robustness and efficacy for both use cases on a wide variety of spatially varying materials.",
    "scholar_publication": "ACM SIGGRAPH 2023 Conference …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_277",
    "authors": "Connor Lin, Koki Nagano, Jan Kautz, Eric Chan, Umar Iqbal, Leonidas Guibas, Gordon Wetzstein, Sameh Khamis",
    "title": "Single-shot Implicit Morphable Faces With Consistent Texture Parameterization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591494",
    "pdf_link": null,
    "abstract": "There is a growing demand for the accessible creation of high-quality 3D avatars that are animatable and customizable. Although 3D morphable models provide intuitive control for editing and animation, and robustness for single-view face reconstruction, they cannot easily capture geometric and appearance details. Methods based on neural implicit representations, such as signed distance functions (SDF) or neural radiance fields, approach photo-realism, but are difficult to animate and do not generalize well to unseen data. To tackle this problem, we propose a novel method for constructing implicit 3D morphable face models that are both generalizable and intuitive for editing. Trained from a collection of high-quality 3D scans, our face model is parameterized by geometry, expression, and texture latent codes with a learned SDF and explicit UV texture parameterization. Once trained, we can reconstruct an avatar from a single in-the-wild image by leveraging the learned prior to project the image into the latent space of our model. Our implicit morphable face models can be used to render an avatar from novel views, animate facial expressions by modifying expression codes, and edit textures by directly painting on the learned UV-texture maps. We demonstrate quantitatively and qualitatively that our method improves upon photo-realism, geometry, and expression accuracy compared to state-of-the-art methods.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_816",
    "authors": "Andrey Voynov, Kfir Aberman, Daniel Cohen-Or",
    "title": "Sketch-guided Text-to-image Diffusion Models",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591560",
    "pdf_link": null,
    "abstract": "Text-to-Image models have introduced a remarkable leap in the evolution of machine learning, demonstrating high-quality synthesis of images from a given text-prompt. However, these powerful pretrained models still lack control handles that can guide spatial properties of the synthesized images. In this work, we introduce a universal approach to guide a pretrained text-to-image diffusion model, with a spatial map from another domain (e.g., sketch) during inference time. Unlike previous works, our method does not require to train a dedicated model or a specialized encoder for the task. Our key idea is to train a Latent Guidance Predictor (LGP) - a small, per-pixel, Multi-Layer Perceptron (MLP) that maps latent features of noisy images to spatial maps, where the deep features are extracted from the core Denoising Diffusion Probabilistic Model (DDPM) network. The LGP is trained only on a few thousand images and constitutes a differential guiding map predictor, over which the loss is computed and propagated back to push the intermediate images to agree with the spatial map. The per-pixel training offers flexibility and locality which allows the technique to perform well on out-of-domain sketches, including free-hand style drawings. We take a particular focus on the sketch-to-image translation task, revealing a robust and expressive way to generate images that follow the guidance of a sketch of arbitrary style or domain.",
    "scholar_publication": "ACM SIGGRAPH 2023 conference …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_186",
    "authors": "Lin Gao, Feng-Lin Liu, Shu-Yu Chen, Kaiwen Jiang, Chun-Peng Li, Yu-Kun Lai, Hongbo Fu",
    "title": "SketchFaceNeRF: Sketch-based Facial Generation and Editing in Neural Radiance Fields",
    "paper_url": "https://orca.cardiff.ac.uk/id/eprint/159468/",
    "pdf_link": null,
    "abstract": "Realistic 3D facial generation based on Neural Radiance Fields (NeRFs) from 2D sketches benefits various applications. Despite the high realism of free-view rendering results of NeRFs, it is tedious and difficult for artists to achieve detailed 3D control and manipulation. Meanwhile, due to its conciseness and expressiveness, sketching has been widely used for 2D facial image generation and editing. Applying sketching to NeRFs is challenging due to the inherent uncertainty for 3D generation with 2D constraints, a significant gap in content richness when generating faces from sparse sketches, and potential inconsistencies for sequential multi-view editing given only 2D sketch inputs. To address these challenges, we present SketchFaceNeRF, a novel sketch-based 3D facial NeRF generation and editing method, to produce free-view photo-realistic images. To solve the challenge of sketch sparsity, we introduce a Sketch Tri-plane Prediction net to first inject the appearance into sketches, thus generating features given reference images to allow color and texture control. Such features are then lifted into compact 3D tri-planes to supplement the absent 3D information, which is important for improving robustness and faithfulness. However, during editing, consistency for unseen or unedited 3D regions is difficult to maintain due to limited spatial hints in sketches. We thus adopt a Mask Fusion module to transform free-view 2D masks (inferred from sketch editing operations) into the tri-plane space as 3D masks, which guide the fusion of the original and sketch-based generated faces to synthesize edited faces. We further design an optimization approach with a novel space loss to improve identity retention and editing faithfulness. Our pipeline enables users to flexibly manipulate faces from different viewpoints in 3D space, easily designing desirable facial models. Extensive experiments validate that our approach is superior to the state-of-the-art 2D sketch-based image generation and editing approaches in realism and faithfulness.",
    "scholar_publication": "ACM Transactions on …, 2023 - orca.cardiff.ac.uk"
  },
  {
    "paper_id": "papers_682",
    "authors": "Michal Piovarci, Alexandre Chapiro, Bernd Bickel",
    "title": "Skin-Screen: A Computational Fabrication Framework for Color Tattoos",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592432",
    "pdf_link": null,
    "abstract": "Tattoos are a highly popular medium, with both artistic and medical applications. Although the mechanical process of tattoo application has evolved historically, the results are reliant on the artisanal skill of the artist. This can be especially challenging for some skin tones, or in cases where artists lack experience. We provide the first systematic overview of tattooing as a computational fabrication technique. We built an automated tattooing rig and a recipe for the creation of silicone sheets mimicking realistic skin tones, which allowed us to create an accurate model predicting tattoo appearance. This enables several exciting applications including tattoo previewing, color retargeting, novel ink spectra optimization, color-accurate prosthetics, and more.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_459",
    "authors": "Jiong Chen, Fernando de Goes, Mathieu Desbrun",
    "title": "Somigliana Coordinates:  An Elasticity-derived Approach for Cage Deformation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591519",
    "pdf_link": null,
    "abstract": "In this paper, we present a novel cage deformer based on elasticity-derived matrix-valued coordinates. In order to bypass the typical shearing artifacts and lack of volume control of existing cage deformers, we promote a more elastic behavior of the cage deformation by deriving our coordinates from the Somigliana identity, a boundary integral formulation based on the fundamental solution of linear elasticity. Given an initial cage and its deformed pose, the deformation of the cage interior is deduced from these Somigliana coordinates via a corotational scheme, resulting in a matrix-weighted combination of both vertex positions and face normals of the cage. Our deformer thus generalizes Green coordinates, while producing physically-plausible spatial deformations that are invariant under similarity transformations and with interactive bulging control. We demonstrate the efficiency and versatility of our method through a series of examples in 2D and 3D.",
    "scholar_publication": "ACM SIGGRAPH 2023 Conference …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_699",
    "authors": "Alexandros D. Keros, Kartic Subr",
    "title": "Spectral Coarsening With Hodge Laplacians",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591544",
    "pdf_link": null,
    "abstract": "Many computational algorithms applied to geometry operate on discrete representations of shape. It is sometimes necessary to first simplify, or coarsen, representations found in modern datasets for practicable or expedited processing. The utility of a coarsening algorithm depends on both, the choice of representation as well as the specific processing algorithm or operator. e.g. simulation using the Finite Element Method, calculating Betti numbers, etc. We propose a novel method that can coarsen triangle meshes, tetrahedral meshes and simplicial complexes. Our method allows controllable preservation of salient features from the high-resolution geometry and can therefore be customized to different applications. Salient properties are typically captured by local shape descriptors via linear differential operators – variants of Laplacians. Eigenvectors of their discretized matrices yield a useful spectral domain for geometry processing (akin to the famous Fourier spectrum which uses eigenfunctions of the derivative operator). Existing methods for spectrum-preserving coarsening use zero-dimensional discretizations of Laplacian operators (defined on vertices). We propose a generalized spectral coarsening method that considers multiple Laplacian operators defined in different dimensionalities in tandem. Our simple algorithm greedily decides the order of contractions of simplices based on a quality function per simplex. The quality function quantifies the error due to removal of that simplex on a chosen band within the spectrum of the coarsened geometry.",
    "scholar_publication": "ACM SIGGRAPH 2023 Conference Proceedings, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_220",
    "authors": "Yingsi Qin, Wei-Yu Chen, Matthew O'Toole, Aswin C. Sankaranarayanan",
    "title": "Split-Lohmann Multifocal Displays",
    "paper_url": "https://par.nsf.gov/biblio/10433220",
    "pdf_link": null,
    "abstract": "… multifocal display using focus tunable lenses, and progressively work our way to the proposed Split-Lohmann display… for a multifocal display, which involves a translating display behind …",
    "scholar_publication": "ACM Transactions on …, 2023 - par.nsf.gov"
  },
  {
    "paper_id": "paperstog_113",
    "authors": "Beibei Wang, Wenhua Jin, Miloš Hašan, Ling-Qi Yan, Ling-Qi Yan, Beibei Wang",
    "title": "SpongeCake: A Layered Microflake Surface Appearance Model",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3546940",
    "pdf_link": null,
    "abstract": "In this article, we propose SpongeCake: A layered BSDF model where each layer is a volumetric scattering medium, defined using microflake or other phase functions. We omit any reflecting and refracting interfaces between the layers. The first advantage of this formulation is that an exact and analytic solution for single scattering, regardless of the number of volumetric layers, can be derived. We propose to approximate multiple scattering by an additional single-scattering lobe with modified parameters and a Lambertian lobe. We use a parameter mapping neural network to find the parameters of the newly added lobes to closely approximate the multiple scattering effect. Despite the absence of layer interfaces, we demonstrate that many common material effects can be achieved with layers of SGGX microflake and other volumes with appropriate parameters. A normal mapping effect can also be achieved through mapping of microflake orientations, which avoids artifacts common in standard normal maps. Thanks to the analytical formulation, our model is very fast to evaluate and sample. Through various parameter settings, our model is able to handle many types of materials, like plastics, wood, cloth, and so on, opening a number of practical applications.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2022 - dl.acm.org"
  },
  {
    "paper_id": "papers_679",
    "authors": "Kenji Tojo, Ariel Shamir, Bernd Bickel, Nobuyuki Umetani",
    "title": "Stealth Shaper: Reflectivity Optimization as Surface Stylization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591542",
    "pdf_link": null,
    "abstract": "We present a technique to optimize the reflectivity of a surface while preserving its overall shape. The naïve optimization of the mesh vertices using the gradients of reflectivity simulations results in undesirable distortion. In contrast, our robust formulation optimizes the surface normal as an independent variable that bridges the reflectivity term with differential rendering, and the regularization term with as-rigid-as-possible elastic energy. We further adaptively subdivide the input mesh to improve the convergence. Consequently, our method can minimize the retroreflectivity of a wide range of input shapes, resulting in sharply creased shapes ubiquitous among stealth aircraft and Sci-Fi vehicles. Furthermore, by changing the reward for the direction of the outgoing light directions, our method can be applied to other reflectivity design tasks, such as the optimization of architectural walls to concentrate light in a specific region. We have tested the proposed method using light-transport simulations and real-world 3D-printed objects.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_326",
    "authors": "Chenxi Liu, Toshiki Aoki, Mikhail Bessmeltsev, Alla Sheffer",
    "title": "StripMaker: Perception-driven Learned Vector Sketch Consolidation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592130",
    "pdf_link": null,
    "abstract": "Artist sketches often use multiple overdrawn strokes to depict a single intended curve. Humans effortlessly mentally consolidate such sketches by detecting groups of overdrawn strokes and replacing them with the corresponding intended curves. While this mental process is near instantaneous, manually annotating or retracing sketches to communicate this intended mental image is highly time consuming; yet most sketch applications are not designed to handle overdrawing and can only operate on overdrawing-free, consolidated sketches. We propose StripMaker, a new and robust learning based method for automatic consolidation of raw vector sketches. We avoid the need for an unsustainably large manually annotated learning corpus by leveraging observations about artist workflow and perceptual cues viewers employ when mentally consolidating sketches. We train two perception-aware classifiers that assess the likelihood that a pair of stroke groups jointly depicts the same intended curve: our first classifier is purely local and only accounts for the properties of the evaluated strokes; our second classifier incorporates global context and is designed to operate on approximately consolidated sketches. We embed these classifiers within a consolidation framework that leverages artist workflow: we first process strokes in the order they were drawn and use our local classifier to arrive at an approximate consolidation output, then use the contextual classifier to refine this output and finalize the consolidated result. We validate StripMaker by comparing its results to manual consolidation outputs and algorithmic alternatives. StripMaker achieves comparable performance to manual consolidation. In a comparative study participants preferred our results by a 53% margin over those of the closest algorithmic alternative (67% versus 14%, other/neither 19%).",
    "scholar_publication": "ACM Transactions on Graphics …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_454",
    "authors": "Lizhen Wang, Xiaochen Zhao, Jingxiang Sun, Yuxiang Zhang, Hongwen Zhang, Tao Yu, Yebin Liu",
    "title": "StyleAvatar: Real-time Photo-realistic Neural Portrait Avatar From a Single Video",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591517",
    "pdf_link": null,
    "abstract": "Face reenactment methods attempt to restore and re-animate portrait videos as realistically as possible. Existing methods face a dilemma in quality versus controllability: 2D GAN-based methods achieve higher image quality but suffer in fine-grained control of facial attributes compared with 3D counterparts. In this work, we propose StyleAvatar, a real-time photo-realistic portrait avatar reconstruction method using StyleGAN-based networks, which can generate high-fidelity portrait avatars with faithful expression control. We expand the capabilities of StyleGAN by introducing a compositional representation and a sliding window augmentation method, which enable faster convergence and improve translation generalization. Specifically, we divide the portrait scenes into three parts for adaptive adjustments: facial region, non-facial foreground region, and the background. Besides, our network leverages the best of UNet, StyleGAN and time coding for video learning, which enables high-quality video generation. Furthermore, a sliding window augmentation method together with a pre-training strategy are proposed to improve translation generalization and training performance, respectively. The proposed network can converge within two hours while ensuring high image quality and a forward rendering time of only 20 milliseconds. Furthermore, we propose a real-time live system, which further pushes research into applications. Results and experiments demonstrate the superiority of our method in terms of image quality, full portrait video generation, and real-time re-animation compared to existing facial reenactment methods. Training and inference code for this paper are at https://github.com/LizhenWangT/StyleAvatar.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_374",
    "authors": "Paul Zhang, Zoë Marschner, Justin Solomon, Rasmus Tamstorf",
    "title": "Sum-of-squares Collision Detection for Curved Shapes and Paths",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591507",
    "pdf_link": null,
    "abstract": "Sum-of-Squares Programming (SOSP) has recently been introduced to graphics as a unified way to address a large set of difficult problems involving higher order primitives. Unfortunately, a challenging aspect of this approach is the computational cost—especially for problems involving multiple geometries like collision detection. In this paper, we present techniques to reduce the cost of SOSP significantly. We use these improvements to speed up difficult problems like collision detection between Bézier triangles by as much as 300 ×. In addition, motivated by hair bundle simulation, we present SOSP based collision detection on the tapered cubic cylinder. We also present an algebraic formulation of rigid body motion enabling SOSP based collision detection for curved geometries and trajectories simultaneously. While these new formulations are complex, our speedups make them feasible. These advances improve the applicability of SOSP based collision detection and enable the continued progress of higher-order geometry processing.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_119",
    "authors": "Siyou Lin, Dong Xiao, Zuoqiang Shi, Bin Wang, Siyou Lin",
    "title": "Surface Reconstruction From Point Clouds Without Normals by Parametrizing the Gauss Formula",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3554730",
    "pdf_link": null,
    "abstract": "We propose Parametric Gauss Reconstruction (PGR) for surface reconstruction from point clouds without normals. Our insight builds on the Gauss formula in potential theory, which represents the indicator function of a region as an integral over its boundary. By viewing surface normals and surface element areas as unknown parameters, the Gauss formula interprets the indicator as a member of some parametric function space. We can solve for the unknown parameters using the Gauss formula and simultaneously obtain the indicator function. Our method bypasses the need for accurate input normals as required by most existing non-data-driven methods, while also exhibiting superiority over data-driven methods, since no training is needed. Moreover, by modifying the Gauss formula and employing regularization, PGR also adapts to difficult cases such as noisy inputs, thin structures, sparse or nonuniform points, for which accurate normal estimation becomes quite difficult. Our code is publicly available at https://github.com/jsnln/ParametricGaussRecon.",
    "scholar_publication": "ACM Transactions on Graphics, 2022 - dl.acm.org"
  },
  {
    "paper_id": "papers_501",
    "authors": "Hsueh-Ti Derek Liu, Mark Gillespie, Benjamin Chislett, Nicholas Sharp, Alec Jacobson, Keenan Crane",
    "title": "Surface Simplification Using Intrinsic Error Metrics",
    "paper_url": "https://par.nsf.gov/biblio/10468669",
    "pdf_link": null,
    "abstract": "This paper describes a method for fast simplification of surface meshes. Whereas past methods focus on visual appearance, our goal is to solve equations on the surface. Hence, rather than approximate the extrinsic geometry, we construct a coarseintrinsic triangulationof the input domain. In the spirit of thequadric error metric (QEM), we perform greedy decimation while agglomerating global information about approximation error. In lieu of extrinsic quadrics, however, we store intrinsic tangent vectors that track how far curvature drifts during simplification. This process also yields a bijective map between the fine and coarse mesh, and prolongation operators for both scalar- and vector-valued data. Moreover, we obtain hard guarantees on element quality via intrinsic retriangulation---a feature unique to the intrinsic setting. The overall payoff is a black box approach to geometry processing, which decouples mesh resolution from the size of matrices used to solve equations. We show how our method benefits several fundamental tasks, including geometric multigrid, all-pairs geodesic distance, mean curvature flow, geodesic Voronoi diagrams, and the discrete exponential map.",
    "scholar_publication": "ACM Transactions on …, 2023 - par.nsf.gov"
  },
  {
    "paper_id": "papers_481",
    "authors": "Yuanqi Li, Shun Liu, Xinran Yang, Jianwei Guo, Jie Guo, Yanwen Guo",
    "title": "Surface and Edge Detection for Primitive Fitting of Point Clouds",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591522",
    "pdf_link": null,
    "abstract": "Fitting primitives for point cloud data to obtain a structural representation has been widely adopted for reverse engineering and other graphics applications. Existing segmentation-based approaches only segment primitive patches but ignore edges that indicate boundaries of primitives, leading to inaccurate and incomplete reconstruction. To fill the gap, we present a novel surface and edge detection network (SED-Net) for accurate geometric primitive fitting of point clouds. The key idea is to learn parametric surfaces (including B-spline patches) and edges jointly that can be assembled into a regularized and seamless CAD model in one unified and efficient framework. SED-Net is equipped with a two-branch structure to extract type and edge features and geometry features of primitives. At the core of our network is a two-stage feature fusion mechanism to utilize the type, edge and geometry features fully. Precisely detected surface patches can be employed as contextual information to facilitate the detection of edges and corners. Benefiting from the simultaneous detection of surfaces and edges, we can obtain a parametric and compact model representation. This enables us to represent a CAD model with predefined primitive-specific meshes and also allows users to edit its shape easily. Extensive experiments and comparisons against previous methods demonstrate our effectiveness and superiority.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_104",
    "authors": "Mazdak Abulnaga, Oded Stein, Polina Golland, Justin Solomon, Mazdak Abulnaga",
    "title": "Symmetric Volume Maps: Order-invariant Volumetric Mesh Correspondence With Free Boundary",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3572897",
    "pdf_link": null,
    "abstract": "Although shape correspondence is a central problem in geometry processing, most methods for this task apply only to two-dimensional surfaces. The neglected task of volumetric correspondence—a natural extension relevant to shapes extracted from simulation, medical imaging, and volume rendering—presents unique challenges that do not appear in the two-dimensional case. In this work, we propose a method for mapping between volumes represented as tetrahedral meshes. Our formulation minimizes a distortion energy designed …",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_542",
    "authors": "Sirui Chen, Albert Wu, C. Karen Liu",
    "title": "Synthesizing Dexterous Nonprehensile Pregrasp for Ungraspable Objects",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591528",
    "pdf_link": null,
    "abstract": "Daily objects embedded in a contextual environment are often ungraspable initially. Whether it is a book sandwiched by other books on a fully packed bookshelf or a piece of paper lying flat on the desk, a series of nonprehensile pregrasp maneuvers is required to manipulate the object into a graspable state. Humans are proficient at utilizing environmental contacts to achieve manipulation tasks that are otherwise impossible, but synthesizing such nonprehensile pregrasp behaviors is challenging to existing methods. We present a novel method that combines graph search, optimal control, and a learning-based objective function to synthesize physically realistic and diverse nonprehensile pre-grasp motions that leverage the external contacts. Since the “graspability” of an object in context with its surrounding is difficult to define, we utilize a dataset of dexterous grasps to learn a metric which implicitly takes into account the exposed surface of the object and the finger tip locations. Our method can efficiently discover hand and object trajectories that are certified to be physically feasible by the simulation and kinematically achievable by the dexterous hand. We evaluate our method on eight challenging scenarios where nonprehensile pre-grasps are required to succeed. We also show that our method can be applied to unseen objects different from those in the training dataset. Finally, we report quantitative analyses on generalization and robustness of our method, as well as an ablation study.",
    "scholar_publication": "ACM SIGGRAPH 2023 Conference Proceedings, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_506",
    "authors": "Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael Black, Sanja Fidler, Xue Bin Peng",
    "title": "Synthesizing Physical Character-scene Interactions",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591525",
    "pdf_link": null,
    "abstract": "Movement is how people interact with and affect their environment. For realistic character animation, it is necessary to synthesize such interactions between virtual characters and their surroundings. Despite recent progress in character animation using machine learning, most systems focus on controlling an agent’s movements in fairly simple and homogeneous environments, with limited interactions with other objects. Furthermore, many previous approaches that synthesize human-scene interactions require significant manual labeling of the training data. In contrast, we present a system that uses adversarial imitation learning and reinforcement learning to train physically-simulated characters that perform scene interaction tasks in a natural and life-like manner. Our method learns scene interaction behaviors from large unstructured motion datasets, without manual annotation of the motion data. These scene interactions are learned using an adversarial discriminator that evaluates the realism of a motion within the context of a scene. The key novelty involves conditioning both the discriminator and the policy networks on scene context. We demonstrate the effectiveness of our approach through three challenging scene interaction tasks: carrying, sitting, and lying down, which require coordination of a character’s movements in relation to objects in the environment. Our policies learn to seamlessly transition between different behaviors like idling, walking, and sitting. By randomizing the properties of the objects and their placements during training, our method is able to generalize beyond the objects and scenarios depicted in the training dataset, producing natural character-scene interactions for a wide variety of object shapes and placements. The approach takes physics-based character motion generation a step closer to broad applicability. Please see our supplementary video for more results.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_341",
    "authors": "Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, Daniel Cohen-Or",
    "title": "TEXTure: Text-guided Texturing of 3D Shapes",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591503",
    "pdf_link": null,
    "abstract": "… TEXTure, a novel method for text-guided generation, editing, and transfer of textures for 3D shapes… diffusion model, TEXTure applies an iterative scheme that paints a 3D model from …",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_785",
    "authors": "Kavosh Jazar, Paul G. Kry",
    "title": "Temporal Set Inversion for Animated Implicits",
    "paper_url": "https://search.proquest.com/openview/9fde5cf6b4f96f470e489c7ebe5897a0/1?pq-origsite=gscholar&cbl=18750&diss=y",
    "pdf_link": null,
    "abstract": "We exploit the temporal coherence of closed-form animated implicit surfaces by locally re-evaluating an octree-like discretization of the implicit field only as and where is necessary to rigorously maintain a global error invariant over time, thereby saving resources in static or slowly-evolving areas where per-frame updates are not necessary. We treat implicit surface rendering as a special case of the continuous constraint satisfaction problem of set inversion, which seeks preimages of arbitrary sets under vector-valued functions. From this …",
    "scholar_publication": "2023 - search.proquest.com"
  },
  {
    "paper_id": "papers_753",
    "authors": "William Gao, Noam Aigerman, Thibault Groueix, Vova Kim, Rana Hanocka",
    "title": "TextDeformer: Geometry Manipulation Using Text Guidance",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591552",
    "pdf_link": null,
    "abstract": "We present a technique for automatically producing a deformation of an input triangle mesh, guided solely by a text prompt. Our framework is capable of deformations that produce both large, low-frequency shape changes, and small high-frequency details. Our framework relies on differentiable rendering to connect geometry to powerful pre-trained image encoders, such as CLIP and DINO. Notably, updating mesh geometry by taking gradient steps through differentiable rendering is notoriously challenging, commonly resulting in deformed meshes with significant artifacts. These difficulties are amplified by noisy and inconsistent gradients from CLIP. To overcome this limitation, we opt to represent our mesh deformation through Jacobians, which updates deformations in a global, smooth manner (rather than locally-sub-optimal steps). Our key observation is that Jacobians are a representation that favors smoother, large deformations, leading to a global relation between vertices and pixels, and avoiding localized noisy gradients. Additionally, to ensure the resulting shape is coherent from all 3D viewpoints, we encourage the deep features computed on the 2D encoding of the rendering to be consistent for a given vertex from all viewpoints. We demonstrate that our method is capable of smoothly-deforming a wide variety of source mesh and target text prompts, achieving both large modifications to, e.g., body proportions of animals, as well as adding fine semantic details, such as shoe laces on an army boot and fine details of a face.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_125",
    "authors": "Yana Nehmé, Johanna Delanoy, Florent Dupont, Jean-Philippe Farrugia, Patrick Le Callet, Guillaume Lavoué, Yana Nehmé",
    "title": "Textured Mesh Quality Assessment: Large-scale Dataset and Deep Learning-based Quality Metric",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592786",
    "pdf_link": null,
    "abstract": "Over the past decade, three-dimensional (3D) graphics have become highly detailed to mimic the real world, exploding their size and complexity. Certain applications and device constraints necessitate their simplification and/or lossy compression, which can degrade their visual quality. Thus, to ensure the best Quality of Experience, it is important to evaluate the visual quality to accurately drive the compression and find the right compromise between visual quality and data size. In this work, we focus on subjective and objective quality assessment of textured 3D meshes. We first establish a large-scale dataset, which includes 55 source models quantitatively characterized in terms of geometric, color, and semantic complexity, and corrupted by combinations of five types of compression-based distortions applied on the geometry, texture mapping, and texture image of the meshes. This dataset contains over 343k distorted stimuli. We propose an approach to select a challenging subset of 3,000 stimuli for which we collected 148,929 quality judgments from over 4,500 participants in a large-scale crowdsourced subjective experiment. Leveraging our subject-rated dataset, a learning-based quality metric for 3D graphics was proposed. Our metric demonstrates state-of-the-art results on our dataset of textured meshes and on a dataset of distorted meshes with vertex colors. Finally, we present an application of our metric and dataset to explore the influence of distortion interactions and content characteristics on the perceived quality of compressed textured meshes.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_108",
    "authors": "Avi M. Aizenman, George A. Koulieris, Agostino Gibaldi, Vibhor Sehgal, Dennis M. Levi, Martin S. Banks, Avi M. Aizenman",
    "title": "The Statistics of Eye Movements and Binocular Disparities in VR Gaming Headsets Should Drive Headset Design",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3549529",
    "pdf_link": null,
    "abstract": "… of the control system work to drive their respective outputs to the … that we hope will influence HMD and video-game design. … It would be useful to expand this analysis to other headsets …",
    "scholar_publication": "ACM transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_461",
    "authors": "Valentin Deschaintre, Julia Guerrero Viu, Diego Gutierrez, Tamy Boubekeur, Belen Masia",
    "title": "The Visual Language of Fabrics",
    "paper_url": "https://zaguan.unizar.es/record/130031",
    "pdf_link": null,
    "abstract": "… tractable we focus on the particular class of fabrics. We choose this class since fabrics exhibit … We first build a large dataset, text2fabric, relating photorealistic renderings of digital fabrics …",
    "scholar_publication": "2023 - zaguan.unizar.es"
  },
  {
    "paper_id": "papers_812",
    "authors": "Jin-San Cheng, Bingwei Zhang, Yikun Xiao, Ming Li",
    "title": "Topology Driven Approximation to Rational Surface-surface Intersection via Interval Algebraic Topology Analysis",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592452",
    "pdf_link": null,
    "abstract": "Computing the intersection between two parametric surfaces (SSI) is one of the most fundamental problems in geometric and solid modeling. Maintaining the SSI topology is critical to its computation robustness. We propose a topology-driven hybrid symbolic-numeric framework to approximate rational parametric surface-surface intersection (SSI) based on a concept of interval algebraic topology analysis (IATA), which configures within a 4D interval box the SSI topology. We map the SSI topology to an algebraic system's solutions within the framework, classify and enumerate all topological cases as a mixture of four fundamental cases (or their specific sub-cases). Various complicated topological situations are covered, such as cusp points or curves, tangent points (isolated or not) or curves, tiny loops, self-intersections, or their mixtures. The theoretical formulation is also implemented numerically using advanced real solution isolation techniques, and computed within a topology-driven framework which maximally utilizes the advantages of the topology maintenance of algebraic analysis, the robustness of iterative subdivision, and the efficiency of forward marching. The approach demonstrates improved robustness under benchmark topological cases when compared with available open-source and commercial solutions, including IRIT, SISL, and Parasolid.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_290",
    "authors": "Yunxiang Zhang, Kenneth Chen, Qi Sun",
    "title": "Toward Optimized VR/AR Ergonomics: Modeling and Predicting User Neck Muscle Contraction",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591495",
    "pdf_link": null,
    "abstract": "Ergonomic efficiency is essential to the mass and prolonged adoption of VR/AR experiences. While VR/AR head-mounted displays unlock users’ natural wide-range head movements during viewing, their neck muscle comfort is inevitably compromised by the added hardware weight. Unfortunately, little quantitative knowledge for understanding and addressing such an issue is available so far. Leveraging electromyography devices, we measure, model, and predict VR users’ neck muscle contraction levels (MCL) while they move their heads to interact with the virtual environment. Specifically, by learning from collected physiological data, we develop a bio-physically inspired computational model to predict neck MCL under diverse head kinematic states. Beyond quantifying the cumulative MCL of completed head movements, our model can also predict potential MCL requirements with target head poses only. A series of objective evaluations and user studies demonstrate its prediction accuracy and generality, as well as its ability in reducing users’ neck discomfort by optimizing the layout of visual targets. We hope this research will motivate new ergonomic-centered designs for VR/AR and interactive graphics applications. Source code is released at: https://github.com/NYU-ICL/xr-ergonomics-neck-comfort.",
    "scholar_publication": "ACM SIGGRAPH 2023 conference …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_521",
    "authors": "Brooke Krajancich, Petr Kellnhofer, Gordon Wetzstein",
    "title": "Towards Attention–Aware Foveated Rendering",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592406",
    "pdf_link": null,
    "abstract": "Foveated graphics is a promising approach to solving the bandwidth challenges of immersive virtual and augmented reality displays by exploiting the falloff in spatial acuity in the periphery of the visual field. However, the perceptual models used in these applications neglect the effects of higherlevel cognitive processing, namely the allocation of visual attention, and are thus overestimating sensitivity in the periphery in many scenarios. Here, we introduce the first attention-aware model of contrast sensitivity. We conduct user studies to measure contrast sensitivity under different attention distributions and show that sensitivity in the periphery drops significantly when the user is required to allocate attention to the fovea. We motivate the development of future foveation models with another user study and demonstrate that tolerance for foveation in the periphery is significantly higher when the user is concentrating on a task in the fovea. Analysis of our model predicts significant bandwidth savings over those afforded by current models. As such, our work forms the foundation for attention-aware foveated graphics techniques.",
    "scholar_publication": "ACM Transactions on Graphics …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_458",
    "authors": "Elena Garces, Victor Arellano, Carlos Rodriguez-Pardo, David Pascual, Sergio Suja, Jorge Lopez-Moreno",
    "title": "Towards Material Digitization With a Dual-scale Optical System",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592147",
    "pdf_link": null,
    "abstract": "Existing devices for measuring material appearance in spatially-varying samples are limited to a single scale, either micro or mesoscopic. This is a practical limitation when the material has a complex multi-scale structure. In this paper, we present a system and methods to digitize materials at two scales, designed to include high-resolution data in spatially-varying representations at larger scales. We design and build a hemispherical light dome able to digitize flat material samples up to 11x11cm. We estimate geometric properties, anisotropic reflectance and transmittance at the microscopic level using polarized directional lighting with a single orthogonal camera. Then, we propagate this structured information to the mesoscale, using a neural network trained with the data acquired by the device and image-to-image translation methods. To maximize the compatibility of our digitization, we leverage standard BSDF models commonly adopted in the industry. Through extensive experiments, we demonstrate the precision of our device and the quality of our digitization process using a set of challenging real-world material samples and validation scenes. Further, we demonstrate the optical resolution and potential of our device for acquiring more complex material representations by capturing microscopic attributes which affect the global appearance: we characterize the properties of textile materials such as the yarn twist or the shape of individual fly-out fibers. We also release the SEDDIDOME dataset of materials, including raw data captured by the machine and optimized parameteres.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_687",
    "authors": "Philip Voglreiter, Bernhard Kerbl, Alexander Weinrauch, Joerg Hermann Mueller, Thomas Neff, Markus Steinberger, Dieter Schmalstieg",
    "title": "Trim Regions for Online Computation of From-Region Potentially Visible Sets",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592434",
    "pdf_link": null,
    "abstract": "Visibility computation is a key element in computer graphics applications. More specifically, a from-region potentially visible set (PVS) is an established tool in rendering acceleration, but its high computational cost means a from-region PVS is almost always precomputed. Precomputation restricts the use of PVS to static scenes and leads to high storage cost, in particular, if we need fine-grained regions. For dynamic applications, such as streaming content over a variable-bandwidth network, online PVS computation with configurable region size is required. We address this need with trim regions, a new method for generating from-region PVS for arbitrary scenes in real time. Trim regions perform controlled erosion of object silhouettes in image space, implicitly applying the shrinking theorem known from previous work. Our algorithm is the first that applies automatic shrinking to unconstrained 3D scenes, including non-manifold meshes, and does so in real time using an efficient GPU execution model. We demonstrate that our algorithm generates a tight PVS for complex scenes and outperforms previous online methods for from-viewpoint and from-region PVS. It runs at 60 Hz for realistic game scenes consisting of millions of triangles and computes PVS with a tightness matching or surpassing existing approaches.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "paperstog_123",
    "authors": "Jie Guo, Shuichang Lai, Qinghao Tu, Chengzhi Tao, Changqing Zou, Yanwen Guo, Jie Guo",
    "title": "Ultra-high Resolution SVBRDF Recovery From a Single Image",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3593798",
    "pdf_link": null,
    "abstract": "Existing convolutional neural networks have achieved great success in recovering Spatially Varying Bidirectional Surface Reflectance Distribution Function (SVBRDF) maps from a single image. However, they mainly focus on handling low-resolution (e.g., 256 × 256) inputs. Ultra-High Resolution (UHR) material maps are notoriously difficult to acquire by existing networks because (1) finite computational resources set bounds for input receptive fields and output resolutions, and (2) convolutional layers operate locally and lack the ability to capture long-range structural dependencies in UHR images. We propose an implicit neural reflectance model and a divide-and-conquer solution to address these two challenges simultaneously. We first crop a UHR image into low-resolution patches, each of which are processed by a local feature extractor to extract important details. To fully exploit long-range spatial dependency and ensure global coherency, we incorporate a global feature extractor and several coordinate-aware feature assembly modules into our pipeline. The global feature extractor contains several lightweight material vision transformers that have a global receptive field at each scale and have the ability to infer long-term relationships in the material. After decoding globally coherent feature maps assembled by coordinate-aware feature assembly modules, the proposed end-to-end method is able to generate UHR SVBRDF maps from a single image with fine spatial details and consistent global structures.",
    "scholar_publication": "ACM Transactions on Graphics, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_806",
    "authors": "Dani Valevski, Matan Kalman, Eyal Molad, Eyal Segalis, Yossi Matias, Yaniv Leviathan",
    "title": "UniTune: Text-driven Image Editing by Fine Tuning a Diffusion Model on a Single Image",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592451",
    "pdf_link": null,
    "abstract": "Text-driven image generation methods have shown impressive results recently, allowing casual users to generate high quality images by providing textual descriptions. However, similar capabilities for editing existing images are still out of reach. Text-driven image editing methods usually need edit masks, struggle with edits that require significant visual changes and cannot easily keep specific details of the edited portion. In this paper we make the observation that image-generation models can be converted to image-editing models simply by fine-tuning them on a single image. We also show that initializing the stochastic sampler with a noised version of the base image before the sampling and interpolating relevant details from the base image after sampling further increase the quality of the edit operation. Combining these observations, we propose UniTune, a novel image editing method. UniTune gets as input an arbitrary image and a textual edit description, and carries out the edit while maintaining high fidelity to the input image. UniTune does not require additional inputs, like masks or sketches, and can perform multiple edits on the same image without retraining. We test our method using the Imagen model in a range of different use cases. We demonstrate that it is broadly applicable and can perform a surprisingly wide range of expressive editing operations, including those requiring significant visual changes that were previously impossible.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_213",
    "authors": "Dongliang Cao, Paul Roetzer, Florian Bernard",
    "title": "Unsupervised Learning of Robust Spectral Shape Matching",
    "paper_url": "https://arxiv.org/abs/2304.14419",
    "pdf_link": null,
    "abstract": "We propose a novel learning-based approach for robust 3D shape matching. Our method builds upon deep functional maps and can be trained in a fully unsupervised manner. Previous deep functional map methods mainly focus on predicting optimised functional maps alone, and then rely on off-the-shelf post-processing to obtain accurate point-wise maps during inference. However, this two-stage procedure for obtaining point-wise maps often yields sub-optimal performance. In contrast, building upon recent insights about the relation between functional maps and point-wise maps, we propose a novel unsupervised loss to couple the functional maps and point-wise maps, and thereby directly obtain point-wise maps without any post-processing. Our approach obtains accurate correspondences not only for near-isometric shapes, but also for more challenging non-isometric shapes and partial shapes, as well as shapes with different discretisation or topological noise. Using a total of nine diverse datasets, we extensively evaluate the performance and demonstrate that our method substantially outperforms previous state-of-the-art methods, even compared to recent supervised methods. Our code is available at https://github.com/dongliangcao/Unsupervised-Learning-of-Robust-Spectral-Shape-Matching.",
    "scholar_publication": "arXiv preprint arXiv:2304.14419, 2023 - arxiv.org"
  },
  {
    "paper_id": "papers_380",
    "authors": "Guoqing Yang, Fuyou Xue, Qi Zhang, Ke Xie, Chi-Wing Fu, Hui Huang",
    "title": "UrbanBIS: A Large-scale Benchmark for Fine-grained Urban Building Instance Segmentation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591508",
    "pdf_link": null,
    "abstract": "We present the UrbanBIS benchmark for large-scale 3D urban understanding, supporting practical urban-level semantic and building-level instance segmentation. UrbanBIS comprises six real urban scenes, with 2.5 billion points, covering a vast area of 10.78 km2 and 3,370 buildings, captured by 113,346 views of aerial photogrammetry. Particularly, UrbanBIS provides not only semantic-level annotations on a rich set of urban objects, including buildings, vehicles, vegetation, roads, and bridges, but also instance-level annotations on the buildings. Further, UrbanBIS is the first 3D dataset that introduces fine-grained building sub-categories, considering a wide variety of shapes for different building types. Besides, we propose B-Seg, a building instance segmentation method to establish UrbanBIS. B-Seg adopts an end-to-end framework with a simple yet effective strategy for handling large-scale point clouds. Compared with mainstream methods, B-Seg achieves better accuracy with faster inference speed on UrbanBIS. In addition to the carefully-annotated point clouds, UrbanBIS provides high-resolution aerial-acquisition photos and high-quality large-scale 3D reconstruction models, which shall facilitate a wide range of studies such as multi-view stereo, urban LOD generation, aerial path planning, autonomous navigation, road network extraction, and so on, thus serving as an important platform for many intelligent city applications. UrbanBIS and related code can be downloaded at https://vcc.tech/UrbanBIS.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_206",
    "authors": "Yu Wang, Minghao Guo, Justin Solomon",
    "title": "Variational Quasi-harmonic Maps for Computing Diffeomorphisms",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592105",
    "pdf_link": null,
    "abstract": "Computation of injective (or inversion-free) maps is a key task in geometry processing, physical simulation, and shape optimization. Despite being a longstanding problem, it remains challenging due to its highly nonconvex and combinatoric nature. We propose computation of variational quasi-harmonic maps to obtain smooth inversion-free maps. Our work is built on a key observation about inversion-free maps: A planar map is a diffeomorphism if and only if it is quasi-harmonic and satisfies a special Cauchy boundary condition. We hence equate the inversion-free mapping problem to an optimal control problem derived from our theoretical result, in which we search in the space of parameters that define an elliptic PDE. We show that this problem can be solved by minimizing within a family of functionals. Similarly, our discretized functionals admit exactly injective maps as the minimizers, empirically producing inversion-free discrete maps of triangle meshes. We design efficient numerical procedures for our problem that prioritize robust convergence paths. Experiments show that on challenging examples our methods can achieve up to orders of magnitude improvement over state-of-the-art, in terms of speed or quality. Moreover, we demonstrate how to optimize a generic energy in our framework while restricting to quasi-harmonic maps.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_549",
    "authors": "Tong Zhao, Laurent Busé, David Cohen-Steiner, Tamy Boubekeur, Jean-Marc Thiery, Pierre Alliez",
    "title": "Variational Shape Reconstruction via Quadric Error Metrics",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591529",
    "pdf_link": null,
    "abstract": "Inspired by the strengths of quadric error metrics initially designed for mesh decimation, we propose a concise mesh reconstruction approach for 3D point clouds. Our approach proceeds by clustering the input points enriched with quadric error metrics, where the generator of each cluster is the optimal 3D point for the sum of its quadric error metrics. This approach favors the placement of generators on sharp features, and tends to equidistribute the error among clusters. We reconstruct the output surface mesh from the adjacency between clusters and a constrained binary solver. We combine our clustering process with an adaptive refinement driven by the error. Compared to prior art, our method avoids dense reconstruction prior to simplification and produces immediately an optimized mesh.",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_625",
    "authors": "Nagabhushan Somraj, Rajiv Soundararajan",
    "title": "ViP-NeRF: Visibility Prior for Sparse Input Neural Radiance Fields",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591539",
    "pdf_link": null,
    "abstract": "Neural radiance fields (NeRF) have achieved impressive performances in view synthesis by encoding neural representations of a scene. However, NeRFs require hundreds of images per scene to synthesize photo-realistic novel views. Training them on sparse input views leads to overfitting and incorrect scene depth estimation resulting in artifacts in the rendered novel views. Sparse input NeRFs were recently regularized by providing dense depth estimated from pre-trained networks as supervision, to achieve improved performance over sparse depth constraints. However, we find that such depth priors may be inaccurate due to generalization issues. Instead, we hypothesize that the visibility of pixels in different input views can be more reliably estimated to provide dense supervision. In this regard, we compute a visibility prior through the use of plane sweep volumes, which does not require any pre-training. By regularizing the NeRF training with the visibility prior, we successfully train the NeRF with few input views. We reformulate the NeRF to also directly output the visibility of a 3D point from a given viewpoint to reduce the training time with the visibility constraint. On multiple datasets, our model outperforms the competing sparse input NeRF models including those that use learned priors. The source code for our model can be found on our project page: https://nagabhushansn95.github.io/publications/2023/ViP-NeRF.html.",
    "scholar_publication": "ACM SIGGRAPH 2023 conference …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_558",
    "authors": "Emilie Yu, Cuong Nguyen, Kevin Matzen, Oliver Wang, Rubaiat Habib Kazi, Adrien Bousseau",
    "title": "VideoDoodles: Hand-drawn Animations on Videos With Scene-aware Canvases",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592413",
    "pdf_link": null,
    "abstract": "We present an interactive system to ease the creation of so-called video doodles - videos on which artists insert hand-drawn animations for entertainment or educational purposes. Video doodles are challenging to create because to be convincing, the inserted drawings must appear as if they were part of the captured scene. In particular, the drawings should undergo tracking, perspective deformations and occlusions as they move with respect to the camera and to other objects in the scene - visual effects that are difficult to reproduce with existing 2D video editing software. Our system supports these effects by relying on planar canvases that users position in a 3D scene reconstructed from the video. Furthermore, we present a custom tracking algorithm that allows users to anchor canvases to static or dynamic objects in the scene, such that the canvases move and rotate to follow the position and direction of these objects. When testing our system, novices could create a variety of short animated clips in a dozen of minutes, while professionals praised its speed and ease of use compared to existing tools.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_642",
    "authors": "Diego Royo, Talha Sultan, Adolfo Muñoz, Khadijeh Masumnia-Bisheh, Eric Brandt, Diego Gutierrez, Andreas Velten, Julio Marco",
    "title": "Virtual Mirrors: Non-line-of-sight Imaging Beyond the Third Bounce",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592429",
    "pdf_link": null,
    "abstract": "Non-line-of-sight (NLOS) imaging methods are capable of reconstructing complex scenes that are not visible to an observer using indirect illumination. However, they assume only third-bounce illumination, so they are currently limited to single-corner configurations, and present limited visibility when imaging surfaces at certain orientations. To reason about and tackle these limitations, we make the key observation that planar diffuse surfaces behave specularly at wavelengths used in the computational wave-based NLOS imaging domain. We call such surfaces virtual mirrors. We leverage this observation to expand the capabilities of NLOS imaging using illumination beyond the third bounce, addressing two problems: imaging single-corner objects at limited visibility angles, and imaging objects hidden behind two corners. To image objects at limited visibility angles, we first analyze the reflections of the known illuminated point on surfaces of the scene as an estimator of the position and orientation of objects with limited visibility. We then image those limited visibility objects by computationally building secondary apertures at other surfaces that observe the target object from a direct visibility perspective. Beyond single-corner NLOS imaging, we exploit the specular behavior of virtual mirrors to image objects hidden behind a second corner by imaging the space behind such virtual mirrors, where the mirror image of objects hidden around two corners is formed. No specular surfaces were involved in the making of this paper.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_496",
    "authors": "Rohan Sawhney, Bailey Miller, Ioannis Gkioulekas, Keenan Crane",
    "title": "Walk on Stars: A Grid-free Monte Carlo Method for PDEs With Neumann Boundary Conditions",
    "paper_url": "https://arxiv.org/abs/2302.11815",
    "pdf_link": null,
    "abstract": "Grid-free Monte Carlo methods based on the walk on spheres (WoS) algorithm solve fundamental partial differential equations (PDEs) like the Poisson equation without discretizing the problem domain or approximating functions in a finite basis. Such methods hence avoid aliasing in the solution, and evade the many challenges of mesh generation. Yet for problems with complex geometry, practical grid-free methods have been largely limited to basic Dirichlet boundary conditions. We introduce the walk on stars (WoSt) algorithm, which solves linear elliptic PDEs with arbitrary mixed Neumann and Dirichlet boundary conditions. The key insight is that one can efficiently simulate reflecting Brownian motion (which models Neumann conditions) by replacing the balls used by WoS with star-shaped domains. We identify such domains via the closest point on the visibility silhouette, by simply augmenting a standard bounding volume hierarchy with normal information. Overall, WoSt is an easy modification of WoS, and retains the many attractive features of grid-free Monte Carlo methods such as progressive and view-dependent evaluation, trivial parallelization, and sublinear scaling to increasing geometric detail.",
    "scholar_publication": "arXiv preprint arXiv …, 2023 - arxiv.org"
  },
  {
    "paper_id": "papers_499",
    "authors": "Nicole Feng, Mark Gillespie, Keenan Crane",
    "title": "Winding Numbers on Discrete Surfaces",
    "paper_url": "https://par.nsf.gov/biblio/10468668",
    "pdf_link": null,
    "abstract": "In the plane, thewinding numberis the number of times a curve wraps around a given point. Winding numbers are a basic component of geometric algorithms such as point-in-polygon tests, and their generalization to data with noise or topological errors has proven valuable for geometry processing tasks ranging from surface reconstruction to mesh booleans. However, standard definitions do not immediately apply on surfaces, where not all curves bound regions. We develop a meaningful generalization, starting with the well-known relationship between winding numbers and harmonic functions. By processing the derivatives of such functions, we can robustly filter out components of the input that do not bound any region. Ultimately, our algorithm yields (i) a closed, completed version of the input curves, (ii) integer labels for regions that are meaningfully bounded by these curves, and (iii) the complementary curves that do not bound any region. The main computational cost is solving a standard Poisson equation, or for surfaces with nontrivial topology, a sparse linear program. We also introduce special basis functions to represent singularities that naturally occur at endpoints of open curves.",
    "scholar_publication": "ACM Transactions on Graphics, 2023 - par.nsf.gov"
  },
  {
    "paper_id": "papers_301",
    "authors": "Shir Iluz, Yael Vinker, Amir Hertz, Daniel Berio, Daniel Cohen-Or, Ariel Shamir",
    "title": "Word-as-image for Semantic Typography",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592123",
    "pdf_link": null,
    "abstract": "A word-as-image is a semantic typography technique where a word illustration presents a visualization of the meaning of the word, while also preserving its readability. We present a method to create word-as-image illustrations automatically. This task is highly challenging as it requires semantic understanding of the word and a creative idea of where and how to depict these semantics in a visually pleasing and legible manner. We rely on the remarkable ability of recent large pretrained language-vision models to distill textual concepts visually. We target simple, concise, black-and-white designs that convey the semantics clearly. We deliberately do not change the color or texture of the letters and do not use embellishments. Our method optimizes the outline of each letter to convey the desired concept, guided by a pretrained Stable Diffusion model. We incorporate additional loss terms to ensure the legibility of the text and the preservation of the style of the font. We show high quality and engaging results on numerous examples and compare to alternative techniques. Code and demo will be available at our project page.",
    "scholar_publication": "ACM Transactions on …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_431",
    "authors": "Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, Jun-Yan Zhu",
    "title": "Zero-shot Image-to-image Translation",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3588432.3591513",
    "pdf_link": null,
    "abstract": "… -zero, a diffusion-based image-to-image translation method that allows users to specify the edit direction on-the-fly (eg, cat → dog). We perform various translation tasks on both real (top …",
    "scholar_publication": "ACM SIGGRAPH 2023 …, 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_721",
    "authors": "Minseok Chae, Kiseung Bang, Dongheon Yoo, Yoonchan Jeong",
    "title": "Étendue Expansion in Holographic Near Eye Displays Through Sparse Eye-box Generation Using Lens Array Eyepiece",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592441",
    "pdf_link": null,
    "abstract": "In this paper, we present a novel method the étendue expansion of near-eye holographic displays through the generation of a sparse eye-box. Conventional holographic near-eye displays have suffered from narrow field of view or narrow eye-box due to the limited étendue supported by a spatial light modulator. We focus on the fact that these displays typically form a dense eye-box, which could be an excessive investment of the limited étendue. By rearranging the eye-box in a sparse manner, the practical étendue can be extended. With a properly designed sparse eye-box shape, it can provide the 3D holographic images and ensure continuous light entrance to the pupil. To create a sparse eye-box, we utilize a lens array as an eyepiece lens. We optimize the spatial light modulator's phase profile for the proposed system and analyze the impact of the use of the lens array eyepiece on the holographic image quality. In particular, we focus on the lens array specification of the lenslet pitch and the focal length, deriving feasible specifications based on our analysis. We experimentally demonstrate a near eye see-through display using the proposed system and verify the étendue expansion.",
    "scholar_publication": "ACM Transactions on Graphics (TOG), 2023 - dl.acm.org"
  },
  {
    "paper_id": "papers_448",
    "authors": "Zeqiang Lai, Kaixuan Wei, Ying Fu, Philipp Härtel, Felix Heide",
    "title": "∇-Prox: Differentiable Proximal Algorithm Modeling for Large-scale Optimization",
    "paper_url": "https://dl.acm.org/doi/abs/10.1145/3592144",
    "pdf_link": null,
    "abstract": "Tasks across diverse application domains can be posed as large-scale optimization problems, these include graphics, vision, machine learning, imaging, health, scheduling, planning, and energy system forecasting. Independently of the application domain, proximal algorithms have emerged as a formal optimization method that successfully solves a wide array of existing problems, often exploiting problem-specific structures in the optimization. Although model-based formal optimization provides a principled approach to problem modeling with convergence guarantees, at first glance, this seems to be at odds with black-box deep learning methods. A recent line of work shows that, when combined with learning-based ingredients, model-based optimization methods are effective, interpretable, and allow for generalization to a wide spectrum of applications with little or no extra training data. However, experimenting with such hybrid approaches for different tasks by hand requires domain expertise in both proximal optimization and deep learning, which is often error-prone and time-consuming. Moreover, naively unrolling these iterative methods produces lengthy compute graphs, which when differentiated via autograd techniques results in exploding memory consumption, making batch-based training challenging. In this work, we introduce ∇-Prox, a domain-specific modeling language and compiler for large-scale optimization problems using differentiable proximal algorithms. ∇-Prox allows users to specify optimization objective functions of unknowns concisely at a high level, and intelligently compiles the problem into compute and memory-efficient differentiable solvers. One of the core features of ∇-Prox is its full differentiability, which supports hybrid model- and learning-based solvers integrating proximal optimization with neural network pipelines. Example applications of this methodology include learning-based priors and/or sample-dependent inner-loop optimization schedulers, learned with deep equilibrium learning or deep reinforcement learning. With a few lines of code, we show ∇-Prox can generate performant solvers for a range of image optimization problems, including end-to-end computational optics, image deraining, and compressive magnetic resonance imaging. We also demonstrate ∇-Prox can be used in a completely orthogonal application domain of energy system planning, an essential task in the energy crisis and the clean energy transition, where it outperforms state-of-the-art CVXPY and commercial Gurobi solvers.",
    "scholar_publication": "ACM Transactions on Graphics …, 2023 - dl.acm.org"
  }
]