[
  {
    "paper_id": "nvidia_0",
    "authors": "Rohan Yadav, Shiv Sundrum, Wonchan Lee, Michael Garland, Michael Bauer, Alex Aiken, Fredrik Kjolstad",
    "title": "Composing Distributed Computations Through Task and Kernel Fusion",
    "paper_url": "https://research.nvidia.com/publication/2025-03_composing-distributed-computations-through-task-and-kernel-fusion",
    "pdf_link": "https://arxiv.org/pdf/2406.18109.pdf",
    "abstract": "We introduce Diffuse, a system that dynamically performs task and kernel fusion in distributed, task-based runtime systems. The key component of Diffuse is an intermediate representation of distributed computation that enables the necessary analyses for the fusion of distributed tasks to be performed in a scalable manner. We pair task fusion with a JIT compiler to fuse together the kernels within fused tasks. We show empirically that Diffuse\u2019s intermediate representation is general enough to be a target for two real-world, task-based libraries (cuPyNumeric and Legate Sparse), letting Diffuse find optimization opportunities across function and library boundaries. Diffuse accelerates unmodified applications developed by composing task-based libraries by 1.86x on average (geo-mean), and by between 0.93x\u201310.7x on up to 128 GPUs. Diffuse also finds optimization opportunities missed by the original application developers, enabling high-level Python programs to match or exceed the performance of an explicitly parallel MPI library.",
    "topics": [
      "High Performance Computing"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on a system for task and kernel fusion in distributed computation, with no mention of deep - learning models (especially transformer - based), large - scale data training, or models for general - purpose use across multiple domains. It is mainly about optimizing distributed computing systems rather than foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on a software system for task and kernel fusion in distributed computation. It does not mention any concepts related to robotic hardware, input sensors, mechanical kinematics, or learning - based control algorithms for robots. It is a purely algorithmic and software - only implementation without clear connections to robotic systems or hardware."
  },
  {
    "paper_id": "nvidia_1",
    "authors": "Rohan Yadav, Michael Bauer, David Broman, Michael Garland, Alex Aiken, Fredrik Kjolstad",
    "title": "Automatic Tracing in Task-Based Runtime Systems",
    "paper_url": "https://research.nvidia.com/publication/2025-03_automatic-tracing-task-based-runtime-systems",
    "pdf_link": "https://arxiv.org/pdf/2406.18111.pdf",
    "abstract": "Implicitly parallel task-based runtime systems often perform dynamic analysis to discover dependencies in and extract parallelism from sequential programs. Dependence analysis becomes expensive as task granularity drops below a threshold. Tracing techniques have been developed where programmers annotate repeated program fragments (traces) issued by the application, and the runtime system memoizes the dependence analysis for those fragments, greatly reducing overhead when the fragments are executed again. However, manual trace annotation can be brittle and not easily applicable to complex programs built through the composition of independent components. We introduce Apophenia, a system that automatically traces the dependence analysis of task-based runtime systems, removing the burden of manual annotations from programmers and enabling new and complex programs to be traced. Apophenia identifies traces dynamically through a series of dynamic string analyses, which find repeated program fragments in the stream of tasks issued to the runtime system. We show that Apophenia is able to come between 0.92x\u20131.03x the performance of manually traced programs, and is able to effectively trace previously untraced programs to yield speedups of between 0.91x\u20132.82x on the Perlmutter and Eos supercomputers.",
    "topics": [
      "High Performance Computing"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on automatic tracing in task - based runtime systems to reduce dependence analysis overhead, without involving deep - learning models (especially transformer - based), large - scale data training, or serving as a general - purpose backbone for multiple downstream tasks across domains.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on automatic tracing in task - based runtime systems, involving software - only techniques for sequential program analysis and parallelism extraction. It lacks any mention of robotic hardware systems, input sensors, mechanical kinematics, or learning - based algorithms for controlling such systems, and is thus outside the robotics research domain."
  },
  {
    "paper_id": "nvidia_2",
    "authors": "Ci-Siang Lin, Chien-Yi Wang, Frank Wang, Min-Hung Chen",
    "title": "Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation",
    "paper_url": "https://research.nvidia.com/publication/2025-02_semantic-prompt-learning-weakly-supervised-semantic-segmentation",
    "pdf_link": "https://arxiv.org/pdf/2401.11791.pdf",
    "abstract": "Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation models using image data with only image-level supervision. Since precise pixel-level annotations are not accessible, existing methods typically focus on producing pseudo masks for training segmentation models by refining CAM-like heatmaps. However, the produced heatmaps may capture only the discriminative image regions of object categories or the associated co-occurring backgrounds. To address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS) framework, which learns to effectively prompt the CLIP latent space to enhance the semantic alignment between the segmented regions and the target object categories. More specifically, we propose Contrastive Prompt Learning and Prompt-guided Semantic Refinement to learn the prompts that adequately describe and suppress the co-occurring backgrounds associated with each object category. In this way, SemPLeS can perform better semantic alignment between object regions and class labels, resulting in desired pseudo masks for training segmentation models. The proposed SemPLeS framework achieves competitive performance on standard WSSS benchmarks, PASCAL VOC 2012 and MS COCO 2014, and shows compatibility with other WSSS methods.",
    "topics": [
      "Computer Vision"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on weakly - supervised semantic segmentation, a narrow application. It proposes a framework to address issues in this specific task and there is no mention of large - scale pre - trained models, multi - modality, or cross - domain applications, so it lacks broader foundational impact on general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on a weakly - supervised semantic segmentation framework, which is a purely algorithmic study without any clear connections to robotic systems, hardware components, or any of the key indicators of robotics research."
  },
  {
    "paper_id": "nvidia_3",
    "authors": "Ming-Yu Liu, Many other contributors at https://d1qx31qr3h6wln.cloudfront.net/publications/NVIDIA%20Cosmos_4.pdf",
    "title": "Cosmos World Foundation Model Platform for Physical AI",
    "paper_url": "https://research.nvidia.com/publication/2025-01_cosmos-world-foundation-model-platform-physical-ai",
    "pdf_link": "https://arxiv.org/pdf/2501.03575.pdf",
    "abstract": "Physical AI needs to be trained digitally first. It needs a digital twin of itself, the policy model, and adigital twin of the world, the world model. In this paper, we present the Cosmos World Foundation ModelPlatform to help developers build customized world models for their Physical AI setups. We positiona world foundation model as a general-purpose world model that can be fine-tuned into customizedworld models for downstream applications. Our platform covers a video curation pipeline, pre-trainedworld foundation models, examples of post-training of pre-trained world foundation models, and videotokenizers. To help Physical AI builders solve the most critical problems of our society, we make ourplatform open-source and our models open-weight with permissive licenses available via NVIDIA Cosmos.",
    "topics": [
      "Generative AI",
      "Robotics"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper presents a Cosmos World Foundation Model Platform with pre - trained world foundation models. These models are described as general - purpose and can be fine - tuned for downstream applications, which aligns with the concept of foundation models capable of serving as backbones for various tasks, thus having broader implications for general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on a digital platform for building world models for Physical AI. It mainly involves software - only implementations like video curation, pre - trained models, and video tokenizers, without clear connections to robotic hardware systems, input sensors, mechanical kinematics, or learning - based algorithms controlling physical actuators."
  },
  {
    "paper_id": "nvidia_4",
    "authors": "Chun-Mao Lai, Hsiang-Chun Wang, Ping-Chun Hsieh, Frank Wang, Min-Hung Chen, Shao-Hua Sun",
    "title": "Diffusion-Reward Adversarial Imitation Learning",
    "paper_url": "https://research.nvidia.com/publication/2024-12_diffusion-reward-adversarial-imitation-learning",
    "pdf_link": "https://arxiv.org/pdf/2405.16194.pdf",
    "abstract": "Imitation learning aims to learn a policy from observing expert demonstrations without access to reward signals from environments. Generative adversarial imitation learning (GAIL) formulates imitation learning as adversarial learning, employing a generator policy learning to imitate expert behaviors and discriminator learning to distinguish the expert demonstrations from agent trajectories. Despite its encouraging results, GAIL training is often brittle and unstable. Inspired by the recent dominance of diffusion models in generative modeling, this work proposes Diffusion-Reward Adversarial Imitation Learning (DRAIL), which integrates a diffusion model into GAIL, aiming to yield more precise and smoother rewards for policy learning. Specifically, we propose a diffusion discriminative classifier to construct an enhanced discriminator; then, we design diffusion rewards based on the classifier's output for policy learning. We conduct extensive experiments in navigation, manipulation, and locomotion, verifying DRAIL's effectiveness compared to prior imitation learning methods. Moreover, additional experimental results demonstrate the generalizability and data efficiency of DRAIL. Visualized learned reward functions of GAIL and DRAIL suggest that DRAIL can produce more precise and smoother rewards. Code available at this https URL.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Generative AI",
      "Robotics"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper integrates a diffusion model into generative adversarial imitation learning. Diffusion models are a key indicator of foundation model research. The proposed method shows generalizability and is tested in multiple domains (navigation, manipulation, and locomotion), indicating broader impact on general - purpose AI capabilities.",
    "is_robotics": true,
    "robotic_reason": "The paper focuses on imitation learning, which is relevant to robotics as an approach for physical systems. Additionally, the experiments are conducted in navigation, manipulation, and locomotion, which are well - known key indicators of robotics research, suggesting a connection to robotic systems and hardware."
  },
  {
    "paper_id": "nvidia_5",
    "authors": "Yoni Kasten, Wuyue Lu, Haggai Maron",
    "title": "Fast Encoder-Based 3D from Casual Videos via Point Track Processing",
    "paper_url": "https://research.nvidia.com/publication/2024-12_fast-encoder-based-3d-casual-videos-point-track-processing",
    "pdf_link": "https://arxiv.org/pdf/2404.07097.pdf",
    "abstract": "NVIDIA and our third-party partners use cookies and other tools to collect and record information you provide as well as information about your interactions with our websites for performance improvement, analytics, and to assist in our marketing efforts. By clicking Agree\" or \"Manage Settings\", you consent to the use of cookies and other tools as described in our Cookie Policy in accordance with your settings and accept our Terms of Service (which contains important waivers). Please see our Privacy Policy for more information on our privacy practices.",
    "topics": [
      "Computer Vision"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper title focuses on a specific 3D processing task from casual videos, and the abstract is about cookie and privacy policies. There are no mentions of key indicators related to foundation models, and it seems to be a narrow application without broader foundational impact.",
    "is_robotics": false,
    "robotic_reason": "The title focuses on 3D from casual videos via point track processing, and the abstract is about cookie policies and privacy practices. There are no robotics-related concepts, methodologies, or mentions of hardware components in the paper."
  },
  {
    "paper_id": "nvidia_6",
    "authors": "Yuchen Hu, Chen Chen, Huck Yang, Chengwei Qin, Pin-Yu Chen, Eng Siong Chng, Chao Zhang",
    "title": "Self-Taught Recognizer: Toward Unsupervised Adaptation for Speech Foundation Models",
    "paper_url": "https://research.nvidia.com/publication/2024-12_self-taught-recognizer-toward-unsupervised-adaptation-speech-foundation-models",
    "pdf_link": "https://arxiv.org/pdf/2405.14161.pdf",
    "abstract": "We propose an unsupervised adaptation framework, Self-TAught Recognizer (STAR), which leverages unlabeled data to enhance the robustness of automatic speech recognition (ASR) systems in diverse target domains, such as noise and accents. STAR is developed for prevalent speech foundation models based on Transformer-related architecture with auto-regressive decoding (e.g., Whisper, Canary). Specifically, we propose a novel indicator that empirically integrates step-wise information during decoding to assess the token-level quality of pseudo labels without ground truth, thereby guiding model updates for effective unsupervised adaptation. Experimental results show that STAR achieves an average of 13.5% relative reduction in word error rate across 14 target domains, and it sometimes even approaches the upper-bound performance of supervised adaptation. Surprisingly, we also observe that STAR prevents the adapted model from the common catastrophic forgetting problem without recalling source-domain data. Furthermore, STAR exhibits high data efficiency that only requires less than one-hour unlabeled data, and seamless generality to alternative large speech models and speech translation tasks",
    "topics": [
      "Generative AI",
      "Machine Translation",
      "Natural Language Processing",
      "Speech Processing"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper focuses on enhancing speech foundation models with Transformer - related architecture. It uses unlabeled data for unsupervised adaptation of these models across diverse target domains, which shows cross - domain applications. The proposed method can be applied to alternative large speech models and speech translation tasks, indicating broader implications for general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on an unsupervised adaptation framework for speech foundation models in automatic speech recognition systems. It involves purely algorithmic studies and software - only implementations without any clear connections to robotic systems, hardware components, or the key indicators of robotics research."
  },
  {
    "paper_id": "nvidia_7",
    "authors": "Chen Tessler, Kelly Guo, Ofir Nabati, Gal Chechik, Jason Peng",
    "title": "MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting",
    "paper_url": "https://research.nvidia.com/publication/2024-12_maskedmimic-unified-physics-based-character-control-through-masked-motion",
    "pdf_link": "https://arxiv.org/pdf/2409.14393.pdf",
    "abstract": "We introduce MaskedMimic a single unified controller for physically simulated humanoids. Our system is capable of generating a wide range of motions across diverse terrains from intuitive user-defined intents. In this work, we show several applications, including generating full-body motion from partial joint target positions, responding to joystick steering, engaging in object interactions, following paths, interpreting text commands, and even combining these modalities, such as executing text-stylized path following.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Computer Graphics",
      "Generative AI",
      "Robotics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper presents a controller for physically simulated humanoids focused on generating humanoid motions. It doesn't mention large - scale deep - learning models, pre - training on large data, and is mainly for the narrow application of humanoid motion control without broader foundational impact on general - purpose AI capabilities.",
    "is_robotics": true,
    "robotic_reason": "The paper focuses on a unified controller for physically simulated humanoids, which can generate a variety of motions. Concepts like motion generation and interaction with objects are key indicators of robotics research, and the use of physical simulation implies a connection to robotic hardware systems."
  },
  {
    "paper_id": "nvidia_8",
    "authors": "Brian Chao, Manu Gopakumar, Suyeon Choi, Liang Shi, Jonghyun Kim, Gordon Wetzstein",
    "title": "Large \u00c9tendue 3D Holographic Display with Content-adpative Dynamic Fourier Modulation",
    "paper_url": "https://research.nvidia.com/publication/2024-12_large-etendue-3d-holographic-display-content-adpative-dynamic-fourier",
    "pdf_link": "https://arxiv.org/pdf/2409.03143.pdf",
    "abstract": "Abstract",
    "topics": [
      "Computational Photography and Imaging",
      "VR, AR and Display Technology"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper title is about a 3D holographic display with dynamic Fourier modulation, and from the given information, there is no indication of research involving deep - learning models trained on large data, nor does it mention any of the key indicators related to foundation model research. It seems focused on a narrow, specialized application in the field of holographic displays.",
    "is_robotics": false,
    "robotic_reason": "The paper title is about a 3D holographic display with a modulation technique, and based on the provided information, there is no mention of hardware systems with sensors and mechanical kinematics, nor learning - based control algorithms, and no key robotics indicators are present."
  },
  {
    "paper_id": "nvidia_9",
    "authors": "Chen-Chia Chang, Chia-Tung (Mark) Ho, Yaguang Li, Yiran Chen, Haoxing (Mark) Ren",
    "title": "DRC-Coder: Automated DRC Checker Code Generation Using LLM Autonomous Agent",
    "paper_url": "https://research.nvidia.com/publication/2024-11_drc-coder-automated-drc-checker-code-generation-using-llm-autonomous-agent",
    "pdf_link": "https://arxiv.org/pdf/2412.05311.pdf",
    "abstract": "In the advanced technology nodes, the integrated design rule checker (DRC) is often utilized in place and route tools for fast optimization loops for power-performance-area. Implementing integrated DRC checkers to meet the standard of commercial DRC tools demands extensive human expertise to interpret foundry specifications, analyze layouts, and debug code iteratively. However, this labor-intensive process, requiring to be repeated by every update of technology nodes, prolongs the turnaround time of designing circuits. In this paper, we present DRC-Coder, a multi-agent framework with vision capabilities for automated DRC code generation. By incorporating vision language models and large language models (LLM), DRC-Coder can effectively process textual, visual, and layout information to perform rule interpretation and coding by two specialized LLMs. We also design an auto-evaluation function for LLMs to enable DRC code debugging. Experimental results show that targeting on a sub-3nm technology node for a state-of-the-art standard cell layout tool, DRC-Coder achieves perfect F1 score 1.000 in generating DRC codes for meeting the standard of a commercial DRC tool, highly outperforming standard prompting techniques (F1=0.631). DRC-Coder can generate code for each design rule within four minutes on average, which significantly accelerates technology advancement and reduces engineering costs.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Circuits and VLSI Design",
      "Generative AI"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper uses large language models (LLM) and vision language models, which are key indicators of foundation model research. These models are trained on large amounts of data and can handle multi - modal information (textual, visual, and layout), showing general - purpose capabilities applicable to the DRC code generation task, thus having broader implications beyond narrow applications.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on automated DRC code generation using language models for circuit design. It does not mention any hardware systems with input sensors and mechanical kinematics, nor does it involve learning - based algorithms for robotic control. It is a software - only implementation without physical components, thus falling outside the robotics research domain."
  },
  {
    "paper_id": "nvidia_10",
    "authors": "Rafael Valle, Rohan Badlani, Zhifeng Kong, Sang-gil Lee, Arushi Goel, Sungwon Kim, Joao Felipe Santos, Shuqi Dai, Siddharth Gururani, Aya AIJa'fari, Alex Liu, Kevin Shih, Wei Ping, Huck Yang, Bryan Catanzaro",
    "title": "Fugatto 1 - Foundational Generative Audio Transformer Opus 1",
    "paper_url": "https://research.nvidia.com/publication/2024-11_fugatto-1-foundational-generative-audio-transformer-opus-1",
    "pdf_link": "https://d1qx31qr3h6wln.cloudfront.net/publications/FUGATTO.pdf",
    "abstract": "Fugatto is a versatile audio synthesis and transformation model capable of followingfree-form text instructions with optional audio inputs. While large languagemodels (LLMs) trained with text on a simple next-token prediction objective canlearn to infer instructions directly from the data, models trained solely on audiodata lack this capacity. This is because audio data does not inherently contain theinstructions that were used to generate it. To overcome this challenge, we introducea specialized dataset generation approach optimized for producing a wide range ofaudio generation and transformation tasks, ensuring the data reveals meaningfulrelationships between audio and language. Another challenge lies in achievingcompositional abilities \u2013 such as combining, interpolating between, or negatinginstructions \u2013 using data alone. To address it, we propose ComposableART, aninference-time technique that extends classifier-free guidance to compositionalguidance. It enables the seamless and flexible composition of instructions, leadingto highly customizable audio outputs outside the training distribution. Our evaluationsacross a diverse set of tasks demonstrate that Fugatto performs competitivelywith specialized models, while ComposableART enhances its sonic palette andcontrol over synthesis. Most notably, we highlight our framework\u2019s ability tosynthesize emergent sounds \u2013 sonic phenomena that transcend conventional audiogeneration \u2013 unlocking new creative possibilities. Demo Website.",
    "topics": [
      "Generative AI"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper presents Fugatto, a model related to large language models (LLMs). It is trained on a specialized dataset for diverse audio generation and transformation tasks, showing general - purpose modeling and cross - domain application between audio and language, thus having broader implications for general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on audio synthesis and transformation models, specialized dataset generation for audio, and an inference - time technique for compositional guidance in audio. It contains no mention of robotics - related concepts, methodologies, or hardware components."
  },
  {
    "paper_id": "nvidia_11",
    "authors": "Yunchen Yu, Andrea Weidlich, Bruce Walter, Eugene d'Eon, Steve Marschner",
    "title": "Appearance Modeling of Iridescent Feathers with Diverse Nanostructures",
    "paper_url": "https://research.nvidia.com/publication/2024-11_appearance-modeling-iridescent-feathers-diverse-nanostructures",
    "pdf_link": "https://d1qx31qr3h6wln.cloudfront.net/publications/sAqyr-papers_998s4-file1.pdf",
    "abstract": "Many animals exhibit structural colors, which are often iridescent, meaning that the perceived colors change with illumination conditions and viewing perspectives. Biological iridescence is usually caused by multilayers or other periodic structures in animal tissues, which selectively reflect light of certain wavelengths and often result in a shiny appearance - which almost always comes with spatially varying highlights, thanks to randomness and irregularities in the structures. Previous models for biological iridescence tend to each target one specific structure, and most models only compute large-area averages, overlooking spatial variation in iridescent appearance.",
    "topics": [
      "Computer Graphics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on the appearance modeling of iridescent feathers with diverse nanostructures. It addresses a narrow, specialized application of modeling biological iridescence and does not involve deep - learning models trained on large data for generalized factual realities or have broader implications for general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on the appearance modeling of iridescent feathers, dealing with biological optics and appearance modeling. There are no mentions of robotic hardware systems, learning - based control algorithms, or any of the key indicators of robotics research in the title and abstract."
  },
  {
    "paper_id": "nvidia_12",
    "authors": "Clemens Eppner, Adithya Murali, Caelan Garrett, Rowland O'Flaherty, Tucker Hermans, Wei Yang, Dieter Fox",
    "title": "scene_synthesizer: A Python Library for Procedural Scene Generation in Robot Manipulation",
    "paper_url": "https://research.nvidia.com/publication/2024-11_scenesynthesizer-python-library-procedural-scene-generation-robot-manipulation",
    "pdf_link": null,
    "abstract": "NVIDIA and our third-party partners use cookies and other tools to collect and record information you provide as well as information about your interactions with our websites for performance improvement, analytics, and to assist in our marketing efforts. By clicking Agree\" or \"Manage Settings\", you consent to the use of cookies and other tools as described in our Cookie Policy in accordance with your settings and accept our Terms of Service (which contains important waivers). Please see our Privacy Policy for more information on our privacy practices.",
    "topics": [
      "Algorithms and Numerical Methods",
      "Artificial Intelligence and Machine Learning",
      "Computer Graphics",
      "Robotics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper title suggests a library for procedural scene generation in robot manipulation, which is a narrow application. The abstract does not contain relevant information about foundation models. There are no key indicators of foundation model research, and it seems focused on a specialized task without broader general - purpose AI implications.",
    "is_robotics": false,
    "robotic_reason": "The title mentions robot manipulation, which is a robotics-related concept. However, the abstract does not contain any information related to robotics research, such as hardware systems, learning - based algorithms, or key robotics indicators. It mainly focuses on cookie usage and privacy policies, so it cannot be determined that the paper falls within the robotics research domain."
  },
  {
    "paper_id": "nvidia_13",
    "authors": "Siyin Wang, Huck Yang, Ji Wu, Chao Zhang",
    "title": "Bayesian Example Selection Improves In-Context Learning for Speech, Text, and Visual Modalities",
    "paper_url": "https://research.nvidia.com/publication/2024-11_bayesian-example-selection-improves-context-learning-speech-text-and-visual",
    "pdf_link": "https://arxiv.org/pdf/2404.14716.pdf",
    "abstract": "Large language models (LLMs) can adapt to new tasks through in-context learning (ICL) based on a few examples presented in dialogue history without any model parameter update. Despite such convenience, the performance of ICL heavily depends on the quality of the in-context examples presented, which makes the in-context example selection approach a critical choice. This paper proposes a novel Bayesian in-Context example Selection method (ByCS) for ICL. Extending the inference probability conditioned on in-context examples based on Bayes' theorem, ByCS focuses on the inverse inference conditioned on test input. Following the assumption that accurate inverse inference probability (likelihood) will result in accurate inference probability (posterior), in-context examples are selected based on their inverse inference results. Diverse and extensive cross-tasking and cross-modality experiments are performed with speech, text, and image examples. Experimental results show the efficacy and robustness of our ByCS method on various models, tasks and modalities.",
    "topics": [
      "Computer Vision",
      "Natural Language Processing",
      "Speech Processing"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper focuses on large language models (LLMs), which are a key type of foundation model. It proposes a method for in - context learning in LLMs and conducts cross - modality experiments, demonstrating broader general - purpose AI capabilities and cross - domain applications.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on a Bayesian in-Context example Selection method for in-context learning in speech, text, and visual modalities. It is a purely algorithmic study without any clear connections to robotic systems, hardware components, or any of the key robotics research indicators."
  },
  {
    "paper_id": "nvidia_14",
    "authors": "Yusuke Hirota, Ryo Hachiuma, Huck Yang, Yuta Nakashima",
    "title": "From Descriptive Richness to Bias: Unveiling the Dark Side of Generative Image Caption Enrichment",
    "paper_url": "https://research.nvidia.com/publication/2024-11_descriptive-richness-bias-unveiling-dark-side-generative-image-caption",
    "pdf_link": "https://arxiv.org/pdf/2406.13912.pdf",
    "abstract": "Large language models (LLMs) have enhanced the capacity of vision-language models to caption visual text. This generative approach to image caption enrichment further makes textual captions more descriptive, improving alignment with the visual context. However, while many studies focus on benefits of generative caption enrichment (GCE), are there any negative side effects? We compare standard-format captions and recent GCE processes from the perspectives of \"gender bias\" and \"hallucination\", showing that enriched captions suffer from increased gender bias and hallucination. Furthermore, models trained on these enriched captions amplify gender bias by an average of 30.9% and increase hallucination by 59.5%. This study serves as a caution against the trend of making captions more descriptive.",
    "topics": [
      "Computer Vision",
      "Generative AI",
      "Natural Language Processing"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper mentions large language models (LLMs) and vision - language models, which are key indicators of foundation model research. The research has broader implications as it assesses the negative side - effects of a generative approach related to these models, impacting general - purpose AI capabilities in the cross - domain of vision and language.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on the negative side - effects of generative image caption enrichment in vision - language models, such as gender bias and hallucination. It does not involve any robotics - related concepts like reinforcement learning, manipulation tasks, or robotic hardware systems, thus falling outside the robotics research domain."
  },
  {
    "paper_id": "nvidia_15",
    "authors": "Yichen Lu, Jiaqi Song, Huck Yang, Shinji Watanabe",
    "title": "FastAdaSP: Multitask-Adapted Efficient Inference for Large Speech Language Model",
    "paper_url": "https://research.nvidia.com/publication/2024-11_fastadasp-multitask-adapted-efficient-inference-large-speech-language-model",
    "pdf_link": "https://arxiv.org/pdf/2410.03007.pdf",
    "abstract": "In this study, we aim to explore Multitask Speech Language Model (SpeechLM) efficient inference via token reduction. Unlike other modalities such as vision or text, speech has unique temporal dependencies, making previous efficient inference works on other modalities not directly applicable. Furthermore, methods for efficient SpeechLM inference on long sequence and sparse signals remain largely unexplored. Then we propose FastAdaSP, a weighted token merging framework specifically designed for various speech-related tasks to improve the trade-off between efficiency and performance. Experimental results on WavLLM and Qwen-Audio show that our method achieves the state-of-the-art (SOTA) efficiency-performance trade-off compared with other baseline methods. Specifically, FastAdaSP achieved 7x memory efficiency and 1.83x decoding throughput without any degradation on tasks like Emotion Recognition (ER) and Spoken Question Answering (SQA).",
    "topics": [
      "Machine Translation",
      "Natural Language Processing",
      "Speech Processing"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper focuses on a large speech language model, which is related to large language models (LLM). It proposes a method for multitask - adapted efficient inference applicable to various speech - related tasks, showing broader cross - domain capabilities in the speech domain, which aligns with the general - purpose nature of foundation models.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on efficient inference for large speech language models through token reduction. It only involves software - based algorithmic research in the field of speech processing and has no clear connections to robotic systems, hardware components, or any of the key indicators of robotics research."
  },
  {
    "paper_id": "nvidia_16",
    "authors": "William Shen, Caelan Garrett, Ankit Goyal, Tucker Hermans, Fabio Ramos",
    "title": "Differentiable GPU-Parallelized Task and Motion Planning",
    "paper_url": "https://research.nvidia.com/publication/2024-11_differentiable-gpu-parallelized-task-and-motion-planning",
    "pdf_link": "https://arxiv.org/pdf/2411.11833.pdf",
    "abstract": "NVIDIA and our third-party partners use cookies and other tools to collect and record information you provide as well as information about your interactions with our websites for performance improvement, analytics, and to assist in our marketing efforts. By clicking Agree\" or \"Manage Settings\", you consent to the use of cookies and other tools as described in our Cookie Policy in accordance with your settings and accept our Terms of Service (which contains important waivers). Please see our Privacy Policy for more information on our privacy practices.",
    "topics": [
      "Algorithms and Numerical Methods",
      "Artificial Intelligence and Machine Learning",
      "High Performance Computing",
      "Robotics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The title indicates research on task and motion planning, and the abstract is about cookies and privacy policies, not related to deep - learning models trained on large data for general - purpose use and cross - domain applications as required by foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper title mentions task and motion planning, but the abstract is about cookie policies and privacy, not related to robotics. There are no clear connections to robotic systems, hardware components, or the key indicators of robotics research in the given information."
  },
  {
    "paper_id": "nvidia_17",
    "authors": "Zihan Zhou, Animesh Garg, Dieter Fox, Caelan Garrett, Ajay Mandlekar",
    "title": "SPIRE: Synergistic Planning, Imitation, and Reinforcement Learning for Long-Horizon Manipulation",
    "paper_url": "https://research.nvidia.com/publication/2024-11_spire-synergistic-planning-imitation-and-reinforcement-learning-long-horizon",
    "pdf_link": "https://arxiv.org/pdf/2410.18065.pdf",
    "abstract": "NVIDIA and our third-party partners use cookies and other tools to collect and record information you provide as well as information about your interactions with our websites for performance improvement, analytics, and to assist in our marketing efforts. By clicking Agree\" or \"Manage Settings\", you consent to the use of cookies and other tools as described in our Cookie Policy in accordance with your settings and accept our Terms of Service (which contains important waivers). Please see our Privacy Policy for more information on our privacy practices.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Robotics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The title suggests a narrow approach focused on long - horizon manipulation using a combination of planning, imitation, and reinforcement learning. The abstract is about cookie policies and does not indicate any research on large - scale, general - purpose deep - learning models. Overall, there are no key indicators of foundation model research, and it seems to target a specialized application without broader foundational implications.",
    "is_robotics": false,
    "robotic_reason": "Although the paper title mentions reinforcement learning and manipulation, the abstract is about cookie usage and privacy policies, lacking any details on robotic hardware systems, sensory inputs, actuator outputs, or other robotics - related concepts."
  },
  {
    "paper_id": "nvidia_18",
    "authors": "Caelan Garrett, Ajay Mandlekar, Bowen Wen, Dieter Fox",
    "title": "SkillGen: Automated Demonstration Generation for Efficient Skill Learning and Deployment",
    "paper_url": "https://research.nvidia.com/publication/2024-11_skillgen-automated-demonstration-generation-efficient-skill-learning-and",
    "pdf_link": "https://arxiv.org/pdf/2410.18907.pdf",
    "abstract": "NVIDIA and our third-party partners use cookies and other tools to collect and record information you provide as well as information about your interactions with our websites for performance improvement, analytics, and to assist in our marketing efforts. By clicking Agree\" or \"Manage Settings\", you consent to the use of cookies and other tools as described in our Cookie Policy in accordance with your settings and accept our Terms of Service (which contains important waivers). Please see our Privacy Policy for more information on our privacy practices.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Robotics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The title and abstract do not mention any key indicators of foundation model research. The abstract is about cookie usage and privacy policies, and the title seems focused on skill learning and deployment rather than large - scale deep - learning models with broad cross - domain applications.",
    "is_robotics": false,
    "robotic_reason": "The paper title does not clearly indicate a connection to robotics, and the abstract is about cookie usage and privacy policies, lacking any robotics - related concepts, methodologies, or mentions of hardware components."
  },
  {
    "paper_id": "nvidia_19",
    "authors": "Shuo Cheng, Caelan Garrett, Ajay Mandlekar, Danfei Xu",
    "title": "NOD-TAMP: Generalizable Long-Horizon Planning with Neural Object Descriptors",
    "paper_url": "https://research.nvidia.com/publication/2024-11_nod-tamp-generalizable-long-horizon-planning-neural-object-descriptors",
    "pdf_link": "https://arxiv.org/pdf/2311.01530.pdf",
    "abstract": "NVIDIA and our third-party partners use cookies and other tools to collect and record information you provide as well as information about your interactions with our websites for performance improvement, analytics, and to assist in our marketing efforts. By clicking Agree\" or \"Manage Settings\", you consent to the use of cookies and other tools as described in our Cookie Policy in accordance with your settings and accept our Terms of Service (which contains important waivers). Please see our Privacy Policy for more information on our privacy practices.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Robotics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The abstract does not contain any information related to foundation model research, and the title focuses on a specific planning method (NOD - TAMP) without indicating large - scale pre - trained models, multi - modality, or cross - domain applications typically associated with foundation models.",
    "is_robotics": false,
    "robotic_reason": "The paper title does not clearly indicate robotics concepts, and the abstract is about cookie policies, privacy, and terms of service, lacking any mention of robotics - related concepts, methodologies, or hardware components."
  },
  {
    "paper_id": "nvidia_20",
    "authors": "Xi Deng, Lifan Wu, Bruce Walter, Eugene d'Eon, Ravi Ramamoorthi, Steve Marschner, Andrea Weidlich",
    "title": "Reconstructing Translucent Thin Objects from Photos",
    "paper_url": "https://research.nvidia.com/publication/2024-11_reconstructing-translucent-thin-objects-photos",
    "pdf_link": "https://d1qx31qr3h6wln.cloudfront.net/publications/_SIGGRAPH24_translucent_thin_object_reconstruction-8.pdf",
    "abstract": "The joint reconstruction of shape and appearance for translucent objects from real-world data poses a challenge in computer graphics, especially when dealing with complex layered materials like leaves or paper. The traditional assumption of diffuse transmittance falls short, and more accurate Monte-Carlo-based models are often needed to reproduce their appearance. To accurately capture the translucent appearance, an acquisition system needs to be carefully designed. Additionally, there are three challenges for inverse rendering: First, a large number of unknown parameters make the optimization problem difficult. Second, the Monte Carlo (MC) renderer introduces noise, which the optimization is sensitive to, especially when dealing with complex material models such as rough dielectric surfaces and highly scattering participating media. Last, MC estimators using long light paths (up to 32 bounces in our case) create a large computation graph in memory, making the gradient back-propagation costly. To address those challenges, we present an affordable and fast acquisition pipeline that can capture spatially varying reflectance and transmission at the same time, using a two-phase optimization. We first initialize the geometry with the traditional vision method and then fit a simple and fast appearance model. Thereafter, we use the estimated parameters to initialize a second optimization using a more expensive volumetric model, which converges faster and more reliably from this favorable starting position. We also introduce a way to analyze each parameter\u2019s sensitivity to the noise in the measurements, which can be used in optimally selecting useful measurements for optimization. Furthermore, instead of iterating on the camera system, we also introduce a weighted L2 loss as an alternative for selecting useful pixels from existing measurements.",
    "topics": [
      "Computer Graphics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on reconstructing translucent thin objects from photos. It presents an acquisition pipeline and optimization methods for this specific computer - graphics task. There is no mention of large - scale deep learning models (especially transformer - based), multi - modal models, or general - purpose modeling approaches with cross - domain applications. It is a narrow, specialized application without broader implications for general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on reconstructing translucent thin objects in computer graphics. It mainly discusses acquisition systems, inverse - rendering challenges, and optimization methods in a graphics context, without any mention of robotic hardware systems, learning - based control algorithms for robots, or key robotics research indicators."
  },
  {
    "paper_id": "nvidia_21",
    "authors": "Yi Li, Yuquan Deng, Jesse Zhang, Joel Jang, Marius Memmel, Caelan Garrett, Fabio Ramos, Dieter Fox, Anqi Li, Abhishek Gupta, Ankit Goyal",
    "title": "HAMSTER: Hierarchical Action Models for Open-World Robot Manipulation",
    "paper_url": "https://research.nvidia.com/publication/2024-11_hamster-hierarchical-action-models-open-world-robot-manipulation",
    "pdf_link": "https://openreview.net/pdf/df0d7f1dabeea7b68caab801d4faa06d555fd3dc.pdf",
    "abstract": "NVIDIA and our third-party partners use cookies and other tools to collect and record information you provide as well as information about your interactions with our websites for performance improvement, analytics, and to assist in our marketing efforts. By clicking Agree\" or \"Manage Settings\", you consent to the use of cookies and other tools as described in our Cookie Policy in accordance with your settings and accept our Terms of Service (which contains important waivers). Please see our Privacy Policy for more information on our privacy practices.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Natural Language Processing",
      "Robotics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The abstract does not contain any relevant information about the model, and based on the title, it seems focused on a narrow application of hierarchical action models for open - world robot manipulation without clear signs of a large - scale, general - purpose deep - learning model trained on vast data as required for foundation model research.",
    "is_robotics": false,
    "robotic_reason": "Although the paper title mentions 'robot manipulation', the abstract only discusses cookie usage, marketing, and privacy policies, lacking any details about robotics-related concepts, methodologies, or hardware components."
  },
  {
    "paper_id": "nvidia_22",
    "authors": "Nishanth Kumar, Fabio Ramos, Dieter Fox, Caelan Garrett",
    "title": "Open-World Task and Motion Planning via Vision-Language Model Inferred Constraints",
    "paper_url": "https://research.nvidia.com/publication/2024-11_open-world-task-and-motion-planning-vision-language-model-inferred-constraints",
    "pdf_link": "https://arxiv.org/pdf/2411.08253",
    "abstract": "NVIDIA and our third-party partners use cookies and other tools to collect and record information you provide as well as information about your interactions with our websites for performance improvement, analytics, and to assist in our marketing efforts. By clicking Agree\" or \"Manage Settings\", you consent to the use of cookies and other tools as described in our Cookie Policy in accordance with your settings and accept our Terms of Service (which contains important waivers). Please see our Privacy Policy for more information on our privacy practices.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Natural Language Processing",
      "Robotics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The abstract does not pertain to research content. Although the title mentions a Vision - Language Model, there is no information in the abstract about model architecture, training approach, or broader impact on general - purpose AI capabilities, so it cannot be determined as foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper title mentions task and motion planning, which is related to robotics. However, the abstract is about cookie usage and privacy policies, not related to robotics concepts, methodologies, or hardware components. There is no evidence in the abstract to support that the paper falls within the robotics research domain."
  },
  {
    "paper_id": "nvidia_23",
    "authors": "Zhutian Yang, Caelan Garrett, Dieter Fox, Tom\u00e1s Lozano-P\u00e9rez, Leslie Pack Kaelbling",
    "title": "Guiding Long-Horizon Task and Motion Planning with Vision Language Models",
    "paper_url": "https://research.nvidia.com/publication/2024-11_guiding-long-horizon-task-and-motion-planning-vision-language-models",
    "pdf_link": "https://arxiv.org/pdf/2410.02193.pdf",
    "abstract": "NVIDIA and our third-party partners use cookies and other tools to collect and record information you provide as well as information about your interactions with our websites for performance improvement, analytics, and to assist in our marketing efforts. By clicking Agree\" or \"Manage Settings\", you consent to the use of cookies and other tools as described in our Cookie Policy in accordance with your settings and accept our Terms of Service (which contains important waivers). Please see our Privacy Policy for more information on our privacy practices.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Natural Language Processing",
      "Robotics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The abstract does not contain any relevant information about the model architecture, training approach, or general - purpose AI capabilities. Although the title mentions Vision Language Models, without proper details from the abstract, we cannot confirm that it involves research on foundation models as defined.",
    "is_robotics": false,
    "robotic_reason": "The title mentions task and motion planning, which are robotic concepts, but the abstract only focuses on cookie usage, privacy policies, and marketing efforts. There is no mention of robotic hardware systems, input sensors, mechanical kinematics, or learning - based control algorithms, so the paper does not fall within the robotics research domain."
  },
  {
    "paper_id": "nvidia_24",
    "authors": "Yijiang Huang, Caelan Garrett, Caitlin Mueller",
    "title": "Constructability-driven design of frame structures with state-space search methods",
    "paper_url": "https://research.nvidia.com/publication/2024-11_constructability-driven-design-frame-structures-state-space-search-methods",
    "pdf_link": null,
    "abstract": "NVIDIA and our third-party partners use cookies and other tools to collect and record information you provide as well as information about your interactions with our websites for performance improvement, analytics, and to assist in our marketing efforts. By clicking Agree\" or \"Manage Settings\", you consent to the use of cookies and other tools as described in our Cookie Policy in accordance with your settings and accept our Terms of Service (which contains important waivers). Please see our Privacy Policy for more information on our privacy practices.",
    "topics": [
      "Algorithms and Numerical Methods",
      "Artificial Intelligence and Machine Learning",
      "Robotics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper title focuses on constructability - driven design of frame structures using state - space search methods, and the abstract is about cookie usage. There is no mention of deep learning models, large - scale data training, or cross - domain and general - purpose applications, so it does not involve foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper title focuses on constructability - driven design of frame structures, and the abstract is about cookie usage and privacy policies. There are no robotics - related concepts, methodologies, or mentions of hardware components."
  },
  {
    "paper_id": "nvidia_25",
    "authors": "Ryan Hoque, Ajay Mandlekar, Caelan Garrett, Ken Goldberg, Dieter Fox",
    "title": "Interventional Data Generation for Robust and Data-Efficient Robot Imitation Learning",
    "paper_url": "https://research.nvidia.com/publication/2024-10_interventional-data-generation-robust-and-data-efficient-robot-imitation",
    "pdf_link": "https://arxiv.org/pdf/2405.01472.pdf",
    "abstract": "NVIDIA and our third-party partners use cookies and other tools to collect and record information you provide as well as information about your interactions with our websites for performance improvement, analytics, and to assist in our marketing efforts. By clicking Agree\" or \"Manage Settings\", you consent to the use of cookies and other tools as described in our Cookie Policy in accordance with your settings and accept our Terms of Service (which contains important waivers). Please see our Privacy Policy for more information on our privacy practices.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Robotics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper title focuses on interventional data generation for robot imitation learning, a narrow application. The abstract is about cookies and privacy policies, not related to foundation model research indicators. There is no mention of large - scale pre - trained models, cross - domain applications, or general - purpose modeling approaches.",
    "is_robotics": false,
    "robotic_reason": "Although the paper title mentions 'robot imitation learning', the abstract is about cookie usage and privacy policies, lacking any content related to robotic hardware systems, input sensors, mechanical kinematics, or learning - based control algorithms for robots."
  },
  {
    "paper_id": "nvidia_26",
    "authors": "Jishnu Jaykumar P, Kamalesh Palanisamy, Yu-Wei Chao, Xinya Du, Yu Xiang",
    "title": "Proto-CLIP: Vision-Language Prototypical Network for Few-Shot Learning",
    "paper_url": "https://research.nvidia.com/publication/2024-10_proto-clip-vision-language-prototypical-network-few-shot-learning",
    "pdf_link": "https://arxiv.org/pdf/2307.03073.pdf",
    "abstract": "We propose a novel framework for few-shot learning by leveraging large-scale vision-language models such as CLIP. Motivated by unimodal prototypical networks for few-shot learning, we introduce Proto-CLIP which utilizes image prototypes and text prototypes for few-shot learning. Specifically, Proto-CLIP adapts the image and text encoder embeddings from CLIP in a joint fashion using few-shot examples. The embeddings from the two encoders are used to compute the respective prototypes of image classes for classification. During adaptation, we propose aligning the image and text prototypes of the corresponding classes. Such alignment is beneficial for few-shot classification due to the reinforced contributions from both types of prototypes. Proto-CLIP has both training-free and fine-tuned variants. We demonstrate the effectiveness of our method by conducting experiments on benchmark datasets for few-shot learning, as well as in the real world for robot perception. The project page can be found at https://irvlutd.github.io/Proto-CLIP.",
    "topics": [
      "Computer Vision",
      "Robotics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper uses a large - scale vision - language model (CLIP) but focuses on adapting it for few - shot learning, a narrow and specialized application. The main contribution is a framework for few - shot learning rather than research on a general - purpose model with broad cross - domain implications.",
    "is_robotics": false,
    "robotic_reason": "The paper mainly focuses on a novel framework for few - shot learning using vision - language models. Although it mentions an experiment in the real world for robot perception, the core of the research is a purely algorithmic study without clear connections to robotic systems or hardware, thus falling outside the robotics research domain."
  },
  {
    "paper_id": "nvidia_27",
    "authors": "Xiaolin Fang, Caelan Garrett, Clemens Eppner, Tom\u00e1s Lozano-P\u00e9rez, Leslie Pack Kaelbling, Dieter Fox",
    "title": "DiMSam: Diffusion Models as Samplers for Task and Motion Planning under Partial Observability",
    "paper_url": "https://research.nvidia.com/publication/2024-10_dimsam-diffusion-models-samplers-task-and-motion-planning-under-partial",
    "pdf_link": "https://arxiv.org/pdf/2306.13196.pdf",
    "abstract": "NVIDIA and our third-party partners use cookies and other tools to collect and record information you provide as well as information about your interactions with our websites for performance improvement, analytics, and to assist in our marketing efforts. By clicking Agree\" or \"Manage Settings\", you consent to the use of cookies and other tools as described in our Cookie Policy in accordance with your settings and accept our Terms of Service (which contains important waivers). Please see our Privacy Policy for more information on our privacy practices.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Robotics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The abstract is about cookie and privacy policies and contains no relevant information about the research. The title only mentions diffusion models for task and motion planning under partial observability, which seems to be a narrow application without clear indication of broader general - purpose AI capabilities, large - scale training, or cross - domain applications.",
    "is_robotics": false,
    "robotic_reason": "The paper title mentions task and motion planning which is a robotics concept, but the abstract is about cookie policies and privacy practices, not related to robotic hardware systems, input sensors, mechanical kinematics, or learning - based control algorithms. There is no clear connection to robotics in the abstract, so it does not fall within the robotics research domain."
  },
  {
    "paper_id": "nvidia_28",
    "authors": "Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, Arash Vahdat",
    "title": "DiffiT: Diffusion Vision Transformers for Image Generation",
    "paper_url": "https://research.nvidia.com/publication/2024-09_diffit-diffusion-vision-transformers-image-generation",
    "pdf_link": "https://arxiv.org/pdf/2312.02139.pdf",
    "abstract": "Diffusion models with their powerful expressivity and high sample quality have achieved State-Of-The-Art (SOTA) performance in the generative domain. The pioneering Vision Transformer (ViT) has also demonstrated strong modeling capabilities and scalability, especially for recognition tasks. In this paper, we study the effectiveness of ViTs in diffusion-based generative learning and propose a new model denoted as Diffusion Vision Transformers (DiffiT). Specifically, we propose a methodology for finegrained control of the denoising process and introduce the Time-dependant Multihead Self Attention (TMSA) mechanism. DiffiT is surprisingly effective in generating high-fidelity images with significantly better parameter efficiency. We also propose latent and image space DiffiT models and show SOTA performance on a variety of class-conditional and unconditional synthesis tasks at different resolutions. The Latent DiffiT model achieves a new SOTA FID score of 1.73 on ImageNet256 dataset while having 19.85%, 16.88% less parameters than other Transformer-based diffusion models such as MDT and DiT, respectively.",
    "topics": [
      "Artificial Intelligence and Machine Learning"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper combines diffusion models and Vision Transformers, which are deep - learning models. It shows general - purpose capabilities by achieving SOTA performance on various image synthesis tasks at different resolutions, indicating its potential as a backbone for downstream image - related tasks.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on image generation using diffusion vision transformers, presenting algorithm - based methods for fine - grained control and introducing new mechanisms for high - fidelity image synthesis. There are no mentions of hardware systems with input sensors, mechanical kinematics, or learning - based control algorithms for robotic actuator outputs, thus falling outside the robotics research domain."
  },
  {
    "paper_id": "nvidia_29",
    "authors": "David Durst, F. Xie, V. Sarukkai, Brennan Shacklett, Iuri Frosio, Chen Tessler, Joohwan Kim, C. Taylor, G. Bernstein, S. Choudhury, P. Hanrahan, Kayvon Fatahalian",
    "title": "Learning to Move Like Professional Counter-Strike Players",
    "paper_url": "https://research.nvidia.com/publication/2024-09_learning-move-professional-counter-strike-players",
    "pdf_link": "https://arxiv.org/pdf/2408.13934",
    "abstract": "In multiplayer, first-person shooter games like Counter-Strike: Global Offensive (CS:GO), coordinated movement is a critical component of high-level strategic play. However, the complexity of team coordination and the variety of conditions present in popular game maps make it impractical to author hand-crafted movement policies for every scenario. We show that it is possible to take a data-driven approach to creating human-like movement controllers for CS:GO. We curate a team movement dataset comprising 123 hours of professional game play traces, and use this dataset to train a transformer-based movement model that generates human-like team movement for all players in a \"Retakes\" round of the game. Importantly, the movement prediction model is efficient. Performing inference for all players takes less than 0.5 ms per game step (amortized cost) on a single CPU core, making it plausible for use in commercial games today. Human evaluators assess that our model behaves more like humans than both commercially-available bots and procedural movement controllers scripted by experts (16% to 59% higher by TrueSkill rating of \"human-like\"). Using experiments involving in-game bot vs. bot self-play, we demonstrate that our model performs simple forms of teamwork, makes fewer common movement mistakes, and yields movement distributions, player lifetimes, and kill locations similar to those observed in professional CS:GO match play.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Computer Graphics",
      "Esports",
      "Generative AI",
      "Human Computer Interaction",
      "Robotics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on training a transformer - based model for a narrow application of creating human - like movement controllers in the Counter - Strike game. It lacks broader implications and general - purpose AI capabilities as it is confined to a specific game scenario without cross - domain applications or the ability to serve as a versatile backbone for various downstream tasks across multiple domains.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on creating movement controllers for a first - person shooter game (CS:GO) using a data - driven approach. It is a software - only implementation with no mention of robotic hardware systems, input sensors, mechanical kinematics, or learning - based algorithms applied to real - world robotic control. Thus, it falls outside the robotics research domain."
  },
  {
    "paper_id": "nvidia_30",
    "authors": "Matthijs Van keirsbilck, Alex Keller",
    "title": "Conformer without Convolutions",
    "paper_url": "https://research.nvidia.com/publication/2024-09_conformer-without-convolutions",
    "pdf_link": null,
    "abstract": "We analyze the weights of a trained speech-to-text neural network and discover a surprising amount of structure in the temporal convolutions. Based on our observations we propose to completely remove learnable temporal convolutions, and replace them with fixed averaging and shift operations which have no learnable parameters and open the way for significantly faster implementations. In the state-of-the-art models Conformer, Squeezeformer and FastConformer, this improves WER by 0.12%, 0.62%, and 0.20% respectively, while reducing the computational cost.",
    "topics": [
      "Algorithms and Numerical Methods",
      "Artificial Intelligence and Machine Learning"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on optimizing speech - to - text neural networks by removing learnable temporal convolutions. It is a narrow improvement for speech - to - text tasks without broader implications for general - purpose AI capabilities, and does not involve large - scale pre - trained models or cross - domain applications.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on improving speech - to - text neural networks by modifying their temporal convolutions. It is a purely algorithmic study without any clear connection to robotic systems or hardware, thus falling outside the robotics research domain."
  },
  {
    "paper_id": "nvidia_31",
    "authors": "Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, Pavlo Molchanov",
    "title": "LLM Pruning and Distillation in Practice: The Minitron Approach",
    "paper_url": "https://research.nvidia.com/publication/2024-08_llm-pruning-and-distillation-practice-minitron-approach",
    "pdf_link": "https://www.isca-archive.org/interspeech_2024/vankeirsbilck24_interspeech.pdf",
    "abstract": "NVIDIA and our third-party partners use cookies and other tools to collect and record information you provide as well as information about your interactions with our websites for performance improvement, analytics, and to assist in our marketing efforts. By clicking Agree\" or \"Manage Settings\", you consent to the use of cookies and other tools as described in our Cookie Policy in accordance with your settings and accept our Terms of Service (which contains important waivers). Please see our Privacy Policy for more information on our privacy practices.",
    "topics": [
      "Artificial Intelligence and Machine Learning"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The abstract does not contain any relevant research content. Although the title mentions LLM, without research - related information in the abstract, we cannot determine if the paper involves training large - scale models on big data or has broader implications for general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper title focuses on LLM pruning and distillation, and the abstract is about cookie policies and privacy practices. There are no robotics-related concepts, methodologies, or references to hardware components in the paper."
  },
  {
    "paper_id": "nvidia_32",
    "authors": "Jaideep Pathak, Yair Cohen, Piyush Garg, Peter Harrington, Noah Brenowitz, Dale Durran, Morteza Mardani, Arash Vahdat, Shaoming Xu, Karthik Kashinath, Mike Pritchard",
    "title": "Kilometer-Scale Convection Allowing Model Emulation using Generative Diffusion Modeling",
    "paper_url": "https://research.nvidia.com/publication/2024-08_kilometer-scale-convection-allowing-model-emulation-using-generative-diffusion",
    "pdf_link": "https://arxiv.org/pdf/2408.10958",
    "abstract": "Storm-scale convection-allowing models (CAMs) are an important tool for predicting the evolution of thunderstorms and mesoscale convective systems that result in damaging extreme weather. By explicitly resolving convective dynamics within the atmosphere they afford meteorologists the nuance needed to provide outlook on hazard. Deep learning models have thus far not proven skilful at km-scale atmospheric simulation, despite being competitive at coarser resolution with state-of-the-art global, medium-range weather forecasting. We present a generative diffusion model called StormCast, which emulates the high-resolution rapid refresh (HRRR) model\u2014NOAA\u2019s state-of-the-art 3km operational CAM. StormCast autoregressively predicts 99 state variables at km scale using a 1-hour time step, with dense vertical resolution in the atmospheric boundary layer, conditioned on 26 synoptic variables. We present evidence of successfully learnt km-scale dynamics including competitive 1-6 hour forecast skill for composite radar reflectivity alongside physically realistic convective cluster evolution, moist updrafts, and cold pool morphology. StormCast predictions maintain realistic power spectra for multiple predicted variables across multi-hour forecasts. Together, these results establish the potential for autoregressive ML to emulate CAMs \u2013 opening up new km-scale frontiers for regional ML weather prediction and future climate hazard dynamical downscaling.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Climate Simulation",
      "Generative AI"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on using a generative diffusion model for a narrow application of emulating kilometer - scale convection - allowing models in weather prediction. There is no indication of broader cross - domain applications or the model serving as a general - purpose backbone for various downstream tasks, thus it does not meet the criteria for foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on using a generative diffusion model to emulate a weather forecasting model. It is a purely algorithmic study in the field of weather prediction without any clear connection to robotic systems, hardware components, or the key indicators of robotics research."
  },
  {
    "paper_id": "nvidia_33",
    "authors": "Chia-Tung (Mark) Ho, Haoxing (Mark) Ren, Brucek Khailany",
    "title": "VerilogCoder: Autonomous Verilog Coding Agents with Graph-based Planning and Abstract Syntax Tree (AST)-based Waveform Tracing Tool",
    "paper_url": "https://research.nvidia.com/publication/2024-08_verilogcoder-autonomous-verilog-coding-agents-graph-based-planning-and-abstract",
    "pdf_link": "https://arxiv.org/pdf/2408.08927",
    "abstract": "Due to the growing complexity of modern Integrated Circuits (ICs), automating hardware design can prevent a significant amount of human error from the engineering process and result in less errors. Verilog is a popular hardware description language for designing and modeling digital systems; thus, Verilog generation is one of the emerging areas of research to facilitate the design process. In this work, we propose VerilogCoder, a system of multiple Artificial Intelligence (AI) agents for Verilog code generation, to autonomously write Verilog code and fix syntax and functional errors using collaborative Verilog tools (i.e., syntax checker, simulator, and waveform tracer). Firstly, we propose a task planner that utilizes a novel Task and Circuit Relation Graph retrieval method to construct a holistic plan based on module descriptions. To debug and fix functional errors, we develop a novel and efficient abstract syntax tree (AST)-based waveform tracing tool, which is integrated within the autonomous Verilog completion flow. The proposed methodology successfully generates 94.2% syntactically and functionally correct Verilog code, surpassing the state-of-the-art methods by 33.9% on the VerilogEval-Human v2 benchmark.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Circuits and VLSI Design",
      "Generative AI"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper presents VerilogCoder, a system of AI agents for Verilog code generation. It focuses on a narrow and specialized application in hardware design (Verilog code generation) without involving large - scale pre - trained deep learning models or having broader implications for general - purpose AI capabilities, thus not meeting the criteria for foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on autonomous Verilog code generation for integrated circuit design, which is a software - based solution for hardware description language. It does not involve hardware systems with input sensors, mechanical kinematics, or learning - based control algorithms for robotic movement. There are no robotics - related concepts such as reinforcement learning, robot navigation, or manipulation tasks present, and it is a software - only implementation without physical robotic components."
  },
  {
    "paper_id": "nvidia_34",
    "authors": "Yuchen Hu, Chen Chen, Huck Yang, Ruizhe Li, Zhehuai Chen, Eng Siong Chng",
    "title": "GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators",
    "paper_url": "https://research.nvidia.com/publication/2024-08_gentranslate-large-language-models-are-generative-multilingual-speech-and",
    "pdf_link": "https://arxiv.org/pdf/2402.06894 ",
    "abstract": "Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely \"GenTranslate\", which builds upon LLMs to generate better results from the diverse translation versions in N-best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the rich information in N-best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTranslate dataset that contains over 592K hypotheses-translation pairs in 11 languages. Experiments on various speech and machine translation benchmarks (e.g., FLEURS, CoVoST-2, WMT) demonstrate that our GenTranslate significantly outperforms the state-of-the-art model.",
    "topics": [
      "Applied Perception",
      "Artificial Intelligence and Machine Learning",
      "Generative AI",
      "Machine Translation",
      "Natural Language Processing",
      "Speech Processing"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper builds on large language models (LLMs), which are a key type in foundation model research. It uses the general - purpose capabilities of LLMs for multilingual speech and machine translation, showing cross - domain applications. Additionally, it involves LLM finetuning, which is a common approach in foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on a new generative paradigm for multilingual speech and machine translation using large language models. It involves software - only techniques such as beam search decoding and hypothesis selection, and the creation of a dataset for fine - tuning. There are no mentions of hardware systems, input sensors, mechanical kinematics, or any of the key robotics indicators, thus it is not within the robotics research domain."
  },
  {
    "paper_id": "nvidia_35",
    "authors": "Gilad Deutch, Rinon Gal, Daniel Garibi, Or Patashnik, Daniel Cohen-Or",
    "title": "TurboEdit: Text-Based Image Editing Using Few-Step Diffusion Models",
    "paper_url": "https://research.nvidia.com/publication/2024-08_turboedit-text-based-image-editing-using-few-step-diffusion-models",
    "pdf_link": "https://arxiv.org/pdf/2408.00735 ",
    "abstract": "Diffusion models have opened the path to a wide range of text-based image editing frameworks. However, these typically build on the multi-step nature of the diffusion backwards process, and adapting them to distilled, fast-sampling methods has proven surprisingly challenging. Here, we focus on a popular line of text-based editing frameworks - the ``edit-friendly'' DDPM-noise inversion approach. We analyze its application to fast sampling methods and categorize its failures into two classes: the appearance of visual artifacts, and insufficient editing strength. We trace the artifacts to mismatched noise statistics between inverted noises and the expected noise schedule, and suggest a shifted noise schedule which corrects for this offset. To increase editing strength, we propose a pseudo-guidance approach that efficiently increases the magnitude of edits without introducing new artifacts. All in all, our method enables text-based image editing with as few as three diffusion steps, while providing novel insights into the mechanisms behind popular text-based editing approaches.",
    "topics": [
      "Computer Graphics",
      "Computer Vision",
      "Generative AI"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper focuses on diffusion models, which are key indicators of foundation model research. It aims to improve text - based image editing using these models, showing cross - domain applications in text and image, and has broader implications for general - purpose AI capabilities in the field of multimodal processing.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on text - based image editing using diffusion models. It is a purely algorithmic study without any clear connections to robotic systems, hardware components, or the key indicators of robotics research."
  },
  {
    "paper_id": "nvidia_36",
    "authors": "Amirmojtaba Sabour, Sanja Fidler, Karsten Kreis",
    "title": "Align Your Steps: Optimizing Sampling Schedules in Diffusion Models",
    "paper_url": "https://research.nvidia.com/publication/2024-07_align-your-steps-optimizing-sampling-schedules-diffusion-models",
    "pdf_link": "https://arxiv.org/pdf/2404.14507 ",
    "abstract": "Diffusion models (DMs) have established themselves as the state-of-the-art generative modeling approach in the visual domain and beyond. A crucial drawback of DMs is their slow sampling speed, relying on many sequential function evaluations through large neural networks. Sampling from DMs can be seen as solving a differential equation through a discretized set of noise levels known as the sampling schedule. While past works primarily focused on deriving efficient solvers, little attention has been given to finding optimal sampling schedules, and the entire literature relies on hand-crafted heuristics. In this work, for the first time, we propose a general and principled approach to optimizing the sampling schedules of DMs for high-quality outputs, called Align Your Steps. We leverage methods from stochastic calculus and find optimal schedules specific to different solvers, trained DMs and datasets. We evaluate our novel approach on several image, video as well as 2D toy data synthesis benchmarks, using a variety of different samplers, and observe that our optimized schedules outperform previous hand-crafted schedules in almost all experiments. Our method demonstrates the untapped potential of sampling schedule optimization, especially in the few-step synthesis regime.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Generative AI"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper focuses on diffusion models, which are key indicators of foundation model research. It presents a general approach for optimizing sampling schedules in these models, applicable across multiple data types like images, videos, and 2D toy data, showing broader impact on general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on optimizing sampling schedules in diffusion models for data synthesis, which is a purely algorithmic study without any clear connection to robotic systems or hardware components."
  },
  {
    "paper_id": "nvidia_37",
    "authors": "Shih-Yang Liu, Chien-Yi Wang, Hongxu Danny Yin, Pavlo Molchanov, Frank Wang, Kwang-Ting Cheng, Min-Hung Chen",
    "title": "DoRA: Weight-Decomposed Low-Rank Adaptation",
    "paper_url": "https://research.nvidia.com/publication/2024-07_dora-weight-decomposed-low-rank-adaptation",
    "pdf_link": "https://openreview.net/pdf/df0dc78f31394b7e36afad393b90fe0a4b0f6479.pdf ",
    "abstract": "In this ICML'24 Oral paper, we first introduce a novel weight decomposition analysis to investigate the inherent differences between FT and LoRA. Aiming to resemble the learning capacity of FT from the findings, we propose Weight-Decomposed LowRank Adaptation (DoRA). DoRA decomposes the pre-trained weight into two components, magnitude and direction, for fine-tuning, specifically employing LoRA for directional updates to efficiently minimize the number of trainable parameters. By employing DoRA, we enhance both the learning capacity and training stability of LoRA while avoiding any additional inference overhead. DoRA consistently outperforms LoRA on fine-tuning LLaMA, LLaVA, and VL-BART on various downstream tasks, such as commonsense reasoning, visual instruction tuning, and image/video-text understanding. Code available at this https URL.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Computer Vision",
      "Generative AI",
      "Natural Language Processing"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper focuses on fine - tuning pre - trained models (LLaMA, LLaVA, and VL - BART) using a novel method (DoRA). These pre - trained models are large multimodal or language models. The approach aims to enhance learning capacity and training stability for various downstream tasks across different domains, which indicates a broader impact on general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on a novel weight decomposition method (DoRA) for fine - tuning pre - trained models, mainly discussing its performance on language and vision - related downstream tasks. There are no mentions of robotic systems, hardware components, or any of the key robotics research indicators in the title and abstract, so it does not fall within the robotics research domain."
  },
  {
    "paper_id": "nvidia_38",
    "authors": "Ankit Goyal, Valts Blukis, Jie Xu, Yijie Guo, Yu-Wei Chao, Dieter Fox",
    "title": "RVT-2: Learning Precise Manipulation from Few Examples",
    "paper_url": "https://research.nvidia.com/publication/2024-07_rvt-2-learning-precise-manipulation-few-examples",
    "pdf_link": "https://arxiv.org/pdf/2406.08545 ",
    "abstract": "In this work, we study how to build a robotic system that can solve multiple 3D manipulation tasks given language instructions. To be useful in industrial and household domains, such a system should be capable of learning new tasks with few demonstrations and solving them precisely. Prior works, like PerAct and RVT, have studied this problem, however, they often struggle with tasks requiring high precision. We study how to make them more effective, precise, and fast. Using a combination of architectural and system-level improvements, we propose RVT-2, a multitask 3D manipulation model that is 6X faster in training and 2X faster in inference than its predecessor RVT. RVT-2 achieves a new state-of-the-art on RLBench, improving the success rate from 65% to 82%. RVT-2 is also effective in the real world, where it can learn tasks requiring high precision, like picking up and inserting plugs, with just 10 demonstrations. Visual results, code, and trained model are provided at: https://robotic-view-transformer-2.github.io/.",
    "topics": [
      "Computer Vision",
      "Robotics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on a robotic system (RVT - 2) for 3D manipulation tasks. It is mainly aimed at narrow, specialized applications in industrial and household domains, and there is no indication of a large - scale pre - trained model, cross - domain applications, or a general - purpose modeling approach with broader foundational impact, so it does not fall into foundation model research.",
    "is_robotics": true,
    "robotic_reason": "The paper focuses on building a robotic system for 3D manipulation tasks. It aims to improve the performance of such a system, and the model is effective in real - world scenarios for tasks like picking up and inserting plugs. Manipulation tasks are a key indicator of robotics research, and the real - world applicability implies the involvement of hardware systems."
  },
  {
    "paper_id": "nvidia_39",
    "authors": "Rinon Gal, Yael Vinker, Yuval Alaluf, Amit Bermano, Daniel Cohen-Or, Ariel Shamir, Gal Chechik",
    "title": "Breathing Life Into Sketches Using Text-to-Video Priors",
    "paper_url": "https://research.nvidia.com/publication/2024-07_breathing-life-sketches-using-text-video-priors",
    "pdf_link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Gal_Breathing_Life_Into_Sketches_Using_Text-to-Video_Priors_CVPR_2024_paper.pdf ",
    "abstract": "A sketch is one of the most intuitive and versatile tools humans use to convey their ideas visually. An animated sketch opens another dimension to the expression of ideas and is widely used by designers for a variety of purposes. Animating sketches is a laborious process, requiring extensive experience and professional design skills. In this work, we present a method that automatically adds motion to a single-subject sketch (hence, \"breathing life into it\"), merely by providing a text prompt indicating the desired motion. The output is a short animation provided in vector representation, which can be easily edited. Our method does not require extensive training, but instead leverages the motion prior of a large pretrained text-to-video diffusion model using a score-distillation loss to guide the placement of strokes. To promote natural and smooth motion and to better preserve the sketch's appearance, we model the learned motion through two components. The first governs small local deformations and the second controls global affine transformations. Surprisingly, we find that even models that struggle to generate sketch videos on their own can still serve as a useful backbone for animating abstract representations",
    "topics": [
      "Computer Graphics",
      "Computer Vision",
      "Generative AI"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper leverages a large pretrained text - to - video diffusion model, which is a type of model commonly used in foundation model research. The diffusion model, trained on large amounts of data, serves as a backbone for the task of animating sketches, demonstrating cross - domain application from video generation to sketch animation and contributing to general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on animating single - subject sketches using text prompts and a text - to - video diffusion model. It involves software - only techniques for creating animations and does not contain any concepts related to hardware systems, input sensors, mechanical kinematics, or learning - based algorithms for robotic control, thus falling outside the robotics research domain."
  },
  {
    "paper_id": "nvidia_40",
    "authors": "Eugene d'Eon, Andrea Weidlich",
    "title": "VMF Diffuse: A unified rough diffuse BRDF",
    "paper_url": "https://research.nvidia.com/publication/2024-07_vmf-diffuse-unified-rough-diffuse-brdf",
    "pdf_link": "https://diglib.eg.org/handle/10.1111/cgf15149",
    "abstract": "We present a practical analytic BRDF that approximates scattering from a generalized microfacet volume with a von Mises-Fischer NDF. Our BRDF seamlessly blends from smooth Lambertian, through moderately rough height fields with Beckmann-like statistics and into highly rough/porous behaviours that have been lacking from prior models. At maximum roughness, our model reduces to the recent Lambert-sphere BRDF. We validate our model by comparing to simulations of scattering from geometries with randomly-placed Lambertian spheres and show an improvement relative to a rough Beckmann BRDF with very high roughness.",
    "topics": [
      "Computer Graphics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper presents an analytic BRDF for approximating scattering from a microfacet volume. It focuses on a narrow, specialized application in the field of computer graphics related to BRDFs and lacks the characteristics of foundation model research such as large - scale deep learning models, pre - training, and cross - domain or general - purpose capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on presenting and validating an analytic BRDF for approximating scattering, which is a purely algorithmic and software - only study without any clear connection to robotic systems, hardware components, or robotics - related concepts like reinforcement learning, manipulation tasks, etc."
  },
  {
    "paper_id": "nvidia_41",
    "authors": "Francis Williams, Jiahui Huang, Jonathan Swartz, Gergely Klar, Vijay Thakkar, Matthew Cong, Xuanchi Ren, Ruilong Li, Clement Fuji-Tsang, Sanja Fidler, Eftychios Sifakis, Ken Museth",
    "title": "fVDB: A Deep-Learning Framework for Sparse, Large-Scale, and High-Performance Spatial Intelligence",
    "paper_url": "https://research.nvidia.com/publication/2024-07_fvdb-deep-learning-framework-sparse-large-scale-and-high-performance-spatial",
    "pdf_link": "https://arxiv.org/pdf/2407.01781 ",
    "abstract": "We present fVDB, a novel GPU-optimized framework for deep learning on large-scale 3D data. fVDB provides a complete set of differentiable primitives to build deep learning architectures for common tasks in 3D learning such as convolution, pooling, attention, ray-tracing, meshing, etc. fVDB simultaneously provides a much larger feature set (primitives and operators) than established frameworks with no loss in efficiency: our operators match or exceed the performance of other frameworks with narrower scope. Furthermore, fVDB can process datasets with much larger footprint and spatial resolution than prior works, while providing a competitive memory footprint on small inputs. \u00a0To achieve this combination of versatility and performance, fVDB relies on a single novel VDB index grid acceleration structure paired with several key innovations including GPU accelerated sparse grid construction, convolution using tensor cores, fast ray tracing kernels using HDDA, and jagged tensors.\u00a0Our framework is fully integrated with PyTorch enabling interoperability with existing pipelines, and we demonstrate its effectiveness on a number of representative tasks such as large-scale point-cloud segmentation, high resolution 3D generative modeling, unbounded scale Neural Radiance Fields, and large-scale point cloud reconstruction.",
    "topics": [
      "Computer Graphics",
      "Generative AI"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper presents a framework for deep learning on large - scale 3D data. It focuses on building architectures for 3D learning tasks and enhancing performance in this specific domain. There is no mention of large multimodal models, large language models, or other key indicators of foundation model research, and it seems to target narrow 3D learning applications rather than general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on a GPU - optimized deep - learning framework for 3D data processing. It presents algorithms and techniques for deep learning on 3D data, with no clear connections to robotic systems, hardware components, or any of the key robotics research indicators."
  },
  {
    "paper_id": "nvidia_42",
    "authors": "Rohan Sawhney, Daqi Lin, Markus Kettunen, Benedikt Bitterli, Ravi Ramamoorthi, Chris Wyman, Matt Pharr",
    "title": "Decorrelating ReSTIR Samplers via MCMC Mutations",
    "paper_url": "https://research.nvidia.com/publication/2024-07_decorrelating-restir-samplers-mcmc-mutations",
    "pdf_link": "https://research.nvidia.com/labs/prl/sawhney2024decorrelating/restirmcmc2024.pdf ",
    "abstract": "Monte Carlo rendering algorithms often utilize correlations between pixels to improve efficiency and enhance image quality. For real-time applications in particular, repeated reservoir resampling offers a powerful framework to reuse samples both spatially in an image and temporally across multiple frames. While such techniques achieve equal-error up to 100\u00d7 faster for real-time direct lighting [Bitterli et al. 2020] and global illumination [Ouyang et al. 2021; Lin et al. 2021], they are still far from optimal. For instance, spatiotemporal resampling often introduces noticeable correlation artifacts, while reservoirs holding more than one sample suffer from impoverishment in the form of duplicate samples. We demonstrate how interleaving Markov Chain Monte Carlo (MCMC) mutations with reservoir resampling helps alleviate these issues, especially in scenes with glossy materials and difficult-to-sample lighting. Moreover, our approach does not introduce any bias, and in practice, we find considerable improvement in image quality with just a single mutation per reservoir sample in each frame.",
    "topics": [
      "Computer Graphics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on improving Monte Carlo rendering algorithms for real - time applications by interleaving MCMC mutations with reservoir resampling. It does not involve deep learning models trained on large data, nor does it have general - purpose AI capabilities across multiple domains, being a narrow application in rendering.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on Monte Carlo rendering algorithms and using Markov Chain Monte Carlo mutations to improve image quality in real - time applications. It is a purely algorithmic study without any clear connections to robotic systems, hardware components, or the key indicators of robotics research."
  },
  {
    "paper_id": "nvidia_43",
    "authors": "Dario Seyb, Eugene D'eon, Benedikt Bitterli, Wojciech Jarosz",
    "title": "From Microfacets to Participating Media: A Unified Theory of Light Transport with Stochastic Geometry",
    "paper_url": "https://research.nvidia.com/publication/2024-07_microfacets-participating-media-unified-theory-light-transport-stochastic",
    "pdf_link": "https://cs.dartmouth.edu/~wjarosz/publications/seyb24from.pdf ",
    "abstract": "Stochastic geometry models have enjoyed immense success in graphics for modeling interactions of light with complex phenomena such as participating media, rough surfaces, fibers, and more. Although each of these models operates on the same principle of replacing intricate geometry by a random process and deriving the average light transport across all instances thereof, they are each tailored to one specific application and are fundamentally distinct. Each type of stochastic geometry present in the scene is firmly encapsulated in its own appearance model, with its own statistics and light transport average, and no cross-talk between different models or deterministic and stochastic geometry is possible.",
    "topics": [
      "Computer Graphics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on stochastic geometry models for light - transport in graphics, tailored to specific applications. There is no mention of deep - learning models (especially transformer - based), large - scale data training, or general - purpose models for cross - domain applications, thus not meeting the criteria for foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on stochastic geometry models for light transport in graphics, with no mention of hardware systems, input sensors, mechanical kinematics, or learning - based control algorithms relevant to robotics. It is a software - and algorithm - centric study without clear connections to robotic systems or hardware."
  },
  {
    "paper_id": "nvidia_44",
    "authors": "Shlomi Steinberg, Ravi Ramamoorthi, Benedikt Bitterli, Arshiya Mollazainali, Eugene D'eon, Matt Pharr",
    "title": "A Free-Space Diffraction BSDF",
    "paper_url": "https://research.nvidia.com/publication/2024-07_free-space-diffraction-bsdf",
    "pdf_link": "https://research.nvidia.com/labs/rtr/publication/steinberg2024diffraction/steinberg2024_fsd_paper_lowres.pdf ",
    "abstract": "Free-space diffractions are an optical phenomenon where light appears to \u201cbend\u201d around the geometric edges and corners of scene objects. In this paper we present an efficient method to simulate such effects. We derive an edge-based formulation of Fraunhofer diffraction, which is well suited to the common (triangular) geometric meshes used in computer graphics. Our method dynamically constructs a free-space diffraction BSDF by considering the geometry around the intersection point of a ray of light with an object, and we present an importance sampling strategy for these BSDFs. Our method is unique in requiring only ray tracing to produce free-space diffractions, works with general meshes, requires no geometry preprocessing, and is designed to work with path tracers with a linear rendering equation. We show that we are able to reproduce accurate diffraction lobes, and, in contrast to any existing method, are able to handle complex, real-world geometry. This work serves to connect free-space diffractions to the efficient path tracing tools from computer graphics.",
    "topics": [
      "Computer Graphics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on simulating free - space diffraction in computer graphics, presenting a specialized method for this optical phenomenon. It does not involve deep - learning models, large - scale data training, or have broader implications for general - purpose AI capabilities as required for foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on simulating free - space diffraction in computer graphics using ray tracing and importance sampling strategies. It contains no mention of robotics - related concepts such as hardware systems, input sensors, mechanical kinematics, or learning - based control algorithms, nor is there any connection to the key indicators of robotics research."
  },
  {
    "paper_id": "nvidia_45",
    "authors": "Vismay Modi, Nicholas Sharp, Or Perel, Shinjiro Sueda, David I. W. Levin",
    "title": "Simplicits: Mesh-Free, Geometry-Agnostic, Elastic Simulation",
    "paper_url": "https://research.nvidia.com/publication/2024-07_simplicits-mesh-free-geometry-agnostic-elastic-simulation",
    "pdf_link": "https://research.nvidia.com/labs/toronto-ai/simplicits/assets/Simplicits.pdf ",
    "abstract": "The proliferation of 3D representations, from explicit meshes to implicit neural fields and more, motivates the need for simulators agnostic to representation. We present a data-, mesh-, and grid-free solution for elastic simulation for any object in any geometric representation undergoing large, nonlinear deformations. We note that every standard geometric representation can be reduced to an occupancy function queried at any point in space, and we define a simulator atop this common interface. For each object, we fit a small implicit neural network encoding spatially varying weights that act as a reduced deformation basis. These weights are trained to learn physically significant motions in the object via random perturbations. Our loss ensures we find a weight-space basis that best minimizes deformation energy by stochastically evaluating elastic energies through Monte Carlo sampling of the deformation volume.At runtime, we simulate in the reduced basis and sample the deformations back to the original domain.Our experiments demonstrate the versatility, accuracy, and speed of this approach on data including signed distance functions, point clouds, neural primitives, tomography scans, radiance fields, Gaussian splats, surface meshes, and volume meshes, as well as showing a variety of material energies, contact models, and time integration schemes.",
    "topics": [
      "Computer Graphics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on a mesh - free, geometry - agnostic elastic simulation method. It does not involve large - scale deep learning models trained on vast data, and there is no indication of a model that can serve as a versatile backbone for multiple downstream tasks across domains. Instead, it addresses a narrow application in elastic simulation, so it does not meet the criteria for foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on a mesh - free, geometry - agnostic elastic simulation method. It is a purely algorithmic study centered around 3D representation simulation, with no clear connections to robotic systems, hardware components, or any of the key indicators of robotics research."
  },
  {
    "paper_id": "nvidia_46",
    "authors": "",
    "title": "SuperPADL: Scaling Language-Directed Physics-Based Control with Progressive Supervised Distillation",
    "paper_url": "https://research.nvidia.com/publication/2024-07_superpadl-scaling-language-directed-physics-based-control-progressive",
    "pdf_link": "https://arxiv.org/pdf/2407.10481 ",
    "abstract": "Physically-simulated models for human motion can generate high-quality responsive character animations, often in real-time. Natural language serves as a flexible interface for controlling these models, allowing expert and non-expert users to quickly create and edit their animations. Many recent physics-based animation methods, including those that use text interfaces, train control policies using reinforcement learning (RL). However, scaling these methods beyond several hundred motions has remained challenging. Meanwhile, kinematic animation models are able to successfully learn from thousands of diverse motions by leveraging supervised learning methods. Inspired by these successes, in this work we introduce SuperPADL, a scalable framework for physics-based text-to-motion that leverages both RL and supervised learning to train controllers on thousands of diverse motion clips. SuperPADL is trained in stages using progressive distillation, starting with a large number of specialized experts using RL. These experts are then iteratively distilled into larger, more robust policies using a combination of reinforcement learning and supervised learning. Our trained SuperPADL controller runs in real time on a consumer GPU, and can reproduce motions with high fidelity from a dataset of over 5000 skills. Moreover, our policy can naturally transition between skills, allowing for users to interactively craft complicated animations. We experimentally demonstrate that SuperPADL significantly outperforms RL-based baselines at this large data scale.",
    "topics": [
      "Generative AI"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper presents SuperPADL, a framework for physics - based text - to - motion focused on training controllers for human motion animation. It uses RL and supervised learning but lacks key indicators of foundation model research such as large multimodal, language, or vision - language models. It is mainly for a narrow application in animation and has no broader impact on general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on generating human motion animations using physically - simulated models and language - directed control. It uses reinforcement learning and supervised learning for training controllers, but there is no mention of hardware systems with input sensors, mechanical kinematics, or any clear connection to robotic systems, thus falling outside the robotics research domain."
  },
  {
    "paper_id": "nvidia_47",
    "authors": "Alexander Reshetov, David Hart",
    "title": "Modeling Hair Strands with Roving Capsules",
    "paper_url": "https://research.nvidia.com/publication/2024-07_modeling-hair-strands-roving-capsules",
    "pdf_link": "https://dl.acm.org/doi/pdf/10.1145/3641519.3657450 ",
    "abstract": "Hair strands can be modeled by sweeping spheres with varying radii along B\u00e9zier curves. We ray-trace such shapes by finding intersections of a given ray with a set of capsules dynamically defined at runtime. A substantial performance boost is achieved by systematically eliminating parts of the shape that are guaranteed not to intersect with the given ray. The new intersector is more than twice faster than the previously leading phantom algorithm. This improvement results in a 30% overall performance increase, which includes traversal, shading, and the rendering system overhead. In addition, we derive a parametric form of the swept sphere shapes. This provides a deeper understanding of the properties of such objects compared to the offset surfaces obtained by sweeping circles orthogonal to a given curve.",
    "topics": [
      "Computer Graphics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on a narrow application of modeling hair strands and improving the performance of ray - tracing for this specific task. It does not involve deep learning models, large - scale data training, or have broader implications for general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on modeling hair strands through ray - tracing and geometric shape analysis to improve rendering performance. It does not involve any robotic systems, hardware components, or robotics - related concepts such as reinforcement learning, robot navigation, etc."
  },
  {
    "paper_id": "nvidia_48",
    "authors": "Yixin Chen, David Levin, Timothy Langlois",
    "title": "Fluid Control with Laplacian Eigenfunctions",
    "paper_url": "https://research.nvidia.com/publication/2024-07_fluid-control-laplacian-eigenfunctions",
    "pdf_link": "https://dl.acm.org/doi/pdf/10.1145/3641519.3657468",
    "abstract": "Physics-based fluid control has long been a challenging problem in balancing efficiency and accuracy. We introduce a novel physics-based fluid control pipeline using Laplacian Eigenfluids. Utilizing the adjoint method with our provided analytical gradient expressions, the derivative computation of the control problem is efficient and easy to formulate. We demonstrate that our method is fast enough to support real-time fluid simulation, editing, control, and optimal animation generation. Our pipeline naturally supports multi-resolution and frequency control of fluid simulations. We demonstrate the effectiveness and efficiency of our fluid control pipeline through a variety of examples and comparisons.",
    "topics": [
      "Computer Graphics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper presents a physics - based fluid control pipeline using Laplacian Eigenfluids. It focuses on a narrow, specialized application of fluid control, without involving deep - learning models (especially transformer - based), large - scale data training, or having broader implications for general - purpose AI capabilities across multiple domains.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on physics - based fluid control and simulation using Laplacian Eigenfluids. It does not mention any robotics - related concepts such as hardware systems with sensors and mechanical kinematics, or learning - based control algorithms for robotic actuators. It is a software - only implementation centered around fluid control, thus not falling within the robotics research domain."
  },
  {
    "paper_id": "nvidia_49",
    "authors": "Siwei Zhou, Youngha Chang, Nobuhiko Mukai, Hiroaki Santo, Fumio Okura, Yasuyuki Matsushita, Shuang Zhao",
    "title": "Path-space Differentiable Rendering of Implicit Surfaces",
    "paper_url": "https://research.nvidia.com/publication/2024-07_path-space-differentiable-rendering-implicit-surfaces",
    "pdf_link": "https://shuangz.com/projects/psdr-sdf-sg24/psdr-sdf-sg24.pdf ",
    "abstract": "Physics-based differentiable rendering is a key ingredient for integrating forward rendering into probabilistic inference and machine learning pipelines. As a state-of-the-art formulation for differentiable rendering, differential path integrals have enabled the development of efficient Monte Carlo estimators for both interior and boundary integrals. Unfortunately, this formulation has been designed mostly for explicit geometries like polygonal meshes. In this paper, we generalize the theory of differential path integrals to support implicit geometries like level sets and signeddistance functions (SDFs). In addition, we introduce new Monte Carlo estimators for efficiently sampling discontinuity boundaries that are also implicitly specified. We demonstrate the effectiveness of our theory and algorithms using several differentiable-rendering and inverse-rendering examples.",
    "topics": [
      "Computer Graphics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on generalizing the theory of differential path integrals for implicit geometries in differentiable rendering, which is a narrow application in computer graphics without involving deep - learning models trained on large data, nor having broader implications for general - purpose AI capabilities as required by foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on generalizing the theory of differential path integrals for differentiable rendering of implicit geometries and introducing related Monte Carlo estimators. It is a purely algorithmic study without any clear connections to robotic systems or hardware, thus not falling within the robotics research domain."
  },
  {
    "paper_id": "nvidia_50",
    "authors": "Yuta Noma, Silvia Sellan, Nicholas Sharp, Karan Singh, Alec Jacobson",
    "title": "Surface-Filling Curve Flows via Implicit Medial Axes",
    "paper_url": "https://research.nvidia.com/publication/2024-07_surface-filling-curve-flows-implicit-medial-axes",
    "pdf_link": "https://www.dgp.toronto.edu/projects/surface-filling-curves/surface-filling-curves.pdf ",
    "abstract": "We introduce a fast, robust, and user-controllable algorithm to generate surface-filling curves. We compute these curves through the gradient flow of a simple sparse energy, making our method several orders of magnitude faster than previous works. Our algorithm makes minimal assumptions on the topology and resolution of the input surface, achieving improved robustness. Our framework provides tuneable parameters that guide the shape of the output curve, making it ideal for interactive design applications.",
    "topics": [
      "Computer Graphics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper presents an algorithm for generating surface - filling curves, focused on a narrow and specialized application in interactive design. It does not involve deep - learning models, large - scale data training, or have broader implications for general - purpose AI capabilities as required by foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on a fast algorithm for generating surface - filling curves. There are no mentions of hardware systems, input sensors, mechanical kinematics, or learning - based algorithms for controlling robotic systems. It is a purely algorithmic study without clear connections to robotic systems or hardware."
  },
  {
    "paper_id": "nvidia_51",
    "authors": "Bailey Miller, Rohan Sawhney, Keenan Crane, Ioannis Gkioulekas",
    "title": "Walkin' Robin: Walk on Stars with Robin Boundary Conditions",
    "paper_url": "https://research.nvidia.com/publication/2024-07_walkin-robin-walk-stars-robin-boundary-conditions",
    "pdf_link": "https://research.nvidia.com/labs/prl/miller2024wost/WoStRobin.pdf",
    "abstract": "Numerous scientific and engineering applications require solving boundary value problems (BVPs) like the Laplace and Poisson equations on geometrically intricate domains. We describe a unified Monte Carlo approach to solving elliptic BVPs with Dirichlet, Neumann and Robin boundary conditions using the walk on stars algorithm, which unlike conventional numerical methods, does not require any cumbersome finite element mesh generation or global solves. Similar to Monte Carlo rendering, we simulate independent random walks in domains with partially absorbing and reflecting boundaries using a mix of ray intersection and distance queries---our key contribution is the development of a pointwise estimator with bounded walk throughput, which can have orders of magnitude less error in its solution estimate than previous grid-free techniques for BVPs like the walk on boundary method. We also develop bidirectional and boundary value caching strategies to further reduce the variance of our estimator. Our approach is trivial to parallelize, scales favorably with increasing geometric detail, and allows for progressive and view-dependent evaluation.",
    "topics": [
      "Computer Graphics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper presents a Monte Carlo approach for solving elliptic boundary value problems. It does not involve deep - learning models (especially transformer - based), large - scale data training, or have general - purpose AI capabilities applicable across multiple domains as required for foundation model research. It is focused on a specialized numerical problem in scientific and engineering applications.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on a Monte Carlo approach for solving elliptic boundary value problems and does not mention any robotics-related concepts, methodologies, or hardware components. It is a purely algorithmic study without a clear connection to robotic systems."
  },
  {
    "paper_id": "nvidia_52",
    "authors": "Song Zhang, Daqi Lin, Markus Kettunen, Cem Yuksel, Chris Wyman",
    "title": "Area ReSTIR: Resampling for Real-Time Defocus and Antialiasing",
    "paper_url": "https://research.nvidia.com/publication/2024-07_area-restir-resampling-real-time-defocus-and-antialiasing",
    "pdf_link": "https://research.nvidia.com/labs/rtr/publication/zhang2024area/zhang2024area.pdf",
    "abstract": "Recent advancements in spatiotemporal reservoir resampling (ReSTIR) leverage sample reuse from neighbors to efficiently evaluate the path integral.\u00a0 Like rasterization, ReSTIR methods implicitly assume a pinhole camera and evaluate the light arriving at a pixel through a single predetermined subpixel location at a time (e.g., the pixel center).\u00a0 This prevents efficient path reuse in and near pixels with high-frequency details.",
    "topics": [
      "Computer Graphics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on spatiotemporal reservoir resampling for real - time defocus and antialiasing, which is a narrow, specialized application in the field of computer graphics without any indication of involving large - scale deep learning models, multi - modal capabilities, or broader general - purpose AI impacts.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on spatiotemporal reservoir resampling for real - time defocus and antialiasing in a camera model, with no mention of robotic hardware systems, learning - based control algorithms, or any of the key robotics indicators. It is a purely algorithmic study without a clear connection to robotic systems or hardware."
  },
  {
    "paper_id": "nvidia_53",
    "authors": "Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, Hongsheng Li",
    "title": "Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling",
    "paper_url": "https://research.nvidia.com/publication/2024-06_motion-i2v-consistent-and-controllable-image-video-generation-explicit-motion",
    "pdf_link": "https://arxiv.org/pdf/2401.15977 ",
    "abstract": "We introduce Motion-I2V, a novel framework for consistent and controllable image-to-video generation (I2V). In contrast to previous methods that directly learn the complicated image-to-video mapping, Motion-I2V factorizes I2V into two stages with explicit motion modeling. For the first stage, we propose a diffusion-based motion field predictor, which focuses on deducing the trajectories of the reference image's pixels. For the second stage, we propose motion-augmented temporal attention to enhance the limited 1-D temporal attention in video latent diffusion models. This module can effectively propagate reference image's feature to synthesized frames with the guidance of predicted trajectories from the first stage. Compared with existing methods, Motion-I2V can generate more consistent videos even at the presence of large motion and viewpoint variation. By training a sparse trajectory ControlNet for the first stage, Motion-I2V can support users to precisely control motion trajectories and motion regions with sparse trajectory and region annotations. This offers more controllability of the I2V process than solely relying on textual instructions. Additionally, Motion-I2V's second stage naturally supports zero-shot video-to-video translation. Both qualitative and quantitative comparisons demonstrate the advantages of Motion-I2V over prior approaches in consistent and controllable image-to-video generation. Please see our project page at this https URL.",
    "topics": [
      "Computer Graphics",
      "Generative AI"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper uses a diffusion-based approach in its Motion - I2V framework. Diffusion models are key indicators of foundation model research. The framework shows potential for general - purpose applications as it supports zero - shot video - to - video translation, demonstrating broader implications beyond a narrow application.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on a framework for image - to - video generation. It mainly presents algorithmic methods for video synthesis and control, and there is no mention of robotic systems, hardware components, or any of the key indicators of robotics research in the title and abstract."
  },
  {
    "paper_id": "nvidia_54",
    "authors": "Nikhil Mehta, Jonathan Lorraine, Steve Masson, Ramanathan Arunachalam, Zaid Pervaiz Bhat, James Lucas, Arun George Zachariah",
    "title": "Improving Hyperparameter Optimization with Checkpointed Model Weights",
    "paper_url": "https://research.nvidia.com/publication/2024-06_improving-hyperparameter-optimization-checkpointed-model-weights",
    "pdf_link": "https://arxiv.org/pdf/2406.18630 ",
    "abstract": "When training deep learning models, the performance depends largely on the selected hyperparameters. However, hyperparameter optimization (HPO) is often one of the most expensive parts of model design. Classical HPO methods treat this as a black-box optimization problem. However, gray-box HPO methods, which incorporate more information about the setup, have emerged as a promising direction for more efficient optimization. For example, we can use intermediate loss evaluations to terminate bad selections. In this work, we propose an HPO method for neural networks that uses logged checkpoints of the trained weights to guide future hyperparameter selections. Our Forecasting Model Search (FMS) method embeds weights into a Gaussian process deep kernel surrogate model, to be data-efficient with the logged network weights. To facilitate reproducibility and further research, we\u00a0open-source our code.",
    "topics": [
      "Algorithms and Numerical Methods",
      "Artificial Intelligence and Machine Learning"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on hyperparameter optimization for neural networks using checkpointed model weights. It does not involve research on large - scale deep learning models trained on vast data for general - purpose and cross - domain applications as required for foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on hyperparameter optimization for deep - learning models and proposes a method using logged checkpoints of trained weights. It is a purely algorithmic study without any clear connections to robotic systems or hardware, so it does not fall within the robotics research domain."
  },
  {
    "paper_id": "nvidia_55",
    "authors": "Chia-Tung (Mark) Ho, Haoxing (Mark) Ren",
    "title": "Large Language Model (LLM) for Standard Cell Layout Design Optimization",
    "paper_url": "https://research.nvidia.com/publication/2024-06_large-language-model-llm-standard-cell-layout-design-optimization",
    "pdf_link": "https://arxiv.org/pdf/2406.06549 ",
    "abstract": "Standard cells are essential components of modern digital circuit designs. With process technologies advancing toward 2nm, more routability issues have arisen due to the decreasing number of routing tracks, increasing number and complexity of design rules, and strict patterning rules. The state-of-the-art standard cell design automation framework is able to automatically design standard cell layouts in advanced nodes, but it is still struggling to generate highly competitive Performance-Power-Area (PPA) and routable cell layouts for complex sequential cell designs. Consequently, a novel and efficient methodology incorporating the expertise of experienced human designers to incrementally optimize the PPA of cell layouts is highly necessary and essential. High-quality device clustering, with consideration of netlist topology, diffusion sharing/break and routability in the layouts, can reduce complexity and assist in finding highly competitive PPA, and routable layouts faster. In this paper, we leverage the natural language and reasoning ability of Large Language Model (LLM) to generate high-quality cluster constraints incrementally to optimize the cell layout PPA and debug the routability with ReAct prompting. On a benchmark of sequential standard cells in 2nm, we demonstrate that the proposed method not only achieves up to 19.4% smaller cell area, but also generates 23.5% more LVS/DRC clean cell layouts than previous work. In summary, the proposed method not only successfully reduces cell area by 4.65% on average, but also is able to fix routability in the cell layout designs.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Circuits and VLSI Design",
      "Generative AI"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper uses a Large Language Model (LLM), which is a key indicator of foundation model research. The LLM, a deep - learning model trained on large data, is applied to optimize standard cell layout design, showing its use as a general - purpose tool for a specific downstream task, indicating broader implications beyond narrow applications.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on using large language models for standard cell layout design optimization in digital circuits. It does not mention any robotics - related concepts, methodologies, or hardware components as defined in the robotics research domain, and it is a software - based solution for circuit design, not related to robotic systems."
  },
  {
    "paper_id": "nvidia_56",
    "authors": "Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, Karsten Kreis",
    "title": "Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed Diffusion Models",
    "paper_url": "https://research.nvidia.com/publication/2024-06_align-your-gaussians-text-4d-dynamic-3d-gaussians-and-composed-diffusion-models",
    "pdf_link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Ling_Align_Your_Gaussians_Text-to-4D_with_Dynamic_3D_Gaussians_and_Composed_CVPR_2024_paper.pdf ",
    "abstract": "Text-guided diffusion models have revolutionized image and video generation and have also been successfully used for optimization-based 3D object synthesis. Here, we instead focus on the underexplored text-to-4D setting and synthesize dynamic, animated 3D objects using score distillation methods with an additional temporal dimension. Compared to previous work, we pursue a novel compositional generation-based approach, and combine text-to-image, text-to-video, and 3D-aware multiview diffusion models to provide feedback during 4D object optimization, thereby simultaneously enforcing temporal consistency, high-quality visual appearance and realistic geometry. Our method, called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with deformation fields as 4D representation. Crucial to AYG is a novel method to regularize the distribution of the moving 3D Gaussians and thereby stabilize the optimization and induce motion. We also propose a motion amplification mechanism as well as a new autoregressive synthesis scheme to generate and combine multiple 4D sequences for longer generation. These techniques allow us to synthesize vivid dynamic scenes, outperform previous work qualitatively and quantitatively and achieve state-of-the-art text-to-4D performance. Due to the Gaussian 4D representation, different 4D animations can be seamlessly combined, as we demonstrate. AYG opens up promising avenues for animation, simulation and digital content creation as well as synthetic data generation.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Computer Vision",
      "Generative AI"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper uses text-guided diffusion models, which are key indicators of foundation model research. It combines multiple diffusion models for text - to - 4D synthesis, showing a general - purpose approach that has broader implications for animation, simulation, and digital content creation across different domains.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on text - to - 4D synthesis using diffusion models and related algorithms. It does not mention any robotic hardware systems, input sensors, mechanical kinematics, or learning - based algorithms for controlling robotic actuators. It is a purely algorithmic study without clear connections to robotic systems or hardware, thus not falling within the robotics research domain."
  },
  {
    "paper_id": "nvidia_57",
    "authors": "Danah Yatim, Rafail Fridman, Omer Bar-Tal, Yoni Kasten, Tali Dekel",
    "title": "Space-Time Diffusion Features for Zero-Shot Text-Driven Motion Transfer",
    "paper_url": "https://research.nvidia.com/publication/2024-06_space-time-diffusion-features-zero-shot-text-driven-motion-transfer",
    "pdf_link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Yatim_Space-Time_Diffusion_Features_for_Zero-Shot_Text-Driven_Motion_Transfer_CVPR_2024_paper.pdf ",
    "abstract": "NVIDIA and our third-party partners use cookies and other tools to collect and record information you provide as well as information about your interactions with our websites for performance improvement, analytics, and to assist in our marketing efforts. By clicking Agree\" or \"Manage Settings\", you consent to the use of cookies and other tools as described in our Cookie Policy in accordance with your settings and accept our Terms of Service (which contains important waivers). Please see our Privacy Policy for more information on our privacy practices.",
    "topics": [
      "Generative AI"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The abstract does not contain relevant information about the model's architecture, training approach, or broader impact. While the title mentions 'diffusion features', it doesn't clearly indicate a large - scale, general - purpose deep - learning model trained on extensive data for cross - domain applications. There is insufficient evidence to classify it as foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper title and abstract do not contain any robotics-related concepts, methodologies, or indications of hardware components. The abstract is about cookie usage and privacy policies, not relevant to robotics research."
  },
  {
    "paper_id": "nvidia_58",
    "authors": "",
    "title": "Nemotron-4 340B",
    "paper_url": "https://research.nvidia.com/publication/2024-06_nemotron-4-340b",
    "pdf_link": "https://arxiv.org/pdf/2406.11704 ",
    "abstract": "We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base, Nemotron-4-340B-Instruct, and Nemotron-4-340B-Reward. Our models are open access under the NVIDIA Open Model License Agreement, a permissive\u00a0model\u00a0license\u00a0that allows the distribution, modification, and use of the models and their outputs. These models perform competitively to open access models on a wide range of evaluation benchmarks, and were sized to fit on a single DGX H100 with 8 GPUs when deployed in FP8 precision. We believe that the community can benefit from these models in various research studies and commercial applications, especially for generating synthetic data to train smaller language models. Notably, over 98% of data used in our model alignment process is synthetically generated, showcasing the effectiveness of these models in generating synthetic data. To further support open research and facilitate model development, we are also open-sourcing the synthetic data generation pipeline used in our model alignment process.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Generative AI"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper introduces the Nemotron - 4 340B family of large language models (LLMs). These models are trained to perform well on a wide range of evaluation benchmarks, suggesting they can be used as versatile backbones for various downstream tasks. They are also expected to have broad applications in research and commercial areas, such as generating synthetic data for training smaller language models, which shows a broader impact on general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on the release of language models and their applications in generating synthetic data for training smaller language models. It contains no mention of robotics-related concepts such as hardware systems with sensors and mechanical kinematics, nor learning - based control algorithms for robotic actuators. It is a software - only implementation without any clear connection to robotic systems or hardware."
  },
  {
    "paper_id": "nvidia_59",
    "authors": "Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, Mohammad Shoeybi, Bryan Catanzaro",
    "title": "An Empirical Study of Mamba-based Language Models",
    "paper_url": "https://research.nvidia.com/publication/2024-06_empirical-study-mamba-based-language-models",
    "pdf_link": "https://arxiv.org/pdf/2406.07887",
    "abstract": "Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same training data), however, studies so far have only presented small scale experiments (training with <3B parameters and <1T tokens) comparing SSMs to equivalent Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to an 8B-parameter hybrid architecture consisting of 43% Mamba-2, 7% self-attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of natural language tasks, we answer the important question of whether Mamba models can match their Transformer counterparts at larger training budgets. Our results show that while pure SSM-based models match or exceed Transformers on many tasks, both Mamba and Mamba-2 models lag behind Transformer models on tasks which require strong copying or in-context learning abilities (e.g., five-shot MMLU, Phonebook Lookup) or long-context reasoning. In contrast, we find that the 8B-parameter Mamba2-Hybrid exceeds the 8B-parameter Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8\u00d7 faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequence lengths. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Machine Translation",
      "Natural Language Processing"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper compares large - scale language models (8B - parameter) based on SSMs (Mamba, Mamba - 2) and a hybrid architecture with Transformer models. These models are trained on large amounts of data (up to 3.5T tokens) and evaluated on a diverse set of natural language tasks, indicating a general - purpose approach with broader implications for AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on an empirical study of Mamba - based language models, comparing them with Transformer models in natural language tasks. It is a purely algorithmic study without any clear connections to robotic systems, hardware components, or the key indicators of robotics research."
  },
  {
    "paper_id": "nvidia_60",
    "authors": "Honglin Chen, Hsueh-Ti Derek Liu, David I.W. Levin, Changxi Zheng, Alec Jacobson",
    "title": "Stabler Neo-Hookean Simulation: Absolute Eigenvalue Filtering for Projected Newton",
    "paper_url": "https://research.nvidia.com/publication/2024-06_stabler-neo-hookean-simulation-absolute-eigenvalue-filtering-projected-newton",
    "pdf_link": "https://arxiv.org/pdf/2406.05928 ",
    "abstract": "Volume-preserving hyperelastic materials are widely used to model near-incompressible materials such as rubber and soft tissues. However, the numerical simulation of volume-preserving hyperelastic materials is notoriously challenging within this regime due to the non-convexity of the energy function. In this work, we identify the pitfalls of the popular eigenvalue clamping strategy for projecting Hessian matrices to positive semi-definiteness during Newton's method. We introduce a novel eigenvalue filtering strategy for projected Newton's method to stabilize the optimization of Neo-Hookean energy and other volume-preserving variants under high Poisson's ratio (near 0.5) and large initial volume change. Our method only requires a single line of code change in the existing projected Newton framework, while achieving significant improvement in both stability and convergence speed. We demonstrate the effectiveness and efficiency of our eigenvalue projection scheme on a variety of challenging examples and over different deformations on a large dataset.",
    "topics": [
      "Computer Graphics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on a numerical simulation method for volume - preserving hyperelastic materials. It does not involve deep learning models (especially transformer - based), and there are no mentions of key indicators such as LMM, LLM, etc. It is a specialized application in the field of material simulation without broader implications for general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on a novel eigenvalue filtering strategy for the numerical simulation of volume - preserving hyperelastic materials, which is a purely algorithmic study without clear connections to robotic systems or hardware, thus falling outside the robotics research domain."
  },
  {
    "paper_id": "nvidia_61",
    "authors": "Bowen Wen, Wei Yang, Jan Kautz, Stan Birchfield",
    "title": "FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects",
    "paper_url": "https://research.nvidia.com/publication/2024-06_foundationpose-unified-6d-pose-estimation-and-tracking-novel-objects",
    "pdf_link": "https://openaccess.thecvf.com/content/CVPR2024/papers/Wen_FoundationPose_Unified_6D_Pose_Estimation_and_Tracking_of_Novel_Objects_CVPR_2024_paper.pdf",
    "abstract": "We present FoundationPose, a unified foundation model for 6D object pose estimation and tracking, supporting both model-based and model-free setups. Our approach can be instantly applied at test-time to a novel object without fine-tuning, as long as its CAD model is given, or a small number of reference images are captured. We bridge the gap between these two setups with a neural implicit representation that allows for effective novel view synthesis, keeping the downstream pose estimation modules invariant under the same unified framework. Strong generalizability is achieved via large-scale synthetic training, aided by a large language model (LLM), a novel transformer-based architecture, and contrastive learning formulation. Extensive evaluation on multiple public datasets involving challenging scenarios and objects indicate our unified approach outperforms existing methods specialized for each task by a large margin. In addition, it even achieves comparable results to instance-level methods despite the reduced assumptions.",
    "topics": [
      "Applied Perception",
      "Computer Graphics",
      "Computer Vision",
      "Robotics",
      "VR, AR and Display Technology"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper presents FoundationPose, a unified model. It uses a large language model (LLM) and a novel transformer - based architecture, trained on large - scale synthetic data. It can be applied to novel objects without fine - tuning and outperforms specialized methods, showing strong generalizability and cross - task applicability, which aligns with foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on a unified foundation model for 6D object pose estimation and tracking, presenting mainly algorithmic aspects such as neural implicit representation, large - scale synthetic training, and a novel transformer - based architecture. There are no clear connections to robotic systems or hardware, and it seems to be a purely algorithmic study, thus not falling within the robotics research domain."
  },
  {
    "paper_id": "nvidia_62",
    "authors": "Zhenggang Tang, Zhongzheng Ren, Xiaoming Zhao, Bowen Wen, Jonathan Tremblay, Stan Birchfield, Alexander Schwing",
    "title": "NeRFDeformer: NeRF Transformation from a Single View via 3D Scene Flows",
    "paper_url": "https://research.nvidia.com/publication/2024-06_nerfdeformer-nerf-transformation-single-view-3d-scene-flows",
    "pdf_link": "https://arxiv.org/pdf/2406.10543",
    "abstract": "We present a method for automatically modifying a NeRF representation based on a single observation of a non-rigid transformed version of the original scene. Our method defines the transformation as a 3D flow, specifically as a weighted linear blending of rigid transformations of 3D anchor points that are defined on the surface of the scene. In order to identify anchor points, we introduce a novel correspondence algorithm that first matches RGB-based pairs, then leverages multi-view information and 3D reprojection to robustly filter false positives in two steps. We also introduce a new dataset for exploring the problem of modifying a NeRF scene through a single observation. Our dataset contains 113 synthetic scenes leveraging 47 3D assets. We show that our proposed method outperforms NeRF editing methods as well as diffusion-based methods, and we also explore different methods for filtering correspondences.",
    "topics": [
      "Computer Vision"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on a method for modifying a NeRF representation from a single view, which is a narrow application in 3D scene processing. It does not involve large - scale pre - trained models, multi - modal capabilities, or have broader implications for general - purpose AI, thus not meeting the criteria for foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on a method for modifying a NeRF representation with a novel correspondence algorithm and a new dataset. It is a purely algorithmic study without any clear connections to robotic systems or hardware, thus not falling within the robotics research domain."
  },
  {
    "paper_id": "nvidia_63",
    "authors": "Yijia Weng, Bowen Wen, Jonathan Tremblay, Valts Blukis, Dieter Fox, Leo Guibas, Stan Birchfield",
    "title": "Neural Implicit Representation for Building Digital Twins of Unknown Articulated Objects",
    "paper_url": "https://research.nvidia.com/publication/2024-06_neural-implicit-representation-building-digital-twins-unknown-articulated",
    "pdf_link": "https://arxiv.org/pdf/2404.01440",
    "abstract": "We address the problem of building digital twins of unknown articulated objects from two RGBD scans of the object at different articulation states. We decompose the problem into two stages, each addressing distinct aspects. Our method first reconstructs object-level shape at each state, then recovers the underlying articulation model including part segmentation and joint articulations that associate the two states. By explicitly modeling point-level correspondences and exploiting cues from images, 3D reconstructions, and kinematics, our method yields more accurate and stable results compared to prior work. It also handles more than one movable part and does not rely on any object shape or structure priors.",
    "topics": [
      "Computer Vision",
      "Robotics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on building digital twins of unknown articulated objects, which is a narrow and specialized application. There is no mention of large - scale data - trained deep learning models, multi - modal models, or cross - domain applications, so it does not meet the criteria for foundation model research.",
    "is_robotics": true,
    "robotic_reason": "The paper focuses on building digital twins of unknown articulated objects, which involves joint articulations and kinematics. These concepts are related to the mechanical kinematics and joint movements in robotics research, thus indicating that the paper falls within the robotics research domain."
  },
  {
    "paper_id": "nvidia_64",
    "authors": "Zoe (Jing) Xu, Josef Spjut, Ben Boudaoud, Simona Buetti, Alejandro Lleras, Ruth Rosenholtz",
    "title": "Do Action Video Game Players Search Faster Than Non-Players?",
    "paper_url": "https://research.nvidia.com/publication/2024-05_do-action-video-game-players-search-faster-non-players",
    "pdf_link": null,
    "abstract": "Studies have shown that action video game players have enhanced visual abilities in various domains, such as multiple object tracking, size of the useful field of view, and visual search speed and accuracy. These improvements have been attributed to either a general advantage in \u201clearning to learn\u201d abilities, or domain-specific enhancement(s) in the \u201ccommon demands\u201d between specific games and experimental tasks. To investigate these two theories, we conducted six experiments examining whether and how players and non-players differ in various aspects of visual search. First, we used a staircase to determine the minimal display duration (Experiment1a) and target-distractor color difference (Experiment1b) required for participants to successfully identify a target in a color search task. Next, we assessed participants\u2019 search speed and the cost of switching target and distractor identities when there is one (Experiment2a) or multiple distractor types (Experiment2b). Finally, we measured search speed in harder T/L search (Experiment3a) and game-style figure search (Experiment3b). This study is the first to use both a staircase procedure and standard response time measures to discern differences between players and non-players in visual search. The results suggest that players search faster than non-players only in Experiment2, where performance degraded with increased distractor variability for non-players but not for players. Players also exhibited a smaller cost to switching the target and distractor identities. These findings imply that while there might be no overall enhancement in players\u2019 search abilities, they might benefit from holding variable distractor templates and switching their search target, potentially due to gaming experience which often necessitates memorizing and switching among multiple objects to monitor/avoid (as in first-person shooting games). These results support the \u201ccommon demands\u201d theory. In addition, our collected data on the specific games participants play allow for a more systematic evaluation of which games might enhance which search-related abilities.",
    "topics": [
      "Applied Perception",
      "Esports"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on comparing visual search abilities between action video game players and non - players through a series of experiments. It does not involve deep learning models trained on large amounts of data, nor does it have any mention of key indicators of foundation model research, and is thus not related to foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on comparing visual search abilities between action video game players and non - players through a series of experiments. It does not involve hardware systems with input sensors and mechanical kinematics, nor learning - based algorithms for controlling such systems. There are no robotics - related concepts like reinforcement learning, robot navigation, etc. in the paper, making it outside the robotics research domain."
  },
  {
    "paper_id": "nvidia_65",
    "authors": "Setareh Cohan, Guy Tevet, Daniele Reda, Xue Bin Peng, Michiel van de Panne",
    "title": "Flexible Motion In-betweening with Diffusion Models",
    "paper_url": "https://research.nvidia.com/publication/2024-05_flexible-motion-betweening-diffusion-models",
    "pdf_link": "https://arxiv.org/pdf/2405.11126",
    "abstract": "Motion in-betweening, a fundamental technique in animation, has long been recognized as a labor-intensive and challenging process. We investigate the potential of diffusion models in generating diverse human motions guided by keyframes. Unlike previous inbetweening methods, we propose a simple unified model capable of generating precise and diverse motions that conform to a flexible range of user-specified constraints, as well as text conditioning. To this end, we propose Conditional Motion Diffusion In-betweening (CondMDI) which allows for arbitrary dense-or-sparse keyframe placement and partial keyframe constraints and generates high-quality motions that are both diverse and coherent with the given keyframes. We further explore the use of guidance and imputation-based methods for inference-time keyframing. We evaluate the performance of our diffusion-based in-betweening method on the text-conditioned HumanML3D dataset and demonstrate the versatility and efficacy of diffusion models for keyframe in-betweening.",
    "topics": [
      "Generative AI"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper uses diffusion models, which are a key indicator of foundation model research. The proposed unified model can handle various user - specified constraints and text conditioning, showing general - purpose modeling capabilities and potential for broader applications beyond narrow use cases.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on using diffusion models for motion in - betweening in animation, with no mention of robotic hardware systems, input sensors, mechanical kinematics, or learning - based control algorithms for robots. It is a purely algorithmic study without clear connections to robotic systems or hardware."
  },
  {
    "paper_id": "nvidia_66",
    "authors": "Sammy Christen, Lan Feng, Wei Yang, Yu-Wei Chao, Otmar Hilliges, Jie Song",
    "title": "SynH2R: Synthesizing Hand-Object Motions for Learning Human-to-Robot Handovers",
    "paper_url": "https://research.nvidia.com/publication/2024-05_synh2r-synthesizing-hand-object-motions-learning-human-robot-handovers",
    "pdf_link": "https://arxiv.org/pdf/2311.05599",
    "abstract": "Vision-based human-to-robot handover is an important and challenging task in human-robot interaction. Recent work has attempted to train robot policies by interacting with dynamic virtual humans in simulated environments, where the policies can later be transferred to the real world. However, a major bottleneck is the reliance on human motion capture data, which is expensive to acquire and difficult to scale to arbitrary objects and human grasping motions. In this paper, we introduce a framework that can generate plausible human grasping motions suitable for training the robot. To achieve this, we propose a hand-object synthesis method that is designed to generate handover-friendly motions similar to humans. This allows us to generate synthetic training and testing data with 100x more objects than previous work. In our experiments, we show that our method trained purely with synthetic data is competitive with state-of-the-art methods that rely on real human motion data both in simulation and on a real system. In addition, we can perform evaluations on a larger scale compared to prior work. With our newly introduced test set, we show that our model can better scale to a large variety of unseen objects and human motions compared to the baselines.",
    "topics": [
      "Computer Vision",
      "Robotics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on a narrow application of generating synthetic human grasping motions for human - to - robot handovers. It does not involve large - scale pre - trained models, multi - modal models, or general - purpose modeling approaches with cross - domain applications. The model's impact is limited to this specific task and lacks broader implications for general - purpose AI capabilities.",
    "is_robotics": true,
    "robotic_reason": "The paper focuses on vision - based human - to - robot handover, a manipulation task in human - robot interaction. It involves generating synthetic data for training robot policies, with experiments conducted in simulation and on a real system, which clearly connects to robotic systems and hardware."
  },
  {
    "paper_id": "nvidia_67",
    "authors": "Chia-Liang Kuo, Yu-Wei Chao, Yi-Ting Chen",
    "title": "SKT-Hang: Hanging Everyday Objects via Object-Agnostic Semantic Keypoint Trajectory Generation",
    "paper_url": "https://research.nvidia.com/publication/2024-05_skt-hang-hanging-everyday-objects-object-agnostic-semantic-keypoint-trajectory",
    "pdf_link": "https://arxiv.org/pdf/2312.04936 ",
    "abstract": "We study the problem of hanging a wide range of grasped objects on diverse supporting items. Hanging objects is a ubiquitous task that is encountered in numerous aspects of our everyday lives. However, both the objects and supporting items can exhibit substantial variations in their shapes and structures, bringing two challenging issues: (1) determining the task-relevant geometric structures across different objects and supporting items, and (2) identifying a robust action sequence to accommodate the shape variations of supporting items. To this end, we propose Semantic Keypoint Trajectory (SKT), an object-agnostic representation that is highly versatile and applicable to various everyday objects. We also propose Shape-conditioned Trajectory Deformation Network (SCTDN), a model that learns to generate SKT by deforming a template trajectory based on the task-relevant geometric structure features of the supporting items. We conduct extensive experiments and demonstrate substantial improvements in our framework over existing robot hanging methods in the success rate and inference time. Finally, our simulation-trained framework shows promising hanging results in the real world. For videos and supplementary materials, please visit our project webpage: https://hcis-lab.github.io/SKT-Hang/.",
    "topics": [
      "Robotics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on a narrow and specialized application of robot hanging objects. It proposes a model for this specific task without involving large - scale pre - trained models, cross - domain applications, or broader general - purpose AI capabilities.",
    "is_robotics": true,
    "robotic_reason": "The paper focuses on the manipulation task of hanging everyday objects. It proposes methods to address challenges in this task and conducts experiments on existing robot hanging methods. Additionally, it mentions that the simulation - trained framework shows results in the real world, indicating a connection to robotic hardware systems."
  },
  {
    "paper_id": "nvidia_68",
    "authors": "Manu Gopakumar, Gun-Yeal Lee, Suyeon Choi, Brian Chao, Yifan Peng, Jonghyun Kim, Gordon Wetzstein",
    "title": "Full-colour 3D holographic augmented-reality displays with metasurface waveguides",
    "paper_url": "https://research.nvidia.com/publication/2024-05_full-colour-3d-holographic-augmented-reality-displays-metasurface-waveguides",
    "pdf_link": "https://www.nature.com/articles/s41586-024-07386-0.pdf",
    "abstract": "Emerging spatial computing systems seamlessly superimpose digital information on the physical environment observed by a user, enabling transformative experiences across various domains, such as entertainment, education, communication and training1,2,3. However, the widespread adoption of augmented-reality (AR) displays has been limited due to the bulky projection optics of their light engines and their inability to accurately portray three-dimensional (3D) depth cues for virtual content, among other factors4,5. Here we introduce a holographic AR system that overcomes these challenges using a unique combination of inverse-designed full-colour metasurface gratings, a compact dispersion-compensating waveguide geometry and artificial-intelligence-driven holography algorithms. These elements are co-designed to eliminate the need for bulky collimation optics between the spatial light modulator and the waveguide and to present vibrant, full-colour, 3D AR content in a compact device form factor. To deliver unprecedented visual quality with our prototype, we develop an innovative image formation model that combines a physically accurate waveguide model with learned components that are automatically calibrated using camera feedback. Our unique co-design of a nanophotonic metasurface waveguide and artificial-intelligence-driven holographic algorithms represents a significant advancement in creating visually compelling 3D AR experiences in a compact wearable device.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Computer Graphics",
      "VR, AR and Display Technology"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on developing a holographic AR system using nanophotonic metasurface waveguides and AI - driven holographic algorithms for a specific application of 3D AR displays. There is no mention of large - scale deep - learning models (especially transformer - based), pre - trained models, or general - purpose modeling approaches. It is a specialized application without broader foundational impact on general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on the development of a holographic augmented - reality display system, involving metasurface waveguides and holographic algorithms. There are no mentions of hardware systems with input sensors and mechanical kinematics, nor any of the key robotics research indicators. It is mainly a software and nanophotonic hardware study without a clear connection to robotic systems as defined."
  },
  {
    "paper_id": "nvidia_69",
    "authors": "Ali Hatamizadeh, Greg Heinrich, Hongxu Danny Yin, Andrew Tao, Jose M. Alvarez, Jan Kautz, Pavlo Molchanov",
    "title": "FasterViT: Fast Vision Transformers with Hierarchical Attention",
    "paper_url": "https://research.nvidia.com/publication/2024-05_fastervit-fast-vision-transformers-hierarchical-attention",
    "pdf_link": "https://arxiv.org/pdf/2306.06189 ",
    "abstract": "We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Computer Vision"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on designing a hybrid CNN - ViT neural network (FasterViT) for high image throughput in computer vision applications. It is mainly for specific CV tasks like classification, object detection, and segmentation, lacking indications of large - scale pre - training on diverse data, and has a narrow focus on CV rather than broader cross - domain applications or general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on designing a new family of hybrid CNN - ViT neural networks for computer vision applications. It mainly discusses algorithmic improvements in neural networks and their performance on CV tasks. There are no mentions of hardware systems with sensors and mechanical kinematics, and no clear connections to robotics - related concepts such as reinforcement learning, manipulation tasks, or robot navigation. It is a purely algorithmic study without clear links to robotic systems or hardware."
  },
  {
    "paper_id": "nvidia_70",
    "authors": "Katja Schwarz, Seung Wook Kim, Jun Gao, Sanja Fidler, Andreas Geiger, Karsten Kreis",
    "title": "WildFusion: Learning 3D-Aware Latent Diffusion Models in View Space",
    "paper_url": "https://research.nvidia.com/publication/2024-05_wildfusion-learning-3d-aware-latent-diffusion-models-view-space",
    "pdf_link": "https://arxiv.org/pdf/2311.13570",
    "abstract": "Modern learning-based approaches to 3D-aware image synthesis achieve high photorealism and 3D-consistent viewpoint changes for the generated images. Existing approaches represent instances in a shared canonical space. However, for in-the-wild datasets a shared canonical system can be difficult to define or might not even exist. In this work, we instead model instances in view space, alleviating the need for posed images and learned camera distributions. We find that in this setting, existing GAN-based methods are prone to generating flat geometry and struggle with distribution coverage. We hence propose WildFusion, a new approach to 3D-aware image synthesis based on latent diffusion models (LDMs). We first train an autoencoder that infers a compressed latent representation, which additionally captures the images' underlying 3D structure and enables not only reconstruction but also novel view synthesis. To learn a faithful 3D representation, we leverage cues from monocular depth prediction. Then, we train a diffusion model in the 3D-aware latent space, thereby enabling synthesis of high-quality 3D-consistent image samples, outperforming recent state-of-the-art GAN-based methods. Importantly, our 3D-aware LDM is trained without any direct supervision from multiview images or 3D geometry and does not require posed images or learned pose or camera distributions. It directly learns a 3D representation without relying on canonical camera coordinates. This opens up promising research avenues for scalable 3D-aware image synthesis and 3D content creation from in-the-wild image data.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Computer Vision",
      "Generative AI"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper proposes WildFusion, an approach based on latent diffusion models (LDMs), which are key in foundation model research. It aims for scalable 3D - aware image synthesis and 3D content creation from in - the - wild data, suggesting broader general - purpose AI capabilities and cross - domain potential in 3D and image synthesis applications.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on 3D - aware image synthesis using latent diffusion models. It does not mention any hardware systems with sensors and mechanical kinematics, nor any of the key indicators of robotics research. It is a purely algorithmic study without clear connections to robotic systems or hardware."
  },
  {
    "paper_id": "nvidia_71",
    "authors": "Matt Pharr, Bartlomiej Wronski, Marco Salvi, Marcos Fajardo",
    "title": "Filtering After Shading With Stochastic Texture Filtering",
    "paper_url": "https://research.nvidia.com/publication/2024-05_filtering-after-shading-stochastic-texture-filtering",
    "pdf_link": "https://d1qx31qr3h6wln.cloudfront.net/publications/stochtex.pdf",
    "abstract": "2D texture maps and 3D voxel arrays are widely used to add rich detail to the surfaces and volumes of rendered scenes, and filtered texture lookups are integral to producing high-quality imagery. We show that applying the texture filter after evaluating shading generally gives more accurate imagery than filtering textures before BSDF evaluation, as is current practice. These benefits are not merely theoretical, but are apparent in common cases. We demonstrate that practical and efficient filtering after shading is possible through the use of stochastic sampling of texture filters.",
    "topics": [
      "Computer Graphics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on texture filtering in rendered scenes and improving imagery quality. It does not involve deep - learning models trained on large data, and there are no mentions of the key indicators of foundation model research. The work is a narrow application in the field of computer graphics without broader implications for general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on texture filtering for rendered scenes to improve imagery quality, which is a purely algorithmic and software - only topic without any clear connection to robotic systems, hardware components, or robotics - related concepts."
  },
  {
    "paper_id": "nvidia_72",
    "authors": "YuChen Hu, Chen Chen, Huck Yang, Ruizhe Li, Chao Zhang, Pin-Yu Chen, EnSiong Chng",
    "title": "Large Language Models are Efficient Learners of Noise-Robust Speech Recognition",
    "paper_url": "https://research.nvidia.com/publication/2024-05_large-language-models-are-efficient-learners-noise-robust-speech-recognition",
    "pdf_link": "https://arxiv.org/pdf/2401.10446 ",
    "abstract": "Recent advances in large language models (LLMs) have promoted generative error correction (GER) for automatic speech recognition (ASR), which leverages the rich linguistic knowledge and powerful reasoning ability of LLMs to improve recognition results. The latest work proposes a GER benchmark with HyPoradise dataset to learn the mapping from ASR N-best hypotheses to ground-truth transcription by efficient LLM finetuning, which shows great effectiveness but lacks specificity on noise-robust ASR. In this work, we extend the benchmark to noisy conditions and investigate if we can teach LLMs to perform denoising for GER just like what robust ASR do}, where one solution is introducing noise information as a conditioner into LLM. However, directly incorporating noise embeddings from audio encoder could harm the LLM tuning due to cross-modality gap. To this end, we propose to extract a language-space noise embedding from the N-best list to represent the noise conditions of source speech, which can promote the denoising process in GER. Furthermore, in order to enhance its representation ability of audio noise, we design a knowledge distillation (KD) approach via mutual information estimation to distill the real noise information in audio embeddings to our language embedding. Experiments on various latest LLMs demonstrate our approach achieves a new breakthrough with up to 53.9% correction improvement in terms of word error rate while with limited training data. Analysis shows that our language-space noise embedding can well represent the noise conditions of source speech, under which off-the-shelf LLMs show strong ability of language-space denoising.",
    "topics": [
      "Applied Perception",
      "Artificial Intelligence and Machine Learning",
      "Generative AI",
      "Machine Translation",
      "Natural Language Processing",
      "Speech Processing"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper focuses on large language models (LLMs), a key indicator of foundation model research. It finetunes existing LLMs for speech recognition, demonstrating their use as versatile backbones for a downstream task. The research also has broader implications for general - purpose AI capabilities by enhancing the noise - robustness of LLMs in speech recognition.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on improving speech recognition using large language models, proposing methods for generative error correction and noise handling. It contains no mention of hardware systems with sensors and mechanical kinematics, nor any of the key robotics research indicators. It is a purely algorithmic study without a clear connection to robotic systems or hardware."
  },
  {
    "paper_id": "nvidia_73",
    "authors": "Chen Chen, Ruizhe Li, Yuchen Hu, Sabato Marco Siniscalchi, Pin-Yu Chen, Ensiong Chng, Huck Yang",
    "title": "It's Never Too Late: Fusing Acoustic Information into Large Language Models for Automatic Speech Recognition",
    "paper_url": "https://research.nvidia.com/publication/2024-05_it-s-never-too-late-fusing-acoustic-information-large-language-models-automatic",
    "pdf_link": "https://arxiv.org/pdf/2402.05457",
    "abstract": "Recent studies have successfully shown that large language models (LLMs) can be successfully used for generative error correction (GER) on top of the automatic speech recognition (ASR) output. Specifically, an LLM is utilized to carry out a direct mapping from the N-best hypotheses list generated by an ASR system to the predicted output transcription. However, despite its effectiveness, GER introduces extra data uncertainty since the LLM is trained without taking into account acoustic information available in the speech signal. In this work, we aim to overcome such a limitation by infusing acoustic information before generating the predicted transcription through a novel late fusion solution termed Uncertainty-Aware Dynamic Fusion (UADF). UADF is a multimodal fusion approach implemented into an auto-regressive decoding process and works in two stages: (i) It first analyzes and calibrates the token-level LLM decision, and (ii) it then dynamically assimilates the information from the acoustic modality. Experimental evidence collected from various ASR tasks shows that UADF surpasses existing fusion mechanisms in several ways. It yields significant improvements in word error rate (WER) while mitigating data uncertainty issues in LLM and addressing the poor generalization relied with sole modality during fusion. We also demonstrate that UADF seamlessly adapts to audio-visual speech recognition.",
    "topics": [
      "Applied Perception",
      "Artificial Intelligence and Machine Learning",
      "Generative AI",
      "Machine Translation",
      "Natural Language Processing",
      "Speech Processing"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper involves large language models (LLMs), a key indicator of foundation model research. It proposes a multimodal fusion approach to address limitations in using LLMs for speech recognition, which has broader implications for general - purpose AI capabilities and cross - domain applications, as shown by its adaptation to audio - visual speech recognition.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on fusing acoustic information into large language models for automatic speech recognition. It presents a software - only algorithmic solution for ASR tasks without any clear connection to robotic systems, hardware components, or robotics - related concepts such as reinforcement learning, manipulation tasks, etc."
  },
  {
    "paper_id": "nvidia_74",
    "authors": "Albert Wu, Ruocheng Wang, Sirui Chen, Clemens Eppner, C Karen Liu",
    "title": "One-Shot Transfer of Long-Horizon Extrinsic Manipulation Through Contact Retargeting",
    "paper_url": "https://research.nvidia.com/publication/2024-04_one-shot-transfer-long-horizon-extrinsic-manipulation-through-contact",
    "pdf_link": "https://arxiv.org/pdf/2404.07468 ",
    "abstract": "Extrinsic manipulation, the use of environment contacts to achieve manipulation objectives, enables strategies that are otherwise impossible with a parallel jaw gripper. However, orchestrating a long-horizon sequence of contact interactions between the robot, object, and environment is notoriously challenging due to the scene diversity, large action space, and difficult contact dynamics. We observe that most extrinsic manipulation are combinations of short-horizon primitives, each of which depend strongly on initializing from a desirable contact configuration to succeed. Therefore, we propose to generalize one extrinsic manipulation trajectory to diverse objects and environments by retargeting contact requirements. We prepare a single library of robust short-horizon, goal-conditioned primitive policies, and design a framework to compose state constraints stemming from contacts specifications of each primitive. Given a test scene and a single demo prescribing the primitive sequence, our method enforces the state constraints on the test scene and find intermediate goal states using inverse kinematics. The goals are then tracked by the primitive policies. Using a 7+1 DoF robotic arm-gripper system, we achieved an overall success rate of 80.5% on hardware over 4 long-horizon extrinsic manipulation tasks, each with up to 4 primitives. Our experiments cover 10 objects and 6 environment configurations. We further show empirically that our method admits a wide range of demonstrations, and that contact retargeting is indeed the key to successfully combining primitives for long-horizon extrinsic manipulation.",
    "topics": [
      "Robotics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on a narrow robotic extrinsic manipulation task, proposing a method for contact retargeting in this specific domain. It does not involve large - scale deep - learning models trained on vast data for generalized factual realities, nor does it show broader cross - domain applications or general - purpose AI capabilities.",
    "is_robotics": true,
    "robotic_reason": "The paper focuses on extrinsic manipulation, a manipulation task. It uses a 7+1 DoF robotic arm - gripper system (a hardware system) and applies inverse kinematics for motion planning. These elements align with the robotics research domain."
  },
  {
    "paper_id": "nvidia_75",
    "authors": "Rinon Gal, Or Lichter, Elad Richardson, Or Patashnik, Amit H Bermano, Gal Chechik, Daniel Cohen-Or",
    "title": "LCM-Lookahead for Encoder-based Text-to-Image Personalization",
    "paper_url": "https://research.nvidia.com/publication/2024-04_lcm-lookahead-encoder-based-text-image-personalization",
    "pdf_link": "https://arxiv.org/pdf/2404.03620",
    "abstract": "Recent advancements in diffusion models have introduced fast sampling methods that can effectively produce high-quality images in just one or a few denoising steps. Interestingly, when these are distilled from existing diffusion models, they often maintain alignment with the original model, retaining similar outputs for similar prompts and seeds. These properties present opportunities to leverage fast sampling methods as a shortcut-mechanism, using them to create a preview of denoised outputs through which we can backpropagate image-space losses. In this work, we explore the potential of using such shortcut-mechanisms to guide the personalization of text-to-image models to specific facial identities. We focus on encoder-based personalization approaches, and demonstrate that by augmenting their training with a lookahead identity loss, we can achieve higher identity fidelity, without sacrificing layout diversity or prompt alignment.",
    "topics": [
      "Computer Graphics",
      "Computer Vision",
      "Generative AI"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper involves diffusion models, which are a key indicator of foundation model research. It uses existing diffusion models and explores a method for text - to - image model personalization, showing an application on a general - purpose model with potential for broader AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on text - to - image personalization using diffusion models and encoder - based approaches. It involves purely algorithmic studies and software - only implementations without any clear connections to robotic systems, hardware components, or the key indicators of robotics research."
  },
  {
    "paper_id": "nvidia_76",
    "authors": "Yoshinori Nishi, John W. Poulton, Xi Chen, Sanquan Song, Brian Zimmer, Walker Turner, Stephen Tell, Nikola Nedovic, John Wilson, William Dally, Tom Gray",
    "title": "A 0.190-pJ/bit 25.2-Gb/s/wire Inverter-Based AC-Coupled Transceiver for Short-Reach Die-to-Die Interfaces in 5-nm CMOS",
    "paper_url": "https://research.nvidia.com/publication/2024-04_0190-pjbit-252-gbswire-inverter-based-ac-coupled-transceiver-short-reach-die",
    "pdf_link": null,
    "abstract": "This article presents an inverter-based short-reach ac-coupled toggle (ISR-ACT) link targeted for short-reach die-to-die communication over silicon interposer or similar high-density interconnect. The ISR-ACT\u2019s transmitter (TX) sends non-return-to-zero (NRZ) data through a small series capacitor to inject low-swing pulses into the line. These pulses are amplified and latched by a two-stage receiver (RX), where the 1st-stage transimpedance amplifier (TIA) amplifies the pulse data and positive feedback around both stages captures the data and maintains the dc level on the line. Achieving low signal swing through a capacitor divider, the ACT driver consumes less than half the power compared to regular rail-to-rail CMOS drivers while providing dc voltage isolation between the driver and the RX. Fabricated in a 5-nm standard CMOS process, the ISR-ACT link, operating on a 0.75-V supply, shows 0.66 UI margin at 25.2 Gb/s/wire over a 1.2-mm on-chip channel and demonstrates an energy efficiency of 0.190 pJ/bit.",
    "topics": [
      "Circuits and VLSI Design"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on an inverter - based short - reach ac - coupled transceiver for die - to - die communication in a 5 - nm CMOS process. It does not involve deep learning models, training on large amounts of data, or general - purpose modeling for cross - domain applications, but rather a specialized hardware - related solution for a narrow communication application.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on a transceiver for short - reach die - to - die communication in a CMOS process. It is a purely hardware - related study in the field of integrated circuits and does not involve any robotics - related concepts such as learning - based algorithms for robotic control, or hardware systems with input sensors and mechanical kinematics for joint movements."
  },
  {
    "paper_id": "nvidia_77",
    "authors": "Benjamin Watson, Josef Spjut, Joohwan Kim, Byungjoo Lee, Mijin Yoo, Peter Shirley, Rulon Raymond",
    "title": "Is Less More? Rendering for Esports",
    "paper_url": "https://research.nvidia.com/publication/2024-03_less-more-rendering-esports",
    "pdf_link": "https://d1qx31qr3h6wln.cloudfront.net/publications/CG_A_2024__Is_Less_More__Esports_Rendering.pdf",
    "abstract": "Computer graphics research has long prioritized image quality over frame rate. Yet demand for an alternative is growing, with many esports players turning off visual effects to improve frame rates. Is it time for graphics researchers to reconsider their goals? A workshop at the 2023 SIGGRAPH Conference explored this question. Three researchers made provocative presentations, each of which were then discussed by dozens of research and industry attendees. We summarize those presentations and discussions here, concluding with potential research questions, and future plans for esports at SIGGRAPH.",
    "topics": [
      "Applied Perception",
      "Computer Graphics",
      "Esports",
      "Human Computer Interaction",
      "Real-Time Rendering"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on a narrow topic of rendering in esports, discussing the trade - off between image quality and frame rate. It does not involve deep - learning models trained on large data for generalized factual realities or have broader implications for general - purpose AI capabilities as defined in foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on computer graphics research related to esports, specifically the trade - off between image quality and frame rate. There are no mentions of hardware systems with sensors and mechanical kinematics, nor any of the key robotics research indicators. It is a software - oriented study in the field of computer graphics without clear connections to robotic systems or hardware."
  },
  {
    "paper_id": "nvidia_78",
    "authors": "Kevin Xie, Jonathan Lorraine, Tianshi Cao, Jun Gao, James Lucas, Antonio Torralba, Sanja Fidler, Xiaohui Zeng",
    "title": "LATTE3D: Large-scale Amortized Text-To-Enhanced3D Synthesis",
    "paper_url": "https://research.nvidia.com/publication/2024-03_latte3d-large-scale-amortized-text-enhanced3d-synthesis",
    "pdf_link": "https://arxiv.org/pdf/2403.15385",
    "abstract": "Recent text-to-3D generation approaches produce impressive 3D results but require time-consuming optimization that can take up to an hour per prompt. Amortized methods like ATT3D optimize multiple prompts simultaneously to improve efficiency, enabling fast text-to-3D synthesis. However, ATT3D cannot capture high-frequency geometry and texture details and struggles to scale to large prompt sets, so it generalizes poorly. We introduce Latte3D, addressing these limitations to achieve fast, high-quality generation on a significantly larger prompt set. Key to our method is 1) building a scalable architecture for amortized learning and 2) leveraging 3D data during optimization through 3D-aware diffusion priors, shape regularization, and model initialization to achieve robustness to diverse and complex training prompts. Latte3D amortizes both neural field generation and textured surface generation to produce highly detailed textured meshes in a single forward pass. Latte3D generates 3D objects in 400ms, and can be further enhanced with fast test-time optimization.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Computer Graphics",
      "Generative AI"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper introduces Latte3D which uses diffusion priors in its approach. Diffusion models are a key indicator of foundation model research. Also, the model is designed to handle a large prompt set and aims for high - quality, fast 3D generation, showing potential for cross - domain applications in 3D synthesis, thus having broader implications for general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on large - scale amortized text - to - 3D synthesis, which is a purely algorithmic study without any clear connections to robotic systems or hardware. It does not mention any of the key indicators related to robotics research."
  },
  {
    "paper_id": "nvidia_79",
    "authors": "Yang Zhang, Travis M. Bartley, Mariana Graterol-Fuenmayor, Vitaly Lavrukhin, Evelina Bakhturina, Boris Ginsburg",
    "title": "A Chat about Boring Problems: Studying GPT-Based Text Normalization",
    "paper_url": "https://research.nvidia.com/publication/2024-03_chat-about-boring-problems-studying-gpt-based-text-normalization",
    "pdf_link": "https://arxiv.org/pdf/2309.13426",
    "abstract": "Text normalization - the conversion of text from written to spoken form - is traditionally assumed to be an ill-formed task for language modeling. In this work, we argue otherwise. We empirically show the capacity of Large-Language Models (LLM) for text normalization in few-shot scenarios. Combining self-consistency reasoning with linguistic-informed prompt engineering, we find LLM-based text normalization to achieve error rates approximately 40% lower than production-level normalization systems. Further, upon error analysis, we note key limitations in the conventional design of text normalization tasks. We create a new taxonomy of text normalization errors and apply it to results from GPT-3.5-Turbo and GPT-4.0. Through this new framework, we identify strengths and weaknesses of LLM-based TN, opening opportunities for future work.",
    "topics": [
      "Machine Translation",
      "Natural Language Processing",
      "Speech Processing"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper uses Large-Language Models (LLMs), which are a key indicator of foundation model research. It demonstrates the application of LLMs in text normalization, showing the models' versatility across different tasks and broader impact on general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on text normalization using large - language models, which is a purely algorithmic study without any clear connections to robotic systems, hardware components, or robotics - related concepts such as those listed in the key indicators."
  },
  {
    "paper_id": "nvidia_80",
    "authors": "Chia-Tung (Mark) Ho, Ajay Chandna, David Guan, Alvin Ho, Minsoo Kim, Yaguang Li, Haoxing (Mark) Ren",
    "title": "Novel Transformer Model Based Clustering Method for Standard Cell Design Automation",
    "paper_url": "https://research.nvidia.com/publication/2024-03_novel-transformer-model-based-clustering-method-standard-cell-design-automation",
    "pdf_link": "https://dl.acm.org/doi/pdf/10.1145/3626184.3633314",
    "abstract": "Standard cells are essential components of modern digital circuit designs. With process technologies advancing beyond 5nm, more routability issues have arisen due to the decreasing number of rout\u0002ing tracks (RTs), increasing number and complexity of design rules, and strict patterning rules. The standard cell design automation framework is able to automatically design standard cell layouts, but it is struggling to resolve the severe routability issues in advanced nodes. As a result, a better and more efficient standard cell design automation method that can not only resolve the routability issue but also scale to hundreds of transistors to shorten the development time of standard cell libraries is highly needed and essential. High quality device clustering with the considerations of routabil\u0002ity in the layouts of different technology nodes can reduce the complexity and assist finding the routable layouts faster. In this paper, we develop a novel transformer model-based clustering methodology - training the model using LVS/DRC clean cell layouts and leveraging the personalized page rank vectors to cluster the devices with the attentions to netlist graph and learned embeddings from the actual LVS/DRC clean layouts. On a benchmark of 94 complex and hard-to-route standard cells, the proposed method not only generates 15% more LVS/DRC clean layouts, but also achieves average 12.7X faster than previous work. The proposed method can generate 100% LVS/DRC clean cell layouts over 1000 standard cells and achieve 14.5% smaller cell width than an industrial standard cell library.",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Circuits and VLSI Design"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper uses a transformer model for standard cell design automation, a narrow application in digital circuit design. It lacks broader implications for general - purpose AI capabilities and does not show cross - domain applications, so it is not foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on a novel transformer model - based clustering method for standard cell design automation in digital circuits. There are no mentions of robotics - related concepts such as reinforcement learning in robotic contexts, manipulation tasks, or robotic perception, and no connection to hardware systems with input sensors and mechanical kinematics for joint movements, thus it is not within the robotics research domain."
  },
  {
    "paper_id": "nvidia_81",
    "authors": "Rongjian Liang, Anthony Agnesina, Haoxing (Mark) Ren",
    "title": "MedPart: A Multi-Level Evolutionary Differentiable Hypergraph Partitioner",
    "paper_url": "https://research.nvidia.com/publication/2024-03_medpart-multi-level-evolutionary-differentiable-hypergraph-partitioner",
    "pdf_link": "https://dl.acm.org/doi/pdf/10.1145/3626184.3633319",
    "abstract": "State-of-the-art hypergraph partitioners, such as hMETIS, usually adopt a multi-level paradigm for efficiency and scalability. How\u0002ever, they are prone to getting trapped in local minima due to their reliance on refinement heuristics and overlooking global struc\u0002tural information during coarsening. SpecPart, the most advanced academic hypergraph partitioning refinement method, improves partitioning by leveraging spectral information. Still, its success depends heavily on the quality of initial input solutions. This work introduces MedPart, a multi-level evolutionary differentiable hy\u0002pergraph partitioner. MedPart follows the multi-level paradigm but addresses its limitations by using fast spectral coarsening and introducing a novel evolutionary differentiable algorithm to op\u0002timize each coarsening level. Moreover, by analogy between hy\u0002pergraph partitioning and deep graph learning, our evolutionary differentiable algorithm can be accelerated with deep graph learning toolkits on GPUs. Experiments on public benchmarks consistently show MedPart outperforming hMETIS and achieving up to a 30% improvement in cut size for some benchmarks compared to the best-published solutions, including those from SpecPart\u2014moreover, MedPart\u2019s runtime scales linearly with the number of hyperedges.",
    "topics": [
      "Circuits and VLSI Design"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on a hypergraph partitioner named MedPart, which is a specialized algorithm for hypergraph partitioning. It does not involve deep - learning models trained on large amounts of data for generalized factual realities, nor does it show any of the key indicators related to foundation model research. Instead, it aims at a narrow application of hypergraph partitioning without broader foundational impact on general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on a multi - level evolutionary differentiable hypergraph partitioner, which is a purely algorithmic study without any clear connection to robotic systems, hardware components, or robotics - related concepts such as reinforcement learning, motion planning, etc."
  },
  {
    "paper_id": "nvidia_82",
    "authors": "Rongjian Liang, Anthony Agnesina, Wen-Hao Liu, Haoxing (Mark) Ren",
    "title": "GPU/ML-Enhanced Large Scale Global Routing Contest",
    "paper_url": "https://research.nvidia.com/publication/2024-03_gpuml-enhanced-large-scale-global-routing-contest",
    "pdf_link": "https://dl.acm.org/doi/pdf/10.1145/3626184.3639693",
    "abstract": "Modern VLSI design flows demand scalable global routing techniques applicable across diverse design stages. In response, the ISPD 2024 contest pioneers the first GPU/ML-enhanced global routing competition, selecting advancements in GPU-accelerated computing platforms and machine learning techniques to address scalability challenges. Large-scale benchmarks, containing up to 50 million cells, offer test cases to assess global routers' runtime and memory scalability. The contest provides simplified input/output formats and performance metrics, framing global routing challenges as mathematical optimization problems and encouraging diverse participation. Two sets of evaluation metrics are introduced: the primary one concentrates on global routing applications to guide post-placement optimization and detailed routing, focusing on congestion resolution and runtime scalability. Special honor is given based on the second set of metrics, placing additional emphasis on runtime efficiency and aiming at guiding early-stage planning.",
    "topics": [
      "Circuits and VLSI Design"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on a GPU/ML - enhanced global routing contest in VLSI design, which is a narrow and specialized application in the field of integrated circuit design. It does not involve deep - learning models trained on large amounts of data for general - purpose use across multiple domains, and there is no mention of the key indicators related to foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on GPU/ML - enhanced global routing in VLSI design, framing it as a mathematical optimization problem. It only discusses software - based techniques for routing in integrated circuits and has no mention of robotic hardware systems, input sensors, mechanical kinematics, or the control of such systems with learning - based algorithms, thus not falling within the robotics research domain."
  },
  {
    "paper_id": "nvidia_83",
    "authors": "Or Patashnik, Rinon Gal, Daniel Cohen-Or, Jun-Yan Zhu, Fernando De la Torre",
    "title": "Consolidating Attention Features for Multi-view Image Editing",
    "paper_url": "https://research.nvidia.com/publication/2024-02_consolidating-attention-features-multi-view-image-editing",
    "pdf_link": "https://arxiv.org/pdf/2402.14792",
    "abstract": "Large-scale text-to-image models enable a wide range of image editing techniques, using text prompts or even spatial controls. However, applying these editing methods to multi-view images depicting a single scene leads to 3D-inconsistent results. In this work, we focus on spatial control-based geometric manipulations and introduce a method to consolidate the editing process across various views. We build on two insights: (1) maintaining consistent features throughout the generative process helps attain consistency in multi-view editing, and (2) the queries in self-attention layers significantly influence the image structure. Hence, we propose to improve the geometric consistency of the edited images by enforcing the consistency of the queries. To do so, we introduce QNeRF, a neural radiance field trained on the internal query features of the edited images. Once trained, QNeRF can render 3D-consistent queries, which are then softly injected back into the self-attention layers during generation, greatly improving multi-view consistency. We refine the process through a progressive, iterative method that better consolidates queries across the diffusion timesteps. We compare our method to a range of existing techniques and demonstrate that it can achieve better multi-view consistency and higher fidelity to the input scene. These advantages allow us to train NeRFs with fewer visual artifacts, that are better aligned with the target geometry.",
    "topics": [
      "Computer Graphics",
      "Computer Vision",
      "Generative AI"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper builds on large - scale text - to - image models, which are a type of pre - trained models. It aims to improve multi - view image editing, showing cross - domain application in the intersection of text - to - image generation and 3D image consistency. This research has broader implications for general - purpose AI capabilities in the field of image and 3D content generation.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on multi - view image editing techniques, including improving geometric consistency of edited images through neural radiance fields and query consistency. It is a purely algorithmic study without any clear connection to robotic systems or hardware, so it does not fall within the robotics research domain."
  },
  {
    "paper_id": "nvidia_84",
    "authors": "Pontus Ebelin",
    "title": "Evaluating and Improving Rendered Visual Experiences: Metrics, Compression, Higher Frame Rates & Recoloring",
    "paper_url": "https://research.nvidia.com/publication/2024-02_evaluating-and-improving-rendered-visual-experiences",
    "pdf_link": null,
    "abstract": "Rendered imagery is presented to us daily. Special effects in movies, video games, scientific visualizations, and marketing catalogs all often rely on images generated through computer graphics. However, with all the possibilities that rendering offers come also a plethora of challenges. This thesis proposes novel ways of evaluating the visual errors caused when some of those challenges are not completely overcome. The thesis also suggests ways to improve on the visual experience observers have when viewing rendered content.In the introduction of this thesis, I provide an overview of a subset of the many and fantastic aspects of the human visual system. I also describe how images are rendered using computer graphics, some of the related challenges, and how the final result is displayed to users. Finally, I discuss some of the basics of image and video quality assessment. The scientific publications contained in this thesis focus on image quality metrics, compression, and rendering at high frame rates. In addition, one paper considers the recoloring of images with the goal of giving people with color vision deficiencies an improved visual experience in a process known as daltonization.Papers I\u2013III suggest ways to evaluate and communicate the errors that users may see in rendered images. In those papers, an image\u2019s error is determined by how much it visually differs from a perfect-quality version of the same view. The focus is on the error map, an image that indicates the magnitude and locations of errors. In Paper IV, tools proposed in the first three papers are used to convey how a novel material texture compression algorithm results in lower visual error compared to competing techniques at similar, low bit rates. To achieve good quality at high compression rates, the proposed algorithm exploits similarities in the textures used for materials.Starting with Paper V, the thesis puts increased emphasis on temporal effects. That paper estimates the temporal edge detection filters in human vision, while previous research had mainly examined spatial edge detection filters. Paper VI demonstrates how perceived quality in rendering can be improved by leveraging the human visual system. The paper suggests a method for rendering ~4\u00d7 more frames per second, which, paired with content-dependent sampling patterns and reconstruction, improves the overall visual experience of rendered image sequences despite reducing the quality of individual frames. This thesis\u2019 final paper, Paper VII, presents a real-time daltonization algorithm that recolors images in a temporally consistent manner, so as to avoid flickering hue changes in image sequences, which are often an issue for competing algorithms that target single images. The proposed recoloring preserves luminance and, thus, the important visual ques it provides.",
    "topics": [
      "Applied Perception",
      "Computer Graphics",
      "Real-Time Rendering"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on evaluating and improving rendered visual experiences, such as image quality metrics, compression, and recoloring for specific applications. It does not involve deep - learning models trained on large data for generalized factual realities, nor does it show characteristics of foundation model research like LMM, LLM, etc., and has no broader impact on general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on evaluating and improving rendered visual experiences in computer graphics, including image quality metrics, compression, frame rates, and recoloring. It does not mention any robotics-related concepts, methodologies, or hardware components, and is a software - only implementation without connections to robotic systems."
  },
  {
    "paper_id": "nvidia_85",
    "authors": "Yoad Tewel, Omri Kaduri, Rinon Gal, Yoni Kasten, Lior Wolf, Gal Chechik, Yuval Atzmon",
    "title": "ConsiStory: Training-Free Consistent Text-to-Image Generation",
    "paper_url": "https://research.nvidia.com/publication/2024-02_consistory-training-free-consistent-text-image-generation",
    "pdf_link": "https://arxiv.org/pdf/2402.03286",
    "abstract": "Text-to-image models offer a new level of creative flexibility by allowing users to guide the image generation process through natural language. However, using these models to consistently portray the same subject across diverse prompts remains challenging. Existing approaches fine-tune the model to teach it new words that describe specific user-provided subjects or add image conditioning to the model. These methods require lengthy per-subject optimization or large-scale pre-training. Moreover, they struggle to align generated images with text prompts and face difficulties in portraying multiple subjects. Here, we present ConsiStory, a training-free approach that enables consistent subject generation by sharing the internal activations of the pretrained model. We introduce a subject-driven shared attention block and correspondence-based feature injection to promote subject consistency between images. Additionally, we develop strategies to encourage layout diversity while maintaining subject consistency. We compare ConsiStory to a range of baselines, and demonstrate state-of-the-art performance on subject consistency and text alignment, without requiring a single optimization step. Finally, ConsiStory can naturally extend to multi-subject scenarios, and even enable training-free personalization for common objects.",
    "topics": [
      "Generative AI"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper uses a pre - trained model and aims to improve text - to - image generation. Pre - trained models are key indicators of foundation model research. The approach also extends to multi - subject scenarios and enables training - free personalization for common objects, showing cross - domain and general - purpose application capabilities, which aligns with the characteristics of foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on text - to - image generation and presents a training - free approach for consistent subject generation in this domain. It contains no mention of robotics - related concepts such as hardware systems, input sensors, mechanical kinematics, or learning - based algorithms for robotic control, and is a purely algorithmic study without a clear connection to robotic systems or hardware."
  },
  {
    "paper_id": "nvidia_86",
    "authors": "Yuanxing Duan, Fangyin Wei, Qiyu Dai, Yuhang He, Wenzheng Chen, Baoquan Chen",
    "title": "4D-Rotor Gaussian Splatting: Towards Efficient Novel View Synthesis for Dynamic Scenes",
    "paper_url": "https://research.nvidia.com/publication/2024-02_4d-rotor-gaussian-splatting-towards-efficient-novel-view-synthesis-dynamic",
    "pdf_link": "https://arxiv.org/pdf/2402.03307",
    "abstract": "We consider the problem of novel-view synthesis (NVS) for dynamic scenes. Recent neural approaches have accomplished exceptional NVS results for static 3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior efforts often encode dynamics by learning a canonical space plus implicit or explicit deformation fields, which struggle in challenging scenarios like sudden movements or generating high-fidelity renderings. In this paper, we introduce 4D Gaussian Splatting (4DRotorGS), a novel method that represents dynamic scenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D Gaussian Splatting in static scenes. We model dynamics at each timestamp by temporally slicing the 4D Gaussians, which naturally compose dynamic 3D Gaussians and can be seamlessly projected into images. As an explicit spatial-temporal representation, 4DRotorGS demonstrates powerful capabilities for modeling complicated dynamics and fine details--especially for scenes with abrupt motions. We further implement our temporal slicing and splatting techniques in a highly optimized CUDA acceleration framework, achieving real-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and 583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions showcase the superior efficiency and effectiveness of 4DRotorGS, which consistently outperforms existing methods both quantitatively and qualitatively.",
    "topics": [
      "Computer Graphics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on a narrow application of novel - view synthesis for dynamic scenes using 4D Gaussian Splatting. It does not involve large - scale pre - trained deep learning models, multi - modal or cross - domain capabilities, nor does it have broader implications for general - purpose AI, thus not meeting the criteria for foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on novel - view synthesis for dynamic scenes using a 4D Gaussian Splatting method and its CUDA acceleration. It presents a purely algorithmic study without any clear connections to robotic systems, hardware components, or the key indicators of robotics research."
  },
  {
    "paper_id": "nvidia_87",
    "authors": "Pontus Ebelin, Gyorgy Denes, Tomas Akenine-M\u00f6ller, Kalle \u00c5str\u00f6m, Magnus Oskarsson, William H. McIlhagga",
    "title": "Estimates of Temporal Edge Detection Filters in Human Vision",
    "paper_url": "https://research.nvidia.com/publication/2024-01_ted",
    "pdf_link": "https://dl.acm.org/doi/pdf/10.1145/3639052",
    "abstract": "Edge detection is an important process in human visual processing. However, as far as we know, few attempts have been made to map the temporal edge detection filters in human vision. To that end, we devised a user study and collected data from which we derived estimates of human temporal edge detection filters based on three different models, including the derivative of the infinite symmetric exponential function and temporal contrast sensitivity function. We analyze our findings using several different methods, including extending the filter to higher frequencies than were shown during the experiment. In addition, we show a proof of concept that our filter may be used in spatiotemporal image quality metrics by incorporating it into a flicker detection pipeline.",
    "topics": [
      "Applied Perception"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on estimating temporal edge - detection filters in human vision, which is a narrow, specialized application. It does not involve large - scale deep learning models trained on vast data, nor does it have cross - domain or general - purpose AI capabilities as required by foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on estimating temporal edge detection filters in human vision through a user study and data - based analysis. It does not involve hardware systems with sensors and mechanical kinematics, and no robotics - related concepts such as reinforcement learning, manipulation tasks, etc. are present. It is a study centered on human vision rather than robotics."
  },
  {
    "paper_id": "nvidia_88",
    "authors": "Freya Shah, Taylor Patti, Oriol Rubies-Bigorda, Susanne F. Yelin",
    "title": "Quantum computing with subwavelength atomic arrays",
    "paper_url": "https://research.nvidia.com/publication/2024-01_quantum-computing-subwavelength-atomic-arrays",
    "pdf_link": "https://arxiv.org/pdf/2306.08555",
    "abstract": "Photon-mediated interactions in subwavelength atomic arrays have numerous applications in quantum science. In this paper, we explore the potential of three-level quantum emitters, or \u201cimpurities\u201d embedded in a two-dimensional atomic array to serve as a platform for quantum computation. By exploiting the altered behavior of impurities as a result of the induced dipole-dipole interactions mediated by subwavelength arrays, we design and simulate a set of universal quantum gates consisting of the square root iSWAP and single-qubit rotations. We demonstrate that these gates have very high fidelities due to the long atomic dipole-dipole coherence times, as long as the atoms remain within a proximal range. Finally, we design and simulate quantum circuits leading to the generation of the maximally entangled two-qubit Bell states, as well as the entangled three-qubit Greenberger-Horne-Zeilinger state. These findings establish subwavelength emitter arrays as an alternative platform for quantum computation and quantum simulation.",
    "topics": [
      "Quantum Computing"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on quantum computing using subwavelength atomic arrays. It does not involve deep - learning models, especially transformer - based ones, and lacks the key indicators of foundation model research. The work is a specialized study in quantum science with no clear broader implications for general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on quantum computing and simulation using subwavelength atomic arrays. It does not involve hardware systems with input sensors, mechanical kinematics, and joint movements, nor does it mention any learning - based control algorithms or key robotics indicators. It is a purely algorithmic and software - simulation study without clear connections to robotic systems or hardware."
  },
  {
    "paper_id": "nvidia_89",
    "authors": "Aleksandr Laptev, Boris Ginsburg",
    "title": "Fast Entropy-Based Methods of Word-Level Confidence Estimation for End-to-End Automatic Speech Recognition",
    "paper_url": "https://research.nvidia.com/publication/2024-01_fast-entropy-based-methods-word-level-confidence-estimation-end-end-automatic",
    "pdf_link": "https://arxiv.org/pdf/2212.08703",
    "abstract": "This paper presents a class of new fast non-trainable entropy-based confidence estimation methods for automatic speech recognition. We show how per-frame entropy values can be normalized and aggregated to obtain a confidence measure per unit and per word for Connectionist Temporal Classification (CTC) and Recurrent Neural Network Transducer (RNN-T) models. Proposed methods have similar computational complexity to the traditional method based on the maximum per-frame probability, but they are more adjustable, have a wider effective threshold range, and better push apart the confidence distributions of correct and incorrect words. We evaluate the proposed confidence measures on LibriSpeech test sets, and show that they are up to 2 and 4 times better than confidence estimation based on the maximum per-frame probability at detecting incorrect words for Conformer-CTC and Conformer-RNN-T models, respectively.",
    "topics": [
      "Speech Processing"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on developing fast entropy - based confidence estimation methods for automatic speech recognition. It uses specific models (CTC, RNN - T, Conformer - CTC, Conformer - RNN - T) for a narrow application in speech recognition without involving large - scale pre - trained models, general - purpose modeling, or cross - domain applications, thus not meeting the criteria for foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on entropy - based confidence estimation methods for automatic speech recognition. It is a purely algorithmic study with no clear connections to robotic systems, hardware components, or any of the key robotics indicators."
  },
  {
    "paper_id": "nvidia_90",
    "authors": "Jia-Mu Sun, Jie Yang, Kaichun Mo, Yu-Kun Lai, Leonidas Guibas, Lin Gao",
    "title": "Haisor: Human-aware Indoor Scene Optimization via Deep Reinforcement Learning",
    "paper_url": "https://research.nvidia.com/publication/2024-01_haisor-human-aware-indoor-scene-optimization-deep-reinforcement-learning",
    "pdf_link": "https://dl.acm.org/doi/pdf/10.1145/3632947",
    "abstract": "3D scene synthesis facilitates and benefits many real-world applications. Most scene generators focus on making indoor scenes plausible via learning from training data and leveraging extra constraints such as adjacency and symmetry. Although the generated 3D scenes are mostly plausible with visually realistic layouts, they can be functionally unsuitable for human users to navigate and interact with furniture. Our key observation is that human activity plays a critical role and sufficient free space is essential for human-scene interactions. This is exactly where many existing synthesized scenes fail\u2014the seemingly correct layouts are often not fit for living. To tackle this, we present a human-aware optimization framework Haisor for 3D indoor scene arrangement via reinforcement learning, which aims to find an action sequence to optimize the indoor scene layout automatically. Based on the hierarchical scene graph representation, an optimal action sequence is predicted and performed via Deep Q-Learning with Monte Carlo Tree Search (MCTS), where MCTS is our key feature to search for the optimal solution in long-term sequences and large action space. Multiple human-aware rewards are designed as our core criteria of human-scene interaction, aiming to identify the next smart action by leveraging powerful reinforcement learning. Our framework is optimized end-to-end by giving the indoor scenes with part-level furniture layout including part mobility information. Furthermore, our methodology is extensible and allows utilizing different reward designs to achieve personalized indoor scene synthesis. Extensive experiments demonstrate that our approach optimizes the layout of 3D indoor scenes in a human-aware manner, which is more realistic and plausible than original state-of-the-art generator results, and our approach produces superior smart actions, outperforming alternative baselines",
    "topics": [
      "Computer Graphics"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on a narrow application of optimizing 3D indoor scene layouts using deep reinforcement learning. It does not involve large multimodal, language, or pre - trained models, and lacks the broader general - purpose and cross - domain aspects characteristic of foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on 3D indoor scene optimization using reinforcement learning. It lacks any mention of hardware systems with sensors and mechanical kinematics, and there are no connections to typical robotics concepts such as manipulation, navigation of robots, or control of robotic actuators. It is a software - based algorithmic study without clear links to robotic hardware."
  },
  {
    "paper_id": "nvidia_91",
    "authors": "Dvir Samuel, Rami Ben-Ari, Simon Raviv, Nir Darshan, Gal Chechik",
    "title": "Generating images of rare concepts using pre-trained diffusion models",
    "paper_url": "https://research.nvidia.com/publication/2024-01_generating-images-rare-concepts-using-pre-trained-diffusion-models",
    "pdf_link": "https://arxiv.org/pdf/2304.14530",
    "abstract": "Text-to-image diffusion models can synthesize high-quality images, but they have various limitations. Here we highlight a common failure mode of these models, namely, generating uncommon concepts and structured concepts like hand palms. We show that their limitation is partly due to the long-tail nature of their training data: web-crawled data sets are strongly unbalanced, causing models to under-represent concepts from the tail of the distribution. We characterize the effect of unbalanced training data on text-to-image models and offer a remedy. We show that rare concepts can be correctly generated by carefully selecting suitable generation seeds in the noise space, using a small reference set of images, a technique that we call SeedSelect. SeedSelect does not require retraining or finetuning the diffusion model. We assess the faithfulness, quality and diversity of SeedSelect in creating rare objects and generating complex formations like hand images, and find it consistently achieves superior performance. We further show the advantage of SeedSelect in semantic data augmentation. Generating semantically appropriate images can successfully improve performance in few-shot recognition benchmarks, for classes from the head and from the tail of the training data of diffusion models",
    "topics": [
      "Artificial Intelligence and Machine Learning",
      "Computer Graphics",
      "Generative AI"
    ],
    "is_foundation_model": true,
    "foundation_reason": "The paper uses pre - trained diffusion models, which are a key indicator of foundation model research. It aims to address limitations of these general - purpose models for broader image generation tasks and shows advantages in semantic data augmentation and few - shot recognition, indicating broader impact on general - purpose AI capabilities.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on text - to - image diffusion models, aiming to solve the problem of generating rare concepts in these models. It does not mention any robotics - related concepts such as reinforcement learning in robotic contexts, robotic perception, or involve hardware systems with input sensors and mechanical kinematics, so it does not fall within the robotics research domain."
  },
  {
    "paper_id": "nvidia_92",
    "authors": "Vahid Noroozi, Somshubra Majumdar, Ankur Kumar, Jagadeesh Balam, Boris Ginsburg",
    "title": "Stateful Conformer with Cache-based Inference for Streaming Automatic Speech Recognition",
    "paper_url": "https://research.nvidia.com/publication/2023-12_stateful-conformer-cache-based-inference-streaming-automatic-speech-recognition",
    "pdf_link": "https://arxiv.org/pdf/2312.17279",
    "abstract": "In this paper, we propose an efficient and accurate streaming speech recognition model based on the FastConformer architecture. We adapted the FastConformer architecture for streaming applications through: (1) constraining both the look-ahead and past contexts in the encoder, and (2) introducing an activation caching mechanism to enable the non-autoregressive encoder to operate autoregressively during inference. The proposed model is thoughtfully designed in a way to eliminate the accuracy disparity between the train and inference time which is common for many streaming models. Furthermore, our proposed encoder works with various decoder configurations including Connectionist Temporal Classification (CTC) and RNN-Transducer (RNNT) decoders. Additionally, we introduced a hybrid CTC/RNNT architecture which utilizes a shared encoder with both a CTC and RNNT decoder to boost the accuracy and save computation. We evaluate the proposed model on LibriSpeech dataset and a multi-domain large scale dataset and demonstrate that it can achieve better accuracy with lower latency and inference time compared to a conventional buffered streaming model baseline. We also showed that training a model with multiple latencies can achieve better accuracy than single latency models while it enables us to support multiple latencies with a single model. Our experiments also showed the hybrid architecture would not only speedup the convergence of the CTC decoder but also improves the accuracy of streaming models compared to single decoder models.",
    "topics": [
      "Speech Processing"
    ],
    "is_foundation_model": false,
    "foundation_reason": "The paper focuses on adapting the FastConformer architecture for streaming speech recognition, a narrow application. It does not involve large - scale pre - trained models, and there is no indication of broader cross - domain or general - purpose AI capabilities, thus not meeting the criteria for foundation model research.",
    "is_robotics": false,
    "robotic_reason": "The paper focuses on developing a streaming speech recognition model and evaluating it on speech datasets. It does not involve any robotics-related concepts such as reinforcement learning, robot navigation, or robotic perception, nor does it mention any hardware systems with sensors and mechanical kinematics, thus falling outside the robotics research domain."
  }
]